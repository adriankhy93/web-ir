{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from serpapi import GoogleSearch\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There are 3 levels of scraping required\n",
    "1. Scrape professor information from `https://ac.cs.tsinghua.edu.cn/faculty.html`\n",
    "2. Scrape paper information using SerpAPI\n",
    "3. Scrape abstracts from arXiv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scrape professor information from `https://ac.cs.tsinghua.edu.cn/faculty.html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parase_url(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_profs_infos(soup):\n",
    "    \"\"\"Extracts names, links, titles, emails, area_of_interests from  \"https://ac.cs.tsinghua.edu.cn/faculty.html\"\n",
    "\n",
    "    Args:\n",
    "        soup (html): html of \"https://ac.cs.tsinghua.edu.cn/faculty.html\"\n",
    "\n",
    "    Returns:\n",
    "        names, links, titles, emails, area_of_interests\n",
    "    \"\"\"\n",
    "    extracted_profs = soup.find_all(class_=\"card-content\")\n",
    "    names = [prof.span.text for prof in extracted_profs]\n",
    "    links = [prof.a[\"href\"] for prof in extracted_profs]\n",
    "\n",
    "    prof_infos_list = [prof.p.text for prof in extracted_profs]\n",
    "    area_of_interests = [\n",
    "        prof_infos.split(\"Area of Research Interests\")[1][1:].strip()\n",
    "        for prof_infos in prof_infos_list\n",
    "    ]  # area of interest\n",
    "    titles = [prof_infos.split(\"\\n\")[0] for prof_infos in prof_infos_list]  # title\n",
    "    emails = [\n",
    "        prof_infos.split(\"\\n\")[1].split(\" \")[-1] for prof_infos in prof_infos_list\n",
    "    ]  # emails\n",
    "\n",
    "    return names, links, titles, emails, area_of_interests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://ac.cs.tsinghua.edu.cn/faculty.html\"\n",
    "\n",
    "soup = parase_url(URL)  # get html\n",
    "names, links, titles, emails, area_of_interests = get_profs_infos(\n",
    "    soup\n",
    ")  # extract names, links, titles, emails, area_of_interests\n",
    "\n",
    "N = len(names)\n",
    "\n",
    "prof_df = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": range(N),\n",
    "        \"name\": names,\n",
    "        \"title\": titles,\n",
    "        \"email\": emails,\n",
    "        \"area_of_interest\": area_of_interests,\n",
    "        \"link\": links,\n",
    "    }\n",
    ")  # send to pd.DataFrame\n",
    "\n",
    "prof_df.to_csv(\"prof_info.csv\", index=False)  # save file\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scrape paper information using SerpAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERP_API_KEY = \"3fb66225911929b1e816688b7da5c052c289e6b1f797b9ff0500916033e73e08\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_scholar_search_params(query):\n",
    "    \"\"\"Prepares params for SerpAPI\n",
    "\n",
    "    Args:\n",
    "        query (str): the query to scrape from google scholar. Use \"*prof name* tsinghua\"\n",
    "\n",
    "    Returns:\n",
    "        dict: params for SerpAPI\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"engine\": \"google_scholar\",\n",
    "        \"q\": query,\n",
    "        \"api_key\": SERP_API_KEY,\n",
    "        \"hl\": \"en\",\n",
    "        \"as_ylo\": 2017,\n",
    "        \"num\": 20,\n",
    "    }\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def perform_google_search(params):\n",
    "    \"\"\"Scrapes google scholar using SerpAPI\n",
    "\n",
    "    Args:\n",
    "        params (dict): params for SerpAPI\n",
    "\n",
    "    Returns:\n",
    "        dict: results of scraping\n",
    "    \"\"\"\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    organic_results = results[\"organic_results\"]\n",
    "    return organic_results\n",
    "\n",
    "\n",
    "def get_paper_details(organic_results):\n",
    "    \"\"\"Extracts title, link, snippet, number cited from organic_results\n",
    "\n",
    "    Args:\n",
    "        organic_results (dict): scrape output from serpAPI\n",
    "\n",
    "    Returns:\n",
    "        lists of extracted results\n",
    "    \"\"\"\n",
    "    title_list = []\n",
    "    link_list = []\n",
    "    snippet_list = []\n",
    "    num_cited_list = []\n",
    "\n",
    "    for r in organic_results:\n",
    "        title = r[\"title\"]\n",
    "        link = r[\"link\"]\n",
    "        snippet = r[\"snippet\"]\n",
    "\n",
    "        if \"cited_by\" in r[\"inline_links\"]:\n",
    "            num_cited = r[\"inline_links\"][\"cited_by\"][\"total\"]\n",
    "        else:\n",
    "            num_cited = 0\n",
    "\n",
    "        title_list.append(title)\n",
    "        link_list.append(link)\n",
    "        snippet_list.append(snippet)\n",
    "        num_cited_list.append(num_cited)\n",
    "\n",
    "    return title_list, link_list, snippet_list, num_cited_list\n",
    "\n",
    "\n",
    "def obtain_full_paper_df(names):\n",
    "    \"\"\"For each prof, perform scraping to obtain most relevant recent papers\n",
    "\n",
    "    Args:\n",
    "        names (list): names of professors\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Final results of scraping using serpAPI\n",
    "    \"\"\"\n",
    "    full_paper_df = pd.DataFrame(\n",
    "        columns=[\"prof_id\", \"title\", \"link\", \"snippet\", \"num_cited\"]\n",
    "    )\n",
    "    for i, n in enumerate(names):\n",
    "        query = n + \" tsinghua\"\n",
    "        params = get_google_scholar_search_params(query)\n",
    "        organic_results = perform_google_search(params)\n",
    "        title_list, link_list, snippet_list, num_cited_list = get_paper_details(\n",
    "            organic_results\n",
    "        )\n",
    "        paper_result_df = pd.DataFrame(\n",
    "            {\n",
    "                \"prof_id\": i,\n",
    "                \"title\": title_list,\n",
    "                \"link\": link_list,\n",
    "                \"snippet\": snippet_list,\n",
    "                \"num_cited\": num_cited_list,\n",
    "            }\n",
    "        )\n",
    "        full_paper_df = pd.concat([full_paper_df, paper_result_df])\n",
    "    return full_paper_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = prof_df[\"name\"].tolist()  # get prof names\n",
    "paper_data_df = obtain_full_paper_df(names)  # scrape and return results\n",
    "paper_data_df.to_csv(\"paper_data.csv\", index=False)  # save to file\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scrape abstracts from arXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_punct_with_space(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "\n",
    "def replace_multiple_spaces_with_single_space(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "\n",
    "def preprocess_pipeline(text):\n",
    "    preproc_text = text.lower()\n",
    "    preproc_text = replace_punct_with_space(preproc_text)\n",
    "    preproc_text = replace_multiple_spaces_with_single_space(preproc_text)\n",
    "    return preproc_text\n",
    "\n",
    "\n",
    "def get_init_arxiv_search_url(title):\n",
    "    plain_title = preprocess_pipeline(title)\n",
    "    query_insert = \"+\".join(plain_title.split())\n",
    "    arxiv_search_url = (\n",
    "        f\"https://arxiv.org/search/?query={query_insert}+&searchtype=all&source=header\"\n",
    "    )\n",
    "    return arxiv_search_url\n",
    "\n",
    "\n",
    "def get_arxiv_parper_link(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    arxiv_link_block = soup.find(class_=\"list-title is-inline-block\")\n",
    "    arxiv_paper_link = None\n",
    "    if arxiv_link_block:\n",
    "        arxiv_paper_link = arxiv_link_block.a[\"href\"]\n",
    "\n",
    "    return arxiv_paper_link\n",
    "\n",
    "\n",
    "def get_abstract(arxiv_paper_link):\n",
    "    page = requests.get(arxiv_paper_link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    abstract = soup.find(class_=\"abstract mathjax\").text\n",
    "\n",
    "    return abstract\n",
    "\n",
    "\n",
    "def get_abstract_pipeline(title):\n",
    "    arxiv_search_url = get_init_arxiv_search_url(title)\n",
    "    arxiv_paper_link = get_arxiv_parper_link(arxiv_search_url)\n",
    "    abstract = \"\"\n",
    "    if arxiv_paper_link:\n",
    "        abstract = get_abstract(arxiv_paper_link)\n",
    "\n",
    "    return abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_data_df[\"abstract\"] = \"\"\n",
    "for i, title in tqdm(enumerate(paper_data_df[\"title\"])):\n",
    "    abstract = get_abstract_pipeline(title)\n",
    "    paper_data_df.loc[i, \"abstract\"] = abstract\n",
    "\n",
    "    paper_data_df.to_csv(\"paper_data_w_abstract.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
