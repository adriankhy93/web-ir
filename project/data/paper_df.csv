paper_id,paper_link,paper_title,paper_abstract
0,https://arxiv.org/abs/2304.02948,FengWu: Pushing the Skillful Global Medium-range Weather Forecast beyond 10 Days Lead,"We present FengWu, an advanced data-driven global medium-range weather forecast system based on Artificial Intelligence (AI). Different from existing data-driven weather forecast methods, FengWu solves the medium-range forecast problem from a multi-modal and multi-task perspective. Specifically, a deep learning architecture equipped with model-specific encoder-decoders and cross-modal fusion Transformer is elaborately designed, which is learned under the supervision of an uncertainty loss to balance the optimization of different predictors in a region-adaptive manner. Besides this, a replay buffer mechanism is introduced to improve medium-range forecast performance. With 39-year data training based on the ERA5 reanalysis, FengWu is able to accurately reproduce the atmospheric dynamics and predict the future land and atmosphere states at 37 vertical levels on a 0.25° latitude-longitude resolution. Hindcasts of 6-hourly weather in 2018 based on ERA5 demonstrate that FengWu performs better than GraphCast in predicting 80\% of the 880 reported predictands, e.g., reducing the root mean square error (RMSE) of 10-day lead global z500 prediction from 733 to 651 $m^{2}/s^2$. In addition, the inference cost of each iteration is merely 600ms on NVIDIA Tesla A100 hardware. The results suggest that FengWu can significantly improve the forecast skill and extend the skillful global medium-range weather forecast out to 10.75 days lead (with ACC of z500 > 0.6) for the first time."
1,https://arxiv.org/abs/2303.02635,VTQA: Visual Text Question Answering via Entity Alignment and Cross-Media Reasoning,"The ideal form of Visual Question Answering requires understanding, grounding and reasoning in the joint space of vision and language and serves as a proxy for the AI task of scene understanding. However, most existing VQA benchmarks are limited to just picking the answer from a pre-defined set of options and lack attention to text. We present a new challenge with a dataset that contains 23,781 questions based on 10124 image-text pairs. Specifically, the task requires the model to align multimedia representations of the same entity to implement multi-hop reasoning between image and text and finally use natural language to answer the question. The aim of this challenge is to develop and benchmark models that are capable of multimedia entity alignment, multi-step reasoning and open-ended answer generation."
2,https://arxiv.org/abs/2212.02122,CLIPVG: Text-Guided Image Manipulation Using Differentiable Vector Graphics,"Considerable progress has recently been made in leveraging CLIP (Contrastive Language-Image Pre-Training) models for text-guided image manipulation. However, all existing works rely on additional generative models to ensure the quality of results, because CLIP alone cannot provide enough guidance information for fine-scale pixel-level changes. In this paper, we introduce CLIPVG, a text-guided image manipulation framework using differentiable vector graphics, which is also the first CLIP-based general image manipulation framework that does not require any additional generative models. We demonstrate that CLIPVG can not only achieve state-of-art performance in both semantic correctness and synthesis quality, but also is flexible enough to support various applications far beyond the capability of all existing methods."
3,https://arxiv.org/abs/2209.08455,TODE-Trans: Transparent Object Depth Estimation with Transformer,"Transparent objects are widely used in industrial automation and daily life. However, robust visual recognition and perception of transparent objects have always been a major challenge. Currently, most commercial-grade depth cameras are still not good at sensing the surfaces of transparent objects due to the refraction and reflection of light. In this work, we present a transformer-based transparent object depth estimation approach from a single RGB-D input. We observe that the global characteristics of the transformer make it easier to extract contextual information to perform depth estimation of transparent areas. In addition, to better enhance the fine-grained features, a feature fusion module (FFM) is designed to assist coherent prediction. Our empirical evidence demonstrates that our model delivers significant improvements in recent popular datasets, e.g., 25% gain on RMSE and 21% gain on REL compared to previous state-of-the-art convolutional-based counterparts in ClearGrasp dataset. Extensive results show that our transformer-based model enables better aggregation of the object's RGB and inaccurate depth information to obtain a better depth representation. Our code and the pre-trained model will be available at https://github.com/yuchendoudou/TODE."
4,https://arxiv.org/abs/2209.06122,What You See is What You Grasp: User-Friendly Grasping Guided by Near-eye-tracking,"This work presents a next-generation human-robot interface that can infer and realize the user's manipulation intention via sight only. Specifically, we develop a system that integrates near-eye-tracking and robotic manipulation to enable user-specified actions (e.g., grasp, pick-and-place, etc), where visual information is merged with human attention to create a mapping for desired robot actions. To enable sight guided manipulation, a head-mounted near-eye-tracking device is developed to track the eyeball movements in real-time, so that the user's visual attention can be identified. To improve the grasping performance, a transformer based grasp model is then developed. Stacked transformer blocks are used to extract hierarchical features where the volumes of channels are expanded at each stage while squeezing the resolution of feature maps. Experimental validation demonstrates that the eye-tracking system yields low gaze estimation error and the grasping system yields promising results on multiple grasping datasets. This work is a proof of concept for gaze interaction-based assistive robot, which holds great promise to help the elder or upper limb disabilities in their daily lives. A demo video is available at https://www.youtube.com/watch?v=yuZ1hukYUrM"
5,https://arxiv.org/abs/2207.13505,Multi-Forgery Detection Challenge 2022: Push the Frontier of Unconstrained and Diverse Forgery Detection,"In this paper, we present the Multi-Forgery Detection Challenge held concurrently with the IEEE Computer Society Workshop on Biometrics at CVPR 2022. Our Multi-Forgery Detection Challenge aims to detect automatic image manipulations including but not limited to image editing, image synthesis, image generation, image photoshop, etc. Our challenge has attracted 674 teams from all over the world, with about 2000 valid result submission counts. We invited the Top 10 teams to present their solutions to the challenge, from which three teams are awarded prizes in the grand finale. In this paper, we present the solutions from the Top 3 teams, in order to boost the research work in the field of image forgery detection."
6,https://arxiv.org/abs/2202.12550,Modulated Collective Behaviors and Condensation of Bacteria,"Bacteria can spontaneously develop collective motions by aligning their motions in dense systems. Here, we show that bacteria can also respond collectively to an alternating electrical field and form dynamic clusters oscillating at the same frequency of the field. As the dynamic clusters go beyond a critical size, they split into smaller ones spontaneously. The critical size for splitting depends on the frequency of electric field and the concentration of bacteria. We show that instead of their biological activity, the physical properties of bacteria as charged particles are responsible for the formation of dynamic clusters. Electroconvective flows across the system play the key role in stabilizing the clusters. However, to form clusters, collective hydrodynamic cooperation between bacteria is important such that no aggregation occurs in dilute suspensions. The findings in this study illustrate that bio-systems can respond collectively to an external field, promising an effective way to control and modulate the behavior of organisms. Moreover, the controlled aggregation and condensation of bacteria offer a robust approach to improve the local concentration of bacteria for early and rapid detection, which has wide applications in clinics."
7,https://arxiv.org/abs/2112.08037,LookinGood^π: Real-time Person-independent Neural Re-rendering for High-quality Human Performance Capture,"We propose LookinGood^π, a novel neural re-rendering approach that is aimed to (1) improve the rendering quality of the low-quality reconstructed results from human performance capture system in real-time; (2) improve the generalization ability of the neural rendering network on unseen people. Our key idea is to utilize the rendered image of reconstructed geometry as the guidance to assist the prediction of person-specific details from few reference images, thus enhancing the re-rendered result. In light of this, we design a two-branch network. A coarse branch is designed to fix some artifacts (i.e. holes, noise) and obtain a coarse version of the rendered input, while a detail branch is designed to predict ""correct"" details from the warped references. The guidance of the rendered image is realized by blending features from two branches effectively in the training of the detail branch, which improves both the warping accuracy and the details' fidelity. We demonstrate that our method outperforms state-of-the-art methods at producing high-fidelity images on unseen people."
8,https://arxiv.org/abs/2011.01613,Towards a Universal Gating Network for Mixtures of Experts,"The combination and aggregation of knowledge from multiple neural networks can be commonly seen in the form of mixtures of experts. However, such combinations are usually done using networks trained on the same tasks, with little mention of the combination of heterogeneous pre-trained networks, especially in the data-free regime. This paper proposes multiple data-free methods for the combination of heterogeneous neural networks, ranging from the utilization of simple output logit statistics, to training specialized gating networks. The gating networks decide whether specific inputs belong to specific networks based on the nature of the expert activations generated. The experiments revealed that the gating networks, including the universal gating approach, constituted the most accurate approach, and therefore represent a pragmatic step towards applications with heterogeneous mixtures of experts in a data-free regime. The code for this project is hosted on github at https://github.com/cwkang1998/network-merging."
9,https://arxiv.org/abs/2001.07450,Occlum: Secure and Efficient Multitasking Inside a Single Enclave of Intel SGX,"Intel Software Guard Extensions (SGX) enables user-level code to create private memory regions called enclaves, whose code and data are protected by the CPU from software and hardware attacks outside the enclaves. Recent work introduces library operating systems (LibOSes) to SGX so that legacy applications can run inside enclaves with few or even no modifications. As virtually any non-trivial application demands multiple processes, it is essential for LibOSes to support multitasking. However, none of the existing SGX LibOSes support multitasking both securely and efficiently.
  This paper presents Occlum, a system that enables secure and efficient multitasking on SGX. We implement the LibOS processes as SFI-Isolated Processes (SIPs). SFI is a software instrumentation technique for sandboxing untrusted modules (called domains). We design a novel SFI scheme named MPX-based, Multi-Domain SFI (MMDSFI) and leverage MMDSFI to enforce the isolation of SIPs. We also design an independent verifier to ensure the security guarantees of MMDSFI. With SIPs safely sharing the single address space of an enclave, the LibOS can implement multitasking efficiently. The Occlum LibOS outperforms the state-of-the-art SGX LibOS on multitasking-heavy workloads by up to 6,600X on micro-benchmarks and up to 500X on application benchmarks."
10,https://arxiv.org/abs/1912.01224,Robust Invisible Hyperlinks in Physical Photographs Based on 3D Rendering Attacks,"In the era of multimedia and Internet, people are eager to obtain information from offline to online. Quick Response (QR) codes and digital watermarks help us access information quickly. However, QR codes look ugly and invisible watermarks can be easily broken in physical photographs. Therefore, this paper proposes a novel method to embed hyperlinks into natural images, making the hyperlinks invisible for human eyes but detectable for mobile devices. Our method is an end-to-end neural network with an encoder to hide information and a decoder to recover information. From original images to physical photographs, camera imaging process will introduce a series of distortion such as noise, blur, and light. To train a robust decoder against the physical distortion from the real world, a distortion network based on 3D rendering is inserted between the encoder and the decoder to simulate the camera imaging process. Besides, in order to maintain the visual attraction of the image with hyperlinks, we propose a loss function based on just noticeable difference (JND) to supervise the training of encoder. Experimental results show that our approach outperforms the previous method in both simulated and real situations."
11,https://arxiv.org/abs/1810.12752,Long Short-Term Attention,"Attention is an important cognition process of humans, which helps humans concentrate on critical information during their perception and learning. However, although many machine learning models can remember information of data, they have no the attention mechanism. For example, the long short-term memory (LSTM) network is able to remember sequential information, but it cannot pay special attention to part of the sequences. In this paper, we present a novel model called long short-term attention (LSTA), which seamlessly integrates the attention mechanism into the inner cell of LSTM. More than processing long short term dependencies, LSTA can focus on important information of the sequences with the attention mechanism. Extensive experiments demonstrate that LSTA outperforms LSTM and related models on the sequence learning tasks."
12,https://arxiv.org/abs/1809.09395,A Case for Asymmetric Non-Volatile Memory Architecture,"The byte-addressable Non-Volatile Memory (NVM) is a promising technology since it simultaneously provides DRAM-like performance, disk-like capacity, and persistency. The current NVM deployment is symmetric, where NVM devices are directly attached to servers. Due to the higher density, NVM provides larger capacity and can be shared among servers. Unfortunately, in the symmetric setting, the availability of NVM devices is affected by the specific machine it is attached to. High availability can be realized by replicating data to NVM on a remote machine. However, it requires full replication of data structure in local memory, limiting the size of the working set. This paper rethinks NVM deployment and makes a case for the asymmetric NVM architecture, which decouples servers from persistent data storage. In the proposed AsymNVM architecture, NVM devices (back-end nodes) can be shared by multiple servers (front-end nodes) and provide recoverable persistent data structures. The asymmetric architecture is made possible by RDMA, and follows the recent industry trend of resource disaggregation. We build AsymNVM framework based on AsymNVM architecture that implements: 1) high performance persistent data structure update; 2) NVM data management; 3) concurrency control; and 4) crash-consistency and replication. The central idea is to use operation logs to reduce the stall due to RDMA writes and enable efficient batching and caching in front-end nodes. To evaluation performance, we construct eight widely used data structures and two applications based on AsymNVM framework, and use traces of industry workloads. In a cluster with ten machines, the results show that AsymNVM achieves comparable performance to the best possible symmetric architecture while avoiding all the drawbacks with disaggregation. Compared to the baseline AsymNVM, speedup brought by the proposed optimizations is 6~22x."
13,https://arxiv.org/abs/1806.01489,Unfolding of a diblock chain and its anomalous diffusion induced by active particles,"We study the structural and dynamical behaviors of a diblock copolymer chain in a bath of active Brownian particles (ABPs) by extensive Brownian dynamics simulation in a two-dimensional model system. Specifically, the A block of chain is self-attractive, while the B block is self-repulsive. We find, beyond a threshold, the A block unfolds with a pattern like extracting a woolen string from a ball. The critical force decreases with the increase of the B block length,NB, for short cases, then keeps a constant with further increase of NB. In addition, we find a power law exists between the unfolding time of chain and active force, Fa, as well as NB. Finally, we focus on the translational and rotational diffusion of chain, and find that both of them remain supper-diffusive at the long time limit for small active forces due to an asymmetry distribution of ABPs. Our results open new routes for manipulating polymer's behaviors with ABPs."
14,https://arxiv.org/abs/1805.12292,Transition between globule and stretch states of a self-attracting chain in the repulsive active particle bath,"Folding and unfolding of biopolymers are often manipulated in experiment by tuning pH, temperature, single-molecule force or shear field. Here we carry out Brownian dynamics simulations to explore the behavior of a single self-attracting chain in the suspension of self-propelling particles (SPPs). As the propelling force increases, globule-stretch (G-S) transition of the chain happens due to the enhanced disturbance from SPPs. Two distinct mechanisms of the transition in the limits of low and high rotational diffusion rates of SPPs have been observed: shear effect at low rate and collision-induced melting at high rate. The G-S and S-G (stretch-globule) curves form hysteresis loop at low rate, while they merge at high rate. Besides, we find two competing effects result in the non-monotonic dependence of the G-S transition on the SPP density at low rate. Our results suggest an alternative approach to manipulating the folding and unfolding of (bio)polymers by utilizing active agents."
15,https://arxiv.org/abs/1805.06708,Beating of grafted chains induced by active Brownian particles,"We study the interplay between active Brownian particles (ABPs) and a hairy surface in two dimensional geometry. We find that the increase of propelling force leads to and enhances inhomogeneous accumulation of ABPs inside the brush region. Oscillation of chain bundles (beating like cilia) is found in company with the formation and disassembly of dynamic cluster of ABPs at large propelling forces. Meanwhile chains are stretched and pushed down due to the effective shear force by ABPs. The decrease of the average brush thickness with propelling force reflects the growth of the beating amplitude of chain bundles. Furthermore, the beating phenomenon is investigated in a simple single-chain system. We find that the chain swings regularly with a major oscillatory period, which increases with chain length and decreases with the increase of propelling force. We build a theory to describe the phenomenon and the predictions on the relationship between period and amplitude for various chain lengths and propelling forces agree very well with simulation data."
16,https://arxiv.org/abs/1705.05054,Spontaneous symmetry breaking induced unidirectional rotation of chain-grafted colloid in the active bath,"Exploiting the energy of randomly moving active agents such as bacteria is a fascinating way to power a microdevice. Here we show, by simulations, that a chain-grafted disk-like colloid can rotate unidirectionally when immersed in a thin film of active particle suspension. The spontaneous symmetry breaking of chain configurations is the origin of the unidirectional rotation. Long persistence time, large propelling force and/or small rotating friction are keys to keeping the broken symmetry and realizing the rotation. In the rotating state, we find very simple linear relations, e.g. between mean angular speed and propelling force. The time-evolving asymmetry of chain configurations reveals that there are two types of non-rotating state. Our findings provide new insights into the phenomena of spontaneous symmetry breaking in active systems with flexible objects and also open the way to conceive new soft/deformable microdevices."
17,https://arxiv.org/abs/1606.08594,Application of Origen2.1 in the decay photon spectrum calculation of spallation products,"Origen2.1 is a widely used computer code for calculating the burnup, decay, and processing of radioactive materials. However, the nuclide library of Origen2.1 is used for existing reactors like pressurized water reactor, to calculate the photon spectrum released by the decay of spallation products, we have made specific libraries for the ADS tungsten spallation target, based on the results given by a Monte Carlo code: FLUKA. All the data used to make the Origen2.1 libraries is obtained from Nuclear structure & decay Data (NuDat2.6). The accumulated activity of spallation products and the contribution of nuclides to photon emission are given in this paper."
18,https://arxiv.org/abs/1512.07805,RFP: A Remote Fetching Paradigm for RDMA-Accelerated Systems,"Remote Direct Memory Access (RDMA) is an efficient way to improve the performance of traditional client-server systems. Currently, there are two main design paradigms for RDMA-accelerated systems. The first allows the clients to directly operate the server's memory and totally bypasses the CPUs at server side. The second follows the traditional server-reply paradigm, which asks the server to write results back to the clients. However, the first method has to expose server's memory and needs tremendous re-design of upper-layer software, which is complex, unsafe, error-prone, and inefficient. The second cannot achieve high input/output operations per second (IOPS), because it employs out-bound RDMA-write at server side which is not efficient.
  We find that the performance of out-bound RDMA-write and in-bound RDMA-read is asymmetric and the latter is 5 times faster than the former. Based on this observation, we propose a novel design paradigm named Remote Fetching Paradigm (RFP). In RFP, the server is still responsible for processing requests from the clients. However, counter-intuitively, instead of sending results back to the clients through out-bound RDMA-write, the server only writes the results in local memory buffers, and the clients use in-bound RDMA-read to remotely fetch these results. Since in-bound RDMA-read achieves much higher IOPS than out-bound RDMA-write, our model is able to bring higher performance than the traditional models.
  In order to prove the effectiveness of RFP, we design and implement an RDMA-accelerated in-memory key-value store following the RFP model. To further improve the IOPS, we propose an optimization mechanism that combines status checking and result fetching. Experiment results show that RFP can improve the IOPS by 160%~310% against state-of-the-art models for in-memory key-value stores."
19,https://arxiv.org/abs/1511.08573,Deformation of a soft boundary induced and enhanced by enclosed active particles,We simulate a two dimensional model of self-propelled particles confined by a deformable boundary. The particles tend to accumulate near the boundary and the shape of the boundary deforms upon the collisions. We find that there are two typical stages in the variation of the morphology with the increase of active force. One is at small force characterized by radially inhomogeneous redistribution of particles and suppression of local fluctuations of the boundary. The other is at large force featured by angularly redistribution of particles and global shape deformation of the boundary. The last two processes are strongly cooperative. We also find different mechanisms in the particle redistribution and opposite force-dependences of the rate of the shape variation at low and high particle concentrations.
20,https://arxiv.org/abs/1508.06077,New Field Model of Polymer/Nanoparticle Mixture-Realizing Discreteness in the Continuous Description,"Field-theoretical method is efficient in predicting the assembling structures of polymeric systems. However, for the polymer/nanoparticle mixture, the continuous density description is not suitable to capture the realistic assembly of particles, especially when the size of particle is much larger than the polymer segment. Here, we developed a field-based model, in which the particles are eventually discrete and hence it can overcome the drawbacks of the conventional field descriptions, e.g., inadequate and crude treatment on the polymer-particle interface and the excluded-volume interaction. We applied the model to study the simplest system of nanoparticles immersed in dense homopolymer solution. Our model can address the depletion effect and interfacial interaction in a more delicate way. Insights into the enthalpic and/or entropic origin of the structural variation due to the competition between depletion and interfacial interaction are obtained. New phenomena such as depletion-enhanced bridging aggregation are observed in the case of strong interfacial attraction and large depletion length. This approach is readily extendable to studying more complex polymer-based nanocomposites or biology-related systems, such as dendrimer/drug encapsulation and membrane/particle assembly."
21,https://arxiv.org/abs/1508.05765,Brush in the bath of active particles: anomalous stretching of chains and distribution of particles,"The interaction between polymer brush and colloidal particles has been intensively studied in the last two decades. Here we consider a flat chain-grafted substrate immersed in a bath of active particles. Simulations show that an increase in the self-propelling force causes an increase in the number of particles that penetrate into the brush. Anomalously, the particle density inside the main body of the brush eventually becomes higher than that outside the brush at very large self-propelling force. The grafted chains are further stretched due to the steric repulsion from the intruded particles. Upon the increase of the self-propelling force, distinct stretching behaviors of chains were observed for low and high grafting densities. Surprisingly, we found a weak descent of the end-to-end distance of chains for high grafting density and very large force which is reminiscent of the compression effect of a chain in the active bath."
22,https://arxiv.org/abs/cond-mat/0611105,Layer-by-layer assembly of colloidal particles deposited onto the polymer-grafted elastic substrate,"We demonstrate a novel route of spatially organizing the colloid arrangements on the polymer-grafted substrate by use of self-consistent field and density functional theories. We find that grafting of polymers onto a substrate can effectively control spatial dispersions of deposited colloids as a result of the balance between colloidal settling force and entropically elastic force of brushes, and colloids can form unexpected ordered structures on a grafting substrate. The depositing process of colloidal particles onto the elastic ""soft"" substrate includes two steps: brush-mediated one-dimensional arrangement of colloidal crystals and controlled layer-by-layer growth driven entropically by non-adsorbing polymer solvent with increasing the particles. The result indicates a possibility for the production of highly ordered and defect-free structures by simply using the grafted substrate instead of periodically patterned templates, under appropriate selection of colloidal size, effective depositing potential, and brush coverage density."
23,https://arxiv.org/abs/cond-mat/0610513,Interactions between colloidal particles induced by polymer brushes grafted onto the substrate,"We investigate the interaction energy between two colloidal particles on or immersed in nonadsorbing polymer brushes grafted onto the substrate as a function of the separation of the particles by use of self-consistent field theory calculation. Depending on the colloidal size and the penetration depth, we demonstrate an existence of repulsive energy barrier of several $k_{B}T$, which can be interpreted by separating the interaction energy into three parts: colloids-polymer interfacial energy, entropic contribution due to ``depletion zone"" overlap of colloidal particles, and entropically elastic energy of grafted chains by compression of particles. The existence of repulsive barrier which is of entirely entropic origin, can lead to kinetic stabilization of the mixture rather than depletion flocculation or phase separation. Therefore, the present result may suggest an approach to control the self-assembling behavior of colloids for the formation of target structures, by tuning the colloidal interaction on the grafting substrate under appropriate selection of colloidal size, effective gravity (influencing the penetration depth), and brush coverage density."
24,https://arxiv.org/abs/2305.03935,Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs,"Diffusion models have exhibited excellent performance in various domains. The probability flow ordinary differential equation (ODE) of diffusion models (i.e., diffusion ODEs) is a particular case of continuous normalizing flows (CNFs), which enables deterministic inference and exact likelihood evaluation. However, the likelihood estimation results by diffusion ODEs are still far from those of the state-of-the-art likelihood-based generative models. In this work, we propose several improved techniques for maximum likelihood estimation for diffusion ODEs, including both training and evaluation perspectives. For training, we propose velocity parameterization and explore variance reduction techniques for faster convergence. We also derive an error-bounded high-order flow matching objective for finetuning, which improves the ODE likelihood and smooths its trajectory. For evaluation, we propose a novel training-free truncated-normal dequantization to fill the training-evaluation gap commonly existing in diffusion ODEs. Building upon these techniques, we achieve state-of-the-art likelihood estimation results on image datasets (2.56 on CIFAR-10, 3.43 on ImageNet-32) without variational dequantization or data augmentation."
25,https://arxiv.org/abs/2304.12824,Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning,"Guided sampling is a vital approach for applying diffusion models in real-world tasks that embeds human-defined guidance during the sampling procedure. This paper considers a general setting where the guidance is defined by an (unnormalized) energy function. The main challenge for this setting is that the intermediate guidance during the diffusion sampling procedure, which is jointly defined by the sampling distribution and the energy function, is unknown and is hard to estimate. To address this challenge, we propose an exact formulation of the intermediate guidance as well as a novel training objective named contrastive energy prediction (CEP) to learn the exact guidance. Our method is guaranteed to converge to the exact guidance under unlimited model capacity and data samples, while previous methods can not. We demonstrate the effectiveness of our method by applying it to offline reinforcement learning (RL). Extensive experiments on D4RL benchmarks demonstrate that our method outperforms existing state-of-the-art algorithms. We also provide some examples of applying CEP for image synthesis to demonstrate the scalability of CEP on high-dimensional data."
26,https://arxiv.org/abs/2211.01095,DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models,"Diffusion probabilistic models (DPMs) have achieved impressive success in high-resolution image synthesis, especially in recent large-scale text-to-image generation applications. An essential technique for improving the sample quality of DPMs is guided sampling, which usually needs a large guidance scale to obtain the best sample quality. The commonly-used fast sampler for guided sampling is DDIM, a first-order diffusion ODE solver that generally needs 100 to 250 steps for high-quality samples. Although recent works propose dedicated high-order solvers and achieve a further speedup for sampling without guidance, their effectiveness for guided sampling has not been well-tested before. In this work, we demonstrate that previous high-order fast samplers suffer from instability issues, and they even become slower than DDIM when the guidance scale grows large. To further speed up guided sampling, we propose DPM-Solver++, a high-order solver for the guided sampling of DPMs. DPM-Solver++ solves the diffusion ODE with the data prediction model and adopts thresholding methods to keep the solution matches training data distribution. We further propose a multistep variant of DPM-Solver++ to address the instability issue by reducing the effective step size. Experiments show that DPM-Solver++ can generate high-quality samples within only 15 to 20 steps for guided sampling by pixel-space and latent-space DPMs."
27,https://arxiv.org/abs/2206.11357,GACT: Activation Compressed Training for Generic Network Architectures,"Training large neural network (NN) models requires extensive memory resources, and Activation Compressed Training (ACT) is a promising approach to reduce training memory footprint. This paper presents GACT, an ACT framework to support a broad range of machine learning tasks for generic NN architectures with limited domain knowledge. By analyzing a linearized version of ACT's approximate gradient, we prove the convergence of GACT without prior knowledge on operator type or model architecture. To make training stable, we propose an algorithm that decides the compression ratio for each tensor by estimating its impact on the gradient at run time. We implement GACT as a PyTorch library that readily applies to any NN architecture. GACT reduces the activation memory for convolutional NNs, transformers, and graph NNs by up to 8.1x, enabling training with a 4.2x to 24.7x larger batch size, with negligible accuracy loss. We implement GACT as a PyTorch library at https://github.com/LiuXiaoxuanPKU/GACT-ICML."
28,https://arxiv.org/abs/2206.08869,Fast Lossless Neural Compression with Integer-Only Discrete Flows,"By applying entropy codecs with learned data distributions, neural compressors have significantly outperformed traditional codecs in terms of compression ratio. However, the high inference latency of neural networks hinders the deployment of neural compressors in practical applications. In this work, we propose Integer-only Discrete Flows (IODF), an efficient neural compressor with integer-only arithmetic. Our work is built upon integer discrete flows, which consists of invertible transformations between discrete random variables. We propose efficient invertible transformations with integer-only arithmetic based on 8-bit quantization. Our invertible transformation is equipped with learnable binary gates to remove redundant filters during inference. We deploy IODF with TensorRT on GPUs, achieving 10x inference speedup compared to the fastest existing neural compressors, while retaining the high compression rates on ImageNet32 and ImageNet64."
29,https://arxiv.org/abs/2206.08265,Maximum Likelihood Training for Score-Based Diffusion ODEs by High-Order Denoising Score Matching,"Score-based generative models have excellent performance in terms of generation quality and likelihood. They model the data distribution by matching a parameterized score network with first-order data score functions. The score network can be used to define an ODE (""score-based diffusion ODE"") for exact likelihood evaluation. However, the relationship between the likelihood of the ODE and the score matching objective is unclear. In this work, we prove that matching the first-order score is not sufficient to maximize the likelihood of the ODE, by showing a gap between the maximum likelihood and score matching objectives. To fill up this gap, we show that the negative likelihood of the ODE can be bounded by controlling the first, second, and third-order score matching errors; and we further present a novel high-order denoising score matching method to enable maximum likelihood training of score-based diffusion ODEs. Our algorithm guarantees that the higher-order matching error is bounded by the training error and the lower-order errors. We empirically observe that by high-order score matching, score-based diffusion ODEs achieve better likelihood on both synthetic data and CIFAR-10, while retaining the high generation quality."
30,https://arxiv.org/abs/2206.00927,DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps,"Diffusion probabilistic models (DPMs) are emerging powerful generative models. Despite their high-quality generation performance, DPMs still suffer from their slow sampling as they generally need hundreds or thousands of sequential function evaluations (steps) of large neural networks to draw a sample. Sampling from DPMs can be viewed alternatively as solving the corresponding diffusion ordinary differential equations (ODEs). In this work, we propose an exact formulation of the solution of diffusion ODEs. The formulation analytically computes the linear part of the solution, rather than leaving all terms to black-box ODE solvers as adopted in previous works. By applying change-of-variable, the solution can be equivalently simplified to an exponentially weighted integral of the neural network. Based on our formulation, we propose DPM-Solver, a fast dedicated high-order solver for diffusion ODEs with the convergence order guarantee. DPM-Solver is suitable for both discrete-time and continuous-time DPMs without any further training. Experimental results show that DPM-Solver can generate high-quality samples in only 10 to 20 function evaluations on various datasets. We achieve 4.70 FID in 10 function evaluations and 2.87 FID in 20 function evaluations on the CIFAR10 dataset, and a $4\sim 16\times$ speedup compared with previous state-of-the-art training-free samplers on various datasets."
31,https://arxiv.org/abs/2205.00163,Deep Ensemble as a Gaussian Process Approximate Posterior,"Deep Ensemble (DE) is an effective alternative to Bayesian neural networks for uncertainty quantification in deep learning. The uncertainty of DE is usually conveyed by the functional inconsistency among the ensemble members, say, the disagreement among their predictions. Yet, the functional inconsistency stems from unmanageable randomness and may easily collapse in specific cases. To render the uncertainty of DE reliable, we propose a refinement of DE where the functional inconsistency is explicitly characterized, and further tuned w.r.t. the training data and certain priori beliefs. Specifically, we describe the functional inconsistency with the empirical covariance of the functions dictated by ensemble members, which, along with the mean, define a Gaussian process (GP). Then, with specific priori uncertainty imposed, we maximize functional evidence lower bound to make the GP specified by DE approximate the Bayesian posterior. In this way, we relate DE to Bayesian inference to enjoy reliable Bayesian uncertainty. Moreover, we provide strategies to make the training efficient. Our approach consumes only marginally added training cost than the standard DE, but achieves better uncertainty quantification than DE and its variants across diverse scenarios."
32,https://arxiv.org/abs/2203.06904,Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models,"Despite the success, the process of fine-tuning large-scale PLMs brings prohibitive adaptation costs. In fact, fine-tuning all the parameters of a colossal model and retaining separate instances for different tasks are practically infeasible. This necessitates a new branch of research focusing on the parameter-efficient adaptation of PLMs, dubbed as delta tuning in this paper. In contrast with the standard fine-tuning, delta tuning only fine-tunes a small portion of the model parameters while keeping the rest untouched, largely reducing both the computation and storage costs. Recent studies have demonstrated that a series of delta tuning methods with distinct tuned parameter selection could achieve performance on a par with full-parameter fine-tuning, suggesting a new promising way of stimulating large-scale PLMs. In this paper, we first formally describe the problem of delta tuning and then comprehensively review recent delta tuning approaches. We also propose a unified categorization criterion that divide existing delta tuning methods into three groups: addition-based, specification-based, and reparameterization-based methods. Though initially proposed as an efficient method to steer large models, we believe that some of the fascinating evidence discovered along with delta tuning could help further reveal the mechanisms of PLMs and even deep neural networks. To this end, we discuss the theoretical principles underlying the effectiveness of delta tuning and propose frameworks to interpret delta tuning from the perspective of optimization and optimal control, respectively. Furthermore, we provide a holistic empirical study of representative methods, where results on over 100 NLP tasks demonstrate a comprehensive performance comparison of different approaches. The experimental results also cover the analysis of combinatorial, scaling and transferable properties of delta tuning."
33,https://arxiv.org/abs/2104.14129,ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training,"The increasing size of neural network models has been critical for improvements in their accuracy, but device memory is not growing at the same rate. This creates fundamental challenges for training neural networks within limited memory environments. In this work, we propose ActNN, a memory-efficient training framework that stores randomly quantized activations for back propagation. We prove the convergence of ActNN for general network architectures, and we characterize the impact of quantization on the convergence via an exact expression for the gradient variance. Using our theory, we propose novel mixed-precision quantization strategies that exploit the activation's heterogeneity across feature dimensions, samples, and layers. These techniques can be readily applied to existing dynamic graph frameworks, such as PyTorch, simply by substituting the layers. We evaluate ActNN on mainstream computer vision models for classification, detection, and segmentation tasks. On all these tasks, ActNN compresses the activation to 2 bits on average, with negligible accuracy loss. ActNN reduces the memory footprint of the activation by 12x, and it enables training with a 6.6x to 14x larger batch size."
34,https://arxiv.org/abs/2103.09527,Implicit Normalizing Flows,"Normalizing flows define a probability distribution by an explicit invertible transformation $\boldsymbol{\mathbf{z}}=f(\boldsymbol{\mathbf{x}})$. In this work, we present implicit normalizing flows (ImpFlows), which generalize normalizing flows by allowing the mapping to be implicitly defined by the roots of an equation $F(\boldsymbol{\mathbf{z}}, \boldsymbol{\mathbf{x}})= \boldsymbol{\mathbf{0}}$. ImpFlows build on residual flows (ResFlows) with a proper balance between expressiveness and tractability. Through theoretical analysis, we show that the function space of ImpFlow is strictly richer than that of ResFlows. Furthermore, for any ResFlow with a fixed number of blocks, there exists some function that ResFlow has a non-negligible approximation error. However, the function is exactly representable by a single-block ImpFlow. We propose a scalable algorithm to train and draw samples from ImpFlows. Empirically, we evaluate ImpFlow on several classification and density modeling tasks, and ImpFlow outperforms ResFlow with a comparable amount of parameters on all the benchmarks."
35,https://arxiv.org/abs/2011.10804,BARS: Joint Search of Cell Topology and Layout for Accurate and Efficient Binary ARchitectures,"Binary Neural Networks (BNNs) have received significant attention due to their promising efficiency. Currently, most BNN studies directly adopt widely-used CNN architectures, which can be suboptimal for BNNs. This paper proposes a novel Binary ARchitecture Search (BARS) flow to discover superior binary architecture in a large design space. Specifically, we analyze the information bottlenecks that are related to both the topology and layout architecture design choices. And we propose to automatically search for the optimal information flow. To achieve that, we design a two-level (Macro & Micro) search space tailored for BNNs and apply a differentiable neural architecture search (NAS) to explore this search space efficiently. The macro-level search space includes width and depth decisions, which is required for better balancing the model performance and complexity. We also design the micro-level search space to strengthen the information flow for BNN. %A notable challenge of BNN architecture search lies in that binary operations exacerbate the ""collapse"" problem of differentiable NAS, for which we incorporate various search and derive strategies to stabilize the search process. On CIFAR-10, BARS achieves 1.5% higher accuracy with 2/3 binary operations and 1/10 floating-point operations comparing with existing BNN NAS studies. On ImageNet, with similar resource consumption, BARS-discovered architecture achieves a 6% accuracy gain than hand-crafted binary ResNet-18 architectures and outperforms other binary architectures while fully binarizing the architecture backbone."
36,https://arxiv.org/abs/2010.14298,A Statistical Framework for Low-bitwidth Training of Deep Neural Networks,"Fully quantized training (FQT), which uses low-bitwidth hardware by quantizing the activations, weights, and gradients of a neural network model, is a promising approach to accelerate the training of deep neural networks. One major challenge with FQT is the lack of theoretical understanding, in particular of how gradient quantization impacts convergence properties. In this paper, we address this problem by presenting a statistical framework for analyzing FQT algorithms. We view the quantized gradient of FQT as a stochastic estimator of its full precision counterpart, a procedure known as quantization-aware training (QAT). We show that the FQT gradient is an unbiased estimator of the QAT gradient, and we discuss the impact of gradient quantization on its variance. Inspired by these theoretical results, we develop two novel gradient quantizers, and we show that these have smaller variance than the existing per-tensor quantizer. For training ResNet-50 on ImageNet, our 5-bit block Householder quantizer achieves only 0.5% validation accuracy loss relative to QAT, comparable to the existing INT8 baseline."
37,https://arxiv.org/abs/2006.11436,BEV-Seg: Bird's Eye View Semantic Segmentation Using Geometry and Semantic Point Cloud,"Bird's-eye-view (BEV) is a powerful and widely adopted representation for road scenes that captures surrounding objects and their spatial locations, along with overall context in the scene. In this work, we focus on bird's eye semantic segmentation, a task that predicts pixel-wise semantic segmentation in BEV from side RGB images. This task is made possible by simulators such as Carla, which allow for cheap data collection, arbitrary camera placements, and supervision in ways otherwise not possible in the real world. There are two main challenges to this task: the view transformation from side view to bird's eye view, as well as transfer learning to unseen domains. Existing work transforms between views through fully connected layers and transfer learns via GANs. This suffers from a lack of depth reasoning and performance degradation across domains. Our novel 2-staged perception pipeline explicitly predicts pixel depths and combines them with pixel semantics in an efficient manner, allowing the model to leverage depth information to infer objects' spatial locations in the BEV. In addition, we transfer learning by abstracting high-level geometric features and predicting an intermediate representation that is common across different domains. We publish a new dataset called BEVSEG-Carla and show that our approach improves state-of-the-art by 24% mIoU and performs well when transferred to a new domain."
38,https://arxiv.org/abs/2002.09741,VFlow: More Expressive Generative Flows with Variational Data Augmentation,"Generative flows are promising tractable models for density modeling that define probabilistic distributions with invertible transformations. However, tractability imposes architectural constraints on generative flows, making them less expressive than other types of generative models. In this work, we study a previously overlooked constraint that all the intermediate representations must have the same dimensionality with the original data due to invertibility, limiting the width of the network. We tackle this constraint by augmenting the data with some extra dimensions and jointly learning a generative flow for augmented data as well as the distribution of augmented dimensions under a variational inference framework. Our approach, VFlow, is a generalization of generative flows and therefore always performs better. Combining with existing generative flows, VFlow achieves a new state-of-the-art 2.98 bits per dimension on the CIFAR-10 dataset and is more compact than previous models to reach similar modeling quality."
39,https://arxiv.org/abs/1804.03578,Towards Training Probabilistic Topic Models on Neuromorphic Multi-chip Systems,"Probabilistic topic models are popular unsupervised learning methods, including probabilistic latent semantic indexing (pLSI) and latent Dirichlet allocation (LDA). By now, their training is implemented on general purpose computers (GPCs), which are flexible in programming but energy-consuming. Towards low-energy implementations, this paper investigates their training on an emerging hardware technology called the neuromorphic multi-chip systems (NMSs). NMSs are very effective for a family of algorithms called spiking neural networks (SNNs). We present three SNNs to train topic models. The first SNN is a batch algorithm combining the conventional collapsed Gibbs sampling (CGS) algorithm and an inference SNN to train LDA. The other two SNNs are online algorithms targeting at both energy- and storage-limited environments. The two online algorithms are equivalent with training LDA by using maximum-a-posterior estimation and maximizing the semi-collapsed likelihood, respectively. They use novel, tailored ordinary differential equations for stochastic optimization. We simulate the new algorithms and show that they are comparable with the GPC algorithms, while being suitable for NMS implementation. We also propose an extension to train pLSI and a method to prune the network to obey the limited fan-in of some NMSs."
40,https://arxiv.org/abs/1710.10568,Stochastic Training of Graph Convolutional Networks with Variance Reduction,"Graph convolutional networks (GCNs) are powerful deep neural networks for graph-structured data. However, GCN computes the representation of a node recursively from its neighbors, making the receptive field size grow exponentially with the number of layers. Previous attempts on reducing the receptive field size by subsampling neighbors do not have a convergence guarantee, and their receptive field size per node is still in the order of hundreds. In this paper, we develop control variate based algorithms which allow sampling an arbitrarily small neighbor size. Furthermore, we prove new theoretical guarantee for our algorithms to converge to a local optimum of GCN. Empirical results show that our algorithms enjoy a similar convergence with the exact algorithm using only two neighbors per node. The runtime of our algorithms on a large Reddit dataset is only one seventh of previous neighbor sampling algorithms."
41,https://arxiv.org/abs/1709.05870,ZhuSuan: A Library for Bayesian Deep Learning,"In this paper we introduce ZhuSuan, a python probabilistic programming library for Bayesian deep learning, which conjoins the complimentary advantages of Bayesian methods and deep learning. ZhuSuan is built upon Tensorflow. Unlike existing deep learning libraries, which are mainly designed for deterministic neural networks and supervised tasks, ZhuSuan is featured for its deep root into Bayesian inference, thus supporting various kinds of probabilistic models, including both the traditional hierarchical Bayesian models and recent deep generative models. We use running examples to illustrate the probabilistic programming on ZhuSuan, including Bayesian logistic regression, variational auto-encoders, deep sigmoid belief networks and Bayesian recurrent neural networks."
42,https://arxiv.org/abs/1702.07083,Scalable Inference for Nested Chinese Restaurant Process Topic Models,"Nested Chinese Restaurant Process (nCRP) topic models are powerful nonparametric Bayesian methods to extract a topic hierarchy from a given text corpus, where the hierarchical structure is automatically determined by the data. Hierarchical Latent Dirichlet Allocation (hLDA) is a popular instance of nCRP topic models. However, hLDA has only been evaluated at small scale, because the existing collapsed Gibbs sampling and instantiated weight variational inference algorithms either are not scalable or sacrifice inference quality with mean-field assumptions. Moreover, an efficient distributed implementation of the data structures, such as dynamically growing count matrices and trees, is challenging.
  In this paper, we propose a novel partially collapsed Gibbs sampling (PCGS) algorithm, which combines the advantages of collapsed and instantiated weight algorithms to achieve good scalability as well as high model quality. An initialization strategy is presented to further improve the model quality. Finally, we propose an efficient distributed implementation of PCGS through vectorization, pre-processing, and a careful design of the concurrent data structures and communication strategy.
  Empirical studies show that our algorithm is 111 times more efficient than the previous open-source implementation for hLDA, with comparable or even better model quality. Our distributed implementation can extract 1,722 topics from a 131-million-document corpus with 28 billion tokens, which is 4-5 orders of magnitude larger than the previous largest corpus, with 50 machines in 7 hours."
43,https://arxiv.org/abs/1610.02496,SaberLDA: Sparsity-Aware Learning of Topic Models on GPUs,"Latent Dirichlet Allocation (LDA) is a popular tool for analyzing discrete count data such as text and images. Applications require LDA to handle both large datasets and a large number of topics. Though distributed CPU systems have been used, GPU-based systems have emerged as a promising alternative because of the high computational power and memory bandwidth of GPUs. However, existing GPU-based LDA systems cannot support a large number of topics because they use algorithms on dense data structures whose time and space complexity is linear to the number of topics. In this paper, we propose SaberLDA, a GPU-based LDA system that implements a sparsity-aware algorithm to achieve sublinear time complexity and scales well to learn a large number of topics. To address the challenges introduced by sparsity, we propose a novel data layout, a new warp-based sampling kernel, and an efficient sparse count matrix updating algorithm that improves locality, makes efficient utilization of GPU warps, and reduces memory consumption. Experiments show that SaberLDA can learn from billions-token-scale data with up to 10,000 topics, which is almost two orders of magnitude larger than that of the previous GPU-based systems. With a single GPU card, SaberLDA is able to learn 10,000 topics from a dataset of billions of tokens in a few hours, which is only achievable with clusters with tens of machines before."
44,https://arxiv.org/abs/1602.06049,Scaling up Dynamic Topic Models,"Dynamic topic models (DTMs) are very effective in discovering topics and capturing their evolution trends in time series data. To do posterior inference of DTMs, existing methods are all batch algorithms that scan the full dataset before each update of the model and make inexact variational approximations with mean-field assumptions. Due to a lack of a more scalable inference algorithm, despite the usefulness, DTMs have not captured large topic dynamics.
  This paper fills this research void, and presents a fast and parallelizable inference algorithm using Gibbs Sampling with Stochastic Gradient Langevin Dynamics that does not make any unwarranted assumptions. We also present a Metropolis-Hastings based $O(1)$ sampler for topic assignments for each word token. In a distributed environment, our algorithm requires very little communication between workers during sampling (almost embarrassingly parallel) and scales up to large-scale applications. We are able to learn the largest Dynamic Topic Model to our knowledge, and learned the dynamics of 1,000 topics from 2.6 million documents in less than half an hour, and our empirical results show that our algorithm is not only orders of magnitude faster than the baselines but also achieves lower perplexity."
45,https://arxiv.org/abs/1601.01142,Streaming Gibbs Sampling for LDA Model,"Streaming variational Bayes (SVB) is successful in learning LDA models in an online manner. However previous attempts toward developing online Monte-Carlo methods for LDA have little success, often by having much worse perplexity than their batch counterparts. We present a streaming Gibbs sampling (SGS) method, an online extension of the collapsed Gibbs sampling (CGS). Our empirical study shows that SGS can reach similar perplexity as CGS, much better than SVB. Our distributed version of SGS, DSGS, is much more scalable than SVB mainly because the updates' communication complexity is small."
46,https://arxiv.org/abs/1510.08628,WarpLDA: a Cache Efficient O(1) Algorithm for Latent Dirichlet Allocation,"Developing efficient and scalable algorithms for Latent Dirichlet Allocation (LDA) is of wide interest for many applications. Previous work has developed an O(1) Metropolis-Hastings sampling method for each token. However, the performance is far from being optimal due to random accesses to the parameter matrices and frequent cache misses.
  In this paper, we first carefully analyze the memory access efficiency of existing algorithms for LDA by the scope of random access, which is the size of the memory region in which random accesses fall, within a short period of time. We then develop WarpLDA, an LDA sampler which achieves both the best O(1) time complexity per token and the best O(K) scope of random access. Our empirical results in a wide range of testing conditions demonstrate that WarpLDA is consistently 5-15x faster than the state-of-the-art Metropolis-Hastings based LightLDA, and is comparable or faster than the sparsity aware F+LDA. With WarpLDA, users can learn up to one million topics from hundreds of millions of documents in a few hours, at an unprecedentedly throughput of 11G tokens per second."
47,https://arxiv.org/abs/1508.02268,Dropout Training for SVMs with Data Augmentation,"Dropout and other feature noising schemes have shown promising results in controlling over-fitting by artificially corrupting the training data. Though extensive theoretical and empirical studies have been performed for generalized linear models, little work has been done for support vector machines (SVMs), one of the most successful approaches for supervised learning. This paper presents dropout training for both linear SVMs and the nonlinear extension with latent representation learning. For linear SVMs, to deal with the intractable expectation of the non-smooth hinge loss under corrupting distributions, we develop an iteratively re-weighted least square (IRLS) algorithm by exploring data augmentation techniques. Our algorithm iteratively minimizes the expectation of a re-weighted least square problem, where the re-weights are analytically updated. For nonlinear latent SVMs, we consider learning one layer of latent representations in SVMs and extend the data augmentation technique in conjunction with first-order Taylor-expansion to deal with the intractable expected non-smooth hinge loss and the nonlinearity of latent representations. Finally, we apply the similar data augmentation ideas to develop a new IRLS algorithm for the expected logistic loss under corrupting distributions, and we further develop a non-linear extension of logistic regression by incorporating one layer of latent representations. Our algorithms offer insights on the connection and difference between the hinge loss and logistic loss in dropout training. Empirical results on several real datasets demonstrate the effectiveness of dropout training on significantly boosting the classification accuracy of both linear and nonlinear SVMs. In addition, the nonlinear SVMs further improve the prediction performance on several image datasets."
48,https://arxiv.org/abs/1411.6370,Big Learning with Bayesian Methods,"Explosive growth in data and availability of cheap computing resources have sparked increasing interest in Big learning, an emerging subfield that studies scalable machine learning algorithms, systems, and applications with Big Data. Bayesian methods represent one important class of statistic methods for machine learning, with substantial recent developments on adaptive, flexible and scalable Bayesian learning. This article provides a survey of the recent advances in Big learning with Bayesian methods, termed Big Bayesian Learning, including nonparametric Bayesian methods for adaptively inferring model complexity, regularized Bayesian inference for improving the flexibility via posterior regularization, and scalable algorithms and systems based on stochastic subsampling and distributed computing for dealing with large-scale applications."
49,https://arxiv.org/abs/1404.4171,Dropout Training for Support Vector Machines,"Dropout and other feature noising schemes have shown promising results in controlling over-fitting by artificially corrupting the training data. Though extensive theoretical and empirical studies have been performed for generalized linear models, little work has been done for support vector machines (SVMs), one of the most successful approaches for supervised learning. This paper presents dropout training for linear SVMs. To deal with the intractable expectation of the non-smooth hinge loss under corrupting distributions, we develop an iteratively re-weighted least square (IRLS) algorithm by exploring data augmentation techniques. Our algorithm iteratively minimizes the expectation of a re-weighted least square problem, where the re-weights have closed-form solutions. The similar ideas are applied to develop a new IRLS algorithm for the expected logistic loss under corrupting distributions. Our algorithms offer insights on the connection and difference between the hinge loss and logistic loss in dropout training. Empirical results on several real datasets demonstrate the effectiveness of dropout training on significantly boosting the classification accuracy of linear SVMs."
50,https://arxiv.org/abs/2305.12689,FIT: Far-reaching Interleaved Transformers,"We present FIT: a transformer-based architecture with efficient self-attention and adaptive computation. Unlike original transformers, which operate on a single sequence of data tokens, we divide the data tokens into groups, with each group being a shorter sequence of tokens. We employ two types of transformer layers: local layers operate on data tokens within each group, while global layers operate on a smaller set of introduced latent tokens. These layers, comprising the same set of self-attention and feed-forward layers as standard transformers, are interleaved, and cross-attention is used to facilitate information exchange between data and latent tokens within the same group. The attention complexity is $O(n^2)$ locally within each group of size $n$, but can reach $O(L^{{4}/{3}})$ globally for sequence length of $L$. The efficiency can be further enhanced by relying more on global layers that perform adaptive computation using a smaller set of latent tokens. FIT is a versatile architecture and can function as an encoder, diffusion decoder, or autoregressive decoder. We provide initial evidence demonstrating its effectiveness in high-resolution image understanding and generation tasks. Notably, FIT exhibits potential in performing end-to-end training on gigabit-scale data, such as 6400$\times$6400 images, or 160K tokens (after patch tokenization), within a memory capacity of 16GB, without requiring specific optimizations or model parallelism."
51,https://arxiv.org/abs/2305.07067,SigRec: Automatic Recovery of Function Signatures in Smart Contracts,"Millions of smart contracts have been deployed onto Ethereum for providing various services, whose functions can be invoked. For this purpose, the caller needs to know the function signature of a callee, which includes its function id and parameter types. Such signatures are critical to many applications focusing on smart contracts, e.g., reverse engineering, fuzzing, attack detection, and profiling. Unfortunately, it is challenging to recover the function signatures from contract bytecode, since neither debug information nor type information is present in the bytecode. To address this issue, prior approaches rely on source code, or a collection of known signatures from incomplete databases or incomplete heuristic rules, which, however, are far from adequate and cannot cope with the rapid growth of new contracts. In this paper, we propose a novel solution that leverages how functions are handled by Ethereum virtual machine (EVM) to automatically recover function signatures. In particular, we exploit how smart contracts determine the functions to be invoked to locate and extract function ids, and propose a new approach named type-aware symbolic execution (TASE) that utilizes the semantics of EVM operations on parameters to identify the number and the types of parameters. Moreover, we develop SigRec, a new tool for recovering function signatures from contract bytecode without the need of source code and function signature databases. The extensive experimental results show that SigRec outperforms all existing tools, achieving an unprecedented 98.7 percent accuracy within 0.074 seconds. We further demonstrate that the recovered function signatures are useful in attack detection, fuzzing and reverse engineering of EVM bytecode."
52,https://arxiv.org/abs/2304.06590,Maximizing temporal quantum correlation by approaching an exceptional point,"Quantum correlations, both spatial and temporal, are the central pillars of quantum mechanics. Over the last two decades, a big breakthrough in quantum physics is its complex extension to the non-Hermitian realm, and dizzying varieties of novel phenomena and applications beyond the Hermitian framework have been uncovered. However, unique features of non-Hermitian quantum correlations, especially in the time domain, still remain to be explored. Here, for the first time, we experimentally achieve this goal by using a parity-time (PT )-symmetric trapped-ion system. The upper limit of temporal quantum correlations, known as the algebraic bound, which has so far not been achieved in the standard measurement scenario, is reached here by approaching the exceptional point (EP), thus showing the unexpected ability of EPs in tuning temporal quantum correlation effects. Our study, unveiling the fundamental interplay of non-Hermiticity, nonlinearity, and temporal quantum correlations, provides the first step towards exploring and utilizing various non-Hermitian temporal quantum effects by operating a wide range of EP devices, which are important for both fundamental studies and applications of quantum EP systems."
53,https://arxiv.org/abs/2304.02860,Towards an Effective and Efficient Transformer for Rain-by-snow Weather Removal,"Rain-by-snow weather removal is a specialized task in weather-degraded image restoration aiming to eliminate coexisting rain streaks and snow particles. In this paper, we propose RSFormer, an efficient and effective Transformer that addresses this challenge. Initially, we explore the proximity of convolution networks (ConvNets) and vision Transformers (ViTs) in hierarchical architectures and experimentally find they perform approximately at intra-stage feature learning. On this basis, we utilize a Transformer-like convolution block (TCB) that replaces the computationally expensive self-attention while preserving attention characteristics for adapting to input content. We also demonstrate that cross-stage progression is critical for performance improvement, and propose a global-local self-attention sampling mechanism (GLASM) that down-/up-samples features while capturing both global and local dependencies. Finally, we synthesize two novel rain-by-snow datasets, RSCityScape and RS100K, to evaluate our proposed RSFormer. Extensive experiments verify that RSFormer achieves the best trade-off between performance and time-consumption compared to other restoration methods. For instance, it outperforms Restormer with a 1.53% reduction in the number of parameters and a 15.6% reduction in inference time. Datasets, source code and pre-trained models are available at \url{https://github.com/chdwyb/RSFormer}."
54,https://arxiv.org/abs/2301.12919,A strong X-ray polarization signal from the magnetar 1RXS J170849.0-400910,"Magnetars are the most strongly magnetized neutron stars, and one of the most promising targets for X-ray polarimetric measurements. We present here the first Imaging X-ray Polarimetry Explorer (IXPE) observation of the magnetar 1RXS J170849.0-400910, jointly analysed with a new Swift observation and archival NICER data. The total (energy and phase integrated) emission in the 2-8 keV energy range is linerarly polarized, at a ~35% level. The phase-averaged polarization signal shows a marked increase with energy, ranging from ~20% at 2-3 keV up to ~80% at 6-8 keV, while the polarization angle remain constant. This indicates that radiation is mostly polarized in a single direction. The spectrum is well reproduced by a combination of either two thermal (blackbody) components or a blackbody and a power law. Both the polarization degree and angle also show a variation with the spin phase, and the former is almost anti-correlated with the source counts in the 2-8 keV and 2-4 keV bands. We discuss the possible implications and interpretations, based on a joint analysis of the spectral, polarization and pulsation properties of the source. A scenario in which the surface temperature is not homogeneous, with a hotter cap covered by a gaseous atmosphere and a warmer region in a condensed state, provides a satisfactory description of both the phase- and energy-dependent spectro-polarimetric data. The (comparatively) small size of the two emitting regions, required to explain the observed pulsations, does not allow to reach a robust conclusion about the presence of vacuum birefringence effects."
55,https://arxiv.org/abs/2301.10972,On the Importance of Noise Scheduling for Diffusion Models,"We empirically study the effect of noise scheduling strategies for denoising diffusion generative models. There are three findings: (1) the noise scheduling is crucial for the performance, and the optimal one depends on the task (e.g., image sizes), (2) when increasing the image size, the optimal noise scheduling shifts towards a noisier one (due to increased redundancy in pixels), and (3) simply scaling the input data by a factor of $b$ while keeping the noise schedule function fixed (equivalent to shifting the logSNR by $\log b$) is a good strategy across image sizes. This simple recipe, when combined with recently proposed Recurrent Interface Network (RIN), yields state-of-the-art pixel-based diffusion models for high-resolution images on ImageNet, enabling single-stage, end-to-end generation of diverse and high-fidelity images at 1024$\times$1024 resolution (without upsampling/cascades)."
56,https://arxiv.org/abs/2301.01991,"Bubble or Not: Measurements, Analyses, and Findings on the Ethereum ERC721 and ERC1155 Non-fungible Token Ecosystem","The non-fungible token (NFT) is an emergent type of cryptocurrency that has garnered extensive attention since its inception. The uniqueness, indivisibility and humanistic value of NFTs are the key characteristics that distinguish them from traditional tokens. The market capitalization of NFT reached 21.5 billion USD in 2021, almost 200 times of all previous transactions. However, the subsequent rapid decline in NFT market fever in the second quarter of 2022 casts doubts on the ostensible boom in the NFT market. To date, there has been no comprehensive and systematic study of the NFT trade market or of the NFT bubble and hype phenomenon. To fill this gap, we conduct an in-depth investigation of the whole Ethereum ERC721 and ERC1155 NFT ecosystem via graph analysis and apply several metrics to measure the characteristics of NFTs. By collecting data from the whole blockchain, we construct three graphs, namely NFT create graph, NFT transfer graph, and NFT hold graph, to characterize the NFT traders, analyze the characteristics of NFTs, and discover many observations and insights. Moreover, we propose new indicators to quantify the activeness and value of NFT and propose an algorithm that combines indicators and graph analyses to find bubble NFTs. Real-world cases demonstrate that our indicators and approach can be used to discern bubble NFTs effectively."
57,https://arxiv.org/abs/2301.00559,Precisely Modeling the Potential of a Surface Electrode Ion Trap,"Accurately modeling the potential generated by electrode of a Paul trap is of great importance for either precision metrology or quantum computing using ions in a Paul trap. For a rectangular shaped electrode, we find a simple but highly accurate parametric expression for the spatial field distribution. Using this expression, a method based on multi-objective optimization is presented to accurately characterize the spatial field strength due to the electrodes and also the stray electric field. This method allows to utilize many different types of data for optimization, such as the equilibrium position of ions in a linear string, trap frequencies and the equilibrium position of a single ion, which therefore greatly improves the model accuracy. The errors of predicted secular frequencies and average ion position are less than $\pm 0.5\%$ and 1.2 $μ$m respectively, much better than the ones predicted by existing method."
58,https://arxiv.org/abs/2301.00354,RiskProp: Account Risk Rating on Ethereum via De-anonymous Score and Network Propagation,"As one of the most popular blockchain platforms supporting smart contracts, Ethereum has caught the interest of both investors and criminals. Differently from traditional financial scenarios, executing Know Your Customer verification on Ethereum is rather difficult due to the pseudonymous nature of the blockchain. Fortunately, as the transaction records stored in the Ethereum blockchain are publicly accessible, we can understand the behavior of accounts or detect illicit activities via transaction mining. Existing risk control techniques have primarily been developed from the perspectives of de-anonymizing address clustering and illicit account classification. However, these techniques cannot be used to ascertain the potential risks for all accounts and are limited by specific heuristic strategies or insufficient label information. These constraints motivate us to seek an effective rating method for quantifying the spread of risk in a transaction network. To the best of our knowledge, we are the first to address the problem of account risk rating on Ethereum by proposing a novel model called RiskProp, which includes a de-anonymous score to measure transaction anonymity and a network propagation mechanism to formulate the relationships between accounts and transactions. We demonstrate the effectiveness of RiskProp in overcoming the limitations of existing models by conducting experiments on real-world datasets from Ethereum. Through case studies on the detected high-risk accounts, we demonstrate that the risk assessment by RiskProp can be used to provide warnings for investors and protect them from possible financial losses, and the superior performance of risk score-based account classification experiments further verifies the effectiveness of our rating method."
59,https://arxiv.org/abs/2212.11972,Scalable Adaptive Computation for Iterative Generation,"We present the Recurrent Interface Network (RIN), a neural net architecture that allocates computation adaptively to the input according to the distribution of information, allowing it to scale to iterative generation of high-dimensional data. Hidden units of RINs are partitioned into the interface, which is locally connected to inputs, and latents, which are decoupled from inputs and can exchange information globally. The RIN block selectively reads from the interface into latents for high-capacity processing, with incremental updates written back to the interface. Stacking multiple blocks enables effective routing across local and global levels. While routing adds overhead, the cost can be amortized in recurrent computation settings where inputs change gradually while more global context persists, such as iterative generation using diffusion models. To this end, we propose a latent self-conditioning technique that ""warm-starts"" the latents at each iteration of the generation process. When applied to diffusion models operating directly on pixels, RINs yield state-of-the-art image and video generation without cascades or guidance, while being domain-agnostic and up to 10$\times$ more efficient compared to specialized 2D and 3D U-Nets."
60,https://arxiv.org/abs/2212.03464,An automated approach to extracting positive and negative clinical research results,"Failure is common in clinical trials since the successful failures presented in negative results always indicate the ways that should not be taken. In this paper, we proposed an automated approach to extracting positive and negative clinical research results by introducing a PICOE (Population, Intervention, Comparation, Outcome, and Effect) framework to represent randomized controlled trials (RCT) reports, where E indicates the effect between a specific I and O. We developed a pipeline to extract and assign the corresponding statistical effect to a specific I-O pair from natural language RCT reports. The extraction models achieved a high degree of accuracy for ICO and E descriptive words extraction through two rounds of training. By defining a threshold of p-value, we find in all Covid-19 related intervention-outcomes pairs with statistical tests, negative results account for nearly 40%. We believe that this observation is noteworthy since they are extracted from the published literature, in which there is an inherent risk of reporting bias, preferring to report positive results rather than negative results. We provided a tool to systematically understand the current level of clinical evidence by distinguishing negative results from the positive results."
61,https://arxiv.org/abs/2211.11317,DeSTSeg: Segmentation Guided Denoising Student-Teacher for Anomaly Detection,"Visual anomaly detection, an important problem in computer vision, is usually formulated as a one-class classification and segmentation task. The student-teacher (S-T) framework has proved to be effective in solving this challenge. However, previous works based on S-T only empirically applied constraints on normal data and fused multi-level information. In this study, we propose an improved model called DeSTSeg, which integrates a pre-trained teacher network, a denoising student encoder-decoder, and a segmentation network into one framework. First, to strengthen the constraints on anomalous data, we introduce a denoising procedure that allows the student network to learn more robust representations. From synthetically corrupted normal images, we train the student network to match the teacher network feature of the same images without corruption. Second, to fuse the multi-level S-T features adaptively, we train a segmentation network with rich supervision from synthetic anomaly masks, achieving a substantial performance improvement. Experiments on the industrial inspection benchmark dataset demonstrate that our method achieves state-of-the-art performance, 98.6% on image-level AUC, 75.8% on pixel-level average precision, and 76.4% on instance-level average precision."
62,https://arxiv.org/abs/2211.02652,AntFuzzer: A Grey-Box Fuzzing Framework for EOSIO Smart Contracts,"In the past few years, several attacks against the vulnerabilities of EOSIO smart contracts have caused severe financial losses to this prevalent blockchain platform. As a lightweight test-generation approach, grey-box fuzzing can open up the possibility of improving the security of EOSIO smart contracts. However, developing a practical grey-box fuzzer for EOSIO smart contracts from scratch is time-consuming and requires a deep understanding of EOSIO internals. In this work, we proposed AntFuzzer, the first highly extensible grey-box fuzzing framework for EOSIO smart contracts. AntFuzzer implements a novel approach that interfaces AFL to conduct AFL-style grey-box fuzzing on EOSIO smart contracts. Compared to black-box fuzzing tools, AntFuzzer can effectively trigger those hard-to-cover branches. It achieved an improvement in code coverage on 37.5% of smart contracts in our benchmark dataset. AntFuzzer provides unified interfaces for users to easily develop new detection plugins for continually emerging vulnerabilities. We have implemented 6 detection plugins on AntFuzzer to detect major vulnerabilities of EOSIO smart contracts. In our large-scale fuzzing experiments on 4,616 real-world smart contracts, AntFuzzer successfully detected 741 vulnerabilities. The results demonstrate the effectiveness and efficiency of AntFuzzer and our detection pl"
63,https://arxiv.org/abs/2210.06366,A Generalist Framework for Panoptic Segmentation of Images and Videos,"Panoptic segmentation assigns semantic and instance ID labels to every pixel of an image. As permutations of instance IDs are also valid solutions, the task requires learning of high-dimensional one-to-many mapping. As a result, state-of-the-art approaches use customized architectures and task-specific loss functions. We formulate panoptic segmentation as a discrete data generation problem, without relying on inductive bias of the task. A diffusion model based on analog bits is used to model panoptic masks, with a simple, generic architecture and loss function. By simply adding past predictions as a conditioning signal, our method is capable of modeling video (in a streaming setting) and thereby learns to track object instances automatically. With extensive experiments, we demonstrate that our generalist approach can perform competitively to state-of-the-art specialist methods in similar settings."
64,https://arxiv.org/abs/2209.07254,Experimental violation of Leggett-Garg inequality in a three-level trapped-ion system,"Leggett-Garg inequality (LGI) studies the temporal correlation in the evolution of physical systems. Classical systems obey the LGI but quantum systems may violate it. The extent of the violation depends on the dimension of the quantum system and the state update rule. In this work, we experimentally test the LGI in a three-level trapped-ion system under the model of a large spin precessing in a magnetic field. The Von Neumann and Lüders state update rules are employed in our system for direct comparative analysis. The maximum observed value of Leggett-Garg correlator under the Von Neumann state update rule is $K_3 = 1.739 \pm 0.014$, which demonstrates a violation of the Lüders bound by 17 standard deviations and is by far the most significant violation in natural three-level systems."
65,https://arxiv.org/abs/2208.04202,Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning,"We present Bit Diffusion: a simple and generic approach for generating discrete data with continuous state and continuous time diffusion models. The main idea behind our approach is to first represent the discrete data as binary bits, and then train a continuous diffusion model to model these bits as real numbers which we call analog bits. To generate samples, the model first generates the analog bits, which are then thresholded to obtain the bits that represent the discrete variables. We further propose two simple techniques, namely Self-Conditioning and Asymmetric Time Intervals, which lead to a significant improvement in sample quality. Despite its simplicity, the proposed approach can achieve strong performance in both discrete image generation and image captioning tasks. For discrete image generation, we significantly improve previous state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens) and ImageNet-64x64 (which has 12K discrete 8-bit tokens), outperforming the best autoregressive model in both sample quality (measured by FID) and efficiency. For image captioning on MS-COCO dataset, our approach achieves competitive results compared to autoregressive models."
66,https://arxiv.org/abs/2208.01841,Robust Learning of Deep Time Series Anomaly Detection Models with Contaminated Training Data,"Time series anomaly detection (TSAD) is an important data mining task with numerous applications in the IoT era. In recent years, a large number of deep neural network-based methods have been proposed, demonstrating significantly better performance than conventional methods on addressing challenging TSAD problems in a variety of areas. Nevertheless, these deep TSAD methods typically rely on a clean training dataset that is not polluted by anomalies to learn the ""normal profile"" of the underlying dynamics. This requirement is nontrivial since a clean dataset can hardly be provided in practice. Moreover, without the awareness of their robustness, blindly applying deep TSAD methods with potentially contaminated training data can possibly incur significant performance degradation in the detection phase. In this work, to tackle this important challenge, we firstly investigate the robustness of commonly used deep TSAD methods with contaminated training data which provides a guideline for applying these methods when the provided training data are not guaranteed to be anomaly-free. Furthermore, we propose a model-agnostic method which can effectively improve the robustness of learning mainstream deep TSAD models with potentially contaminated data. Experiment results show that our method can consistently prevent or mitigate performance degradation of mainstream deep TSAD models on widely used benchmark datasets."
67,https://arxiv.org/abs/2207.05631,DGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization,"Most reinforcement learning algorithms seek a single optimal strategy that solves a given task. However, it can often be valuable to learn a diverse set of solutions, for instance, to make an agent's interaction with users more engaging, or improve the robustness of a policy to an unexpected perturbance. We propose Diversity-Guided Policy Optimization (DGPO), an on-policy algorithm that discovers multiple strategies for solving a given task. Unlike prior work, it achieves this with a shared policy network trained over a single run. Specifically, we design an intrinsic reward based on an information-theoretic diversity objective. Our final objective alternately constraints on the diversity of the strategies and on the extrinsic reward. We solve the constrained optimization problem by casting it as a probabilistic inference task and use policy iteration to maximize the derived lower bound. Experimental results show that our method efficiently discovers diverse strategies in a wide variety of reinforcement learning tasks. Compared to baseline methods, DGPO achieves comparable rewards, while discovering more diverse strategies, and often with better sample efficiency."
68,https://arxiv.org/abs/2206.07669,A Unified Sequence Interface for Vision Tasks,"While language tasks are naturally expressed in a single, unified, modeling framework, i.e., generating sequences of tokens, this has not been the case in computer vision. As a result, there is a proliferation of distinct architectures and loss functions for different vision tasks. In this work we show that a diverse set of ""core"" computer vision tasks can also be unified if formulated in terms of a shared pixel-to-sequence interface. We focus on four tasks, namely, object detection, instance segmentation, keypoint detection, and image captioning, all with diverse types of outputs, e.g., bounding boxes or dense masks. Despite that, by formulating the output of each task as a sequence of discrete tokens with a unified interface, we show that one can train a neural network with a single model architecture and loss function on all these tasks, with no task-specific customization. To solve a specific task, we use a short prompt as task description, and the sequence output adapts to the prompt so it can produce task-specific output. We show that such a model can achieve competitive performance compared to well-established task-specific models."
69,https://arxiv.org/abs/2206.03659,Scalable Online Disease Diagnosis via Multi-Model-Fused Actor-Critic Reinforcement Learning,"For those seeking healthcare advice online, AI based dialogue agents capable of interacting with patients to perform automatic disease diagnosis are a viable option. This application necessitates efficient inquiry of relevant disease symptoms in order to make accurate diagnosis recommendations. This can be formulated as a problem of sequential feature (symptom) selection and classification for which reinforcement learning (RL) approaches have been proposed as a natural solution. They perform well when the feature space is small, that is, the number of symptoms and diagnosable disease categories is limited, but they frequently fail in assignments with a large number of features. To address this challenge, we propose a Multi-Model-Fused Actor-Critic (MMF-AC) RL framework that consists of a generative actor network and a diagnostic critic network. The actor incorporates a Variational AutoEncoder (VAE) to model the uncertainty induced by partial observations of features, thereby facilitating in making appropriate inquiries. In the critic network, a supervised diagnosis model for disease predictions is involved to precisely estimate the state-value function. Furthermore, inspired by the medical concept of differential diagnosis, we combine the generative and diagnosis models to create a novel reward shaping mechanism to address the sparse reward problem in large search spaces. We conduct extensive experiments on both synthetic and real-world datasets for empirical evaluations. The results demonstrate that our approach outperforms state-of-the-art methods in terms of diagnostic accuracy and interaction efficiency while also being more effectively scalable to large search spaces. Besides, our method is adaptable to both categorical and continuous features, making it ideal for online applications."
70,https://arxiv.org/abs/2205.11423,Decoder Denoising Pretraining for Semantic Segmentation,"Semantic segmentation labels are expensive and time consuming to acquire. Hence, pretraining is commonly used to improve the label-efficiency of segmentation models. Typically, the encoder of a segmentation model is pretrained as a classifier and the decoder is randomly initialized. Here, we argue that random initialization of the decoder can be suboptimal, especially when few labeled examples are available. We propose a decoder pretraining approach based on denoising, which can be combined with supervised pretraining of the encoder. We find that decoder denoising pretraining on the ImageNet dataset strongly outperforms encoder-only supervised pretraining. Despite its simplicity, decoder denoising pretraining achieves state-of-the-art results on label-efficient semantic segmentation and offers considerable gains on the Cityscapes, Pascal Context, and ADE20K datasets."
71,https://arxiv.org/abs/2205.09723,Robust and Efficient Medical Imaging with Self-Supervision,"Recent progress in Medical Artificial Intelligence (AI) has delivered systems that can reach clinical expert level performance. However, such systems tend to demonstrate sub-optimal ""out-of-distribution"" performance when evaluated in clinical settings different from the training environment. A common mitigation strategy is to develop separate systems for each clinical setting using site-specific data [1]. However, this quickly becomes impractical as medical data is time-consuming to acquire and expensive to annotate [2]. Thus, the problem of ""data-efficient generalization"" presents an ongoing difficulty for Medical AI development. Although progress in representation learning shows promise, their benefits have not been rigorously studied, specifically for out-of-distribution settings. To meet these challenges, we present REMEDIS, a unified representation learning strategy to improve robustness and data-efficiency of medical imaging AI. REMEDIS uses a generic combination of large-scale supervised transfer learning with self-supervised learning and requires little task-specific customization. We study a diverse range of medical imaging tasks and simulate three realistic application scenarios using retrospective data. REMEDIS exhibits significantly improved in-distribution performance with up to 11.5% relative improvement in diagnostic accuracy over a strong supervised baseline. More importantly, our strategy leads to strong data-efficient generalization of medical imaging AI, matching strong supervised baselines using between 1% to 33% of retraining data across tasks. These results suggest that REMEDIS can significantly accelerate the life-cycle of medical imaging AI development thereby presenting an important step forward for medical imaging AI to deliver broad impact."
72,https://arxiv.org/abs/2205.03553,From Heavy Rain Removal to Detail Restoration: A Faster and Better Network,"The dense rain accumulation in heavy rain can significantly wash out images and thus destroy the background details of images. Although existing deep rain removal models lead to improved performance for heavy rain removal, we find that most of them ignore the detail reconstruction accuracy of rain-free images. In this paper, we propose a dual-stage progressive enhancement network (DPENet) to achieve effective deraining with structure-accurate rain-free images. Two main modules are included in our framework, namely a rain streaks removal network (R$^2$Net) and a detail reconstruction network (DRNet). The former aims to achieve accurate rain removal, and the latter is designed to recover the details of rain-free images. We introduce two main strategies within our networks to achieve trade-off between the effectiveness of deraining and the detail restoration of rain-free images. Firstly, a dilated dense residual block (DDRB) within the rain streaks removal network is presented to aggregate high/low level features of heavy rain. Secondly, an enhanced residual pixel-wise attention block (ERPAB) within the detail reconstruction network is designed for context information aggregation. We also propose a comprehensive loss function to highlight the marginal and regional accuracy of rain-free images. Extensive experiments on benchmark public datasets show both efficiency and effectiveness of the proposed method in achieving structure-preserving rain-free images for heavy rain removal. The source code and pre-trained models can be found at \url{https://github.com/wybchd/DPENet}."
73,https://arxiv.org/abs/2202.03643,SNPSFuzzer: A Fast Greybox Fuzzer for Stateful Network Protocols using Snapshots,"Greybox fuzzing has been widely used in stateless programs and has achieved great success. However, most state-of-the-art greybox fuzzers generally have the problems of slow speed and shallow state depth coverage in the process of fuzzing stateful network protocol programs which are able to remember and store details of the interactions. The existing greybox fuzzers for network protocol programs send a series of well-defined prefix sequences of input messages first and then send mutated messages to test the target state of a stateful network protocol. The process mentioned above causes a high time cost. In this paper, we propose SNPSFuzzer, a fast greybox fuzzer for stateful network protocol using snapshots. SNPSFuzzer dumps the context information when the network protocol program is under a specific state and restores it when the state needs to be fuzzed. Furthermore, we design a message chain analysis algorithm to explore more and deeper network protocol states. Our evaluation shows that, compared with the state-of-the-art network protocol greybox fuzzer AFLNET, SNPSFuzzer increases the speed of network protocol fuzzing by 112.0%-168.9% and improves path coverage by 21.4%-27.5% within 24 hours. Moreover, SNPSFuzzer exposes a previously unreported vulnerability in program Tinydtls."
74,https://arxiv.org/abs/2112.01028,Parallel-Electromagnetically-Induced-Transparency Near Ground-State Cooling of a Trapped-ion Crystal,"We theoretically propose and experimentally demonstrate a parallel-electromagnetically-induced transparency (parallel-EIT) cooling technique for ion crystals in the Paul trap. It has less stringent requirements on the cooling resonance condition than the standard electromagnetically-induced transparency (EIT) cooling, thus allowing, in principle, to simultaneously cool the motional mode spectrum with an arbitrary range. A proof-of-principle validation for this cooling scheme is experimentally demonstrated with up to 4 trapped 40Ca+ ions. We observe simultaneous near-ground-state cooling for all motional modes with best average phonon number about 0.2. By tuning the trap frequency in a large range to imitate a broadband motional mode spectrum, we can still reach almost the same cooling limit for all the modes while standard EIT cooling shows limited cooling range. Our method has a simple experimental configuration, requiring only appropriate modulation of the probe beam of standard EIT cooling, and can be applied to various types of ions (e.g., 171Yb+, 40Ca+). This cooling scheme provides a powerful tool for the initialization of the trapped-ion quantum computers and simulators."
75,https://arxiv.org/abs/2111.12406,Auto robust relative radiometric normalization via latent change noise modelling,"Relative radiometric normalization(RRN) of different satellite images of the same terrain is necessary for change detection, object classification/segmentation, and map-making tasks. However, traditional RRN models are not robust, disturbing by object change, and RRN models precisely considering object change can not robustly obtain the no-change set. This paper proposes auto robust relative radiometric normalization methods via latent change noise modeling. They utilize the prior knowledge that no change points possess small-scale noise under relative radiometric normalization and that change points possess large-scale radiometric noise after radiometric normalization, combining the stochastic expectation maximization method to quickly and robustly extract the no-change set to learn the relative radiometric normalization mapping functions. This makes our model theoretically grounded regarding the probabilistic theory and mathematics deduction. Specifically, when we select histogram matching as the relative radiometric normalization learning scheme integrating with the mixture of Gaussian noise(HM-RRN-MoG), the HM-RRN-MoG model achieves the best performance. Our model possesses the ability to robustly against clouds/fogs/changes. Our method naturally generates a robust evaluation indicator for RRN that is the no-change set root mean square error. We apply the HM-RRN-MoG model to the latter vegetation/water change detection task, which reduces the radiometric contrast and NDVI/NDWI differences on the no-change set, generates consistent and comparable results. We utilize the no-change set into the building change detection task, efficiently reducing the pseudo-change and boosting the precision."
76,https://arxiv.org/abs/2111.01004,Improving Contrastive Learning on Imbalanced Seed Data via Open-World Sampling,"Contrastive learning approaches have achieved great success in learning visual representations with few labels of the target classes. That implies a tantalizing possibility of scaling them up beyond a curated ""seed"" benchmark, to incorporating more unlabeled images from the internet-scale external sources to enhance its performance. However, in practice, larger amount of unlabeled data will require more computing resources due to the bigger model size and longer training needed. Moreover, open-world unlabeled data usually follows an implicit long-tail class or attribute distribution, many of which also do not belong to the target classes. Blindly leveraging all unlabeled data hence can lead to the data imbalance as well as distraction issues. This motivates us to seek a principled approach to strategically select unlabeled data from an external source, in order to learn generalizable, balanced and diverse representations for relevant classes. In this work, we present an open-world unlabeled data sampling framework called Model-Aware K-center (MAK), which follows three simple principles: (1) tailness, which encourages sampling of examples from tail classes, by sorting the empirical contrastive loss expectation (ECLE) of samples over random data augmentations; (2) proximity, which rejects the out-of-distribution outliers that may distract training; and (3) diversity, which ensures diversity in the set of sampled examples. Empirically, using ImageNet-100-LT (without labels) as the seed dataset and two ""noisy"" external data sources, we demonstrate that MAK can consistently improve both the overall representation quality and the class balancedness of the learned features, as evaluated via linear classifier evaluation on full-shot and few-shot settings. The code is available at: https://github.com/VITA-Group/MAK"
77,https://arxiv.org/abs/2110.13359,Investigation of the Effect of Quantum Measurement on Parity-Time Symmetry,"Symmetry, including the parity-time ($\mathcal{PT}$)-symmetry, is a striking topic, widely discussed and employed in many fields. It is well-known that quantum measurement can destroy or disturb quantum systems. However, can and how does quantum measurement destroy the symmetry of the measured system? To answer the pertinent question, we establish the correlation between the quantum measurement and Floquet $\mathcal{PT}$-symmetry and investigate for the first time how the measurement frequency and measurement strength affect the $\mathcal{PT}$-symmetry of the measured system using the $^{40}\mathrm{Ca}^{+}$ ion. It is already shown that the measurement at high frequencies would break the $\mathcal{PT}$ symmetry. Notably, even for an inadequately fast measurement frequency, if the measurement strength is sufficiently strong, the $\mathcal{PT}$ symmetry breaking can occur. The current work can enhance our knowledge of quantum measurement and symmetry and may inspire further research on the effect of quantum measurement on symmetry."
78,https://arxiv.org/abs/2110.07858,Understanding and Improving Robustness of Vision Transformers through Patch-based Negative Augmentation,"We investigate the robustness of vision transformers (ViTs) through the lens of their special patch-based architectural structure, i.e., they process an image as a sequence of image patches. We find that ViTs are surprisingly insensitive to patch-based transformations, even when the transformation largely destroys the original semantics and makes the image unrecognizable by humans. This indicates that ViTs heavily use features that survived such transformations but are generally not indicative of the semantic class to humans. Further investigations show that these features are useful but non-robust, as ViTs trained on them can achieve high in-distribution accuracy, but break down under distribution shifts. From this understanding, we ask: can training the model to rely less on these features improve ViT robustness and out-of-distribution performance? We use the images transformed with our patch-based operations as negatively augmented views and offer losses to regularize the training away from using non-robust features. This is a complementary view to existing research that mostly focuses on augmenting inputs with semantic-preserving transformations to enforce models' invariance. We show that patch-based negative augmentation consistently improves robustness of ViTs across a wide set of ImageNet based robustness benchmarks. Furthermore, we find our patch-based negative augmentation are complementary to traditional (positive) data augmentation, and together boost the performance further."
79,https://arxiv.org/abs/2110.04507,TiKick: Towards Playing Multi-agent Football Full Games from Single-agent Demonstrations,"Deep reinforcement learning (DRL) has achieved super-human performance on complex video games (e.g., StarCraft II and Dota II). However, current DRL systems still suffer from challenges of multi-agent coordination, sparse rewards, stochastic environments, etc. In seeking to address these challenges, we employ a football video game, e.g., Google Research Football (GRF), as our testbed and develop an end-to-end learning-based AI system (denoted as TiKick) to complete this challenging task. In this work, we first generated a large replay dataset from the self-playing of single-agent experts, which are obtained from league training. We then developed a distributed learning system and new offline algorithms to learn a powerful multi-agent AI from the fixed single-agent dataset. To the best of our knowledge, Tikick is the first learning-based AI system that can take over the multi-agent Google Research Football full game, while previous work could either control a single agent or experiment on toy academic scenarios. Extensive experiments further show that our pre-trained model can accelerate the training process of the modern multi-agent algorithm and our method achieves state-of-the-art performances on various academic scenarios."
80,https://arxiv.org/abs/2110.03939,Ranking Cost: Building An Efficient and Scalable Circuit Routing Planner with Evolution-Based Optimization,"Circuit routing has been a historically challenging problem in designing electronic systems such as very large-scale integration (VLSI) and printed circuit boards (PCBs). The main challenge is that connecting a large number of electronic components under specific design rules involves a very large search space. Early solutions are typically designed with hard-coded heuristics, which suffer from problems of non-optimal solutions and lack of flexibility for new design needs. Although a few learning-based methods have been proposed recently, they are typically cumbersome and hard to extend to large-scale applications. In this work, we propose a new algorithm for circuit routing, named Ranking Cost, which innovatively combines search-based methods (i.e., A* algorithm) and learning-based methods (i.e., Evolution Strategies) to form an efficient and trainable router. In our method, we introduce a new set of variables called cost maps, which can help the A* router to find out proper paths to achieve the global objective. We also train a ranking parameter, which can produce the ranking order and further improve the performance of our method. Our algorithm is trained in an end-to-end manner and does not use any artificial data or human demonstration. In the experiments, we compare with the sequential A* algorithm and a canonical reinforcement learning approach, and results show that our method outperforms these baselines with higher connectivity rates and better scalability."
81,https://arxiv.org/abs/2109.10852,Pix2seq: A Language Modeling Framework for Object Detection,"We present Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural network to perceive the image and generate the desired sequence. Our approach is based mainly on the intuition that if a neural network knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms."
82,https://arxiv.org/abs/2109.05125,"MURAL: Multimodal, Multitask Retrieval Across Languages","Both image-caption pairs and translation pairs provide the means to learn deep representations of and connections between languages. We use both types of pairs in MURAL (MUltimodal, MUltitask Representations Across Languages), a dual encoder that solves two tasks: 1) image-text matching and 2) translation pair matching. By incorporating billions of translation pairs, MURAL extends ALIGN (Jia et al. PMLR'21)--a state-of-the-art dual encoder learned from 1.8 billion noisy image-text pairs. When using the same encoders, MURAL's performance matches or exceeds ALIGN's cross-modal retrieval performance on well-resourced languages across several datasets. More importantly, it considerably improves performance on under-resourced languages, showing that text-text learning can overcome a paucity of image-caption examples for these languages. On the Wikipedia Image-Text dataset, for example, MURAL-base improves zero-shot mean recall by 8.1% on average for eight under-resourced languages and by 6.8% on average when fine-tuning. We additionally show that MURAL's text representations cluster not only with respect to genealogical connections but also based on areal linguistics, such as the Balkan Sprachbund."
83,https://arxiv.org/abs/2106.08554,iBatch: Saving Ethereum Fees via Secure and Cost-Effective Batching of Smart-Contract Invocations,"This paper presents iBatch, a middleware system running on top of an operational Ethereum network to enable secure batching of smart-contract invocations against an untrusted relay server off-chain. iBatch does so at a low overhead by validating the server's batched invocations in smart contracts without additional states. The iBatch mechanism supports a variety of policies, ranging from conservative to aggressive batching, and can be configured adaptively to the current workloads. iBatch automatically rewrites smart contracts to integrate with legacy applications and support large-scale deployment.
  For cost evaluation, we develop a platform with fast and cost-accurate transaction replaying, build real transaction benchmarks on popular Ethereum applications, and build a functional prototype of iBatch on Ethereum. The evaluation results show that iBatch saves 14.6%-59.1% Gas cost per invocation with a moderate 2-minute delay and 19.06%-31.52% Ether cost per invocation with a delay of 0.26-1.66 blocks."
84,https://arxiv.org/abs/2106.07631,Improved Transformer for High-Resolution GANs,"Attention-based models, exemplified by the Transformer, can effectively model long range dependency, but suffer from the quadratic complexity of self-attention operation, making them difficult to be adopted for high-resolution image generation based on Generative Adversarial Networks (GANs). In this paper, we introduce two key ingredients to Transformer to address this challenge. First, in low-resolution stages of the generative process, standard global self-attention is replaced with the proposed multi-axis blocked self-attention which allows efficient mixing of local and global attention. Second, in high-resolution stages, we drop self-attention while only keeping multi-layer perceptrons reminiscent of the implicit neural function. To further improve the performance, we introduce an additional self-modulation component based on cross-attention. The resulting model, denoted as HiT, has a nearly linear computational complexity with respect to the image size and thus directly scales to synthesizing high definition images. We show in the experiments that the proposed HiT achieves state-of-the-art FID scores of 30.83 and 2.95 on unconditional ImageNet $128 \times 128$ and FFHQ $256 \times 256$, respectively, with a reasonable throughput. We believe the proposed HiT is an important milestone for generators in GANs which are completely free of convolutions. Our code is made publicly available at https://github.com/google-research/hit-gan"
85,https://arxiv.org/abs/2105.12723,"Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding","Hierarchical structures are popular in recent vision transformers, however, they require sophisticated designs and massive datasets to work well. In this paper, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical way. We find that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simplified architecture that requires minor code changes upon the original vision transformer. The benefits of the proposed judiciously-selected design are threefold: (1) NesT converges faster and requires much less training data to achieve good generalization on both ImageNet and small datasets like CIFAR; (2) when extending our key ideas to image generation, NesT leads to a strong decoder that is 8$\times$ faster than previous transformer-based generators; and (3) we show that decoupling the feature learning and abstraction processes via this nested hierarchy in our design enables constructing a novel method (named GradCAT) for visually interpreting the learned model. Source code is available https://github.com/google-research/nested-transformer."
86,https://arxiv.org/abs/2105.08397,StackVAE-G: An efficient and interpretable model for time series anomaly detection,"Recent studies have shown that autoencoder-based models can achieve superior performance on anomaly detection tasks due to their excellent ability to fit complex data in an unsupervised manner. In this work, we propose a novel autoencoder-based model, named StackVAE-G that can significantly bring the efficiency and interpretability to multivariate time series anomaly detection. Specifically, we utilize the similarities across the time series channels by the stacking block-wise reconstruction with a weight-sharing scheme to reduce the size of learned models and also relieve the overfitting to unknown noises in the training data. We also leverage a graph learning module to learn a sparse adjacency matrix to explicitly capture the stable interrelation structure among multiple time series channels for the interpretable pattern reconstruction of interrelated channels. Combining these two modules, we introduce the stacking block-wise VAE (variational autoencoder) with GNN (graph neural network) model for multivariate time series anomaly detection. We conduct extensive experiments on three commonly used public datasets, showing that our model achieves comparable (even better) performance with the state-of-the-art modelsand meanwhile requires much less computation and memory cost. Furthermore, we demonstrate that the adjacency matrix learned by our model accurately captures the interrelation among multiple channels, and can provide valuable information for failure diagnosis applications."
87,https://arxiv.org/abs/2105.00132,Targeting the Weakest Link: Social Engineering Attacks in Ethereum Smart Contracts,"Ethereum holds multiple billions of U.S. dollars in the form of Ether cryptocurrency and ERC-20 tokens, with millions of deployed smart contracts algorithmically operating these funds. Unsurprisingly, the security of Ethereum smart contracts has been under rigorous scrutiny. In recent years, numerous defense tools have been developed to detect different types of smart contract code vulnerabilities. When opportunities for exploiting code vulnerabilities diminish, the attackers start resorting to social engineering attacks, which aim to influence humans -- often the weakest link in the system. The only known class of social engineering attacks in Ethereum are honeypots, which plant hidden traps for attackers attempting to exploit existing vulnerabilities, thereby targeting only a small population of potential victims.
  In this work, we explore the possibility and existence of new social engineering attacks beyond smart contract honeypots. We present two novel classes of Ethereum social engineering attacks - Address Manipulation and Homograph - and develop six zero-day social engineering attacks. To show how the attacks can be used in popular programming patterns, we conduct a case study of five popular smart contracts with combined market capitalization exceeding $29 billion, and integrate our attack patterns in their source codes without altering their existing functionality. Moreover, we show that these attacks remain dormant during the test phase but activate their malicious logic only at the final production deployment. We further analyze 85,656 open-source smart contracts, and discover that 1,027 of them can be used for the proposed social engineering attacks. We conduct a professional opinion survey with experts from seven smart contract auditing firms, corroborating that the exposed social engineering attacks bring a major threat to the smart contract systems."
88,https://arxiv.org/abs/2104.07307,NT5?! Training T5 to Perform Numerical Reasoning,"Numerical reasoning over text (NRoT) presents unique challenges that are not well addressed by existing pre-training objectives. We explore five sequential training schedules that adapt a pre-trained T5 model for NRoT. Our final model is adapted from T5, but further pre-trained on three datasets designed to strengthen skills necessary for NRoT and general reading comprehension before being fine-tuned on the Discrete Reasoning over Text (DROP) dataset. The training improves DROP's adjusted F1 performance (a numeracy-focused score) from 45.90 to 70.83. Our model closes in on GenBERT (72.4), a custom BERT-Base model using the same datasets with significantly more parameters. We show that training the T5 multitasking framework with multiple numerical reasoning datasets of increasing difficulty, good performance on DROP can be achieved without manually engineering partitioned functionality between distributed and symbol modules."
89,https://arxiv.org/abs/2104.06697,Revisiting Hierarchical Approach for Persistent Long-Term Video Prediction,"Learning to predict the long-term future of video frames is notoriously challenging due to inherent ambiguities in the distant future and dramatic amplifications of prediction error through time. Despite the recent advances in the literature, existing approaches are limited to moderately short-term prediction (less than a few seconds), while extrapolating it to a longer future quickly leads to destruction in structure and content. In this work, we revisit hierarchical models in video prediction. Our method predicts future frames by first estimating a sequence of semantic structures and subsequently translating the structures to pixels by video-to-video translation. Despite the simplicity, we show that modeling structures and their dynamics in the discrete semantic structure space with a stochastic recurrent estimator leads to surprisingly successful long-term prediction. We evaluate our method on three challenging datasets involving car driving and human dancing, and demonstrate that it can generate complicated scene structures and motions over a very long time horizon (i.e., thousands frames), setting a new standard of video prediction with orders of magnitude longer prediction time than existing approaches. Full videos and codes are available at https://1konny.github.io/HVP/."
90,https://arxiv.org/abs/2103.12933,Nucleon isovector scalar charge from overlap fermions,"We calculate the nucleon isovector scalar charge in lattice QCD using overlap fermions on five ensembles of gauge configurations generated by the RBC/UKQCD collaboration using the domain-wall quark action with $2+1$ dynamical flavors. The five ensembles cover five pion masses, $m_π\approx$ 139, 171, 302, 337 and 371 MeV, and four lattice spacings, $a \approx $ 0.06, 0.08, 0.11 and 0.14 fm. Three to six valence quark masses are computed on each ensemble to investigate the pion mass dependence. The extrapolation to the physical pion mass, continuum and infinite volume limits is obtained by a global fit of all data to a formula originated from partially quenched chiral perturbation theory. The excited-states contamination is carefully analyzed with 3--5 sink-source separations and multi-state fits. Our final result, in the $\overline{\text{MS}}$ scheme at 2 GeV, is $g_{S}^{u-d}= 0.94 (10)_{stat}(8)_{sys}$, where the first error is the statistical error and the second is the systematic error."
91,https://arxiv.org/abs/2101.05224,Big Self-Supervised Models Advance Medical Image Classification,"Self-supervised pretraining followed by supervised fine-tuning has seen success in image recognition, especially when labeled examples are scarce, but has received limited attention in medical image analysis. This paper studies the effectiveness of self-supervised learning as a pretraining strategy for medical image classification. We conduct experiments on two distinct tasks: dermatology skin condition classification from digital camera images and multi-label chest X-ray classification, and demonstrate that self-supervised learning on ImageNet, followed by additional self-supervised learning on unlabeled domain-specific medical images significantly improves the accuracy of medical image classifiers. We introduce a novel Multi-Instance Contrastive Learning (MICLe) method that uses multiple images of the underlying pathology per patient case, when available, to construct more informative positive pairs for self-supervised learning. Combining our contributions, we achieve an improvement of 6.7% in top-1 accuracy and an improvement of 1.1% in mean AUC on dermatology and chest X-ray classification respectively, outperforming strong supervised baselines pretrained on ImageNet. In addition, we show that big self-supervised models are robust to distribution shift and can learn efficiently with a small number of labeled medical images."
92,https://arxiv.org/abs/2012.01370,CLUE: Towards Discovering Locked Cryptocurrencies in Ethereum,"As the most popular blockchain that supports smart contracts, there are already more than 296 thousand kinds of cryptocurrencies built on Ethereum. However, not all cryptocurrencies can be controlled by users. For example, some money is permanently locked in wallets' accounts due to attacks. In this paper, we conduct the first systematic investigation on locked cryptocurrencies in Ethereum. In particular, we define three categories of accounts with locked cryptocurrencies and develop a novel tool named CLUE to discover them. Results show that there are more than 216 million dollars value of cryptocurrencies locked in Ethereum. We also analyze the reasons (i.e., attacks/behaviors) why cryptocurrencies are locked. Because the locked cryptocurrencies can never be controlled by users, avoid interacting with the accounts discovered by CLUE and repeating the same mistakes again can help users to save money."
93,https://arxiv.org/abs/2012.01065,BSODA: A Bipartite Scalable Framework for Online Disease Diagnosis,"A growing number of people are seeking healthcare advice online. Usually, they diagnose their medical conditions based on the symptoms they are experiencing, which is also known as self-diagnosis. From the machine learning perspective, online disease diagnosis is a sequential feature (symptom) selection and classification problem. Reinforcement learning (RL) methods are the standard approaches to this type of tasks. Generally, they perform well when the feature space is small, but frequently become inefficient in tasks with a large number of features, such as the self-diagnosis. To address the challenge, we propose a non-RL Bipartite Scalable framework for Online Disease diAgnosis, called BSODA. BSODA is composed of two cooperative branches that handle symptom-inquiry and disease-diagnosis, respectively. The inquiry branch determines which symptom to collect next by an information-theoretic reward. We employ a Product-of-Experts encoder to significantly improve the handling of partial observations of a large number of features. Besides, we propose several approximation methods to substantially reduce the computational cost of the reward to a level that is acceptable for online services. Additionally, we leverage the diagnosis model to estimate the reward more precisely. For the diagnosis branch, we use a knowledge-guided self-attention model to perform predictions. In particular, BSODA determines when to stop inquiry and output predictions using both the inquiry and diagnosis models. We demonstrate that BSODA outperforms the state-of-the-art methods on several public datasets. Moreover, we propose a novel evaluation method to test the transferability of symptom checking methods from synthetic to real-world tasks. Compared to existing RL baselines, BSODA is more effectively scalable to large search spaces."
94,https://arxiv.org/abs/2011.02803,Intriguing Properties of Contrastive Losses,"We study three intriguing properties of contrastive learning. First, we generalize the standard contrastive loss to a broader family of losses, and we find that various instantiations of the generalized loss perform similarly under the presence of a multi-layer non-linear projection head. Second, we study if instance-based contrastive learning (with a global image representation) can learn well on images with multiple objects present. We find that meaningful hierarchical local features can be learned despite the fact that these objectives operate on global instance-level features. Finally, we study the phenomenon of feature suppression among competing features shared across augmented views, such as ""color distribution"" vs ""object class"". We construct datasets with explicit and controllable competing features, and show that, for contrastive learning, a few bits of easy-to-learn shared features can suppress, and even fully prevent, the learning of other sets of competing features. In scenarios where there are multiple objects in an image, the dominant object would suppress the learning of smaller objects. Existing contrastive learning methods critically rely on data augmentation to favor certain sets of features over others, and could suffer from learning saturation for scenarios where existing augmentations cannot fully address the feature suppression. This poses open challenges to existing contrastive learning techniques."
95,https://arxiv.org/abs/2010.16402,Why Do Better Loss Functions Lead to Less Transferable Features?,"Previous work has proposed many new loss functions and regularizers that improve test accuracy on image classification tasks. However, it is not clear whether these loss functions learn better representations for downstream tasks. This paper studies how the choice of training objective affects the transferability of the hidden representations of convolutional neural networks trained on ImageNet. We show that many objectives lead to statistically significant improvements in ImageNet accuracy over vanilla softmax cross-entropy, but the resulting fixed feature extractors transfer substantially worse to downstream tasks, and the choice of loss has little effect when networks are fully fine-tuned on the new tasks. Using centered kernel alignment to measure similarity between hidden representations of networks, we find that differences among loss functions are apparent only in the last few layers of the network. We delve deeper into representations of the penultimate layer, finding that different objectives and hyperparameter combinations lead to dramatically different levels of class separation. Representations with higher class separation obtain higher accuracy on the original task, but their features are less useful for downstream tasks. Our results suggest there exists a trade-off between learning invariant features for the original task and features relevant for transfer tasks."
96,https://arxiv.org/abs/2010.13902,Graph Contrastive Learning with Augmentations,"Generalizable, transferrable, and robust representation learning on graph-structured data remains a challenge for current graph neural networks (GNNs). Unlike what has been developed for convolutional neural networks (CNNs) for image data, self-supervised learning and pre-training are less explored for GNNs. In this paper, we propose a graph contrastive learning (GraphCL) framework for learning unsupervised representations of graph data. We first design four types of graph augmentations to incorporate various priors. We then systematically study the impact of various combinations of graph augmentations on multiple datasets, in four different settings: semi-supervised, unsupervised, and transfer learning as well as adversarial attacks. The results show that, even without tuning augmentation extents nor using sophisticated GNN architectures, our GraphCL framework can produce graph representations of similar or better generalizability, transferrability, and robustness compared to state-of-the-art methods. We also investigate the impact of parameterized graph augmentation extents and patterns, and observe further performance gains in preliminary experiments. Our codes are available at https://github.com/Shen-Lab/GraphCL."
97,https://arxiv.org/abs/2010.13337,Robust Pre-Training by Adversarial Contrastive Learning,"Recent work has shown that, when integrated with adversarial training, self-supervised pre-training can lead to state-of-the-art robustness In this work, we improve robustness-aware self-supervised pre-training by learning representations that are consistent under both data augmentations and adversarial perturbations. Our approach leverages a recent contrastive learning framework, which learns representations by maximizing feature consistency under differently augmented views. This fits particularly well with the goal of adversarial robustness, as one cause of adversarial fragility is the lack of feature invariance, i.e., small input perturbations can result in undesirable large changes in features or even predicted labels. We explore various options to formulate the contrastive task, and demonstrate that by injecting adversarial perturbations, contrastive pre-training can lead to models that are both label-efficient and robust. We empirically evaluate the proposed Adversarial Contrastive Learning (ACL) and show it can consistently outperform existing methods. For example on the CIFAR-10 dataset, ACL outperforms the previous state-of-the-art unsupervised robust pre-training approach by 2.99% on robust accuracy and 2.14% on standard accuracy. We further demonstrate that ACL pre-training can improve semi-supervised adversarial training, even when only a few labeled examples are available. Our codes and pre-trained models have been released at: https://github.com/VITA-Group/Adversarial-Contrastive-Learning."
98,https://arxiv.org/abs/2010.10784,Learning to Embed Categorical Features without Embedding Tables for Recommendation,"Embedding learning of categorical features (e.g. user/item IDs) is at the core of various recommendation models including matrix factorization and neural collaborative filtering. The standard approach creates an embedding table where each row represents a dedicated embedding vector for every unique feature value. However, this method fails to efficiently handle high-cardinality features and unseen feature values (e.g. new video ID) that are prevalent in real-world recommendation systems. In this paper, we propose an alternative embedding framework Deep Hash Embedding (DHE), replacing embedding tables by a deep embedding network to compute embeddings on the fly. DHE first encodes the feature value to a unique identifier vector with multiple hashing functions and transformations, and then applies a DNN to convert the identifier vector to an embedding. The encoding module is deterministic, non-learnable, and free of storage, while the embedding network is updated during the training time to learn embedding generation. Empirical results show that DHE achieves comparable AUC against the standard one-hot full embedding, with smaller model sizes. Our work sheds light on the design of DNN-based alternative embedding schemes for categorical features without using embedding table lookup."
99,https://arxiv.org/abs/2009.02663,DEFECTCHECKER: Automated Smart Contract Defect Detection by Analyzing EVM Bytecode,"Smart contracts are Turing-complete programs running on the blockchain. They are immutable and cannot be modified, even when bugs are detected. Therefore, ensuring smart contracts are bug-free and well-designed before deploying them to the blockchain is extremely important. A contract defect is an error, flaw or fault in a smart contract that causes it to produce an incorrect or unexpected result, or to behave in unintended ways. Detecting and removing contract defects can avoid potential bugs and make programs more robust. Our previous work defined 20 contract defects for smart contracts and divided them into five impact levels. According to our classification, contract defects with seriousness level between 1-3 can lead to unwanted behaviors, e.g., a contract being controlled by attackers. In this paper, we propose DefectChecker, a symbolic execution-based approach and tool to detect eight contract defects that can cause unwanted behaviors of smart contracts on the Ethereum blockchain platform. DefectChecker can detect contract defects from smart contracts bytecode. We compare DefectChecker with key previous works, including Oyente, Mythril and Securify by using an open-source dataset. Our experimental results show that DefectChecker performs much better than these tools in terms of both speed and accuracy. We also applied DefectChecker to 165,621 distinct smart contracts on the Ethereum platform. We found that 25,815 of these smart contracts contain at least one of the contract defects that belongs to impact level 1-3, including some real-world attacks."
100,https://arxiv.org/abs/2009.00607,Characterizing Erasable Accounts in Ethereum,"Being the most popular permissionless blockchain that supports smart contracts, Ethereum allows any user to create accounts on it. However, not all accounts matter. For example, the accounts due to attacks can be removed. In this paper, we conduct the first investigation on erasable accounts that can be removed to save system resources and even users' money (i.e., ETH or gas). In particular, we propose and develop a novel tool named GLASER, which analyzes the State DataBase of Ethereum to discover five kinds of erasable accounts. The experimental results show that GLASER can accurately reveal 508,482 erasable accounts and these accounts lead to users wasting more than 106 million dollars. GLASER can help stop further economic loss caused by these detected accounts. Moreover, GLASER characterizes the attacks/behaviors related to detected erasable accounts through graph analysis."
101,https://arxiv.org/abs/2007.12865,Self-supervised Learning for Large-scale Item Recommendations,"Large scale recommender models find most relevant items from huge catalogs, and they play a critical role in modern search and recommendation systems. To model the input space with large-vocab categorical features, a typical recommender model learns a joint embedding space through neural networks for both queries and items from user feedback data. However, with millions to billions of items in the corpus, users tend to provide feedback for a very small set of them, causing a power-law distribution. This makes the feedback data for long-tail items extremely sparse.
  Inspired by the recent success in self-supervised representation learning research in both computer vision and natural language understanding, we propose a multi-task self-supervised learning (SSL) framework for large-scale item recommendations. The framework is designed to tackle the label sparsity problem by learning better latent relationship of item features. Specifically, SSL improves item representation learning as well as serving as additional regularization to improve generalization. Furthermore, we propose a novel data augmentation method that utilizes feature correlations within the proposed framework.
  We evaluate our framework using two real-world datasets with 500M and 1B training examples respectively. Our results demonstrate the effectiveness of SSL regularization and show its superior performance over the state-of-the-art regularization techniques. We also have already launched the proposed techniques to a web-scale commercial app-to-app recommendation system, with significant improvements top-tier business metrics demonstrated in A/B experiments on live traffic. Our online results also verify our hypothesis that our framework indeed improves model performance even more on slices that lack supervision."
102,https://arxiv.org/abs/2007.09696,STAN: Towards Describing Bytecodes of Smart Contract,"More than eight million smart contracts have been deployed into Ethereum, which is the most popular blockchain that supports smart contract. However, less than 1% of deployed smart contracts are open-source, and it is difficult for users to understand the functionality and internal mechanism of those closed-source contracts. Although a few decompilers for smart contracts have been recently proposed, it is still not easy for users to grasp the semantic information of the contract, not to mention the potential misleading due to decompilation errors. In this paper, we propose the first system named STAN to generate descriptions for the bytecodes of smart contracts to help users comprehend them. In particular, for each interface in a smart contract, STAN can generate four categories of descriptions, including functionality description, usage description, behavior description, and payment description, by leveraging symbolic execution and NLP (Natural Language Processing) techniques. Extensive experiments show that STAN can generate adequate, accurate, and readable descriptions for contract's bytecodes, which have practical value for users."
103,https://arxiv.org/abs/2006.16467,Observation of $\mathcal{PT}$-symmetric quantum coherence in a single ion system,"Parity-time($\mathcal{PT}$)-symmetric systems, featuring real eigenvalues despite its non-Hermitian nature, have been widely utilized to achieve exotic functionalities in the classical realm, such as loss-induced transparency or lasing revival. By approaching the exceptional point (EP) or the coalescences of both eigenvalues and eigenstates, unconventional effects are also expected to emerge in pure quantum $\mathcal{PT}$ devices. Here, we report experimental evidences of spontaneous $\mathcal{PT}$ symmetry breaking in a single cold $^{40}\mathrm{Ca}^{+}$ ion, and more importantly, a counterintuitive effect of perfect quantum coherence occurring at the EP. Excellent agreement between experimental results and theoretical predictions is identified. In view of the versatile role of cold ions in building quantum memory or processor, our experiment provides a new platform to explore and utilize pure quantum EP effects, with diverse applications in quantum engineering of trapped ions."
104,https://arxiv.org/abs/2006.10029,Big Self-Supervised Models are Strong Semi-Supervised Learners,"One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9% ImageNet top-1 accuracy with just 1% of the labels ($\le$13 labeled images per class) using ResNet-50, a $10\times$ improvement in label efficiency over the previous state-of-the-art. With 10% of labels, ResNet-50 trained with our method achieves 77.5% top-1 accuracy, outperforming standard supervised training with all of the labels."
105,https://arxiv.org/abs/2006.02595,Image Augmentations for GAN Training,"Data augmentations have been widely studied to improve the accuracy and robustness of classifiers. However, the potential of image augmentation in improving GAN models for image synthesis has not been thoroughly investigated in previous studies. In this work, we systematically study the effectiveness of various existing augmentation techniques for GAN training in a variety of settings. We provide insights and guidelines on how to augment images for both vanilla GANs and GANs with regularizations, improving the fidelity of the generated images substantially. Surprisingly, we find that vanilla GANs attain generation quality on par with recent state-of-the-art results if we use augmentations on both real and generated images. When this GAN training is combined with other augmentation-based regularization techniques, such as contrastive loss and consistency regularization, the augmentations further improve the quality of generated images. We provide new state-of-the-art results for conditional generation on CIFAR-10 with both consistency loss and contrastive loss as additional regularizations."
106,https://arxiv.org/abs/2005.03908,Estimation of the Laser Frequency Nosie Spectrum by Continuous Dynamical Decoupling,"Decoherence induced by the laser frequency noise is one of the most important obstacles in the quantum information processing. In order to suppress this decoherence, the noise power spectral density needs to be accurately characterized. In particular, the noise spectrum measurement based on the coherence characteristics of qubits would be a meaningful and still challenging method. Here, we theoretically analyze and experimentally obtain the spectrum of laser frequency noise based on the continuous dynamical decoupling technique. We first estimate the mixture-noise (including laser and magnetic noises) spectrum up to $(2π)$530 kHz by monitoring the transverse relaxation from an initial state $+X$, followed by a gradient descent data process protocol. Then the contribution from the laser noise is extracted by enconding the qubits on different Zeeman sublevels. We also investigate two sufficiently strong noise components by making an analogy between these noises and driving lasers whose linewidth assumed to be negligible. This method is verified experimentally and finally helps to characterize the noise."
107,https://arxiv.org/abs/2002.08530,Learning Multi-granular Quantized Embeddings for Large-Vocab Categorical Features in Recommender Systems,"Recommender system models often represent various sparse features like users, items, and categorical features via embeddings. A standard approach is to map each unique feature value to an embedding vector. The size of the produced embedding table grows linearly with the size of the vocabulary. Therefore, a large vocabulary inevitably leads to a gigantic embedding table, creating two severe problems: (i) making model serving intractable in resource-constrained environments; (ii) causing overfitting problems. In this paper, we seek to learn highly compact embeddings for large-vocab sparse features in recommender systems (recsys). First, we show that the novel Differentiable Product Quantization (DPQ) approach can generalize to recsys problems. In addition, to better handle the power-law data distribution commonly seen in recsys, we propose a Multi-Granular Quantized Embeddings (MGQE) technique which learns more compact embeddings for infrequent items. We seek to provide a new angle to improve recommendation performance with compact model sizes. Extensive experiments on three recommendation tasks and two datasets show that we can achieve on par or better performance, with only ~20% of the original model size."
108,https://arxiv.org/abs/2002.05709,A Simple Framework for Contrastive Learning of Visual Representations,"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels."
109,https://arxiv.org/abs/2002.01187,Bilinear Fractional Integral Operators,"We study the bilinear fractional integral considered by Kenig and Stein, where linear combinations of variables with matrix coefficients are involved. Under more general settings, we give a complete characterization of the corresponding parameters for which the bilinear fractional integral is bounded from $L^{p_1}(\mathbb R^{n_1}) \times L^{p_2}(\mathbb R^{n_2})$ to $L^q(\mathbb R^m)$."
110,https://arxiv.org/abs/2001.07384,Understanding Why Neural Networks Generalize Well Through GSNR of Parameters,"As deep neural networks (DNNs) achieve tremendous success across many application domains, researchers tried to explore in many aspects on why they generalize well. In this paper, we provide a novel perspective on these issues using the gradient signal to noise ratio (GSNR) of parameters during training process of DNNs. The GSNR of a parameter is defined as the ratio between its gradient's squared mean and variance, over the data distribution. Based on several approximations, we establish a quantitative relationship between model parameters' GSNR and the generalization gap. This relationship indicates that larger GSNR during training process leads to better generalization performance. Moreover, we show that, different from that of shallow models (e.g. logistic regression, support vector machines), the gradient descent optimization dynamics of DNNs naturally produces large GSNR during training, which is probably the key to DNNs' remarkable generalization ability."
111,https://arxiv.org/abs/1912.03712,Hardy-Littlewood-Sobolev Inequality on Mixed-Norm Lebesgue Spaces,"We consider the Hardy-Littlewood-Sobolev inequality on mixed-norm Lebesgue spaces. We give a complete characterization of indices $\vec p$ and $\vec q$ such that the Riesz potential is bounded from $L^{\vec p}$ to $L^{\vec q}$, including all the endpoint cases. As a result, we get the mixed-norm Hardy-Littlewood-Sobolev inequality."
112,https://arxiv.org/abs/1911.09071,The Origins and Prevalence of Texture Bias in Convolutional Neural Networks,"Recent work has indicated that, unlike humans, ImageNet-trained CNNs tend to classify images by texture rather than by shape. How pervasive is this bias, and where does it come from? We find that, when trained on datasets of images with conflicting shape and texture, CNNs learn to classify by shape at least as easily as by texture. What factors, then, produce the texture bias in CNNs trained on ImageNet? Different unsupervised training objectives and different architectures have small but significant and largely independent effects on the level of texture bias. However, all objectives and architectures still lead to models that make texture-based classification decisions a majority of the time, even if shape information is decodable from their hidden representations. The effect of data augmentation is much larger. By taking less aggressive random crops at training time and applying simple, naturalistic augmentation (color distortion, noise, and blur), we train models that classify ambiguous images by shape a majority of the time, and outperform baselines on out-of-distribution test sets. Our results indicate that apparent differences in the way humans and ImageNet-trained CNNs process images may arise not primarily from differences in their internal workings, but from differences in the data that they see."
113,https://arxiv.org/abs/1911.05317,Convenient Real-Time Monitoring of the Contamination of Surface Ion Trap,"Recent studies indicated that contamination by adatoms on the surface ion trap can generate contact potential, leading to fluctuations in patch potential. By investigating contamination induced by surface adatoms during a loading process, a direct physical image of the contamination process and the relationship between the capacitance change and the contamination from surface adatoms is examined theoretically and experimentally. From the relationship, the contamination by surface adatoms and the effect of in situ treatment process can be monitored by the capacitance between electrodes in real time. This study is foundational to further research on anomalous heating with practical applications in quantum information processing from surface ion traps."
114,https://arxiv.org/abs/1911.00601,Inducing Metallicity in Graphene Nanoribbons via Zero-Mode Superlattices,"The design and fabrication of robust metallic states in graphene nanoribbons (GNRs) is a significant challenge since lateral quantum confinement and many-electron interactions tend to induce electronic band gaps when graphene is patterned at nanometer length scales. Recent developments in bottom-up synthesis have enabled the design and characterization of atomically-precise GNRs, but strategies for realizing GNR metallicity have been elusive. Here we demonstrate a general technique for inducing metallicity in GNRs by inserting a symmetric superlattice of zero-energy modes into otherwise semiconducting GNRs. We verify the resulting metallicity using scanning tunneling spectroscopy as well as first-principles density-functional theory and tight binding calculations. Our results reveal that the metallic bandwidth in GNRs can be tuned over a wide range by controlling the overlap of zero-mode wavefunctions through intentional sublattice symmetry-breaking."
115,https://arxiv.org/abs/1908.09756,Differentiable Product Quantization for End-to-End Embedding Compression,"Embedding layers are commonly used to map discrete symbols into continuous embedding vectors that reflect their semantic meanings. Despite their effectiveness, the number of parameters in an embedding layer increases linearly with the number of symbols and poses a critical challenge on memory and storage constraints. In this work, we propose a generic and end-to-end learnable compression framework termed differentiable product quantization (DPQ). We present two instantiations of DPQ that leverage different approximation techniques to enable differentiability in end-to-end learning. Our method can readily serve as a drop-in alternative for any existing embedding layer. Empirically, DPQ offers significant compression ratios (14-238$\times$) at negligible or no performance cost on 10 datasets across three different language tasks."
116,https://arxiv.org/abs/1907.03371,A coupled-channel lattice study on the resonance-like structure $Z_c(3900)$,"In this exploratory study, near-threshold scattering of $D$ and $\bar{D}^*$ meson is investigated using lattice QCD with $N_f=2+1+1$ twisted mass fermion configurations. The calculation is performed within the coupled-channel Lüscher's finite-size formalism. The study focuses on the channel with $I^G(J^{PC})=1^+(1^{+-})$ where the resonance-like structure $Z_c(3900)$ was discovered. We first identify the most relevant two channels of the problem and the lattice study is performed within the two-channel scattering model. Combined with a two-channel Ross-Shaw theory, scattering parameters are extracted from the energy levels by solving the generalized eigenvalue problem. Our results on the scattering length parameters suggest that, at the particular lattice parameters that we studied, the best fitted parameters do not correspond to a peak behavior in the elastic scattering cross section near the threshold. Furthermore, within the zero-range Ross-Shaw theory, the scenario of a narrow resonance close to the threshold is disfavored beyond $3σ$ level."
117,https://arxiv.org/abs/1907.00505,Few-Shot Representation Learning for Out-Of-Vocabulary Words,"Existing approaches for learning word embeddings often assume there are sufficient occurrences for each word in the corpus, such that the representation of words can be accurately estimated from their contexts. However, in real-world scenarios, out-of-vocabulary (a.k.a. OOV) words that do not appear in training corpus emerge frequently. It is challenging to learn accurate representations of these words with only a few observations. In this paper, we formulate the learning of OOV embeddings as a few-shot regression problem, and address it by training a representation function to predict the oracle embedding vector (defined as embedding trained with abundant observations) based on limited observations. Specifically, we propose a novel hierarchical attention-based architecture to serve as the neural regression function, with which the context information of a word is encoded and aggregated from K observations. Furthermore, our approach can leverage Model-Agnostic Meta-Learning (MAML) for adapting the learned model to the new corpus fast and robustly. Experiments show that the proposed approach significantly outperforms existing methods in constructing accurate embeddings for OOV words, and improves downstream tasks where these embeddings are utilized."
118,https://arxiv.org/abs/1906.05078,Semiconductor ring laser frequency combs induced by phase turbulence,"Semiconductor ring lasers are miniaturized devices that operate on clockwise and counterclockwise modes. These modes are not coupled in the absence of intracavity reflectors, which prevents the formation of a standing wave in the cavity and, consequently, of a population inversion grating. This should inhibit the onset of multimode emission driven by spatial hole burning. Here we show that, despite this notion, ring quantum cascade lasers inherently operate in phase-locked multimode states, that switch on and off as the pumping level is progressively increased. By rewriting the master equation of lasers with fast gain media in the form of the complex Ginzburg-Landau equation, we show that ring frequency combs stem from a phase instability---a phenomenon also known in superconductors and Bose-Einstein condensates. The instability is due to coupling of the amplitude and phase modulation of the optical field in a semiconductor laser, which plays the role of a Kerr nonlinearity, highlighting a connection between laser and microresonator frequency combs."
119,https://arxiv.org/abs/1905.13728,Pre-Training Graph Neural Networks for Generic Structural Feature Extraction,"Graph neural networks (GNNs) are shown to be successful in modeling applications with graph structures. However, training an accurate GNN model requires a large collection of labeled data and expressive features, which might be inaccessible for some applications. To tackle this problem, we propose a pre-training framework that captures generic graph structural information that is transferable across tasks. Our framework can leverage the following three tasks: 1) denoising link reconstruction, 2) centrality score ranking, and 3) cluster preserving. The pre-training procedure can be conducted purely on the synthetic graphs, and the pre-trained GNN is then adapted for downstream applications. With the proposed pre-training procedure, the generic structural information is learned and preserved, thus the pre-trained GNN requires less amount of labeled data and fewer domain-specific features to achieve high performance on different downstream tasks. Comprehensive experiments demonstrate that our proposed framework can significantly enhance the performance of various tasks at the level of node, link, and graph."
120,https://arxiv.org/abs/1905.11023,An Optimal Game Approach for Heterogeneous Vehicular Network Selection with Varying Network Performance,"Most conventional heterogeneous network selection strategies applied in heterogeneous vehicular network regard the performance of each network constant in various traffic scenarios. This assumption leads such strategies to be ineffective in the real-world performance-changing scenarios. To solve this problem, we propose an optimal game approach for heterogeneous vehicular network selection under conditions in which the performance parameters of some networks are changing. Terminals attempting to switch to the network with higher evaluation is formulated as a multi-play non-cooperative game. Heterogeneous vehicular network characteristics are thoroughly accounted for to adjust the game strategy and adapt to the vehicular environment for stability and rapid convergence. A multi-play non-cooperative game model is built to formulate network selection. A probabilistic strategy is used to gradually drive players toward convergence to prevent instability. Furthermore, a system prototype was built at the Connected and Automated Vehicle Test bed of Chang'an University (CAVTest). Its corresponding test results indicate that the proposed approach can effectively suppress the ping-pong effect caused by massive handoffs due to varying network performance and thus well outperforms the single-play strategy."
121,https://arxiv.org/abs/1905.04579,Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification,"Graph Neural Nets (GNNs) have received increasing attentions, partially due to their superior performance in many node and graph classification tasks. However, there is a lack of understanding on what they are learning and how sophisticated the learned graph functions are. In this work, we propose a dissection of GNNs on graph classification into two parts: 1) the graph filtering, where graph-based neighbor aggregations are performed, and 2) the set function, where a set of hidden node features are composed for prediction. To study the importance of both parts, we propose to linearize them separately. We first linearize the graph filtering function, resulting Graph Feature Network (GFN), which is a simple lightweight neural net defined on a \textit{set} of graph augmented features. Further linearization of GFN's set function results in Graph Linear Network (GLN), which is a linear function. Empirically we perform evaluations on common graph classification benchmarks. To our surprise, we find that, despite the simplification, GFN could match or exceed the best accuracies produced by recently proposed GNNs (with a fraction of computation cost), while GLN underperforms significantly. Our results demonstrate the importance of non-linear set function, and suggest that linear graph filtering with non-linear set function is an efficient and powerful scheme for modeling existing graph classification benchmarks."
122,https://arxiv.org/abs/1905.01467,Defining Smart Contract Defects on Ethereum,"Smart contracts are programs running on a blockchain. They are immutable to change, and hence can not be patched for bugs once deployed. Thus it is critical to ensure they are bug-free and well-designed before deployment. A Contract defect is an error, flaw or fault in a smart contract that causes it to produce an incorrect or unexpected result, or to behave in unintended ways. The detection of contract defects is a method to avoid potential bugs and improve the design of existing code. Since smart contracts contain numerous distinctive features, such as the gas system. decentralized, it is important to find smart contract specified defects. To fill this gap, we collected smart-contract-related posts from Ethereum StackExchange, as well as real-world smart contracts. We manually analyzed these posts and contracts; using them to define 20 kinds of contract defects. We categorized them into indicating potential security, availability, performance, maintainability and reusability problems. To validate if practitioners consider these contract as harmful, we created an online survey and received 138 responses from 32 different countries. Feedback showed these contract defects are harmful and removing them would improve the quality and robustness of smart contracts. We manually identified our defined contract defects in 587 real world smart contract and publicly released our dataset. Finally, we summarized 5 impacts caused by contract defects. These help developers better understand the symptoms of the defects and removal priority."
123,https://arxiv.org/abs/1904.01098,Unsupervised Inductive Graph-Level Representation Learning via Graph-Graph Proximity,"We introduce a novel approach to graph-level representation learning, which is to embed an entire graph into a vector space where the embeddings of two graphs preserve their graph-graph proximity. Our approach, UGRAPHEMB, is a general framework that provides a novel means to performing graph-level embedding in a completely unsupervised and inductive manner. The learned neural network can be considered as a function that receives any graph as input, either seen or unseen in the training set, and transforms it into an embedding. A novel graph-level embedding generation mechanism called Multi-Scale Node Attention (MSNA), is proposed. Experiments on five real graph datasets show that UGRAPHEMB achieves competitive accuracy in the tasks of graph classification, similarity ranking, and graph visualization."
124,https://arxiv.org/abs/1902.04527,Extension of Multilinear Fractional Integral Operators to Linear Operators on Lebesgue Spaces with Mixed Norms,"In [C. E. Kenig and E. M. Stein, Multilinear estimates and fractional integration, Math. Res. Lett., 6(1):1-15, 1999], the following type of multilinear fractional integral \[
  \int_{\mathbb{R}^{mn}} \frac{f_1(l_1(x_1,\ldots,x_m,x))\cdots f_{m+1}(l_{m+1}(x_1,\ldots,x_m,x))}{(|x_1|+\ldots+|x_m|)^λ} dx_1\ldots dx_m \] was studied, where $l_i$ are linear maps from $\mathbb{R}^{(m+1)n}$ to $\mathbb{R}^n$ satisfying certain conditions. They proved the boundedness of such multilinear fractional integral from $L^{p_1}\times \ldots \times L^{p_{m+1}}$ to $L^q$ when the indices satisfy the homogeneity condition. In this paper, we show that the above multilinear fractional integral extends to a linear operator for functions in the mixed-norm Lebesgue space $L^{\vec p}$ which contains $L^{p_1}\times \ldots \times L^{p_{m+1}}$ as a subset. Under less restrictions on the linear maps $l_i$, we give a complete characterization of the indices $\vec p$, $q$ and $λ$ for which such an operator is bounded from $L^{\vec p}$ to $L^q$. And for $m=1$ or $n=1$, we give necessary and sufficient conditions on $(l_1, \ldots, l_{m+1})$, $\vec p=(p_1,\ldots, p_{m+1})$, $q$ and $λ$ such that the operator is bounded."
125,https://arxiv.org/abs/1901.11250,Versatile surface ion trap for effective cooling and large-scale trapping of ions,"Scaling up and effective cooling of ions in surface ion trap are central challenges in quantum computing and quantum simulation with trapped ions. In this theoretical study, we propose a versatile surface ion trap. In the manipulation zone of our trap, a symmetric seven-wire geometry enables innate principle-axes rotation of two parallel linear ion chains, which facilitates the cooling of ions along all principle trap axes. To alleviate contaminating the manipulation zone during ion loading, a symmetric five-wire geometry is designed as the loading zone. And a ""fork junction"" connects the loading and manipulation zones, which also enables the shuttling and reordering of ions. A multi-objective optimization procedure suitable for arbitrary junction designs is described in detail, and we present the corresponding optimal results for the key components of our trap. The proposed versatile trap can be used in the construction of large-scale ion quantum processors. The trap also can be used as the multi-ion-mixer or the efficient ion beam splitter, which has the potential applications in quantum simulation and quantum computing, the research of 2D dimensional ion crystals and the guides of quantum microscope, like an electron beam splitter used for quantum matter-wave optics experiments. Interesting topics involving the spin-spin interactions between two ion chains can also be simulated in our trap."
126,https://arxiv.org/abs/1901.10668,Doubly Sparse: Sparse Mixture of Sparse Experts for Efficient Softmax Inference,"Computations for the softmax function are significantly expensive when the number of output classes is large. In this paper, we present a novel softmax inference speedup method, Doubly Sparse Softmax (DS-Softmax), that leverages sparse mixture of sparse experts to efficiently retrieve top-k classes. Different from most existing methods that require and approximate a fixed softmax, our method is learning-based and can adapt softmax weights for a better inference speedup. In particular, our method learns a two-level hierarchy which divides entire output class space into several partially overlapping experts. Each expert is sparse and only contains a subset of output classes. To find top-k classes, a sparse mixture enables us to find the most probable expert quickly, and the sparse expert enables us to search within a small-scale softmax. We empirically conduct evaluation on several real-world tasks, including neural machine translation, language modeling and image classification, and demonstrate that significant computation reductions can be achieved at no performance loss."
127,https://arxiv.org/abs/1811.11212,Self-Supervised GANs via Auxiliary Rotation Loss,"Conditional GANs are at the forefront of natural image synthesis. The main drawback of such models is the necessity for labeled data. In this work we exploit two popular unsupervised learning techniques, adversarial training and self-supervision, and take a step towards bridging the gap between conditional and unconditional GANs. In particular, we allow the networks to collaborate on the task of representation learning, while being adversarial with respect to the classic GAN game. The role of self-supervision is to encourage the discriminator to learn meaningful feature representations which are not forgotten during training. We test empirically both the quality of the learned image representations, and the quality of the synthesized images. Under the same conditions, the self-supervised GAN attains a similar performance to state-of-the-art conditional counterparts. Finally, we show that this approach to fully unsupervised learning can be scaled to attain an FID of 23.4 on unconditional ImageNet generation."
128,https://arxiv.org/abs/1811.06170,Scheme and Experimental Demonstration of Fully Atomic Weak Value Amplification,"In this paper, we explore the possibilities of realizing weak value amplification (WVA) using purely atomic degrees of freedom. Our scheme identifies the internal electronic states and external motional states of a single trapped $^{40}$Ca$^+$ ion as the system degree and pointer degree respectively, and their controllable weak coupling is provided by a bichromatic light field. In our experimental demonstration, by performing appropriate postselection on the internal states, a position displacement of 4 angstroms (in phase space) of the trapped ion is amplified to 10 nanometers. The sensitivity of the amplification effect to the relative phase of the quantum state is also demonstrated. The high operational flexibility of this procedure allows fully exploration of the peculiarities of WVA."
129,https://arxiv.org/abs/1810.11598,Self-Supervised GAN to Counter Forgetting,"GANs involve training two networks in an adversarial game, where each network's task depends on its adversary. Recently, several works have framed GAN training as an online or continual learning problem. We focus on the discriminator, which must perform classification under an (adversarially) shifting data distribution. When trained on sequential tasks, neural networks exhibit \emph{forgetting}. For GANs, discriminator forgetting leads to training instability. To counter forgetting, we encourage the discriminator to maintain useful representations by adding a self-supervision. Conditional GANs have a similar effect using labels. However, our self-supervised GAN does not require labels, and closes the performance gap between conditional and unconditional models. We show that, in doing so, the self-supervised discriminator learns better representations than regular GANs."
130,https://arxiv.org/abs/1810.05050,A broadband achromatic polarization-insensitive metalens consisting of anisotropic nanostructures,"Metasurfaces have attracted widespread attention due to an increasing demand of compact and wearable optical devices. For many applications, polarization-insensitive metasurfaces are highly desirable and appear to limit the choice of their constituent elements to isotropic nanostructures. This greatly restricts the degrees of geometric parameters available in designing each nanostructure. Here, we demonstrate a polarization-insensitive metalens using otherwise anisotropic nanofins which offer additional control over the dispersion and phase of the output light. As a result, we can render a metalens achromatic and polarization-insensitive across nearly the entire visible spectrum from wavelength 460 nm to 700 nm, while maintaining diffraction-limited performance. The metalens is comprised of just a single layer of TiO2 nanofins and has a numerical aperture of 0.2 with a diameter of 26.4 um. The generality of our polarization-insensitive design allows it to be implemented in a plethora of other metasurface devices with applications ranging from imaging to virtual/augmented reality."
131,https://arxiv.org/abs/1810.01365,On Self Modulation for Generative Adversarial Networks,"Training Generative Adversarial Networks (GANs) is notoriously challenging. We propose and study an architectural modification, self-modulation, which improves GAN performance across different data sets, architectures, losses, regularizers, and hyperparameter settings. Intuitively, self-modulation allows the intermediate feature maps of a generator to change as a function of the input noise vector. While reminiscent of other conditioning techniques, it requires no labeled data. In a large-scale empirical study we observe a relative decrease of $5\%-35\%$ in FID. Furthermore, all else being equal, adding this modification to the generator leads to improved performance in $124/144$ ($86\%$) of the studied settings. Self-modulation is a simple architectural change that requires no additional parameter tuning, which suggests that it can be applied readily to any GAN."
132,https://arxiv.org/abs/1808.05689,SimGNN: A Neural Network Approach to Fast Graph Similarity Computation,"Graph similarity search is among the most important graph-based applications, e.g. finding the chemical compounds that are most similar to a query compound. Graph similarity computation, such as Graph Edit Distance (GED) and Maximum Common Subgraph (MCS), is the core operation of graph similarity search and many other applications, but very costly to compute in practice. Inspired by the recent success of neural network approaches to several graph applications, such as node or graph classification, we propose a novel neural network based approach to address this classic yet challenging graph problem, aiming to alleviate the computational burden while preserving a good performance.
  The proposed approach, called SimGNN, combines two strategies. First, we design a learnable embedding function that maps every graph into a vector, which provides a global summary of a graph. A novel attention mechanism is proposed to emphasize the important nodes with respect to a specific similarity metric. Second, we design a pairwise node comparison method to supplement the graph-level embeddings with fine-grained node-level information. Our model achieves better generalization on unseen graphs, and in the worst case runs in quadratic time with respect to the number of nodes in two graphs. Taking GED computation as an example, experimental results on three real graph datasets demonstrate the effectiveness and efficiency of our approach. Specifically, our model achieves smaller error rate and great time reduction compared against a series of baselines, including several approximation algorithms on GED computation, and many existing graph neural network based models. To the best of our knowledge, we are among the first to adopt neural networks to explicitly model the similarity between two graphs, and provide a new direction for future research on graph similarity computation and graph similarity search."
133,https://arxiv.org/abs/1806.09464,Learning K-way D-dimensional Discrete Codes for Compact Embedding Representations,"Conventional embedding methods directly associate each symbol with a continuous embedding vector, which is equivalent to applying a linear transformation based on a ""one-hot"" encoding of the discrete symbols. Despite its simplicity, such approach yields the number of parameters that grows linearly with the vocabulary size and can lead to overfitting. In this work, we propose a much more compact K-way D-dimensional discrete encoding scheme to replace the ""one-hot"" encoding. In the proposed ""KD encoding"", each symbol is represented by a $D$-dimensional code with a cardinality of $K$, and the final symbol embedding vector is generated by composing the code embedding vectors. To end-to-end learn semantically meaningful codes, we derive a relaxed discrete optimization approach based on stochastic gradient descent, which can be generally applied to any differentiable computational graph with an embedding layer. In our experiments with various applications from natural language processing to graph convolutional networks, the total size of the embedding layer can be reduced up to 98\% while achieving similar or better performance."
134,https://arxiv.org/abs/1805.06470,Topological Band Engineering of Graphene Nanoribbons,"Topological insulators (TIs) are an emerging class of materials that host highly robust in-gap surface/interface states while maintaining an insulating bulk. While most notable scientific advancements in this field have been focused on TIs and related topological crystalline insulators in 2D and 3D, more recent theoretical work has predicted the existence of 1D symmetry-protected topological phases in graphene nanoribbons (GNRs). The topological phase of these laterally-confined, semiconducting strips of graphene is determined by their width, edge shape, and the terminating unit cell, and is characterized by a Z2 invariant (similar to 1D solitonic systems). Interfaces between topologically distinct GNRs characterized by different Z2 are predicted to support half-filled in-gap localized electronic states which can, in principle, be utilized as a tool for material engineering. Here we present the rational design and experimental realization of a topologically-engineered GNR superlattice that hosts a 1D array of such states, thus generating otherwise inaccessible electronic structure. This strategy also enables new end states to be engineered directly into the termini of the 1D GNR superlattice. Atomically-precise topological GNR superlattices were synthesized from molecular precursors on a Au(111) surface under ultra-high vacuum (UHV) conditions and characterized by low temperature scanning tunneling microscopy (STM) and spectroscopy (STS). Our experimental results and first-principles calculations reveal that the frontier band structure of these GNR superlattices is defined purely by the coupling between adjacent topological interface states. This novel manifestation of 1D topological phases presents an entirely new route to band engineering in 1D materials based on precise control of their electronic topology, and is a promising platform for future studies of 1D quantum spin physics."
135,https://arxiv.org/abs/1804.08052,HeteroMed: Heterogeneous Information Network for Medical Diagnosis,"With the recent availability of Electronic Health Records (EHR) and great opportunities they offer for advancing medical informatics, there has been growing interest in mining EHR for improving quality of care. Disease diagnosis due to its sensitive nature, huge costs of error, and complexity has become an increasingly important focus of research in past years. Existing studies model EHR by capturing co-occurrence of clinical events to learn their latent embeddings. However, relations among clinical events carry various semantics and contribute differently to disease diagnosis which gives precedence to a more advanced modeling of heterogeneous data types and relations in EHR data than existing solutions. To address these issues, we represent how high-dimensional EHR data and its rich relationships can be suitably translated into HeteroMed, a heterogeneous information network for robust medical diagnosis. Our modeling approach allows for straightforward handling of missing values and heterogeneity of data. HeteroMed exploits metapaths to capture higher level and semantically important relations contributing to disease diagnosis. Furthermore, it employs a joint embedding framework to tailor clinical event representations to the disease diagnosis goal. To the best of our knowledge, this is the first study to use Heterogeneous Information Network for modeling clinical data and disease diagnosis. Experimental results of our study show superior performance of HeteroMed compared to prior methods in prediction of exact diagnosis codes and general disease cohorts. Moreover, HeteroMed outperforms baseline models in capturing similarities of clinical events which are examined qualitatively through case studies."
136,https://arxiv.org/abs/1802.06993,A Survey on the Security of Blockchain Systems,"Since its inception, the blockchain technology has shown promising application prospects. From the initial cryptocurrency to the current smart contract, blockchain has been applied to many fields. Although there are some studies on the security and privacy issues of blockchain, there lacks a systematic examination on the security of blockchain systems. In this paper, we conduct a systematic study on the security threats to blockchain and survey the corresponding real attacks by examining popular blockchain systems. We also review the security enhancement solutions for blockchain, which could be used in the development of various blockchain systems, and suggest some future directions to stir research efforts into this area."
137,https://arxiv.org/abs/1712.06438,An Adaptive Gas Cost Mechanism for Ethereum to Defend Against Under-Priced DoS Attacks,"The gas mechanism in Ethereum charges the execution of every operation to ensure that smart contracts running in EVM (Ethereum Virtual Machine) will be eventually terminated. Failing to properly set the gas costs of EVM operations allows attackers to launch DoS attacks on Ethereum. Although Ethereum recently adjusted the gas costs of EVM operations to defend against known DoS attacks, it remains unknown whether the new setting is proper and how to configure it to defend against unknown DoS attacks. In this paper, we make the first step to address this challenging issue by first proposing an emulation-based framework to automatically measure the resource consumptions of EVM operations. The results reveal that Ethereum's new setting is still not proper. Moreover, we obtain an insight that there may always exist exploitable under-priced operations if the cost is fixed. Hence, we propose a novel gas cost mechanism, which dynamically adjusts the costs of EVM operations according to the number of executions, to thwart DoS attacks. This method punishes the operations that are executed much more frequently than before and lead to high gas costs. To make our solution flexible and secure and avoid frequent update of Ethereum client, we design a special smart contract that collaborates with the updated EVM for dynamic parameter adjustment. Experimental results demonstrate that our method can effectively thwart both known and unknown DoS attacks with flexible parameter settings. Moreover, our method only introduces negligible additional gas consumption for benign users."
138,https://arxiv.org/abs/1712.06263,"The relationship between trading volumes, number of transactions, and stock volatility in GARCH models","We examine the relationship between trading volumes, number of transactions, and volatility using daily stock data of the Tokyo Stock Exchange. Following the mixture of distributions hypothesis, we use trading volumes and the number of transactions as proxy for the rate of information arrivals affecting stock volatility. The impact of trading volumes or number of transactions on volatility is measured using the generalized autoregressive conditional heteroscedasticity (GARCH) model. We find that the GARCH effects, that is, persistence of volatility, is not always removed by adding trading volumes or number of transactions, indicating that trading volumes and number of transactions do not adequately represent the rate of information arrivals."
139,https://arxiv.org/abs/1712.01064,Iterated and Mixed Weak Norms with Applications to Geometric Inequalities,"In this paper, we consider a new weak norm, iterated weak norm in Lebesgue spaces with mixed norms. We study properties of the mixed weak norm and the iterated weak norm and present the relationship between the two weak norms. Even for the ordinary Lebesgue spaces, the two weak norms are not equivalent and any one of them can not control the other one. We give some convergence and completeness results for both weak norms. We study the convergence in truncated norm, which is a substitution of the convergence in measure for mixed Lebesgue spaces. And we give a characterization of the convergence in truncated norm. We show that Hölder's inequality is not always true on mixed weak spaces and we give a complete characterization of indices which admit Hölder's inequality. As applications, we establish some geometric inequalities related to fractional integration in mixed weak spaces and in iterated weak spaces which essentially generalize the Hardy-Littlewood-Sobolev inequality."
140,https://arxiv.org/abs/1711.09343,Phase and dispersion engineering of metalenses: broadband achromatic focusing and imaging in the visible,"Metasurfaces have the potential to miniaturize and improve the performance of any optical element, with applications spanning telecommunications, computing and wearable optics. However, the ability to retain functionality over a continuous, broad range of wavelengths is essential for their widespread adoption. So far, efforts to achieve this have been limited to either a reflection configuration or operation at wavelengths other than the visible. Here, we show that by judicious choice of nanostructures, one can independently control both the phase and group delay, leading to broadband transmissive achromatic metalenses and metalenses with tunable dispersion across the visible spectrum. We demonstrate diffraction-limited broadband focusing and achromatic imaging using metalenses across wavelengths from 470 nm to 670 nm, with focal length change of a few percent across this 200 nm bandwidth. This approach requires only a single layer metasurface whose thickness is on the wavelength scale, and does not involve spatial multiplexing or cascading."
141,https://arxiv.org/abs/1711.03067,Learning K-way D-dimensional Discrete Code For Compact Embedding Representations,"Embedding methods such as word embedding have become pillars for many applications containing discrete structures. Conventional embedding methods directly associate each symbol with a continuous embedding vector, which is equivalent to applying linear transformation based on ""one-hot"" encoding of the discrete symbols. Despite its simplicity, such approach yields number of parameters that grows linearly with the vocabulary size and can lead to overfitting. In this work we propose a much more compact K-way D-dimensional discrete encoding scheme to replace the ""one-hot"" encoding. In ""KD encoding"", each symbol is represented by a $D$-dimensional code, and each of its dimension has a cardinality of $K$. The final symbol embedding vector can be generated by composing the code embedding vectors. To learn the semantically meaningful code, we derive a relaxed discrete optimization technique based on stochastic gradient descent. By adopting the new coding system, the efficiency of parameterization can be significantly improved (from linear to logarithmic), and this can also mitigate the over-fitting problem. In our experiments with language modeling, the number of embedding parameters can be reduced by 97\% while achieving similar or better performance."
142,https://arxiv.org/abs/1706.07881,On Sampling Strategies for Neural Network-based Collaborative Filtering,"Recent advances in neural networks have inspired people to design hybrid recommendation algorithms that can incorporate both (1) user-item interaction information and (2) content information including image, audio, and text. Despite their promising results, neural network-based recommendation algorithms pose extensive computational costs, making it challenging to scale and improve upon. In this paper, we propose a general neural network-based recommendation framework, which subsumes several existing state-of-the-art recommendation algorithms, and address the efficiency issue by investigating sampling strategies in the stochastic gradient descent training for the framework. We tackle this issue by first establishing a connection between the loss functions and the user-item interaction bipartite graph, where the loss function terms are defined on links while major computation burdens are located at nodes. We call this type of loss functions ""graph-based"" loss functions, for which varied mini-batch sampling strategies can have different computational costs. Based on the insight, three novel sampling strategies are proposed, which can significantly improve the training efficiency of the proposed framework (up to $\times 30$ times speedup in our experiments), as well as improving the recommendation performance. Theoretical analysis is also provided for both the computational cost and the convergence. We believe the study of sampling strategies have further implications on general graph-based loss functions, and would also enable more research under the neural network-based recommendation framework."
143,https://arxiv.org/abs/1706.01084,Joint Text Embedding for Personalized Content-based Recommendation,"Learning a good representation of text is key to many recommendation applications. Examples include news recommendation where texts to be recommended are constantly published everyday. However, most existing recommendation techniques, such as matrix factorization based methods, mainly rely on interaction histories to learn representations of items. While latent factors of items can be learned effectively from user interaction data, in many cases, such data is not available, especially for newly emerged items.
  In this work, we aim to address the problem of personalized recommendation for completely new items with text information available. We cast the problem as a personalized text ranking problem and propose a general framework that combines text embedding with personalized recommendation. Users and textual content are embedded into latent feature space. The text embedding function can be learned end-to-end by predicting user interactions with items. To alleviate sparsity in interaction data, and leverage large amount of text data with little or no user interactions, we further propose a joint text embedding model that incorporates unsupervised text embedding with a combination module. Experimental results show that our model can significantly improve the effectiveness of recommendation systems on real-world datasets."
144,https://arxiv.org/abs/1703.03994,Under-Optimized Smart Contracts Devour Your Money,"Smart contracts are full-fledged programs that run on blockchains (e.g., Ethereum, one of the most popular blockchains). In Ethereum, gas (in Ether, a cryptographic currency like Bitcoin) is the execution fee compensating the computing resources of miners for running smart contracts. However, we find that under-optimized smart contracts cost more gas than necessary, and therefore the creators or users will be overcharged. In this work, we conduct the first investigation on Solidity, the recommended compiler, and reveal that it fails to optimize gas-costly programming patterns. In particular, we identify 7 gas-costly patterns and group them to 2 categories. Then, we propose and develop GASPER, a new tool for automatically locating gas-costly patterns by analyzing smart contracts' bytecodes. The preliminary results on discovering 3 representative patterns from 4,240 real smart contracts show that 93.5%, 90.1% and 80% contracts suffer from these 3 patterns, respectively."
145,https://arxiv.org/abs/1701.00452,HSEARCH: fast and accurate protein sequence motif search and clustering,"Protein motifs are conserved fragments occurred frequently in protein sequences. They have significant functions, such as active site of an enzyme. Search and clustering protein sequence motifs are computational intensive. Most existing methods are not fast enough to analyze large data sets for motif finding or achieve low accuracy for motif clustering. We present a new protein sequence motif finding and clustering algorithm, called HSEARCH. It converts fixed length protein sequences to data points in high dimensional space, and applies locality-sensitive hashing to fast search homologous protein sequences for a motif. HSEARCH is significantly faster than the brute force algorithm for protein motif finding and achieves high accuracy for protein motif clustering."
146,https://arxiv.org/abs/1612.08207,Ideology Detection for Twitter Users with Heterogeneous Types of Links,"The problem of ideology detection is to study the latent (political) placement for people, which is traditionally studied on politicians according to their voting behaviors. Recently, more and more studies begin to address the ideology detection problem for ordinary users based on their online behaviors that can be captured by social media, e.g., Twitter. As far as we are concerned, however, the vast majority of the existing methods on ideology detection on social media have oversimplified the problem as a binary classification problem (i.e., liberal vs. conservative). Moreover, though social links can play a critical role in deciding one's ideology, most of the existing work ignores the heterogeneous types of links in social media. In this paper we propose to detect \emph{numerical} ideology positions for Twitter users, according to their \emph{follow}, \emph{mention}, and \emph{retweet} links to a selected set of politicians. A unified probabilistic model is proposed that can (1) explain the reasons why links are built among people in terms of their ideology, (2) integrate heterogeneous types of links together in determining people's ideology, and (3) automatically learn the quality of each type of links in deciding one's ideology. Experiments have demonstrated the advantages of our model in terms of both ranking and political leaning classification accuracy. It is shown that (1) using multiple types of links is better than using any single type of links alone to determine one's ideology, and (2) our model is even more superior than baselines when dealing with people that are sparsely linked in one type of links. We also show that the detected ideology for Twitter users aligns with our intuition quite well."
147,https://arxiv.org/abs/1612.02814,Task-Guided and Path-Augmented Heterogeneous Network Embedding for Author Identification,"In this paper, we study the problem of author identification under double-blind review setting, which is to identify potential authors given information of an anonymized paper. Different from existing approaches that rely heavily on feature engineering, we propose to use network embedding approach to address the problem, which can automatically represent nodes into lower dimensional feature vectors. However, there are two major limitations in recent studies on network embedding: (1) they are usually general-purpose embedding methods, which are independent of the specific tasks; and (2) most of these approaches can only deal with homogeneous networks, where the heterogeneity of the network is ignored. Hence, challenges faced here are two folds: (1) how to embed the network under the guidance of the author identification task, and (2) how to select the best type of information due to the heterogeneity of the network.
  To address the challenges, we propose a task-guided and path-augmented heterogeneous network embedding model. In our model, nodes are first embedded as vectors in latent feature space. Embeddings are then shared and jointly trained according to task-specific and network-general objectives. We extend the existing unsupervised network embedding to incorporate meta paths in heterogeneous networks, and select paths according to the specific task. The guidance from author identification task for network embedding is provided both explicitly in joint training and implicitly during meta path selection. Our experiments demonstrate that by using path-augmented network embedding with task guidance, our model can obtain significantly better accuracy at identifying the true authors comparing to existing methods."
148,https://arxiv.org/abs/1608.07502,Entity Embedding-based Anomaly Detection for Heterogeneous Categorical Events,"Anomaly detection plays an important role in modern data-driven security applications, such as detecting suspicious access to a socket from a process. In many cases, such events can be described as a collection of categorical values that are considered as entities of different types, which we call heterogeneous categorical events. Due to the lack of intrinsic distance measures among entities, and the exponentially large event space, most existing work relies heavily on heuristics to calculate abnormal scores for events. Different from previous work, we propose a principled and unified probabilistic model APE (Anomaly detection via Probabilistic pairwise interaction and Entity embedding) that directly models the likelihood of events. In this model, we embed entities into a common latent space using their observed co-occurrence in different events. More specifically, we first model the compatibility of each pair of entities according to their embeddings. Then we utilize the weighted pairwise interactions of different entity types to define the event probability. Using Noise-Contrastive Estimation with ""context-dependent"" noise distribution, our model can be learned efficiently regardless of the large event space. Experimental results on real enterprise surveillance data show that our methods can accurately detect abnormal events compared to other state-of-the-art abnormal detection techniques."
149,https://arxiv.org/abs/1606.05208,On some determinant and matrix inequalities with a geometrical flavour,"In this paper we study some determinant inequalities and matrix inequalities which have a geometrical flavour. We first examine some inequalities which place work of Macbeath [13] in a more general setting and also relate to recent work of Gressman [8]. In particular, we establish optimisers for these determinant inequalities. We then use these inequalities to establish our main theorem which gives a geometric inequality of matrix type which improves and extends some inequalities of Christ in [5]."
150,https://arxiv.org/abs/2211.05368,A Comprehensive Survey on Distributed Training of Graph Neural Networks,"Graph neural networks (GNNs) have been demonstrated to be a powerful algorithmic model in broad application fields for their effectiveness in learning over graphs. To scale GNN training up for large-scale and ever-growing graphs, the most promising solution is distributed training which distributes the workload of training across multiple computing nodes. However, the workflows, computational patterns, communication patterns, and optimization techniques of distributed GNN training remain preliminarily understood. In this paper, we provide a comprehensive survey of distributed GNN training by investigating various optimization techniques used in distributed GNN training. First, distributed GNN training is classified into several categories according to their workflows. In addition, their computational patterns and communication patterns, as well as the optimization techniques proposed by recent work are introduced. Second, the software frameworks and hardware platforms of distributed GNN training are also introduced for a deeper understanding. Third, distributed GNN training is compared with distributed training of deep neural networks, emphasizing the uniqueness of distributed GNN training. Finally, interesting issues and opportunities in this field are discussed."
151,https://arxiv.org/abs/2210.02414,GLM-130B: An Open Bilingual Pre-trained Model,"We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and disconvergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization, without quantization aware training and with almost no performance loss, making it the first among 100B-scale models. More importantly, the property allows its effective inference on 4$\times$RTX 3090 (24G) or 8$\times$RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at https://github.com/THUDM/GLM-130B ."
152,https://arxiv.org/abs/2208.01190,"Toward 6G TK$μ$ Extreme Connectivity: Architecture, Key Technologies and Experiments","Sixth-generation (6G) networks are evolving towards new features and order-of-magnitude enhancement of systematic performance metrics compared to the current 5G. In particular, the 6G networks are expected to achieve extreme connectivity performance with Tbps-scale data rate, Kbps/Hz-scale spectral efficiency, and $μ$s-scale latency. To this end, an original three-layer 6G network architecture is designed to realise uniform full-spectrum cell-free radio access and provide task-centric agile proximate support for diverse applications. The designed architecture is featured by super edge node (SEN) which integrates connectivity, computing, AI, data, etc. On this basis, a technological framework of pervasive multi-level (PML) AI is established in the centralised unit to enable task-centric near-real-time resource allocation and network automation. We then introduce a radio access network (RAN) architecture of full spectrum uniform cell-free networks, which is among the most attractive RAN candidates for 6G TK$μ$ extreme connectivity. A few most promising key technologies, i.e., cell-free massive MIMO, photonics-assisted Terahertz wireless access and spatiotemporal two-dimensional channel coding are further discussed. A testbed is implemented and extensive trials are conducted to evaluate innovative technologies and methodologies. The proposed 6G network architecture and technological framework demonstrate exciting potentials for full-service and full-scenario applications."
153,https://arxiv.org/abs/2207.10083,"Mixed-Precision Inference Quantization: Radically Towards Faster inference speed, Lower Storage requirement, and Lower Loss","Based on the model's resilience to computational noise, model quantization is important for compressing models and improving computing speed. Existing quantization techniques rely heavily on experience and ""fine-tuning"" skills. In the majority of instances, the quantization model has a larger loss than a full precision model. This study provides a methodology for acquiring a mixed-precise quantization model with a lower loss than the full precision model. In addition, the analysis demonstrates that, throughout the inference process, the loss function is mostly affected by the noise of the layer inputs. In particular, we will demonstrate that neural networks with massive identity mappings are resistant to the quantization method. It is also difficult to improve the performance of these networks using quantization."
154,https://arxiv.org/abs/2204.13304,Programming Matrices as Staged Sparse Rows to Generate Efficient Matrix-free Differential Equation Solver,"Solving differential equations is a critical task in scientific computing. Domain-specific languages (DSLs) have been a promising direction in achieving performance and productivity, but the current state of the art only supports stencil computation, leaving solvers requiring loop-carried dependencies aside. Alternatively, sparse matrices can represent such equation solvers and are more general than existing DSLs, but the performance is sacrificed.
  This paper points out that sparse matrices can be represented as programs instead of data, having both the generality from the matrix-based representation and the performance from program optimizations. Based on the idea, we propose the Staged Sparse Row (SSR) sparse matrix representation that can efficiently cover applications on structured grids. With SSR representation, users can intuitively define SSR matrices using generator functions and use SSR matrices through a concise object-oriented interface. SSR matrices can then be chained and applied to construct the algorithm, including those with loop-carried dependences. We then apply a set of dedicated optimizations, and ultimately simplify the SSR matrix-based codes into straightforward matrix-free ones, which are efficient and friendly for further analysis.
  Implementing BT pseudo application in the NAS Parallel Benchmark, with less than $10\%$ lines of code compared with the matrix-free reference FORTRAN implementation, we achieved up to $92.8\%$ performance. Implementing a matrix-free variant for the High-Performance Conjugate Gradient benchmark, we achieve $3.29\times$ performance compared with the reference implementation, while our implementation shares the same algorithm on the same programming abstraction, which is sparse matrices."
155,https://arxiv.org/abs/2203.09615,Canvas: Isolated and Adaptive Swapping for Multi-Applications on Remote Memory,"Remote memory techniques for datacenter applications have recently gained a great deal of popularity. Existing remote memory techniques focus on the efficiency of a single application setting only. However, when multiple applications co-run on a remote-memory system, significant interference could occur, resulting in unexpected slowdowns even if the same amounts of physical resources are granted to each application. This slowdown stems from massive sharing in applications' swap data paths. Canvas is a redesigned swap system that fully isolates swap paths for remote-memory applications. Canvas allows each application to possess its dedicated swap partition, swap cache, prefetcher, and RDMA bandwidth. Swap isolation lays a foundation for adaptive optimization techniques based on each application's own access patterns and needs. We develop three such techniques: (1) adaptive swap entry allocation, (2) semantics-aware prefetching, and (3) two-dimensional RDMA scheduling. A thorough evaluation with a set of widely-deployed applications demonstrates that Canvas minimizes performance variation and dramatically reduces performance degradation."
156,https://arxiv.org/abs/2202.05137,Quantization in Layer's Input is Matter,"In this paper, we will show that the quantization in layer's input is more important than parameters' quantization for loss function. And the algorithm which is based on the layer's input quantization error is better than hessian-based mixed precision layout algorithm."
157,https://arxiv.org/abs/2104.10569,GraphTheta: A Distributed Graph Neural Network Learning System With Flexible Training Strategy,"Graph neural networks (GNNs) have been demonstrated as a powerful tool for analyzing non-Euclidean graph data. However, the lack of efficient distributed graph learning systems severely hinders applications of GNNs, especially when graphs are big and GNNs are relatively deep. Herein, we present GraphTheta, the first distributed and scalable graph learning system built upon vertex-centric distributed graph processing with neural network operators implemented as user-defined functions. This system supports multiple training strategies and enables efficient and scalable big-graph learning on distributed (virtual) machines with low memory. To facilitate graph convolutions, GraphTheta puts forward a new graph learning abstraction named NN-TGAR to bridge the gap between graph processing and graph deep learning. A distributed graph engine is proposed to conduct the stochastic gradient descent optimization with a hybrid-parallel execution, and a new cluster-batched training strategy is supported. We evaluate GraphTheta using several datasets with network sizes ranging from small-, modest- to large-scale. Experimental results show that GraphTheta can scale well to 1,024 workers for training an in-house developed GNN on an industry-scale Alipay dataset of 1.4 billion nodes and 4.1 billion attributed edges, with a cluster of CPU virtual machines (dockers) of small memory each (5$\sim$12GB). Moreover, GraphTheta can outperform DistDGL by up to $2.02\times$, with better scalability, and GraphLearn by up to $30.56\times$. As for model accuracy, GraphTheta is capable of learning as good GNNs as existing frameworks. To the best of our knowledge, this work presents the largest edge-attributed GNN learning task in the literature."
158,https://arxiv.org/abs/2101.06911,DFOGraph: An I/O- and Communication-Efficient System for Distributed Fully-out-of-Core Graph Processing,"With the magnitude of graph-structured data continually increasing, graph processing systems that can scale-out and scale-up are needed to handle extreme-scale datasets. While existing distributed out-of-core solutions have made it possible, they suffer from limited performance due to excessive I/O and communication costs. We present DFOGraph, a distributed fully-out-of-core graph processing system that applies and assembles multiple techniques to enable I/O- and communication-efficient processing. DFOGraph builds upon two-level column-oriented partition with adaptive compressed representations to allow fine-grained selective computation and communication, and it only issues necessary disk and network requests. Our evaluation shows DFOGraph achieves performance comparable to GridGraph and FlashGraph (>2.52x and 1.06x) on a single machine and outperforms Chaos and HybridGraph significantly (>12.94x and >10.82x) when scaling out."
159,https://arxiv.org/abs/2009.09442,TADOC: Text Analytics Directly on Compression,"This article provides a comprehensive description of Text Analytics Directly on Compression (TADOC), which enables direct document analytics on compressed textual data. The article explains the concept of TADOC and the challenges to its effective realizations. Additionally, a series of guidelines and technical solutions that effectively address those challenges, including the adoption of a hierarchical compression method and a set of novel algorithms and data structure designs, are presented. Experiments on six data analytics tasks of various complexities show that TADOC can save 90.8% storage space and 87.9% memory usage, while halving data processing times."
160,https://arxiv.org/abs/2008.07141,AIPerf: Automated machine learning as an AI-HPC benchmark,"The plethora of complex artificial intelligence (AI) algorithms and available high performance computing (HPC) power stimulates the expeditious development of AI components with heterogeneous designs. Consequently, the need for cross-stack performance benchmarking of AI-HPC systems emerges rapidly. The de facto HPC benchmark LINPACK can not reflect AI computing power and I/O performance without representative workload. The current popular AI benchmarks like MLPerf have fixed problem size therefore limited scalability. To address these issues, we propose an end-to-end benchmark suite utilizing automated machine learning (AutoML), which not only represents real AI scenarios, but also is auto-adaptively scalable to various scales of machines. We implement the algorithms in a highly parallel and flexible way to ensure the efficiency and optimization potential on diverse systems with customizable configurations. We utilize operations per second (OPS), which is measured in an analytical and systematic approach, as the major metric to quantify the AI performance. We perform evaluations on various systems to ensure the benchmark's stability and scalability, from 4 nodes with 32 NVIDIA Tesla T4 (56.1 Tera-OPS measured), up to 512 nodes with 4096 Huawei Ascend 910 (194.53 Peta-OPS measured), and the results show near-linear weak scalability. With flexible workload and single metric, our benchmark can scale and rank AI-HPC easily."
161,https://arxiv.org/abs/2004.00803,RisGraph: A Real-Time Streaming System for Evolving Graphs to Support Sub-millisecond Per-update Analysis at Millions Ops/s,"Evolving graphs in the real world are large-scale and constantly changing, as hundreds of thousands of updates may come every second. Monotonic algorithms such as Reachability and Shortest Path are widely used in real-time analytics to gain both static and temporal insights and can be accelerated by incremental computing. Existing streaming systems adopt the incremental computing model and achieve either low latency or high throughput, but not both. However, both high throughput and low latency are required in real scenarios such as financial fraud detection. This paper presents RisGraph, a real-time streaming system that provides low-latency analysis for each update with high throughput. RisGraph addresses the challenge with localized data access and inter-update parallelism. We propose a data structure named Indexed Adjacency Lists and use sparse arrays and Hybrid Parallel Mode to enable localized data access. To achieve inter-update parallelism, we propose a domain-specific concurrency control mechanism based on the classification of safe and unsafe updates. Experiments show that RisGraph can ingest millions of updates per second for graphs with several hundred million vertices and billions of edges, and the P999 processing time latency is within 20 milliseconds. RisGraph achieves orders-of-magnitude improvement on throughput when analyses are executed for each update without batching and performs better than existing systems with batches of up to 20 million updates."
162,https://arxiv.org/abs/1910.05773,LiveGraph: A Transactional Graph Storage System with Purely Sequential Adjacency List Scans,"The specific characteristics of graph workloads make it hard to design a one-size-fits-all graph storage system. Systems that support transactional updates use data structures with poor data locality, which limits the efficiency of analytical workloads or even simple edge scans. Other systems run graph analytics workloads efficiently, but cannot properly support transactions.
  This paper presents LiveGraph, a graph storage system that outperforms both the best graph transactional systems and the best systems for real-time graph analytics on fresh data. LiveGraph does that by ensuring that adjacency list scans, a key operation in graph workloads, are purely sequential: they never require random accesses even in presence of concurrent transactions. This is achieved by combining a novel graph-aware data structure, the Transactional Edge Log (TEL), together with a concurrency control mechanism that leverages TEL's data layout. Our evaluation shows that LiveGraph significantly outperforms state-of-the-art (graph) database solutions on both transactional and real-time analytical workloads."
163,https://arxiv.org/abs/1801.00746,Bridging the Gap Between Neural Networks and Neuromorphic Hardware with A Neural Network Compiler,"Different from developing neural networks (NNs) for general-purpose processors, the development for NN chips usually faces with some hardware-specific restrictions, such as limited precision of network signals and parameters, constrained computation scale, and limited types of non-linear functions.
  This paper proposes a general methodology to address the challenges. We decouple the NN applications from the target hardware by introducing a compiler that can transform an existing trained, unrestricted NN into an equivalent network that meets the given hardware's constraints. We propose multiple techniques to make the transformation adaptable to different kinds of NN chips, and reliable for restrict hardware constraints.
  We have built such a software tool that supports both spiking neural networks (SNNs) and traditional artificial neural networks (ANNs). We have demonstrated its effectiveness with a fabricated neuromorphic chip and a processing-in-memory (PIM) design. Tests show that the inference error caused by this solution is insignificant and the transformation time is much shorter than the retraining time. Also, we have studied the parameter-sensitivity evaluations to explore the tradeoffs between network error and resource utilization for different transformation strategies, which could provide insights for co-design optimization of neuromorphic hardware and software."
164,https://arxiv.org/abs/2305.15715,Polarization Criteria in Targeted SETI Observation,"In this letter, we propose a new method for distinguishing extraterrestrial intelligence (ETI) signals from the radio frequency interference (RFI) by exploiting the polarization features. The linearly polarized component of Stokes parameters should vary with the parallactic angle in sinusoidal form for ETI signal, while such linearly polarized component should remains relatively stable for terrestrial RFI. To witness such sinusoidal variations, usually, at least 4-8 hours of observation time is required. Polarization in search for extraterrestrial intelligence (SETI) also allow us to study the radio stellar bursts of M-type stars, which is relevant to the habitability of exoplanets. Compared with the frequency drift method, polarization method can effectively reduce the observation time required for signal identification, and also improve the process of signal identification."
165,https://arxiv.org/abs/2305.14725,AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes,"We propose attribute-aware multimodal entity linking, where the input is a mention described with a text and image, and the goal is to predict the corresponding target entity from a multimodal knowledge base (KB) where each entity is also described with a text description, a visual image and a set of attributes and values. To support this research, we construct AMELI, a large-scale dataset consisting of 18,472 reviews and 35,598 products. To establish baseline performance on AMELI, we experiment with the current state-of-the-art multimodal entity linking approaches and our enhanced attribute-aware model and demonstrate the importance of incorporating the attribute information into the entity linking process. To be best of our knowledge, we are the first to build benchmark dataset and solutions for the attribute-aware multimodal entity linking task. Datasets and codes will be made publicly available."
166,https://arxiv.org/abs/2305.14686,Harmonic Measures and Numerical Computation of Cauchy Problems for Laplace Equations,"It is well known that Cauchy problem for Laplace equations is an ill-posed problem in Hadamard's sense. Small deviations in Cauchy data may lead to large errors in the solutions. It is observed that if a bound is imposed on the solution, there exists a conditional stability estimate. This gives a reasonable way to construct stable algorithms. However, it is impossible to have good results at all points in the domain. Although numerical methods for Cauchy problems for Laplace equations have been widely studied for quite a long time, there are still some unclear points, for example, how to evaluate the numerical solutions, which means whether we can approximate the Cauchy data well and keep the bound of the solution, and at which points the numerical results are reliable? In this paper, we will prove the conditional stability estimate which is quantitatively related to harmonic measures. The harmonic measure can be used as an indicate function to pointwisely evaluate the numerical result, which further enables us to find a reliable subdomain where the local convergence rate is higher than a certain order."
167,https://arxiv.org/abs/2305.13812,Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality,"Contrastively trained vision-language models have achieved remarkable progress in vision and language representation learning, leading to state-of-the-art models for various downstream multimodal tasks. However, recent research has highlighted severe limitations of these models in their ability to perform compositional reasoning over objects, attributes, and relations. Scene graphs have emerged as an effective way to understand images compositionally. These are graph-structured semantic representations of images that contain objects, their attributes, and relations with other objects in a scene. In this work, we consider the scene graph parsed from text as a proxy for the image scene graph and propose a graph decomposition and augmentation framework along with a coarse-to-fine contrastive learning objective between images and text that aligns sentences of various complexities to the same image. Along with this, we propose novel negative mining techniques in the scene graph space for improving attribute binding and relation understanding. Through extensive experiments, we demonstrate the effectiveness of our approach that significantly improves attribute binding, relation understanding, systematic generalization, and productivity on multiple recently proposed benchmarks (For example, improvements upto $18\%$ for systematic generalization, $16.5\%$ for relation understanding over a strong baseline), while achieving similar or better performance than CLIP on various general multimodal tasks."
168,https://arxiv.org/abs/2305.10791,BrutePrint: Expose Smartphone Fingerprint Authentication to Brute-force Attack,"Fingerprint authentication has been widely adopted on smartphones to complement traditional password authentication, making it a tempting target for attackers. The smartphone industry is fully aware of existing threats, and especially for the presentation attack studied by most prior works, the threats are nearly eliminated by liveness detection and attempt limit. In this paper, we study the seemingly impossible fingerprint brute-force attack on off-the-shelf smartphones and propose a generic attack framework. We implement BrutePrint to automate the attack, that acts as a middleman to bypass attempt limit and hijack fingerprint images. Specifically, the bypassing exploits two zero-day vulnerabilities in smartphone fingerprint authentication (SFA) framework, and the hijacking leverages the simplicity of SPI protocol. Moreover, we consider a practical cross-device attack scenario and tackle the liveness and matching problems with neural style transfer (NST). We also propose a method based on neural style transfer to generate valid brute-forcing inputs from arbitrary fingerprint images. A case study shows that we always bypasses liveness detection and attempt limit while 71% spoofs are accepted. We evaluate BrutePrint on 10 representative smartphones from top-5 vendors and 3 typical types of applications involving screen lock, payment, and privacy. As all of them are vulnerable to some extent, fingerprint brute-force attack is validated on on all devices except iPhone, where the shortest time to unlock the smartphone without prior knowledge about the victim is estimated at 40 minutes. Furthermore, we suggest software and hardware mitigation measures."
169,https://arxiv.org/abs/2305.07247,Provably Convergent Schrödinger Bridge with Applications to Probabilistic Time Series Imputation,"The Schrödinger bridge problem (SBP) is gaining increasing attention in generative modeling and showing promising potential even in comparison with the score-based generative models (SGMs). SBP can be interpreted as an entropy-regularized optimal transport problem, which conducts projections onto every other marginal alternatingly. However, in practice, only approximated projections are accessible and their convergence is not well understood. To fill this gap, we present a first convergence analysis of the Schrödinger bridge algorithm based on approximated projections. As for its practical applications, we apply SBP to probabilistic time series imputation by generating missing values conditioned on observed data. We show that optimizing the transport cost improves the performance and the proposed algorithm achieves the state-of-the-art result in healthcare and environmental data while exhibiting the advantage of exploring both temporal and feature patterns in probabilistic time series imputation."
170,https://arxiv.org/abs/2305.04752,Reconstruction of the dark energy scalar field potential by Gaussian process,"Dark energy is believed to be responsible for the acceleration of the universe. In this paper, we reconstruct the dark energy scalar field potential $V(φ)$ using the Hubble parameter H(z) through Gaussian Process analysis. Our goal is to investigate dark energy using various H(z) datasets and priors. We find that the choice of prior has little effect on the reconstructed $V(φ)$, but the choice of H(z) dataset has a significant impact. Our result shows that Observational H(z) data (OHD) produces better results in reconstructing $V(φ)$ compared to cosmic chronometers (CC). Additionally, we simulate H(z) data to measure the effect of increasing the number of data points on the accuracy of reconstructed $V(φ)$. We find that doubling the number of H(z) data points can improve the accuracy rate of reconstructed $V(φ)$ by 5$\%$ to 30$\%$."
171,https://arxiv.org/abs/2304.11674,A Lightweight Recurrent Learning Network for Sustainable Compressed Sensing,"Recently, deep learning-based compressed sensing (CS) has achieved great success in reducing the sampling and computational cost of sensing systems and improving the reconstruction quality. These approaches, however, largely overlook the issue of the computational cost; they rely on complex structures and task-specific operator designs, resulting in extensive storage and high energy consumption in CS imaging systems. In this paper, we propose a lightweight but effective deep neural network based on recurrent learning to achieve a sustainable CS system; it requires a smaller number of parameters but obtains high-quality reconstructions. Specifically, our proposed network consists of an initial reconstruction sub-network and a residual reconstruction sub-network. While the initial reconstruction sub-network has a hierarchical structure to progressively recover the image, reducing the number of parameters, the residual reconstruction sub-network facilitates recurrent residual feature extraction via recurrent learning to perform both feature fusion and deep reconstructions across different scales. In addition, we also demonstrate that, after the initial reconstruction, feature maps with reduced sizes are sufficient to recover the residual information, and thus we achieved a significant reduction in the amount of memory required. Extensive experiments illustrate that our proposed model can achieve a better reconstruction quality than existing state-of-the-art CS algorithms, and it also has a smaller number of network parameters than these algorithms. Our source codes are available at: https://github.com/C66YU/CSRN."
172,https://arxiv.org/abs/2304.05555,On the mystery of ultrastable super-Tonks-Girardeau gases under weak dipolar interactions,"The highly excited super-Tonks-Girardeau (sTG) gas was recently observed to be extremely stable in the presence of a weak dipolar repulsion. Here we reveal the underlying reason for this mysterious phenomenon. By exactly solving the trapped small clusters with both contact and dipolar interactions, we show that the reason lies in the distinct spectral responses between sTG gas and its decaying channel (bound state) when turn on a weak dipolar interaction. Specifically, a tiny dipolar force can produce a visible energy shift for the localized bound state, but can hardly affect the extended sTG branch. As a result, the avoided level crossing between two branches is greatly modified in both location and width in the parameter axis of coupling strength, leading to a more (less) stable sTG gas for a repulsive (attractive) dipolar force. These results, consistent with experimental observations, are found to robustly apply to both bosonic and fermionic systems."
173,https://arxiv.org/abs/2304.05026,Excitation and voltage-gated modulation of single-mode dynamics in a planar nano-gap spin Hall nano-oscillator,"We experimentally study the dynamical modes excited by current-induced spin-orbit torque and its electrostatic gating effect in a 3-terminal planar nano-gap spin Hall nano-oscillator (SHNO) with a moderate interfacial perpendicular magnetic anisotropy (IPMA). Both quasilinear propagating spin-wave and localized ""bullet"" modes are achieved and controlled by varying the applied in-plane magnetic field and driving current. The minimum linewidth shows a linear dependence on the actual temperature of the active area, confirming single-mode dynamics based on the nonlinear theory of spin-torque nano-oscillation with a single mode. The observed electrostatic gating tuning oscillation frequency arises from voltage-controlled magnetic anisotropy and threshold current of SHNO via modification of the nonlinear damping and/or the interfacial spin-orbit coupling of the magnetic multilayer. In contrast to previously observed two-mode coexistence degrading the spectral purity in Py/Pt-based SHNOs with a negligible IPMA, a single coherent spin-wave mode with a low driven current can be achieved by selecting the ferromagnet layer with a suitable IPMA because the nonlinear mode coupling can be diminished by bringing in the PMA field to compensate the easy-plane shape anisotropy. Moreover, the simulations demonstrate that the experimentally observed current and gate-voltage modulation of auto-oscillation modes are also closely associated with the nonlinear damping and mode coupling, which are determined by the ellipticity of magnetization precession. The demonstrated nonlinear mode coupling mechanism and electrical control approach of spin-wave modes could provide the clue to facilitate the implementation of the mutual synchronization map for neuromorphic computing applications in SHNO array networks."
174,https://arxiv.org/abs/2304.04207,Scalable Multiple Patterning Layout Decomposition Implemented by a Distribution Evolutionary Algorithm,"As the feature size of semiconductor technology shrinks to 10 nm and beyond, the multiple patterning lithography (MPL) attracts more attention from the industry. In this paper, we model the layout decomposition of MPL as a generalized graph coloring problem, which is addressed by a distribution evolutionary algorithm based on a population of probabilistic model (DEA-PPM). DEA-PPM can strike a balance between decomposition results and running time, being scalable for varied settings of mask number and lithography resolution. Due to its robustness of decomposition results, this could be an alternative technique for multiple patterning layout decomposition in next-generation technology nodes."
175,https://arxiv.org/abs/2304.02273,MMVC: Learned Multi-Mode Video Compression with Block-based Prediction Mode Selection and Density-Adaptive Entropy Coding,"Learning-based video compression has been extensively studied over the past years, but it still has limitations in adapting to various motion patterns and entropy models. In this paper, we propose multi-mode video compression (MMVC), a block wise mode ensemble deep video compression framework that selects the optimal mode for feature domain prediction adapting to different motion patterns. Proposed multi-modes include ConvLSTM-based feature domain prediction, optical flow conditioned feature domain prediction, and feature propagation to address a wide range of cases from static scenes without apparent motions to dynamic scenes with a moving camera. We partition the feature space into blocks for temporal prediction in spatial block-based representations. For entropy coding, we consider both dense and sparse post-quantization residual blocks, and apply optional run-length coding to sparse residuals to improve the compression rate. In this sense, our method uses a dual-mode entropy coding scheme guided by a binary density map, which offers significant rate reduction surpassing the extra cost of transmitting the binary selection map. We validate our scheme with some of the most popular benchmarking datasets. Compared with state-of-the-art video compression schemes and standard codecs, our method yields better or competitive results measured with PSNR and MS-SSIM."
176,https://arxiv.org/abs/2304.00729,Data-Driven Safe Controller Synthesis for Deterministic Systems: A Posteriori Method With Validation Tests,"In this work, we investigate the data-driven safe control synthesis problem for unknown dynamic systems. We first formulate the safety synthesis problem as a robust convex program (RCP) based on notion of control barrier function. To resolve the issue of unknown system dynamic, we follow the existing approach by converting the RCP to a scenario convex program (SCP) by randomly collecting finite samples of system trajectory. However, to improve the sample efficiency to achieve a desired confidence bound, we provide a new posteriori method with validation tests. Specifically, after collecting a set of data for the SCP, we further collect another set of independent \emph{validate data} as posterior information to test the obtained solution. We derive a new overall confidence bound for the safety of the controller that connects the original sample data, the support constraints, and the validation data. The efficiency of the proposed approach is illustrated by a case study of room temperature control. We show that, compared with existing methods, the proposed approach can significantly reduce the required number of sample data to achieve a desired confidence bound."
177,https://arxiv.org/abs/2304.00196,Deformations of objects in $n$-categories,"In this paper, we prove that the deformation theory of an object in an $n$-category is controlled by the its $n$-fold endomorphism algebra. This recovers Lurie's results on deforming objects and categories. We also generalize a previous result by Blanc et al. on deforming a category and an object simultaneously to the case of $n$-categories."
178,https://arxiv.org/abs/2303.14478,DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields,"Recent works such as BARF and GARF can bundle adjust camera poses with neural radiance fields (NeRF) which is based on coordinate-MLPs. Despite the impressive results, these methods cannot be applied to Generalizable NeRFs (GeNeRFs) which require image feature extractions that are often based on more complicated 3D CNN or transformer architectures. In this work, we first analyze the difficulties of jointly optimizing camera poses with GeNeRFs, and then further propose our DBARF to tackle these issues. Our DBARF which bundle adjusts camera poses by taking a cost feature map as an implicit cost function can be jointly trained with GeNeRFs in a self-supervised manner. Unlike BARF and its follow-up works, which can only be applied to per-scene optimized NeRFs and need accurate initial camera poses with the exception of forward-facing scenes, our method can generalize across scenes and does not require any good initialization. Experiments show the effectiveness and generalization ability of our DBARF when evaluated on real-world datasets. Our code is available at \url{https://aibluefisher.github.io/dbarf}."
179,https://arxiv.org/abs/2303.13747,Breakdown of effective-medium theory beyond the critical angle,"Effective-medium theory pertains to the theoretical modelling of homogenization, which aims to replace an inhomogeneous structure of subwavelength-scale constituents with a homogeneous effective medium. The effective-medium theory is fundamental to various realms, including electromagnetics and material science, since it can largely decrease the complexity in the exploration of light-matter interactions by providing simple acceptable approximation. Generally, the effective-medium theory is thought to be applicable to any all-dielectric system with deep-subwavelength constituents, under the condition that the effective medium does not have a critical angle, at which the total internal reflection occurs. Here we reveal a fundamental breakdown of the effective-medium theory that can be applied in very general conditions: showing it for deep-subwavelength all-dielectric multilayers even without critical angle. Our finding relies on an exotic photonic spin Hall effect, which is shown to be ultra-sensitive to the stacking order of deep-subwavelength dielectric layers, since the spin-orbit interaction of light is dependent on slight phase accumulations during the wave propagation. Our results indicate that the photonic spin Hall effect could provide a promising and powerful tool for measuring structural defects for all-dielectric systems even in the extreme nanometer scale."
180,https://arxiv.org/abs/2303.10174,Visual Studio Code in Introductory Computer Science Course: An Experience Report,"Involving integrated development environments (IDEs) in introductory-level (CS1) programming courses is critical. However, it is difficult for instructors to find a suitable IDE that is beginner friendly and supports strong functionality. In this paper, we report the experience of using Visual Studio Code (VS Code) in a CS1 programming course. We describe our motivation for choosing VS Code and how we introduce it to students. We create comprehensive guidance with hierarchical indexing to help students with diverse programming backgrounds. We perform an experimental evaluation of students' programming experience of using VS Code and validate the VS Code together with guidance as a promising solution for CS1 programming courses."
181,https://arxiv.org/abs/2303.05172,The JUNO experiment Top Tracker,"The main task of the Top Tracker detector of the neutrino reactor experiment Jiangmen Underground Neutrino Observatory (JUNO) is to reconstruct and extrapolate atmospheric muon tracks down to the central detector. This muon tracker will help to evaluate the contribution of the cosmogenic background to the signal. The Top Tracker is located above JUNO's water Cherenkov Detector and Central Detector, covering about 60% of the surface above them. The JUNO Top Tracker is constituted by the decommissioned OPERA experiment Target Tracker modules. The technology used consists in walls of two planes of plastic scintillator strips, one per transverse direction. Wavelength shifting fibres collect the light signal emitted by the scintillator strips and guide it to both ends where it is read by multianode photomultiplier tubes. Compared to the OPERA Target Tracker, the JUNO Top Tracker uses new electronics able to cope with the high rate produced by the high rock radioactivity compared to the one in Gran Sasso underground laboratory. This paper will present the new electronics and mechanical structure developed for the Top Tracker of JUNO along with its expected performance based on the current detector simulation."
182,https://arxiv.org/abs/2303.04792,Quantum information phases in space-time: measurement-induced entanglement and teleportation on a noisy quantum processor,"Measurement has a special role in quantum theory: by collapsing the wavefunction it can enable phenomena such as teleportation and thereby alter the ""arrow of time"" that constrains unitary evolution. When integrated in many-body dynamics, measurements can lead to emergent patterns of quantum information in space-time that go beyond established paradigms for characterizing phases, either in or out of equilibrium. On present-day NISQ processors, the experimental realization of this physics is challenging due to noise, hardware limitations, and the stochastic nature of quantum measurement. Here we address each of these experimental challenges and investigate measurement-induced quantum information phases on up to 70 superconducting qubits. By leveraging the interchangeability of space and time, we use a duality mapping, to avoid mid-circuit measurement and access different manifestations of the underlying phases -- from entanglement scaling to measurement-induced teleportation -- in a unified way. We obtain finite-size signatures of a phase transition with a decoding protocol that correlates the experimental measurement record with classical simulation data. The phases display sharply different sensitivity to noise, which we exploit to turn an inherent hardware limitation into a useful diagnostic. Our work demonstrates an approach to realize measurement-induced physics at scales that are at the limits of current NISQ processors."
183,https://arxiv.org/abs/2303.03910,"JUNO sensitivity to $^7$Be, $pep$, and CNO solar neutrinos","The Jiangmen Underground Neutrino Observatory (JUNO), the first multi-kton liquid scintillator detector, which is under construction in China, will have a unique potential to perform a real-time measurement of solar neutrinos well below the few MeV threshold typical for Water Cherenkov detectors. JUNO's large target mass and excellent energy resolution are prerequisites for reaching unprecedented levels of precision. In this paper, we provide estimation of the JUNO sensitivity to 7Be, pep, and CNO solar neutrinos that can be obtained via a spectral analysis above the 0.45 MeV threshold. This study is performed assuming different scenarios of the liquid scintillator radiopurity, ranging from the most opti mistic one corresponding to the radiopurity levels obtained by the Borexino experiment, up to the minimum requirements needed to perform the neutrino mass ordering determination with reactor antineutrinos - the main goal of JUNO. Our study shows that in most scenarios, JUNO will be able to improve the current best measurements on 7Be, pep, and CNO solar neutrino fluxes. We also perform a study on the JUNO capability to detect periodical time variations in the solar neutrino flux, such as the day-night modulation induced by neutrino flavor regeneration in Earth, and the modulations induced by temperature changes driven by helioseismic waves."
184,https://arxiv.org/abs/2303.00965,Detecting Fermi Surface Nesting Effect for Fermionic Dicke Transition by Trap Induced Localization,"Recently, the statistical effect of fermionic superradiance is approved by series of experiments both in free space and in a cavity. The Pauli blocking effect can be visualized by a 1/2 scaling of Dicke transition critical pumping strength against particle number Nat for fermions in a trap. However, the Fermi surface nesting effect, which manifests the enhancement of superradiance by Fermi statistics is still very hard to be identified. Here we studied the influence of localized fermions on the trap edge when both pumping optical lattice and the trap are presented. We find due to localization, the statistical effect in superradiant transition is enhanced. Two new scalings of critical pumping strength are observed as 4/3, and 2/3 for mediate particle number, and the Pauli blocking scaling 1/3 (2d case) in large particle number limit is unaffected. Further, we find the 4/3 scaling is subject to a power law increasing with rising ratio between recoil energy and trap frequency in pumping laser direction. The divergence of this scaling of critical pumping strength against $N_{\rm at}$ in $E_R/ω_x\rightarrow+\infty$ limit can be identified as the Fermi surface nesting effect. Thus we find a practical experimental scheme for visualizing the long-desired Fermi surface nesting effect with the help of trap induced localization in a two-dimensional Fermi gas in a cavity."
185,https://arxiv.org/abs/2302.12669,Design optimization of JUNO-TAO plastic scintillator with WLS-fiber and SiPM readout,"Plastic scintillator (PS) embedding wavelength shifting (WLS) fiber is widely used in high energy particle physics, as muon taggers, and also in medical physics and other applications. In this work, a simulation package is built to evaluate the effects of the diameter and the layout of the optical fiber on the light yield with different configurations. The optimal optical configuration was designed based on the simulation and then validated with two PS prototypes under certain experimental conditions. In the study, the top veto tracker (TVT) of the JUNO-TAO experiment, comprised of 4 layers of 160 strips of PS was designed and evaluated. When a muon tagging efficiency of a PS strip is higher than 99%, the threshold is evaluated. The efficiency of 3-layer out of 4-layer of TVT will be higher than 99% even with the tagging efficiency of a single strip as low as 97% using a threshold of 10 p.e. assuming 40% SiPM PDE."
186,https://arxiv.org/abs/2302.07442,Microwave amplification via interfering multi-photon processes in a half-waveguide quantum electrodynamics system,"We investigate the amplification of a microwave probe signal by a superconducting artificial atom, a transmon, strongly coupled to the end of a one-dimensional semi-infinite transmission line. The end of the transmission line acts as a mirror for microwave fields. Due to the weak anharmonicity of the artificial atom, a strong pump field creates multi-photon excitations among the dressed states. Transitions between these dressed states, Rabi sidebands, give rise to either amplification or attenuation of the weak probe. We obtain a maximum amplitude amplification of about 18 %, higher than in any previous experiment with a single artificial atom, due to constructive interference between Rabi sidebands. We also characterize the noise properties of the system by measuring the spectrum of spontaneous emission."
187,https://arxiv.org/abs/2302.06278,On the nature of X(3960),"A near-threshold enhancement in the $D_s^+ D_s^-$ system, dubbed as $X(3960)$, is observed by the LHCb collaboration recently. A combined analysis on $χ_{c0}(3930)~(\to D^+ D^-)$, $X(3960)~(\to D_s^+ D_s^-)$, and $X(3915)~(\to J/ψω)$ is performed using both a $K$-matrix approach of $D_{(s)}\bar{D}_{(s)}$ four-point contact interactions and a model of Flatté-like parameterizations. The use of the pole counting rule and spectral density function sum rule indicate, under current statistics, that this $D_s^+ D_s^-$ near-threshold state has probably the mixed nature of a $c\bar{c}$ confining state and $D_s^+ D_s^-$ continuum."
188,https://arxiv.org/abs/2301.12135,AdaSfM: From Coarse Global to Fine Incremental Adaptive Structure from Motion,"Despite the impressive results achieved by many existing Structure from Motion (SfM) approaches, there is still a need to improve the robustness, accuracy, and efficiency on large-scale scenes with many outlier matches and sparse view graphs. In this paper, we propose AdaSfM: a coarse-to-fine adaptive SfM approach that is scalable to large-scale and challenging datasets. Our approach first does a coarse global SfM which improves the reliability of the view graph by leveraging measurements from low-cost sensors such as Inertial Measurement Units (IMUs) and wheel encoders. Subsequently, the view graph is divided into sub-scenes that are refined in parallel by a fine local incremental SfM regularised by the result from the coarse global SfM to improve the camera registration accuracy and alleviate scene drifts. Finally, our approach uses a threshold-adaptive strategy to align all local reconstructions to the coordinate frame of global SfM. Extensive experiments on large-scale benchmark datasets show that our approach achieves state-of-the-art accuracy and efficiency."
189,https://arxiv.org/abs/2301.10374,The Structure and Origin of Switchbacks: Parker Solar Probe Observations,"Switchbacks are rapid magnetic field reversals that last from seconds to hours. Current Parker Solar Probe (PSP) observations pose many open questions in regard to the nature of switchbacks. For example, are they stable as they propagate through the inner heliosphere, and how are they formed? In this work, we aim to investigate the structure and origin of switchbacks. In order to study the stability of switchbacks, we suppose the small-scale current sheets therein are generated by magnetic braiding, and they should work to stabilize the switchbacks. With more than one thousand switchbacks identified with PSP observations in seven encounters, we find many more current sheets inside than outside switchbacks, indicating that these microstructures should work to stabilize the S-shaped structures of switchbacks. Additionally, we study the helium variations to trace the switchbacks to their origins. We find both helium-rich and helium-poor populations in switchbacks, implying that the switchbacks could originate from both closed and open magnetic field regions in the Sun. Moreover, we observe that the alpha-proton differential speeds also show complex variations as compared to the local Alfvén speed. The joint distributions of both parameters show that low helium abundance together with low differential speed is the dominant state in switchbacks. The presence of small-scale current sheets in switchbacks along with the helium features are in line with the hypothesis that switchbacks could originate from the Sun via interchange reconnection process. However, other formation mechanisms are not excluded."
190,https://arxiv.org/abs/2301.07547,Quad-cascade picture of turbulence,"Although its ubiquitous emergence in nature and variety of systems, turbulence possesses spatio-temporal chaotic, intermittent fluctuations, and makes it impossible to be precisely predicted. Persistent attempts for almost a century have been devoted to capture the invariant laws and hidden deeply universality out of the vast disorder and chaotic nature of turbulence. The celebrated Kolmogorov -5/3 law is robust, but not comprehensive to describe the diverse turbulences, especially in the turbulence driven by external volume forces, e.g. thermal convection, electrokinetic turbulence and etc. Here, we reveal that the fluxes of kinetic energy and scalar variance must be highly coupled to establish a universal conservation law and consequently we successfully unify a much diversity of scaling laws. As an example, in a microfluidic electrokinetic turbulence, additional scaling of -5/3, -9/5 and -7/3 are experimentally found in the power spectra of concentration. With this proposed model, a full quad-cascade picture is eventually complete to unify the various scaling laws for the most complicated physical problem of turbulence."
191,https://arxiv.org/abs/2301.06044,"Design, construction and commissioning of the PandaX-30T liquid xenon management system","The PandaX-30T is a proposed next-generation experiment to study dark matter and neutrinos using a dual-phase time projection chamber with \textasciitilde30 tons of liquid xenon. An innovative xenon handling subsystem of the PandaX-30T, the First-X, is described in this paper. The First-X is developed to handle liquid xenon safely and efficiently, including liquefying and long-term storing xenon without losses or contamination, and transferring cryogenic liquid xenon between the storage module and the detector safely and effectively without venting out. The storage module of the First-X is five specially designed double-walled cylindrical vessels (Center Tanks) equipped with three heat exchangers each for pressure and temperature regulation. Each Center Tank is designed with a vacuum and multi-layer insulation and a maximum allowable working pressure of 7.1 MPa, allowing 6 tons of xenon to be stored at 165--178 K at 0.1--0.2 MPa in the liquid phase or up to 300 K and up to 6.95 MPa in the supercritical phase. High-pressure storage (\textgreater0.2 MPa) only occurs in case of long-term detector shutdown or lack of nitrogen, ensuring no-loss storage of 6 tons of xenon in the range 178--300 K. In this paper, the thermophysical performances of the First-X and innovative scenarios to conduct non-vented cryogen transportation were experimentally conducted and reported using liquid argon. The non-vented cryogenic liquid filling and pump-assisted cryogenic liquid recovery have been conducted with liquid argon at a mass flow rate of 1390 kg/h, corresponding to a xenon mass flow rate of 2140 kg/h. Compared with the PandaX-4T, the transportation of xenon between the detector and the storage module is conducted in the liquid phase rather than in the gaseous phase, and the filling rate and the recovery rate are increased by approximately 50 times and 30 times, respectively."
192,https://arxiv.org/abs/2212.08502,JUNO Sensitivity on Proton Decay $p\to \barνK^+$ Searches,"The Jiangmen Underground Neutrino Observatory (JUNO) is a large liquid scintillator detector designed to explore many topics in fundamental physics. In this paper, the potential on searching for proton decay in $p\to \barνK^+$ mode with JUNO is investigated.The kaon and its decay particles feature a clear three-fold coincidence signature that results in a high efficiency for identification. Moreover, the excellent energy resolution of JUNO permits to suppress the sizable background caused by other delayed signals. Based on these advantages, the detection efficiency for the proton decay via $p\to \barνK^+$ is 36.9% with a background level of 0.2 events after 10 years of data taking. The estimated sensitivity based on 200 kton-years exposure is $9.6 \times 10^{33}$ years, competitive with the current best limits on the proton lifetime in this channel."
193,https://arxiv.org/abs/2212.05097,Measurement-Induced State Transitions in a Superconducting Qubit: Within the Rotating Wave Approximation,"Superconducting qubits typically use a dispersive readout scheme, where a resonator is coupled to a qubit such that its frequency is qubit-state dependent. Measurement is performed by driving the resonator, where the transmitted resonator field yields information about the resonator frequency and thus the qubit state. Ideally, we could use arbitrarily strong resonator drives to achieve a target signal-to-noise ratio in the shortest possible time. However, experiments have shown that when the average resonator photon number exceeds a certain threshold, the qubit is excited out of its computational subspace, which we refer to as a measurement-induced state transition. These transitions degrade readout fidelity, and constitute leakage which precludes further operation of the qubit in, for example, error correction. Here we study these transitions using a transmon qubit by experimentally measuring their dependence on qubit frequency, average photon number, and qubit state, in the regime where the resonator frequency is lower than the qubit frequency. We observe signatures of resonant transitions between levels in the coupled qubit-resonator system that exhibit noisy behavior when measured repeatedly in time. We provide a semi-classical model of these transitions based on the rotating wave approximation and use it to predict the onset of state transitions in our experiments. Our results suggest the transmon is excited to levels near the top of its cosine potential following a state transition, where the charge dispersion of higher transmon levels explains the observed noisy behavior of state transitions. Moreover, occupation in these higher energy levels poses a major challenge for fast qubit reset."
194,https://arxiv.org/abs/2212.04197,HyperEnclave: An Open and Cross-platform Trusted Execution Environment,"A number of trusted execution environments (TEEs) have been proposed by both academia and industry. However, most of them require specific hardware or firmware changes and are bound to specific hardware vendors (such as Intel, AMD, ARM, and IBM). In this paper, we propose HyperEnclave, an open and cross-platform process-based TEE that relies on the widely-available virtualization extension to create the isolated execution environment. In particular, HyperEnclave is designed to support the flexible enclave operation modes to fulfill the security and performance demands under various enclave workloads. We provide the enclave SDK to run existing SGX programs on HyperEnclave with little or no source code changes. We have implemented HyperEnclave on commodity AMD servers and deployed the system in a world-leading FinTech company to support real-world privacy-preserving computations. The evaluation on both micro-benchmarks and application benchmarks shows the design of HyperEnclave introduces only a small overhead."
195,https://arxiv.org/abs/2212.03561,Searching for $a_0(980)$-meson parton distribution function,"In this paper, we calculate the scalar $a_0(980)$-meson leading-twist wavefunction by using light-cone harmonic oscillator model (LCHO). In which the model parameters are determined by fitting the $ξ$-moments $\langleξ_{a_0}^n\rangle_ζ$ of its light-cone distribution amplitudes. Then, the $a_0(980)$-meson leading-twist light-cone distribution amplitudes with three different scales $ζ= (1.0, 2.0, 5.2)~{\rm GeV}$ are given. After constructing the relationship between $a_0(980)$-meson leading-twist parton distribution functions/valence quark distribution function and its LCHO wavefunction, we exhibit the $q^{a_0}(x,ζ)$ and $x q^{a_0}(x,ζ)$ with different scales. Furthermore, we also calculate the Mellin moments of the $a_0(980)$-meson's valence quark distribution function $\langle x^n q^{a_0}\rangle_ζ$ with $n = (1,2,3)$, i.e. $\langle x q^{a_0}\rangle_{ζ_5} = 0.026$, $\langle x^2 q^{a_0}\rangle_{ζ_5} = 0.017$ and $\langle x^3 q^{a_0}\rangle_{ζ_5} = 0.012$. Finally, the scale evolution for the ratio of the Mellin moments $x^n_{a_0}(ζ,ζ_k)$ are presented."
196,https://arxiv.org/abs/2211.14992,Reconstruction of the event vertex in the PandaX-III experiment with convolution neural network,"The tracks left by charged particles in a gaseous time projection chamber~(TPC) incorporate important information about the interaction process and drift of electrons in gas. The electron diffusion information carried by the tracks is an effective signature to reconstruct $z_0$, the vertex position in drift direction at which the event takes place. In this paper, we propose to reconstruct $z_0$ with convolution neural network~(CNN) in the PandaX-III experiment. A CNN model VGGZ0net is built and validated with Monte Carlo simulation data. It gives $z_0$ with a 11~cm precision for the events above 2~MeV uniformly distributed along a drift distance of 120~cm, and then the electron lifetime can be deduced. The energy resolution of detector is significantly improved after the electron lifetime correction, i.e., from 10.1\% to 4.0\% FWHM at the Q-value of double beta decay of $^{136}$Xe for the scenario with an electron lifetime of 6.5~ms. The CNN model is also successfully applied to the experimental data of the PandaX-III prototype detector for $z_0$ reconstruction."
197,https://arxiv.org/abs/2211.12805,Markov decision processes with maximum entropy rate for Surveillance Tasks,"We consider the problem of synthesizing optimal policies for Markov decision processes (MDP) for both utility objective and security constraint. Specifically, our goal is to maximize the \emph{entropy rate} of the MDP while achieving a surveillance task in the sense that a given region of interest is visited infinitely often with probability one. Such a policy is of our interest since it guarantees both the completion of tasks and maximizes the \emph{unpredictability} of limit behavior of the system. Existing works either focus on the total entropy or do not consider the surveillance tasks which are not suitable for surveillance tasks with infinite horizon. We provide a complete solution to this problem. Specifically, we present an algorithm for synthesizing entropy rate maximizing policies for communicating MDPs. Then based on a new state classification method, we show the entropy rate maximization problem under surveillance task can be effectively solved in polynomial-time. We illustrate the proposed algorithm based on a case study of robot planning scenario."
198,https://arxiv.org/abs/2211.12803,You Don't Know When I Will Arrive: Unpredictable Controller Synthesis for Temporal Logic Tasks,"In this paper, we investigate the problem of synthesizing controllers for temporal logic specifications under security constraint. We assume that there exists a passive intruder (eavesdropper) that can partially observe the behavior of the system. For the purpose of security, we require that the system's behaviors are unpredictable in the sense that the intruder cannot determine for sure that the system will exactly accomplish the task in $K$ steps ahead. This problem is particularly challenging since future information is involved in the synthesis process. We propose a novel information structure that predicts the effect of control in the future. A sound and complete algorithm is developed to synthesize a controller which ensures both task completion and security guarantee. The proposed approach is illustrated by a case study of robot task planning."
199,https://arxiv.org/abs/2211.04728,Overcoming leakage in scalable quantum error correction,"Leakage of quantum information out of computational states into higher energy states represents a major challenge in the pursuit of quantum error correction (QEC). In a QEC circuit, leakage builds over time and spreads through multi-qubit interactions. This leads to correlated errors that degrade the exponential suppression of logical error with scale, challenging the feasibility of QEC as a path towards fault-tolerant quantum computation. Here, we demonstrate the execution of a distance-3 surface code and distance-21 bit-flip code on a Sycamore quantum processor where leakage is removed from all qubits in each cycle. This shortens the lifetime of leakage and curtails its ability to spread and induce correlated errors. We report a ten-fold reduction in steady-state leakage population on the data qubits encoding the logical state and an average leakage population of less than $1 \times 10^{-3}$ throughout the entire device. The leakage removal process itself efficiently returns leakage population back to the computational basis, and adding it to a code circuit prevents leakage from inducing correlated error across cycles, restoring a fundamental assumption of QEC. With this demonstration that leakage can be contained, we resolve a key challenge for practical QEC at scale."
200,https://arxiv.org/abs/2211.04717,Improving Noisy Student Training on Non-target Domain Data for Automatic Speech Recognition,"Noisy Student Training (NST) has recently demonstrated extremely strong performance in Automatic Speech Recognition(ASR). In this paper, we propose a data selection strategy named LM Filter to improve the performance of NST on non-target domain data in ASR tasks. Hypotheses with and without a Language Model are generated and the CER differences between them are utilized as a filter threshold. Results reveal that significant improvements of 10.4% compared with no data filtering baselines. We can achieve 3.31% CER in AISHELL-1 test set, which is best result from our knowledge without any other supervised data. We also perform evaluations on the supervised 1000 hour AISHELL-2 dataset and competitive results of 4.73% CER can be achieved."
201,https://arxiv.org/abs/2211.03893,Query Complexity of the Metric Steiner Tree Problem,"We study the query complexity of the metric Steiner Tree problem, where we are given an $n \times n$ metric on a set $V$ of vertices along with a set $T \subseteq V$ of $k$ terminals, and the goal is to find a tree of minimum cost that contains all terminals in $T$. The query complexity for the related minimum spanning tree (MST) problem is well-understood: for any fixed $\varepsilon > 0$, one can estimate the MST cost to within a $(1+\varepsilon)$-factor using only $\tilde{O}(n)$ queries, and this is known to be tight. This implies that a $(2 + \varepsilon)$-approximate estimate of Steiner Tree cost can be obtained with $\tilde{O}(k)$ queries by simply applying the MST cost estimation algorithm on the metric induced by the terminals.
  Our first result shows that any (randomized) algorithm that estimates the Steiner Tree cost to within a $(5/3 - \varepsilon)$-factor requires $Ω(n^2)$ queries, even if $k$ is a constant. This lower bound is in sharp contrast to an upper bound of $O(nk)$ queries for computing a $(5/3)$-approximate Steiner Tree, which follows from previous work by Du and Zelikovsky.
  Our second main result, and the main technical contribution of this work, is a sublinear query algorithm for estimating the Steiner Tree cost to within a strictly better-than-$2$ factor, with query complexity $\tilde{O}(n^{12/7} + n^{6/7}\cdot k)=\tilde{O}(n^{13/7})=o(n^2)$. We complement this result by showing an $\tildeΩ(n + k^{6/5})$ query lower bound for any algorithm that estimates Steiner Tree cost to a strictly better than $2$ factor. Thus $\tildeΩ(n^{6/5})$ queries are needed to just beat $2$-approximation when $k = Ω(n)$; a sharp contrast to MST cost estimation where a $(1+o(1))$-approximate estimate of cost is achievable with only $\tilde{O}(n)$ queries."
202,https://arxiv.org/abs/2210.11358,Estimating fine age structure and time trends in human contact patterns from coarse contact data: the Bayesian rate consistency model,"Since the emergence of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), many contact surveys have been conducted to measure changes in human interactions in the face of the pandemic and non-pharmaceutical interventions. These surveys were typically conducted longitudinally, using protocols that differ from those used in the pre-pandemic era. We present a model-based statistical approach that can reconstruct contact patterns at 1-year resolution even when the age of the contacts is reported coarsely by 5 or 10-year age bands. This innovation is rooted in population-level consistency constraints in how contacts between groups must add up, which prompts us to call the approach presented here the Bayesian rate consistency model. The model incorporates computationally efficient Hilbert Space Gaussian process priors to infer the dynamics in age- and gender-structured social contacts and is designed to adjust for reporting fatigue in longitudinal surveys. We demonstrate on simulations the ability to reconstruct contact patterns by gender and 1-year age interval from coarse data with adequate accuracy and within a fully Bayesian framework to quantify uncertainty. We investigate the patterns of social contact data collected in Germany from April to June 2020 across five longitudinal survey waves. We reconstruct the fine age structure in social contacts during the early stages of the pandemic and demonstrate that social contacts rebounded in a structured, non-homogeneous manner. We also show that by July 2020, social contact intensities remained well below pre-pandemic values despite a considerable easing of non-pharmaceutical interventions. This model-based inference approach is open access, computationally tractable enabling full Bayesian uncertainty quantification, and readily applicable to contemporary survey data as long as the exact age of survey participants is reported."
203,https://arxiv.org/abs/2210.10255,Observation of non-Abelian exchange statistics on a superconducting processor,"Indistinguishability of particles is a fundamental principle of quantum mechanics. For all elementary and quasiparticles observed to date - including fermions, bosons, and Abelian anyons - this principle guarantees that the braiding of identical particles leaves the system unchanged. However, in two spatial dimensions, an intriguing possibility exists: braiding of non-Abelian anyons causes rotations in a space of topologically degenerate wavefunctions. Hence, it can change the observables of the system without violating the principle of indistinguishability. Despite the well developed mathematical description of non-Abelian anyons and numerous theoretical proposals, their experimental observation has remained elusive for decades. Using a superconducting quantum processor, we prepare the ground state of the surface code and manipulate it via unitary operations to form wavefunctions that are described by non-Abelian anyons. By implementing a unitary protocol to move the anyons, we experimentally verify the fusion rules of non-Abelian Ising anyons and braid them to realize their statistics. Building on our technique, we study the prospect of employing the anyons for quantum computation and utilize braiding to create an entangled state of anyons encoding three logical qubits. Our work represents a key step towards topological quantum computing."
204,https://arxiv.org/abs/2210.08668,Temporal-Spatial dependencies ENhanced deep learning model (TSEN) for household leverage series forecasting,"Analyzing both temporal and spatial patterns for an accurate forecasting model for financial time series forecasting is a challenge due to the complex nature of temporal-spatial dynamics: time series from different locations often have distinct patterns; and for the same time series, patterns may vary as time goes by. Inspired by the successful applications of deep learning, we propose a new model to resolve the issues of forecasting household leverage in China. Our solution consists of multiple RNN-based layers and an attention layer: each RNN-based layer automatically learns the temporal pattern of a specific series with multivariate exogenous series, and then the attention layer learns the spatial correlative weight and obtains the global representations simultaneously. The results show that the new approach can capture the temporal-spatial dynamics of household leverage well and get more accurate and solid predictive results. More, the simulation also studies show that clustering and choosing correlative series are necessary to obtain accurate forecasting results."
205,https://arxiv.org/abs/2210.08437,Model Independent Approach of the JUNO $^8$B Solar Neutrino Program,"The physics potential of detecting $^8$B solar neutrinos is exploited at the Jiangmen Underground Neutrino Observatory (JUNO), in a model independent manner by using three distinct channels of the charged-current (CC), neutral-current (NC) and elastic scattering (ES) interactions. Due to the largest-ever mass of $^{13}$C nuclei in the liquid-scintillator detectors and the potential low background level, $^8$B solar neutrinos would be observable in the CC and NC interactions on $^{13}$C for the first time. By virtue of optimized event selections and muon veto strategies, backgrounds from the accidental coincidence, muon-induced isotopes, and external backgrounds can be greatly suppressed. Excellent signal-to-background ratios can be achieved in the CC, NC and ES channels to guarantee the $^8$B solar neutrino observation. From the sensitivity studies performed in this work, we show that one can reach the precision levels of 5%, 8% and 20% for the $^8$B neutrino flux, $\sin^2θ_{12}$, and $Δm^2_{21}$, respectively, using ten years of JUNO data. It would be unique and helpful to probe the details of both solar physics and neutrino physics. In addition, when combined with SNO, the world-best precision of 3% is expected for the $^8$B neutrino flux measurement."
206,https://arxiv.org/abs/2210.04147,The Possibility of Mirror Planet as Planet Nine in Solar System,"A series of dynamical anomalies in the orbits of distant trans-Neptunian objects points to a new celestial body (usually named Planet Nine) in the solar system. In this draft, we point out that a mirror planet captured from the outer solar system or formed in the solar system is also a possible candidate. The introduction of the mirror matter model is due to an unbroken parity symmetry and is a potential explanation for dark matter. This mirror planet has null or fainter electromagnetic counterparts with a smaller optical radius and might be explored through gravitational effects."
207,https://arxiv.org/abs/2210.03025,Information scrambling of the dilute Bose gas at low temperature,"We calculate the quantum Lyapunov exponent $λ_L$ and butterfly velocity $v_B$ in the dilute Bose gas at temperature $T$ deep in the Bose-Einstein condensation phase. The generalized Boltzmann equation approach is used for calculating out-of-time ordered correlators, from which $λ_L$ and $v_B$ are extracted. At very low temperature where elementary excitations are phonon-like, we find $λ_L\propto T^5$ and $v_B\sim c$, the sound velocity. At relatively high temperature, we have $λ_L\propto T$ and $v_B\sim c(T/T_*)^{0.23}$. We find $λ_L$ is always comparable to the damping rate of a quasiparticle, whose energy depends suitably on $T$. The chaos diffusion constant $D_L=v_B^2/λ_L$, on the other hand, differs from the energy diffusion constant $D_E$. We find $D_E\ll D_L$ at very low temperature and $D_E\gg D_L$ otherwise."
208,https://arxiv.org/abs/2209.09811,Predictive Scale-Bridging Simulations through Active Learning,"Throughout computational science, there is a growing need to utilize the continual improvements in raw computational horsepower to achieve greater physical fidelity through scale-bridging over brute-force increases in the number of mesh elements. For instance, quantitative predictions of transport in nanoporous media, critical to hydrocarbon extraction from tight shale formations, are impossible without accounting for molecular-level interactions. Similarly, inertial confinement fusion simulations rely on numerical diffusion to simulate molecular effects such as non-local transport and mixing without truly accounting for molecular interactions. With these two disparate applications in mind, we develop a novel capability which uses an active learning approach to optimize the use of local fine-scale simulations for informing coarse-scale hydrodynamics. Our approach addresses three challenges: forecasting continuum coarse-scale trajectory to speculatively execute new fine-scale molecular dynamics calculations, dynamically updating coarse-scale from fine-scale calculations, and quantifying uncertainty in neural network models."
209,https://arxiv.org/abs/2209.07757,Readout of a quantum processor with high dynamic range Josephson parametric amplifiers,"We demonstrate a high dynamic range Josephson parametric amplifier (JPA) in which the active nonlinear element is implemented using an array of rf-SQUIDs. The device is matched to the 50 $Ω$ environment with a Klopfenstein-taper impedance transformer and achieves a bandwidth of 250-300 MHz, with input saturation powers up to -95 dBm at 20 dB gain. A 54-qubit Sycamore processor was used to benchmark these devices, providing a calibration for readout power, an estimate of amplifier added noise, and a platform for comparison against standard impedance matched parametric amplifiers with a single dc-SQUID. We find that the high power rf-SQUID array design has no adverse effect on system noise, readout fidelity, or qubit dephasing, and we estimate an upper bound on amplifier added noise at 1.6 times the quantum limit. Lastly, amplifiers with this design show no degradation in readout fidelity due to gain compression, which can occur in multi-tone multiplexed readout with traditional JPAs."
210,https://arxiv.org/abs/2209.07729,On Weighted Graph Sparsification by Linear Sketching,"A seminal work of [Ahn-Guha-McGregor, PODS'12] showed that one can compute a cut sparsifier of an unweighted undirected graph by taking a near-linear number of linear measurements on the graph. Subsequent works also studied computing other graph sparsifiers using linear sketching, and obtained near-linear upper bounds for spectral sparsifiers [Kapralov-Lee-Musco-Musco-Sidford, FOCS'14] and first non-trivial upper bounds for spanners [Filtser-Kapralov-Nouri, SODA'21]. All these linear sketching algorithms, however, only work on unweighted graphs.
  In this paper, we initiate the study of weighted graph sparsification by linear sketching by investigating a natural class of linear sketches that we call incidence sketches, in which each measurement is a linear combination of the weights of edges incident on a single vertex. Our results are:
  1. Weighted cut sparsification: We give an algorithm that computes a $(1 + ε)$-cut sparsifier using $\tilde{O}(n ε^{-3})$ linear measurements, which is nearly optimal.
  2. Weighted spectral sparsification: We give an algorithm that computes a $(1 + ε)$-spectral sparsifier using $\tilde{O}(n^{6/5} ε^{-4})$ linear measurements. Complementing our algorithm, we then prove a superlinear lower bound of $Ω(n^{21/20-o(1)})$ measurements for computing some $O(1)$-spectral sparsifier using incidence sketches.
  3. Weighted spanner computation: We focus on graphs whose largest/smallest edge weights differ by an $O(1)$ factor, and prove that, for incidence sketches, the upper bounds obtained by~[Filtser-Kapralov-Nouri, SODA'21] are optimal up to an $n^{o(1)}$ factor."
211,https://arxiv.org/abs/2209.03994,A Study of Shared-Control with Force Feedback for Obstacle Avoidance in Whole-body Telelocomotion of a Wheeled Humanoid,"Teleoperation has emerged as an alternative solution to fully-autonomous systems for achieving human-level capabilities on humanoids. Specifically, teleoperation with whole-body control is a promising hands-free strategy to command humanoids but demands more physical and mental effort. To mitigate this limitation, researchers have proposed shared-control methods incorporating robot decision-making to aid humans on low-level tasks, further reducing operation effort. However, shared-control methods for wheeled humanoid telelocomotion on a whole-body level has yet to be explored. In this work, we study how whole-body feedback affects the performance of different shared-control methods for obstacle avoidance in diverse environments. A Time-Derivative Sigmoid Function (TDSF) is proposed to generate more intuitive force feedback from obstacles. Comprehensive human experiments were conducted, and the results concluded that force feedback enhances the whole-body telelocomotion performance in unfamiliar environments but could reduce performance in familiar environments. Conveying the robot's intention through haptics showed further improvements since the operator can utilize the force feedback for short-distance planning and visual feedback for long-distance planning."
212,https://arxiv.org/abs/2208.14074,Effective Multi-User Delay-Constrained Scheduling with Deep Recurrent Reinforcement Learning,"Multi-user delay constrained scheduling is important in many real-world applications including wireless communication, live streaming, and cloud computing. Yet, it poses a critical challenge since the scheduler needs to make real-time decisions to guarantee the delay and resource constraints simultaneously without prior information of system dynamics, which can be time-varying and hard to estimate. Moreover, many practical scenarios suffer from partial observability issues, e.g., due to sensing noise or hidden correlation. To tackle these challenges, we propose a deep reinforcement learning (DRL) algorithm, named Recurrent Softmax Delayed Deep Double Deterministic Policy Gradient ($\mathtt{RSD4}$), which is a data-driven method based on a Partially Observed Markov Decision Process (POMDP) formulation. $\mathtt{RSD4}$ guarantees resource and delay constraints by Lagrangian dual and delay-sensitive queues, respectively. It also efficiently tackles partial observability with a memory mechanism enabled by the recurrent neural network (RNN) and introduces user-level decomposition and node-level merging to ensure scalability. Extensive experiments on simulated/real-world datasets demonstrate that $\mathtt{RSD4}$ is robust to system dynamics and partially observable environments, and achieves superior performances over existing DRL and non-DRL-based methods."
213,https://arxiv.org/abs/2208.14030,Spacecraft depth completion based on the gray image and the sparse depth map,"Perceiving the three-dimensional (3D) structure of the spacecraft is a prerequisite for successfully executing many on-orbit space missions, and it can provide critical input for many downstream vision algorithms. In this paper, we propose to sense the 3D structure of spacecraft using light detection and ranging sensor (LIDAR) and a monocular camera. To this end, Spacecraft Depth Completion Network (SDCNet) is proposed to recover the dense depth map based on gray image and sparse depth map. Specifically, SDCNet decomposes the object-level spacecraft depth completion task into foreground segmentation subtask and foreground depth completion subtask, which segments the spacecraft region first and then performs depth completion on the segmented foreground area. In this way, the background interference to foreground spacecraft depth completion is effectively avoided. Moreover, an attention-based feature fusion module is also proposed to aggregate the complementary information between different inputs, which deduces the correlation between different features along the channel and the spatial dimension sequentially. Besides, four metrics are also proposed to evaluate object-level depth completion performance, which can more intuitively reflect the quality of spacecraft depth completion results. Finally, a large-scale satellite depth completion dataset is constructed for training and testing spacecraft depth completion algorithms. Empirical experiments on the dataset demonstrate the effectiveness of the proposed SDCNet, which achieves 0.25m mean absolute error of interest and 0.759m mean absolute truncation error, surpassing state-of-the-art methods by a large margin. The spacecraft pose estimation experiment is also conducted based on the depth completion results, and the experimental results indicate that the predicted dense depth map could meet the needs of downstream vision tasks."
214,https://arxiv.org/abs/2208.11993,A Compacted Structure for Cross-domain learning on Monocular Depth and Flow Estimation,"Accurate motion and depth recovery is important for many robot vision tasks including autonomous driving. Most previous studies have achieved cooperative multi-task interaction via either pre-defined loss functions or cross-domain prediction. This paper presents a multi-task scheme that achieves mutual assistance by means of our Flow to Depth (F2D), Depth to Flow (D2F), and Exponential Moving Average (EMA). F2D and D2F mechanisms enable multi-scale information integration between optical flow and depth domain based on differentiable shallow nets. A dual-head mechanism is used to predict optical flow for rigid and non-rigid motion based on a divide-and-conquer manner, which significantly improves the optical flow estimation performance. Furthermore, to make the prediction more robust and stable, EMA is used for our multi-task training. Experimental results on KITTI datasets show that our multi-task scheme outperforms other multi-task schemes and provide marked improvements on the prediction results."
215,https://arxiv.org/abs/2208.01466,A Secure Dynamic Edge Resource Federation Architecture for Cross-Domain IoT Systems,"The fast integration of 5G communication, Artificial Intelligence (AI), and Internet-of-Things (IoT) technologies is envisioned to enable Next Generation Networks (NGNs) for diverse smart services and user-defined applications for Smart Cities. However, it is still challenging to build a scalable and efficient infrastructure that satisfies the various performance, security, and management demands by heterogeneous IoT applications across multiple administrative domains. This paper presents a dynamic edge resource federation architecture, which integrates the concept of network slicing (NS) and blockchain to improve scalability, dynamicity, and security for multi-domain IoT applications. A NS-enabled dynamic edge resource federation framework adopts intelligent mechanisms to support efficient multi-domain service coordination that satisfies diverse Quality of Service (QoS) and security requirements. We propose a Hierarchical Integrated Federated Ledger (HIFL), which aims to guarantee decentralized security and privacy-preserving properties in multi-domain resource orchestration and service re-adjustment. As a secure-by-design solution, HIFL is promising to support efficient, trust and secured end-to-end IoT services. A preliminary proof-of-concept prototype has been implemented for comparing intra- and inter-domain performance expectations."
216,https://arxiv.org/abs/2207.13070,DeFakePro: Decentralized DeepFake Attacks Detection using ENF Authentication,"Advancements in generative models, like Deepfake allows users to imitate a targeted person and manipulate online interactions. It has been recognized that disinformation may cause disturbance in society and ruin the foundation of trust. This article presents DeFakePro, a decentralized consensus mechanism-based Deepfake detection technique in online video conferencing tools. Leveraging Electrical Network Frequency (ENF), an environmental fingerprint embedded in digital media recording, affords a consensus mechanism design called Proof-of-ENF (PoENF) algorithm. The similarity in ENF signal fluctuations is utilized in the PoENF algorithm to authenticate the media broadcasted in conferencing tools. By utilizing the video conferencing setup with malicious participants to broadcast deep fake video recordings to other participants, the DeFakePro system verifies the authenticity of the incoming media in both audio and video channels."
217,https://arxiv.org/abs/2207.10519,Real-Time Elderly Monitoring for Senior Safety by Lightweight Human Action Recognition,"With an increasing number of elders living alone, care-giving from a distance becomes a compelling need, particularly for safety. Real-time monitoring and action recognition are essential to raise an alert timely when abnormal behaviors or unusual activities occur. While wearable sensors are widely recognized as a promising solution, highly depending on user's ability and willingness makes them inefficient. In contrast, video streams collected through non-contact optical cameras provide richer information and release the burden on elders. In this paper, leveraging the Independently-Recurrent neural Network (IndRNN) we propose a novel Real-time Elderly Monitoring for senior Safety (REMS) based on lightweight human action recognition (HAR) technology. Using captured skeleton images, the REMS scheme is able to recognize abnormal behaviors or actions and preserve the user's privacy. To achieve high accuracy, the HAR module is trained and fine-tuned using multiple databases. An extensive experimental study verified that REMS system performs action recognition accurately and timely. REMS meets the design goals as a privacy-preserving elderly safety monitoring system and possesses the potential to be adopted in various smart monitoring systems."
218,https://arxiv.org/abs/2207.06702,Decoherence and Landauer's Principle in Qubit-Cavity Quantum-Field-Theory Interaction,"We consider quantum decoherence and Landauer's principle in qubit-cavity quantum field theory (QFT) interaction, treating the qubit as the system and cavity QFT as the environment. In particular, we investigate the changes that occur in the system with a pure initial state and environment during the decoherence process, with or without energy dissipation, and compare the results with the case in which the initial state of the system is a mixed state and thus decoherence is absent. When we choose an interaction Hamiltonian such that the energy and coherence of the system change simultaneously, the population change of the system and the energy change are the same when the initial state is mixed. However, the decoherence terms increase the von Neumann entropy of the system. In this case the energy change and decoherence of the system are not independent physical processes. The decoherence process maintains unitarity. On the other hand, if the interaction Hamiltonian does not change the energy of the system, there is only the decoherence effect. The environment will be a distribution in the basis of the displaced number state and always increases the energy. Landauer's principle is satisfied in both cases."
219,https://arxiv.org/abs/2207.06431,Suppressing quantum errors by scaling a surface code logical qubit,"Practical quantum computing will require error rates that are well below what is achievable with physical qubits. Quantum error correction offers a path to algorithmically-relevant error rates by encoding logical qubits within many physical qubits, where increasing the number of physical qubits enhances protection against physical errors. However, introducing more qubits also increases the number of error sources, so the density of errors must be sufficiently low in order for logical performance to improve with increasing code size. Here, we report the measurement of logical qubit performance scaling across multiple code sizes, and demonstrate that our system of superconducting qubits has sufficient performance to overcome the additional errors from increasing qubit number. We find our distance-5 surface code logical qubit modestly outperforms an ensemble of distance-3 logical qubits on average, both in terms of logical error probability over 25 cycles and logical error per cycle ($2.914\%\pm 0.016\%$ compared to $3.028\%\pm 0.023\%$). To investigate damaging, low-probability error sources, we run a distance-25 repetition code and observe a $1.7\times10^{-6}$ logical error per round floor set by a single high-energy event ($1.6\times10^{-7}$ when excluding this event). We are able to accurately model our experiment, and from this model we can extract error budgets that highlight the biggest challenges for future systems. These results mark the first experimental demonstration where quantum error correction begins to improve performance with increasing qubit number, illuminating the path to reaching the logical error rates required for computation."
220,https://arxiv.org/abs/2207.04995,Afterpulse measurement of JUNO 20-inch PMTs,"In this article we present the large photo-multiplier tube (PMT) afterpulse measurement results of Jiangmen Underground Neutrino Observatory (JUNO) experiment. Totally 11 dynode-PMTs (R12860) from Hamamatsu company and 150 micro-channel plate PMTs (MCP-PMTs, GDB-6201) from NNVT company were tested, an afterpulse model is built according to the afterpulse time distribution and probability of occurrence for these two types of PMTs. The average ratio between the total afterpulse charge with the delay between 0.5 $μ$ s and 20 $μ$ s to the primary pulse charge is 5.6%(13.2%) for the tested MCP-PMTs (dynode-PMTs). JUNO experiment will deploy 20,012 20-inch PMTs, and this study will benefit the detector simulation, event reconstruction and data analysis of JUNO experiment."
221,https://arxiv.org/abs/2206.13095,Information geometry under hierarchical quantum measurement,"In most quantum technologies, measurements need to be performed on the parametrized quantum states to transform the quantum information to classical information. The measurements, however, inevitably distort the information. The characterization of the discrepancy is an important subject in quantum information science, which plays a key role in understanding the difference between the structures of the quantum and classical information. Here we analyze the discrepancy in terms of the Fisher information metric and present a framework that can provide analytical bounds on the difference under hierarchical quantum measurements. Specifically, we present a set of analytical bounds on the difference between the quantum and classical Fisher information metric under hierarchical p-local quantum measurements, which are measurements that can be performed collectively on at most p copies of quantum states. The results can be directly transformed to the precision limit in multi-parameter quantum metrology, which leads to characterizations of the tradeoff among the precision of different parameters. The framework also provides a coherent picture for various existing results by including them as special cases."
222,https://arxiv.org/abs/2206.11489,Nearly Minimax Optimal Reinforcement Learning with Linear Function Approximation,"We study reinforcement learning with linear function approximation where the transition probability and reward functions are linear with respect to a feature mapping $\boldsymbolφ(s,a)$. Specifically, we consider the episodic inhomogeneous linear Markov Decision Process (MDP), and propose a novel computation-efficient algorithm, LSVI-UCB$^+$, which achieves an $\widetilde{O}(Hd\sqrt{T})$ regret bound where $H$ is the episode length, $d$ is the feature dimension, and $T$ is the number of steps. LSVI-UCB$^+$ builds on weighted ridge regression and upper confidence value iteration with a Bernstein-type exploration bonus. Our statistical results are obtained with novel analytical tools, including a new Bernstein self-normalized bound with conservatism on elliptical potentials, and refined analysis of the correction term. This is a minimax optimal algorithm for linear MDPs up to logarithmic factors, which closes the $\sqrt{Hd}$ gap between the upper bound of $\widetilde{O}(\sqrt{H^3d^3T})$ in (Jin et al., 2020) and lower bound of $Ω(Hd\sqrt{T})$ for linear MDPs."
223,https://arxiv.org/abs/2206.05254,Formation of robust bound states of interacting microwave photons,"Systems of correlated particles appear in many fields of science and represent some of the most intractable puzzles in nature. The computational challenge in these systems arises when interactions become comparable to other energy scales, which makes the state of each particle depend on all other particles. The lack of general solutions for the 3-body problem and acceptable theory for strongly correlated electrons shows that our understanding of correlated systems fades when the particle number or the interaction strength increases. One of the hallmarks of interacting systems is the formation of multi-particle bound states. In a ring of 24 superconducting qubits, we develop a high fidelity parameterizable fSim gate that we use to implement the periodic quantum circuit of the spin-1/2 XXZ model, an archetypal model of interaction. By placing microwave photons in adjacent qubit sites, we study the propagation of these excitations and observe their bound nature for up to 5 photons. We devise a phase sensitive method for constructing the few-body spectrum of the bound states and extract their pseudo-charge by introducing a synthetic flux. By introducing interactions between the ring and additional qubits, we observe an unexpected resilience of the bound states to integrability breaking. This finding goes against the common wisdom that bound states in non-integrable systems are unstable when their energies overlap with the continuum spectrum. Our work provides experimental evidence for bound states of interacting photons and discovers their stability beyond the integrability limit."
224,https://arxiv.org/abs/2206.03843,A multiferroic two-dimensional electron gas,"Multiferroics are compounds in which at least two ferroic orders coexist - typically (anti)ferromagnetism and ferroelectricity. While magnetic order can arise in both insulating and conducting compounds, ferroelectricity is in principle not allowed in metals although a few two-dimensional (semi)metals were reported to behave as ferroelectrics. Yet, the combination with magnetic order to realize multiferroic metals remains elusive. Here, by combining x-ray spectroscopy and magnetotransport, we show the coexistence of ferroelectricity and magnetism in an oxide-based two-dimensional electron gas (2DEG). The data evidence a non-volatile switching of the polar displacements and of the anomalous Hall effect by the polarization direction, demonstrating a magnetoelectric coupling. Our findings provide new opportunities in quantum matter stemming from the interplay between ferroelectricity, ferromagnetism and Rashba spin-orbit coupling in a 2DEG."
225,https://arxiv.org/abs/2206.01247,A Magnetic Flux Rope Configuration Derived by Optimization of Two-Spacecraft In-situ Measurements,"Increasingly one interplanetary coronal mass ejection (ICME) structure can propagate across more than one spacecraft in the solar wind. This usually happens when two or more spacecraft are nearly radially aligned with a relatively small longitudinal separation angle from one another. This provides multi-point measurements of the same structure and enables better characterization and validation of modeling results of the structures embedded in these ICMEs. We report such an event during October 13-14, 2019 when the Solar TErrestrial RElations Observatory Ahead (STA) spacecraft and the Parker Solar Probe (PSP) crossed one ICME structure at two different locations with nominal separations in both heliocentric distances and the longitudinal angles. We first perform an optimal fitting to the STA in-situ measurements, based on an analytic quasi-three dimensional (3D) model, yielding a minimum reduced $χ^2=0.468$. Then we further apply the optimization approach by combining the magnetic field measurements from both spacecraft along their separate paths across the ICME structure. We find that the output based on the optimization (with the minimum reduced $χ^2=3.15$) of the combined two-spacecraft dataset yields a more consistent result, given the much improved agreement of the model output with PSP data. The result demonstrates a magnetic flux rope configuration with clear 3D spatial variations."
226,https://arxiv.org/abs/2205.13685,Adversarial attacks and defenses in Speaker Recognition Systems: A survey,"Speaker recognition has become very popular in many application scenarios, such as smart homes and smart assistants, due to ease of use for remote control and economic-friendly features. The rapid development of SRSs is inseparable from the advancement of machine learning, especially neural networks. However, previous work has shown that machine learning models are vulnerable to adversarial attacks in the image domain, which inspired researchers to explore adversarial attacks and defenses in Speaker Recognition Systems (SRS). Unfortunately, existing literature lacks a thorough review of this topic. In this paper, we fill this gap by performing a comprehensive survey on adversarial attacks and defenses in SRSs. We first introduce the basics of SRSs and concepts related to adversarial attacks. Then, we propose two sets of criteria to evaluate the performance of attack methods and defense methods in SRSs, respectively. After that, we provide taxonomies of existing attack methods and defense methods, and further review them by employing our proposed criteria. Finally, based on our review, we find some open issues and further specify a number of future directions to motivate the research of SRSs security."
227,https://arxiv.org/abs/2205.08830,Prospects for Detecting the Diffuse Supernova Neutrino Background with JUNO,"We present the detection potential for the diffuse supernova neutrino background (DSNB) at the Jiangmen Underground Neutrino Observatory (JUNO), using the inverse-beta-decay (IBD) detection channel on free protons. We employ the latest information on the DSNB flux predictions, and investigate in detail the background and its reduction for the DSNB search at JUNO. The atmospheric neutrino induced neutral current (NC) background turns out to be the most critical background, whose uncertainty is carefully evaluated from both the spread of model predictions and an envisaged \textit{in situ} measurement. We also make a careful study on the background suppression with the pulse shape discrimination (PSD) and triple coincidence (TC) cuts. With latest DSNB signal predictions, more realistic background evaluation and PSD efficiency optimization, and additional TC cut, JUNO can reach the significance of 3$σ$ for 3 years of data taking, and achieve better than 5$σ$ after 10 years for a reference DSNB model. In the pessimistic scenario of non-observation, JUNO would strongly improve the limits and exclude a significant region of the model parameter space."
228,https://arxiv.org/abs/2205.08629,Mass Testing and Characterization of 20-inch PMTs for JUNO,"Main goal of the JUNO experiment is to determine the neutrino mass ordering using a 20kt liquid-scintillator detector. Its key feature is an excellent energy resolution of at least 3 % at 1 MeV, for which its instruments need to meet a certain quality and thus have to be fully characterized. More than 20,000 20-inch PMTs have been received and assessed by JUNO after a detailed testing program which began in 2017 and elapsed for about four years. Based on this mass characterization and a set of specific requirements, a good quality of all accepted PMTs could be ascertained. This paper presents the performed testing procedure with the designed testing systems as well as the statistical characteristics of all 20-inch PMTs intended to be used in the JUNO experiment, covering more than fifteen performance parameters including the photocathode uniformity. This constitutes the largest sample of 20-inch PMTs ever produced and studied in detail to date, i.e. 15,000 of the newly developed 20-inch MCP-PMTs from Northern Night Vision Technology Co. (NNVT) and 5,000 of dynode PMTs from Hamamatsu Photonics K. K.(HPK)."
229,https://arxiv.org/abs/2205.06187,Efficient Deep Visual and Inertial Odometry with Adaptive Visual Modality Selection,"In recent years, deep learning-based approaches for visual-inertial odometry (VIO) have shown remarkable performance outperforming traditional geometric methods. Yet, all existing methods use both the visual and inertial measurements for every pose estimation incurring potential computational redundancy. While visual data processing is much more expensive than that for the inertial measurement unit (IMU), it may not always contribute to improving the pose estimation accuracy. In this paper, we propose an adaptive deep-learning based VIO method that reduces computational redundancy by opportunistically disabling the visual modality. Specifically, we train a policy network that learns to deactivate the visual feature extractor on the fly based on the current motion state and IMU readings. A Gumbel-Softmax trick is adopted to train the policy network to make the decision process differentiable for end-to-end system training. The learned strategy is interpretable, and it shows scenario-dependent decision patterns for adaptive complexity reduction. Experiment results show that our method achieves a similar or even better performance than the full-modality baseline with up to 78.8% computational complexity reduction for KITTI dataset evaluation. The code is available at https://github.com/mingyuyng/Visual-Selective-VIO."
230,https://arxiv.org/abs/2205.04098,Onset of nonlinear electroosmotic flow under AC electric field,"Nonlinearity of electroosmotic flows (EOFs) is ubiquitous and plays a crucial role in the mass and energy transfer in ion transport, specimen mixing, electrochemistry reaction, and electric energy storage and utilizing. When and how the transition from a linear regime to a nonlinear one is essential for understanding, prohibiting or utilizing nonlinear EOF. However, suffers the lacking of reliable experimental instruments with high spatial and temporal resolutions, the investigation of the onset of nonlinear EOF still stays in theory. Herein, we experimentally studied the velocity fluctuations of EOFs driven by AC electric field via ultra-sensitive fluorescent blinking tricks. The linear and nonlinear AC EOFs are successfully identified from both the time trace and energy spectra of velocity fluctuations. The critical electric field ($E_{A,C}$) separating the two statuses is determined and is discovered by defining a generalized scaling law with respect to the convection velocity ($U$) and AC frequency ($f_f$) as $E_{A,C}$~${f_f}^{0.48-0.027U}$. The universal control parameters are determined with surprising accuracy for governing the status of AC EOFs. We hope the current investigation could be essential in the development of both theory and applications of nonlinear EOF."
231,https://arxiv.org/abs/2204.13249,Sub-percent Precision Measurement of Neutrino Oscillation Parameters with JUNO,"JUNO is a multi-purpose neutrino observatory under construction in the south of China. This publication presents new sensitivity estimates for the measurement of the $Δm^2_{31}$, $Δm^2_{21}$, $\sin^2 θ_{12}$, and $\sin^2 θ_{13}$ oscillation parameters using reactor antineutrinos, which is one of the primary physics goals of the experiment. The sensitivities are obtained using the best knowledge available to date on the location and overburden of the experimental site, the nuclear reactors in the surrounding area and beyond, the detector response uncertainties, and the reactor antineutrino spectral shape constraints expected from the TAO satellite detector. It is found that the $Δm^2_{31}$, $Δm^2_{21}$, and $\sin^2 θ_{12}$ oscillation parameters will be determined to better than 0.5% precision in six years of data collection, which represents approximately an order of magnitude improvement over existing constraints."
232,https://arxiv.org/abs/2204.11372,Noise-resilient Edge Modes on a Chain of Superconducting Qubits,"Inherent symmetry of a quantum system may protect its otherwise fragile states. Leveraging such protection requires testing its robustness against uncontrolled environmental interactions. Using 47 superconducting qubits, we implement the one-dimensional kicked Ising model which exhibits non-local Majorana edge modes (MEMs) with $\mathbb{Z}_2$ parity symmetry. Remarkably, we find that any multi-qubit Pauli operator overlapping with the MEMs exhibits a uniform late-time decay rate comparable to single-qubit relaxation rates, irrespective of its size or composition. This characteristic allows us to accurately reconstruct the exponentially localized spatial profiles of the MEMs. Furthermore, the MEMs are found to be resilient against certain symmetry-breaking noise owing to a prethermalization mechanism. Our work elucidates the complex interplay between noise and symmetry-protected edge modes in a solid-state environment."
233,https://arxiv.org/abs/2204.06096,Nuclear phase retrieval spectroscopy using resonant x-ray scattering,"Light-matter interaction is exploited in spectroscopic techniques to access information about molecular, atomic or nuclear constituents of the sample of interest. While scattered light carries both amplitude and phase information of the electromagnetic field, most of the time the latter is lost in intensity measurements. However, often the phase information is paramount to reconstruct the desired information of the target, as it is well known from coherent x-ray imaging. Here we introduce a new phase retrieval algorithm which allows us to reconstruct the field phase information from two-dimensional time- and energy-resolved spectra. We apply this method to the particular case of x-ray scattering off Mössbauer nuclei at a synchrotron radiation source. Knowledge of the phase allows also for an excellent reconstruction of the energy spectra from experimental data, which could not be achieved with this resolution otherwise. Our approach provides an efficient novel data analysis tool which will benefit x-ray quantum optics and Mössbauer spectroscopy with synchrotron radiation alike."
234,https://arxiv.org/abs/2204.03256,Calibration Strategy of the JUNO-TAO Experiment,"The Taishan Antineutrino Observatory (JUNO-TAO, or TAO) is a satellite detector for the Jiangmen Underground Neutrino Observatory (JUNO). Located near the Taishan reactor, TAO independently measures the reactor's antineutrino energy spectrum with unprecedented energy resolution. To achieve this goal, energy response must be well calibrated. Using the Automated Calibration Unit (ACU) and the Cable Loop System (CLS) of TAO, multiple radioactive sources are deployed to various positions in the detector to perform a precise calibration of energy response. The non-linear energy response can be controlled within 0.6% with different energy points of these radioactive sources. It can be further improved by using $^{12}\rm B$ decay signals produced by cosmic muons. Through the energy non-uniformity calibration, residual non-uniformity is less than 0.2%. The energy resolution degradation and energy bias caused by the residual non-uniformity can be controlled within 0.05% and 0.3%, respectively. In addition, the stability of other detector parameters, such as the gain of each silicon photo-multiplier, can be monitored with a special ultraviolet LED calibration system."
235,https://arxiv.org/abs/2203.15162,A Distribution Evolutionary Algorithm for the Graph Coloring Problem,"Graph coloring is a challenging combinatorial optimization problem with a wide range of applications. In this paper, a distribution evolutionary algorithm based on a population of probability model (DEA-PPM) is developed to address it efficiently. Unlike existing estimation of distribution algorithms where a probability model is updated by generated solutions, DEA-PPM employs a distribution population based on a novel probability model, and an orthogonal exploration strategy is introduced to search the distribution space with the assistance of an refinement strategy. By sampling the distribution population, efficient search in the solution space is realized based on a tabu search process. Meanwhile, DEA-PPM introduces an iterative vertex removal strategy to improve the efficiency of $k$-coloring, and an inherited initialization strategy is implemented to address the chromatic problem well. The cooperative evolution of the distribution population and the solution population leads to a good balance between exploration and exploitation. Numerical results demonstrate that the DEA-PPM of small population size is competitive to the state-of-the-art metaheuristics.utes to its competitiveness to the state-of-the-art metaheuristics."
236,https://arxiv.org/abs/2203.14798,Sublinear Algorithms and Lower Bounds for Estimating MST and TSP Cost in General Metrics,"We consider the design of sublinear space and query complexity algorithms for estimating the cost of a minimum spanning tree (MST) and the cost of a minimum traveling salesman (TSP) tour in a metric on $n$ points. We first consider the $o(n)$-space regime and show that, when the input is a stream of all $\binom{n}{2}$ entries of the metric, for any $α\ge 2$, both MST and TSP cost can be $α$-approximated using $\tilde{O}(n/α)$ space, and that $Ω(n/α^2)$ space is necessary for this task. Moreover, we show that even if the streaming algorithm is allowed $p$ passes over a metric stream, it still requires $\tildeΩ(\sqrt{n/αp^2})$ space.
  We next consider the semi-streaming regime, where computing even the exact MST cost is easy and the main challenge is to estimate TSP cost to within a factor that is strictly better than $2$. We show that, if the input is a stream of all edges of the weighted graph that induces the underlying metric, for any $\varepsilon > 0$, any one-pass $(2-\varepsilon)$-approximation of TSP cost requires $Ω(\varepsilon^2 n^2)$ space; on the other hand, there is an $\tilde{O}(n)$ space two-pass algorithm that approximates the TSP cost to within a factor of 1.96.
  Finally, we consider the query complexity of estimating metric TSP cost to within a factor that is strictly better than $2$, when the algorithm is given access to a matrix that specifies pairwise distances between all points. For MST estimation in this model, it is known that a $(1+\varepsilon)$-approximation is achievable with $\tilde{O}(n/\varepsilon^{O(1)})$ queries. We design an algorithm that performs $\tilde{O}(n^{1.5})$ distance queries and achieves a strictly better than $2$-approximation when either the metric is known to contain a spanning tree supported on weight-$1$ edges or the algorithm is given access to a minimum spanning tree of the graph."
237,https://arxiv.org/abs/2203.13754,Fast fluorescence lifetime imaging analysis via extreme learning machine,"We present a fast and accurate analytical method for fluorescence lifetime imaging microscopy (FLIM) using the extreme learning machine (ELM). We used extensive metrics to evaluate ELM and existing algorithms. First, we compared these algorithms using synthetic datasets. Results indicate that ELM can obtain higher fidelity, even in low-photon conditions. Afterwards, we used ELM to retrieve lifetime components from human prostate cancer cells loaded with gold nanosensors, showing that ELM also outperforms the iterative fitting and non-fitting algorithms. By comparing ELM with a computational efficient neural network, ELM achieves comparable accuracy with less training and inference time. As there is no back-propagation process for ELM during the training phase, the training speed is much higher than existing neural network approaches. The proposed strategy is promising for edge computing with online training."
238,https://arxiv.org/abs/2203.11406,Snowmass2021 Whitepaper: Muonium to antimuonium conversion,The spontaneous muonium to antimuonium conversion is one of the interesting charged lepton flavor violation processes. It serves as a clear indication of new physics and plays an important role in constraining the parameter space beyond Standard Model. MACE is a proposed experiment to probe such a phenomenon and expected to enhance the sensitivity to the conversion probability by more than two orders of magnitude from the current best upper constraint obtained by the PSI experiment two decades ago. Recent developments in the theoretical and experimental aspects to search for such a rare process are summarized.
239,https://arxiv.org/abs/2202.12507,FAEP: Fast Autonomous Exploration Planner for UAV Equipped with Limited FOV Sensor,"Autonomous exploration is one of the important parts to achieve the autonomous operation of Unmanned Aerial Vehicles (UAVs). To improve the efficiency of the exploration process, a fast and autonomous exploration planner (FAEP) is proposed in this paper. We firstly design a novel frontiers exploration sequence generation method to obtain a more reasonable exploration path, which considers not only the flight-level but frontier-level factors into TSP. According to the exploration sequence and the distribution of frontiers, a two-stage heading planning strategy is proposed to cover more frontiers by heading change during an exploration journey. To improve the stability of path searching, a guided kinodynamic path searching based on a guiding path is devised. In addition, a dynamic start point selection method for replanning is also adopted to increase the fluency of flight. We present sufficient benchmark and real-world experiments. Experimental results show the superiority of the proposed exploration planner compared with typical and state-of-the-art methods."
240,https://arxiv.org/abs/2202.10732,Wide-angle giant photonic spin Hall effect,"Photonic spin Hall effect is a manifestation of spin-orbit interaction of light and can be measured by a transverse shift λof photons with opposite spins. The precise measurement of transverse shifts can enable many spin-related applications, such as precise metrology and optical sensing. However, this transverse shift is generally small (i.e. δ/λ<{10}^{-1}, λis the wavelength), which impedes its precise measurement. To-date proposals to generate giant spin Hall effect (namely with δ/λ>{10}^{2}) have severe limitations, particularly its occurrence only over a narrow angular cone (with a width of Δθ<{1}^{\circ}). Here we propose a universal scheme to realize the wide-angle giant photonic spin Hall effect with Δθ>{70}^{\circ} by exploiting the interface between free space and uniaxial epsilon-near-zero media. The underlying mechanism is ascribed to the almost-perfect polarization splitting between s and p polarized waves at the designed interface. Remarkably, this almost-perfect polarization splitting does not resort to the interference effect and is insensitive to the incident angle, which then gives rise to the wide-angle giant photonic spin Hall effect."
241,https://arxiv.org/abs/2202.06472,Asymptotically Unbiased Estimation for Delayed Feedback Modeling via Label Correction,"Alleviating the delayed feedback problem is of crucial importance for the conversion rate(CVR) prediction in online advertising. Previous delayed feedback modeling methods using an observation window to balance the trade-off between waiting for accurate labels and consuming fresh feedback. Moreover, to estimate CVR upon the freshly observed but biased distribution with fake negatives, the importance sampling is widely used to reduce the distribution bias. While effective, we argue that previous approaches falsely treat fake negative samples as real negative during the importance weighting and have not fully utilized the observed positive samples, leading to suboptimal performance.
  In this work, we propose a new method, DElayed Feedback modeling with UnbiaSed Estimation, (DEFUSE), which aim to respectively correct the importance weights of the immediate positive, the fake negative, the real negative, and the delay positive samples at finer granularity. Specifically, we propose a two-step optimization approach that first infers the probability of fake negatives among observed negatives before applying importance sampling. To fully exploit the ground-truth immediate positives from the observed distribution, we further develop a bi-distribution modeling framework to jointly model the unbiased immediate positives and the biased delay conversions. Experimental results on both public and our industrial datasets validate the superiority of DEFUSE. Codes are available at https://github.com/ychen216/DEFUSE.git."
242,https://arxiv.org/abs/2201.11173,Learning Noise via Dynamical Decoupling of Entangled Qubits,"Noise in entangled quantum systems is difficult to characterize due to many-body effects involving multiple degrees of freedom. This noise poses a challenge to quantum computing, where two-qubit gate performance is critical. Here, we develop and apply multi-qubit dynamical decoupling sequences that characterize noise that occurs during two-qubit gates. In our superconducting system comprised of Transmon qubits with tunable couplers, we observe noise that is consistent with flux fluctuations in the coupler that simultaneously affects both qubits and induces noise in their entangling parameter. The effect of this noise on the qubits is very different from the well-studied single-qubit dephasing. Additionally, steps are observed in the decoupled signals, implying the presence of non-Gaussian noise."
243,https://arxiv.org/abs/2201.05702,Port Selection for Fluid Antenna Systems,"Fluid antenna system promises to obtain enormous diversity in the small space of a mobile device by switching the position of the radiating element to the most desirable position from a large number of prescribed locations of the given space. Previous researches have revealed the promising performance of fluid antenna systems if the position with the maximum received signal-to-noise ratio (SNR) is chosen. However, selecting the best position, referred to as port selection, requires a huge number of SNR observations from the ports and may prove to be infeasible. This letter tackles this problem by devising a number of fast port selection algorithms utilizing a combination of machine learning methods and analytical approximation when the system observes only a few ports. Simulation results illustrate that with only 10% of the ports observed, more than an order of magnitude reduction in the outage probability can be achieved. Even in the extreme cases where only one port is observed, considerable performance improvements are possible using the proposed algorithms."
244,https://arxiv.org/abs/2201.05540,Compact Graph Structure Learning via Mutual Information Compression,"Graph Structure Learning (GSL) recently has attracted considerable attentions in its capacity of optimizing graph structure as well as learning suitable parameters of Graph Neural Networks (GNNs) simultaneously. Current GSL methods mainly learn an optimal graph structure (final view) from single or multiple information sources (basic views), however the theoretical guidance on what is the optimal graph structure is still unexplored. In essence, an optimal graph structure should only contain the information about tasks while compress redundant noise as much as possible, which is defined as ""minimal sufficient structure"", so as to maintain the accurancy and robustness. How to obtain such structure in a principled way? In this paper, we theoretically prove that if we optimize basic views and final view based on mutual information, and keep their performance on labels simultaneously, the final view will be a minimal sufficient structure. With this guidance, we propose a Compact GSL architecture by MI compression, named CoGSL. Specifically, two basic views are extracted from original graph as two inputs of the model, which are refinedly reestimated by a view estimator. Then, we propose an adaptive technique to fuse estimated views into the final view. Furthermore, we maintain the performance of estimated views and the final view and reduce the mutual information of every two views. To comprehensively evaluate the performance of CoGSL, we conduct extensive experiments on several datasets under clean and attacked conditions, which demonstrate the effectiveness and robustness of CoGSL."
245,https://arxiv.org/abs/2201.04334,Electrically tunable magnetism and unique intralayer charge transfer in Janus monolayer MnSSe for spintronics applications,"Controlling magnetism and electronic properties of two-dimensional (2D) materials by purely electrical means is crucial and highly sought for high-efficiency spintronics devices since electric field can be easily applied locally compared with magnetic field. The recently discover 2D Janus crystals has provide a new platform for nanoscale electronics and spintronics due to their broken inversion symmetry nature. The intrinsic ferromagnetic Jauns monolayer, and hence the tunable physical properties, is therefore of great interest. Here, through comprehensive density functional theory calculations and Monte Carlo simulations, we unveil that single-layer MnSSe is an intrinsic ferromagnetic half-metal with a direct band gap of 1.14 eV in spin-down channel and a Curie temperature of about 72 K. The exchange coupling can be significantly enhanced or quenched by hole and electron doping, respectively. In particular, a small amount of hole doping MnSSe can tune its magnetization easy axis in between out-of-plane and in-plane directions, which is conducive to designing 2D spin field-effect transistor for spin-dependent transport. We also find a reversible longitudinal interlayer charge transfer between S and Se layers for the first time that is highly sensitive to the applied external electric field. Interestingly, the directions of charge flow and the applied field are the same. The behavior originates from the coexistence and/or the competition of external and built-in fields. These findings, together with the excellent stability and large in-plane stiffness, can greatly facilitate the development of nanoscale electronics and spintronics devices based on 2D MnSSe crystal."
246,https://arxiv.org/abs/2112.14450,"Damping signatures at JUNO, a medium-baseline reactor neutrino oscillation experiment","We study damping signatures at the Jiangmen Underground Neutrino Observatory (JUNO), a medium-baseline reactor neutrino oscillation experiment. These damping signatures are motivated by various new physics models, including quantum decoherence, $ν_3$ decay, neutrino absorption, and wave packet decoherence. The phenomenological effects of these models can be characterized by exponential damping factors at the probability level. We assess how well JUNO can constrain these damping parameters and how to disentangle these different damping signatures at JUNO. Compared to current experimental limits, JUNO can significantly improve the limits on $τ_3/m_3$ in the $ν_3$ decay model, the width of the neutrino wave packet $σ_x$, and the intrinsic relative dispersion of neutrino momentum $σ_{\rm rel}$."
247,https://arxiv.org/abs/2112.10979,"Mott Insulator-Density Ordered Superfluid Transition and ""Shamrock Transition"" in a Frustrated Triangle Lattice","Density order is usually a consequence of the competition between long-range and short-range interactions. Here we report a density ordered superfluid emergent from a homogeneous Mott insulator due to the competition between frustrations and local interactions. This transition is found in a Bose-Hubbard model on a frustrated triangle lattice with an extra pairing term. Further, we find a quantum phase transition between two different density ordered superfluids, which is beyond the Landau-Ginzburg paradigm. Across this transition, a U(1) symmetry is emergent, while the symmetry in each density ordered superfluid is Z2*Z3. Because there emerges a shamrock-like degenerate ground state in parameter space, we call the transition ""shamrock transition"". Effective low energy theories are established for the two transitions mentioned above and we find their resemblance and differences with clock models."
248,https://arxiv.org/abs/2111.09261,Small-scale magnetic flux ropes and their properties based on in-situ measurements from Parker Solar Probe,"We report small-scale magnetic flux ropes via the Parker Solar Probe in situ measurements during the first six encounters and present additional analyses to supplement our prior work in Chen et al. 2021. These flux ropes are detected by the Grad-Shafranov-based algorithm with the duration and scale size ranging from 10 seconds to $\lesssim$1 hour and from a few hundred kilometers to 10$^{-3}$ au, respectively. They include both static structures and those with significant field-aligned plasma flows. Most structures tend to possess large cross helicity, while the residual energy distributes in wide ranges. We find that these dynamic flux ropes mostly propagate anti-sunward, with no preferential sign of magnetic helicity. The magnetic flux function follows a power law and is proportional to scale size. We also present case studies showing reconstructed two-dimensional (2D) configurations, which confirm that the static and dynamic flux ropes have the common configuration of spiral magnetic field lines (also streamlines). Moreover, the existence of such events hints at the interchange reconnection as a possible mechanism to generate flux rope-like structures near the Sun. Lastly, we summarize the major findings and discuss the possible correlation between these flux rope-like structures and turbulence due to the process of local Alfvenic alignment."
249,https://arxiv.org/abs/2111.07474,A Polynomial Lower Bound on the Number of Rounds for Parallel Submodular Function Minimization and Matroid Intersection,"Submodular function minimization (SFM) and matroid intersection are fundamental discrete optimization problems with applications in many fields. It is well known that both of these can be solved making $\mathrm{poly}(N)$ queries to a relevant oracle (evaluation oracle for SFM and rank oracle for matroid intersection), where $N$ denotes the universe size. However, all known polynomial query algorithms are highly adaptive, requiring at least $N$ rounds of querying the oracle. A natural question is whether these can be efficiently solved in a highly parallel manner, namely, with $\mathrm{poly}(N)$ queries using only poly-logarithmic rounds of adaptivity.
  An important step towards understanding the adaptivity needed for efficient parallel SFM was taken recently in the work of Balkanski and Singer who showed that any SFM algorithm making $\mathrm{poly}(N)$ queries necessarily requires $Ω(\log N/\log \log N)$ rounds. This left open the possibility of efficient SFM algorithms in poly-logarithmic rounds. For matroid intersection, even the possibility of a constant round, $\mathrm{poly}(N)$ query algorithm was not hitherto ruled out.
  In this work, we prove that any, possibly randomized, algorithm for submodular function minimization or matroid intersection making $\mathrm{poly}(N)$ queries requires $\tildeΩ\left(N^{1/3}\right)$ rounds of adaptivity. In fact, we show a polynomial lower bound on the number of rounds of adaptivity even for algorithms that make at most $2^{N^{1-δ}}$ queries, for any constant $δ> 0$. Therefore, even though SFM and matroid intersection are efficiently solvable, they are not highly parallelizable in the oracle model."
250,https://arxiv.org/abs/2111.04050,Entropy production and correlation spreading in the interaction between particle detector and thermal baths,"Entropy production is the key to the second law of thermodynamics, and it is well defined by considering a joint unitary evolution of a system $S$ and a thermal environment $E$. However, due to the diversity of the initial state and Hamiltonian of the system and environment, it is hard to evaluate the characterisation of entropy production. In the present work, we propose that the evolution of $S$ and $E$ can be solved non-perturbatively in the framework of Gaussian quantum mechanics (GQM). We study the entropy production and correlation spreading in the interaction between Unruh-DeWitt-like particle detector and thermal baths, where the particle detector is set to be a harmonic oscillator and the thermal baths are made of interacting and noninteracting Gaussian states. We can observe that the entropy production implies quantum recurrence and shows periodicity. In the case of interacting bath, the correlation propagates in a periodic system and leads to a revival of the initial state. Our analysis can be extended to any other models in the framework of GQM, and it may also shed some light on the AdS/CFT correspondence."
251,https://arxiv.org/abs/2111.00287,CdtGRN: Construction of qualitative time-delayed gene regulatory networks with a deep learning method,"Background:Gene regulations often change over time rather than being constant. But many of gene regulatory networks extracted from databases are static. The tumor suppressor gene $P53$ is involved in the pathogenesis of many tumors, and its inhibition effects occur after a certain period. Therefore, it is of great significance to elucidate the regulation mechanism over time points. Result:A qualitative method for representing dynamic gene regulatory network is developed, called CdtGRN. It adopts the combination of convolutional neural networks(CNN) and fully connected networks(DNN) as the core mechanism of prediction. The ionizing radiation Affymetrix dataset (E-MEXP-549) was obtained at ArrayExpress, by microarray gene expression levels predicting relations between regulation. CdtGRN is tested against a time-delayed gene regulatory network with $22,284$ genes related to $P53$. The accuracy of CdtGRN reaches 92.07$\%$ on the classification of conservative verification set, and a kappa coefficient reaches $0.84$ and an average AUC accuracy is 94.25$\%$. This resulted in the construction of. Conclusion:The algorithm and program we developed in our study would be useful for identifying dynamic gene regulatory networks, and objectively analyze the delay of the regulatory relationship by analyzing the gene expression levels at different time points. The time-delayed gene regulatory network of $P53$ is also inferred and represented qualitatively, which is helpful to understand the pathological mechanism of tumors."
252,https://arxiv.org/abs/2110.11147,Unique Continuation on Quadratic Curves for Harmonic Functions,"The unique continuation on quadratic curves for harmonic functions is discussed in this paper. By using complex extension method, the conditional stability of unique continuation along quadratic curves for harmonic functions is illustrated. The numerical algorithm is provided based on collocation method and Tikhonov regularization. The stability estimates on parabolic and hyperbolic curves for harmonic functions are demonstrated by numerical examples respectively."
253,https://arxiv.org/abs/2110.06716,The negative dependence of evacuation time on group size under a binding mechanism,"This paper initiates the analysis of the relation between evacuation time and group size by applying an extended floor field cellular automaton model. Agents with various speeds, a group structure containing leaders and followers, and a dynamic field dependent on local population density are implemented all together in the model. Most importantly, a complete binding mechanism which includes leaders waiting for followers is brought up for the first time. A counterintuitive negative relation between evacuation time and group size is discovered in simulations. An entropy like quantity, namely the mixing index, is constructed to analyze the cause of that relation. It is found that under the binding mechanism, the higher degree of group mixing, the longer the evacuation time will be. Moreover, through a constant scale transformation, it is shown that the mixing index can be a key indicator that contains useful information about the evacuation system."
254,https://arxiv.org/abs/2109.14195,Influence of Binomial Crossover on Approximation Error of Evolutionary Algorithms,"Although differential evolution (DE) algorithms perform well on a large variety of complicated optimization problems, only a few theoretical studies are focused on the working principle of DE algorithms. To make the first attempt to reveal the function of binomial crossover, this paper aims to answer whether it can reduce the approximation error of evolutionary algorithms. By investigating the expected approximation error and the probability of not finding the optimum, we conduct a case study comparing two evolutionary algorithms with and without binomial crossover on two classical benchmark problems: OneMax and Deceptive. It is proven that using binomial crossover leads to the dominance of transition matrices. As a result, the algorithm with binomial crossover asymptotically outperforms that without crossover on both OneMax and Deceptive, and outperforms on OneMax, however, not on Deceptive. Furthermore, an adaptive parameter strategy is proposed which can strengthen the superiority of binomial crossover on Deceptive."
255,https://arxiv.org/abs/2109.06138,Orbital selective switching of ferromagnetism in an oxide quasi two-dimensional electron gas,"Multi-orbital physics in quasi-two-dimensional electron gases (q2DEGs) triggers unique phenomena not observed in bulk materials, such as unconventional superconductivity and magnetism. Here, we investigate the mechanism of orbital selective switching of the spin-polarization in the oxide q2DEG formed at the (001) interface between the LaAlO$_{3}$, EuTiO$_{3}$ and SrTiO$_{3}$ band insulators. By using density functional theory calculations, transport, magnetic and x-ray spectroscopy measurements, we find that the filling of titanium-bands with 3d$_{xz,yz}$ orbital character in the EuTiO3 layer and at the interface with SrTiO$_{3}$ induces an antiferromagnetic to ferromagnetic switching of the exchange interaction between Eu-4f$^{7}$ magnetic moments. The results explain the observation of the carrier density dependent ferromagnetic correlations and anomalous Hall effect in this q2DEG, and demonstrate how combined theoretical and experimental approaches can lead to a deeper understanding of novel electronic phases and serve as a guide for the materials design for advanced electronic applications."
256,https://arxiv.org/abs/2109.05861,Regression Analysis of Correlations for Correlated Data,"Correlated data are ubiquitous in today's data-driven society. A fundamental task in analyzing these data is to understand, characterize and utilize the correlations in them in order to conduct valid inference. Yet explicit regression analysis of correlations has been so far limited to longitudinal data, a special form of correlated data, while implicit analysis via mixed-effects models lacks generality as a full inferential tool. This paper proposes a novel regression approach for modelling the correlation structure, leveraging a new generalized z-transformation. This transformation maps correlation matrices that are constrained to be positive definite to vectors with un-restricted support, and is order-invariant. Building on these two properties, we develop a regression model to relate the transformed parameters to any covariates. We show that coupled with a mean and a variance regression model, the use of maximum likelihood leads to asymptotically normal parameter estimates, and crucially enables statistical inference for all the parameters. The performance of our framework is demonstrated in extensive simulation. More importantly, we illustrate the use of our model with the analysis of the classroom data, a highly unbalanced multilevel clustered data with within-class and within-school correlations, and the analysis of the malaria immune response data in Benin, a longitudinal data with time-dependent covariates in addition to time. Our analyses reveal new insights not previously known."
257,https://arxiv.org/abs/2109.05807,Incompatibility measures in multi-parameter quantum estimation under hierarchical quantum measurements,"The incompatibility of the measurements constraints the achievable precisions in multi-parameter quantum estimation. Understanding the tradeoff induced by such incompatibility is a central topic in quantum metrology. Here we provide an approach to study the incompatibility under general $p$-local measurements, which are the measurements that can be performed collectively on at most $p$ copies of quantum states. We demonstrate the power of the approach by presenting a hierarchy of analytical bounds on the tradeoff among the precision limits of different parameters. These bounds lead to a necessary condition for the saturation of the quantum Cramér-Rao bound under $p$-local measurements, which recovers the partial commutative condition at p=1 and the weak commutative condition at $p=\infty$. As a further demonstration of the power of the framework, we present another set of tradeoff relations with the right logarithmic operators(RLD)."
258,https://arxiv.org/abs/2109.03560,X-GOAL: Multiplex Heterogeneous Graph Prototypical Contrastive Learning,"Graphs are powerful representations for relations among objects, which have attracted plenty of attention. A fundamental challenge for graph learning is how to train an effective Graph Neural Network (GNN) encoder without labels, which are expensive and time consuming to obtain. Contrastive Learning (CL) is one of the most popular paradigms to address this challenge, which trains GNNs by discriminating positive and negative node pairs. Despite the success of recent CL methods, there are still two under-explored problems. First, how to reduce the semantic error introduced by random topology based data augmentations. Traditional CL defines positive and negative node pairs via the node-level topological proximity, which is solely based on the graph topology regardless of the semantic information of node attributes, and thus some semantically similar nodes could be wrongly treated as negative pairs. Second, how to effectively model the multiplexity of the real-world graphs, where nodes are connected by various relations and each relation could form a homogeneous graph layer. To solve these problems, we propose a novel multiplex heterogeneous graph prototypical contrastive leaning (X-GOAL) framework to extract node embeddings. X-GOAL is comprised of two components: the GOAL framework, which learns node embeddings for each homogeneous graph layer, and an alignment regularization, which jointly models different layers by aligning layer-specific node embeddings. Specifically, the GOAL framework captures the node-level information by a succinct graph transformation technique, and captures the cluster-level information by pulling nodes within the same semantic cluster closer in the embedding space. The alignment regularization aligns embeddings across layers at both node and cluster levels. We evaluate X-GOAL on various real-world datasets and downstream tasks to demonstrate its effectiveness."
259,https://arxiv.org/abs/2109.00283,Photonic-enabled radio-frequency self-interference cancellation incorporated in an in-band full-duplex radio-over-fiber system,"A photonic approach for radio-frequency (RF) self-interference cancellation (SIC) incorporated in an in-band full-duplex radio-over-fiber system is proposed. A dual-polarization binary phase-shift keying modulator is used for dual-polarization multiplexing at the central office (CO). A local oscillator signal and an intermediate-frequency signal carrying the downlink data are single-sideband modulated on the two polarization directions of the modulator, respectively. The optical signal is then transmitted to the remote unit, where the optical signals in the two polarization directions are split into two parts. One part is detected to generate the up-converted downlink RF signal, and the other part is re-modulated by the uplink RF signal and the self-interference, which is then transmitted back to the CO for the signal down-conversion and SIC via the optical domain signal adjustment and balanced detection. The functions of SIC, frequency up-conversion, down-conversion, and fiber transmission with dispersion immunity are all incorporated in the system. An experiment is performed. Cancellation depths of more than 39 dB for the single-tone signal and more than 20 dB for the 20-MBaud 16 quadrature amplitude modulation signal are achieved in the back-to-back case. The performance of the system does not have a significant decline when a section of 4.1-km optical fiber is incorporated."
260,https://arxiv.org/abs/2108.11420,Model-based Decision Making with Imagination for Autonomous Parking,"Autonomous parking technology is a key concept within autonomous driving research. This paper will propose an imaginative autonomous parking algorithm to solve issues concerned with parking. The proposed algorithm consists of three parts: an imaginative model for anticipating results before parking, an improved rapid-exploring random tree (RRT) for planning a feasible trajectory from a given start point to a parking lot, and a path smoothing module for optimizing the efficiency of parking tasks. Our algorithm is based on a real kinematic vehicle model; which makes it more suitable for algorithm application on real autonomous cars. Furthermore, due to the introduction of the imagination mechanism, the processing speed of our algorithm is ten times faster than that of traditional methods, permitting the realization of real-time planning simultaneously. In order to evaluate the algorithm's effectiveness, we have compared our algorithm with traditional RRT, within three different parking scenarios. Ultimately, results show that our algorithm is more stable than traditional RRT and performs better in terms of efficiency and quality."
261,https://arxiv.org/abs/2108.08691,A Cuckoo Quantum Evolutionary Algorithm for the Graph Coloring Problem,"Based on the framework of the quantum-inspired evolutionary algorithm, a cuckoo quantum evolutionary algorithm (CQEA) is proposed for solving the graph coloring problem (GCP). To reduce iterations for the search of the chromatic number, the initial quantum population is generated by random initialization assisted by inheritance. Moreover, improvement of global exploration is achieved by incorporating the cuckoo search strategy, and a local search operation, as well as a perturbance strategy, is developed to enhance its performance on GCPs. Numerical results demonstrate that CQEA operates with strong exploration and exploitation abilities, and is competitive to the compared state-of-the-art heuristic algorithms."
262,https://arxiv.org/abs/2108.05493,Violation and Revival of Kramers' Degeneracy in Open Quantum Systems,"Kramers' theorem ensures double degeneracy in the energy spectrum of a time-reversal symmetric fermionic system with half-integer total spin. Here we are now trying to go beyond the closed system and discuss Kramers' degeneracy in open systems out of equilibrium. In this letter, we prove that the Kramers' degeneracy in interacting fermionic systems is equivalent to the degeneracy in the spectra of different spins together with the vanishing of the inter-spin spectrum. We find the violation of Kramers' degeneracy in time-reversal symmetric open quantum systems is locked with whether the system reaches thermal equilibrium. After a sudden coupling to an environment in a time-reversal symmetry preserving way, the Kramers doublet experiences an energy splitting at a short time and then a recovery process. We verified the violation and revival of Kramers' degeneracy in a concrete model of interacting fermions and we find Kramers' degeneracy is restored after the local thermalization time. By contrast, for time-reversal symmetry $\tilde{\cal T}$ with $\tilde{\cal T}^2=1$, we find although there is a violation and revival of spectral degeneracy for different spins, the inter-spin spectral function is always nonzero. We also prove that the degeneracy in spectral function protected by unitary symmetry can be maintained always."
263,https://arxiv.org/abs/2108.04453,Method Towards CVPR 2021 Image Matching Challenge,This report describes Megvii-3D team's approach towards CVPR 2021 Image Matching Workshop.
264,https://arxiv.org/abs/2305.11461,SelfzCoT: a Self-Prompt Zero-shot CoT from Semantic-level to Code-level for a Better Utilization of LLMs,"This paper show a work on better use of LLMs with SelfzCoT a self-prompt zero-shot CoT. Specifically, on the zero-shot arithmetic reasoning tasks, the accuracy of the proposed SelfzCoT is improved with GSM8K from 40.50% to 82.34%, with MultiArith from 79.3% to 94.7%, with ADDSUB from 74.70% to 94.10%, with SingleEq from 78.70% to 91.30%, with AQUA from 31.90% to 82.33%, and with SVAMP from 63.70% to 79.70%. Totally, using the first two lasting path activations to LLM and particularly, the code-level self-prompt, the SelfzCoT has a huge improvement on all six zero-shot arithmetic reasoning tasks. Additionally, our modified zero-shot CoT (MzCoT) also achieves remarkable performance in the reasoning tasks. The accuracy of the proposed MzCoT is enhanced with GSM8K from 40.50% to 76.32%, with MultiArith from 79.3% to 96.97%, with ADDSUB from 74.70% to 92.39%, with SingleEq from 78.70% to 94.60%, with AQUA from 31.90% to 79.90%, and with SVAMP from 63.70% to 81.50%. Notably, SelfzCoT has the best performance on GSM8K among all the recent zero-shot methods."
265,https://arxiv.org/abs/2305.08522,Cross-Modality Time-Variant Relation Learning for Generating Dynamic Scene Graphs,"Dynamic scene graphs generated from video clips could help enhance the semantic visual understanding in a wide range of challenging tasks such as environmental perception, autonomous navigation, and task planning of self-driving vehicles and mobile robots. In the process of temporal and spatial modeling during dynamic scene graph generation, it is particularly intractable to learn time-variant relations in dynamic scene graphs among frames. In this paper, we propose a Time-variant Relation-aware TRansformer (TR$^2$), which aims to model the temporal change of relations in dynamic scene graphs. Explicitly, we leverage the difference of text embeddings of prompted sentences about relation labels as the supervision signal for relations. In this way, cross-modality feature guidance is realized for the learning of time-variant relations. Implicitly, we design a relation feature fusion module with a transformer and an additional message token that describes the difference between adjacent frames. Extensive experiments on the Action Genome dataset prove that our TR$^2$ can effectively model the time-variant relations. TR$^2$ significantly outperforms previous state-of-the-art methods under two different settings by 2.1% and 2.6% respectively."
266,https://arxiv.org/abs/2304.00334,TalkCLIP: Talking Head Generation with Text-Guided Expressive Speaking Styles,"In order to produce facial-expression-specified talking head videos, previous audio-driven one-shot talking head methods need to use a reference video with a matching speaking style (i.e., facial expressions). However, finding videos with a desired style may not be easy, potentially restricting their application. In this work, we propose an expression-controllable one-shot talking head method, dubbed TalkCLIP, where the expression in a speech is specified by the natural language. This would significantly ease the difficulty of searching for a video with a desired speaking style. Here, we first construct a text-video paired talking head dataset, in which each video has alternative prompt-alike descriptions. Specifically, our descriptions involve coarse-level emotion annotations and facial action unit (AU) based fine-grained annotations. Then, we introduce a CLIP-based style encoder that first projects natural language descriptions to the CLIP text embedding space and then aligns the textual embeddings to the representations of speaking styles. As extensive textual knowledge has been encoded by CLIP, our method can even generalize to infer a speaking style whose description has not been seen during training. Extensive experiments demonstrate that our method achieves the advanced capability of generating photo-realistic talking heads with vivid facial expressions guided by text descriptions."
267,https://arxiv.org/abs/2301.01081,StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles,"Different people speak with diverse personalized speaking styles. Although existing one-shot talking head methods have made significant progress in lip sync, natural facial expressions, and stable head motions, they still cannot generate diverse speaking styles in the final talking head videos. To tackle this problem, we propose a one-shot style-controllable talking face generation framework. In a nutshell, we aim to attain a speaking style from an arbitrary reference speaking video and then drive the one-shot portrait to speak with the reference speaking style and another piece of audio. Specifically, we first develop a style encoder to extract dynamic facial motion patterns of a style reference video and then encode them into a style code. Afterward, we introduce a style-controllable decoder to synthesize stylized facial animations from the speech content and style code. In order to integrate the reference speaking style into generated videos, we design a style-aware adaptive transformer, which enables the encoded style code to adjust the weights of the feed-forward layers accordingly. Thanks to the style-aware adaptation mechanism, the reference speaking style can be better embedded into synthesized videos during decoding. Extensive experiments demonstrate that our method is capable of generating talking head videos with diverse speaking styles from only one portrait image and an audio clip while achieving authentic visual effects. Project Page: https://github.com/FuxiVirtualHuman/styletalk."
268,https://arxiv.org/abs/2203.04007,DuMLP-Pin: A Dual-MLP-dot-product Permutation-invariant Network for Set Feature Extraction,"Existing permutation-invariant methods can be divided into two categories according to the aggregation scope, i.e. global aggregation and local one. Although the global aggregation methods, e. g., PointNet and Deep Sets, get involved in simpler structures, their performance is poorer than the local aggregation ones like PointNet++ and Point Transformer. It remains an open problem whether there exists a global aggregation method with a simple structure, competitive performance, and even much fewer parameters. In this paper, we propose a novel global aggregation permutation-invariant network based on dual MLP dot-product, called DuMLP-Pin, which is capable of being employed to extract features for set inputs, including unordered or unstructured pixel, attribute, and point cloud data sets. We strictly prove that any permutation-invariant function implemented by DuMLP-Pin can be decomposed into two or more permutation-equivariant ones in a dot-product way as the cardinality of the given input set is greater than a threshold. We also show that the DuMLP-Pin can be viewed as Deep Sets with strong constraints under certain conditions. The performance of DuMLP-Pin is evaluated on several different tasks with diverse data sets. The experimental results demonstrate that our DuMLP-Pin achieves the best results on the two classification problems for pixel sets and attribute sets. On both the point cloud classification and the part segmentation, the accuracy of DuMLP-Pin is very close to the so-far best-performing local aggregation method with only a 1-2% difference, while the number of required parameters is significantly reduced by more than 85% in classification and 69% in segmentation, respectively. The code is publicly available on https://github.com/JaronTHU/DuMLP-Pin."
269,https://arxiv.org/abs/2102.11149,Phase Space Reconstruction Network for Lane Intrusion Action Recognition,"In a complex road traffic scene, illegal lane intrusion of pedestrians or cyclists constitutes one of the main safety challenges in autonomous driving application. In this paper, we propose a novel object-level phase space reconstruction network (PSRNet) for motion time series classification, aiming to recognize lane intrusion actions that occur 150m ahead through a monocular camera fixed on moving vehicle. In the PSRNet, the movement of pedestrians and cyclists, specifically viewed as an observable object-level dynamic process, can be reconstructed as trajectories of state vectors in a latent phase space and further characterized by a learnable Lyapunov exponent-like classifier that indicates discrimination in terms of average exponential divergence of state trajectories. Additionally, in order to first transform video inputs into one-dimensional motion time series of each object, a lane width normalization based on visual object tracking-by-detection is presented. Extensive experiments are conducted on the THU-IntrudBehavior dataset collected from real urban roads. The results show that our PSRNet could reach the best accuracy of 98.0%, which remarkably exceeds existing action recognition approaches by more than 30%."
270,https://arxiv.org/abs/2102.09780,A Deep Graph Wavelet Convolutional Neural Network for Semi-supervised Node Classification,"Graph convolutional neural network provides good solutions for node classification and other tasks with non-Euclidean data. There are several graph convolutional models that attempt to develop deep networks but do not cause serious over-smoothing at the same time. Considering that the wavelet transform generally has a stronger ability to extract useful information than the Fourier transform, we propose a new deep graph wavelet convolutional network (DeepGWC) for semi-supervised node classification tasks. Based on the optimized static filtering matrix parameters of vanilla graph wavelet neural networks and the combination of Fourier bases and wavelet ones, DeepGWC is constructed together with the reuse of residual connection and identity mappings in network architectures. Extensive experiments on three benchmark datasets including Cora, Citeseer, and Pubmed are conducted. The experimental results demonstrate that our DeepGWC outperforms existing graph deep models with the help of additional wavelet bases and achieves new state-of-the-art performances eventually."
271,https://arxiv.org/abs/2012.06785,DETR for Crowd Pedestrian Detection,"Pedestrian detection in crowd scenes poses a challenging problem due to the heuristic defined mapping from anchors to pedestrians and the conflict between NMS and highly overlapped pedestrians. The recently proposed end-to-end detectors(ED), DETR and deformable DETR, replace hand designed components such as NMS and anchors using the transformer architecture, which gets rid of duplicate predictions by computing all pairwise interactions between queries. Inspired by these works, we explore their performance on crowd pedestrian detection. Surprisingly, compared to Faster-RCNN with FPN, the results are opposite to those obtained on COCO. Furthermore, the bipartite match of ED harms the training efficiency due to the large ground truth number in crowd scenes. In this work, we identify the underlying motives driving ED's poor performance and propose a new decoder to address them. Moreover, we design a mechanism to leverage the less occluded visible parts of pedestrian specifically for ED, and achieve further improvements. A faster bipartite match algorithm is also introduced to make ED training on crowd dataset more practical. The proposed detector PED(Pedestrian End-to-end Detector) outperforms both previous EDs and the baseline Faster-RCNN on CityPersons and CrowdHuman. It also achieves comparable performance with state-of-the-art pedestrian detection methods. Code will be released soon."
272,https://arxiv.org/abs/1811.11057,Fast Object Detection in Compressed Video,"Object detection in videos has drawn increasing attention since it is more practical in real scenarios. Most of the deep learning methods use CNNs to process each decoded frame in a video stream individually. However, the free of charge yet valuable motion information already embedded in the video compression format is usually overlooked. In this paper, we propose a fast object detection method by taking advantage of this with a novel Motion aided Memory Network (MMNet). The MMNet has two major advantages: 1) It significantly accelerates the procedure of feature extraction for compressed videos. It only need to run a complete recognition network for I-frames, i.e. a few reference frames in a video, and it produces the features for the following P frames (predictive frames) with a light weight memory network, which runs fast; 2) Unlike existing methods that establish an additional network to model motion of frames, we take full advantage of both motion vectors and residual errors that are freely available in video streams. To our best knowledge, the MMNet is the first work that investigates a deep convolutional detector on compressed videos. Our method is evaluated on the large-scale ImageNet VID dataset, and the results show that it is 3x times faster than single image detector R-FCN and 10x times faster than high-performance detector MANet at a minor accuracy loss."
273,https://arxiv.org/abs/1809.10198,Recent progress in semantic image segmentation,"Semantic image segmentation, which becomes one of the key applications in image processing and computer vision domain, has been used in multiple domains such as medical area and intelligent transportation. Lots of benchmark datasets are released for researchers to verify their algorithms. Semantic segmentation has been studied for many years. Since the emergence of Deep Neural Network (DNN), segmentation has made a tremendous progress. In this paper, we divide semantic image segmentation methods into two categories: traditional and recent DNN method. Firstly, we briefly summarize the traditional method as well as datasets released for segmentation, then we comprehensively investigate recent methods based on DNN which are described in the eight aspects: fully convolutional network, upsample ways, FCN joint with CRF methods, dilated convolution approaches, progresses in backbone network, pyramid methods, Multi-level feature and multi-stage method, supervised, weakly-supervised and unsupervised methods. Finally, a conclusion in this area is drawn."
274,https://arxiv.org/abs/1807.11699,SegStereo: Exploiting Semantic Information for Disparity Estimation,"Disparity estimation for binocular stereo images finds a wide range of applications. Traditional algorithms may fail on featureless regions, which could be handled by high-level clues such as semantic segments. In this paper, we suggest that appropriate incorporation of semantic cues can greatly rectify prediction in commonly-used disparity estimation frameworks. Our method conducts semantic feature embedding and regularizes semantic cues as the loss term to improve learning disparity. Our unified model SegStereo employs semantic features from segmentation and introduces semantic softmax loss, which helps improve the prediction accuracy of disparity maps. The semantic cues work well in both unsupervised and supervised manners. SegStereo achieves state-of-the-art results on KITTI Stereo benchmark and produces decent prediction on both CityScapes and FlyingThings3D datasets."
275,https://arxiv.org/abs/2305.15262,Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration,"We identify two crucial limitations in the evaluation of recent parallel-integrated method Parallel Context Windows (PCW), which extends the maximum context lengths of language models, e.g., 2048 for LLaMA, by harnessing window-wise attention and positional embedding techniques. We first show that a simple yet strong baseline, weighted sum ensemble, is missing for the in-context few-shot classification. Moreover, on more challenging Chain-of-Thought (CoT) reasoning (e.g., HotpotQA), PCW would present unexpected deterioration regarding question miscomprehension and false inference. Based on our findings, we suggest that the existing PCW design may not guarantee sufficient improvement and practicality in handling lengthy documents in real-world applications. More community efforts on enabling language models' long context understanding ability should be paid."
276,https://arxiv.org/abs/2304.05977,ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation,"We present ImageReward -- the first general-purpose text-to-image human preference reward model -- to address various prevalent issues in generative models and align them with human values and preferences. Its training is based on our systematic annotation pipeline that covers both the rating and ranking components, collecting a dataset of 137k expert comparisons to date. In human evaluation, ImageReward outperforms existing scoring methods (e.g., CLIP by 38.6\%), making it a promising automatic metric for evaluating and improving text-to-image synthesis. The reward model is publicly available via the \texttt{image-reward} package at \url{https://github.com/THUDM/ImageReward}."
277,https://arxiv.org/abs/2304.04779,GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner,"Graph self-supervised learning (SSL), including contrastive and generative approaches, offers great potential to address the fundamental challenge of label scarcity in real-world graph data. Among both sets of graph SSL techniques, the masked graph autoencoders (e.g., GraphMAE)--one type of generative method--have recently produced promising results. The idea behind this is to reconstruct the node features (or structures)--that are randomly masked from the input--with the autoencoder architecture. However, the performance of masked feature reconstruction naturally relies on the discriminability of the input features and is usually vulnerable to disturbance in the features. In this paper, we present a masked self-supervised learning framework GraphMAE2 with the goal of overcoming this issue. The idea is to impose regularization on feature reconstruction for graph SSL. Specifically, we design the strategies of multi-view random re-mask decoding and latent representation prediction to regularize the feature reconstruction. The multi-view random re-mask decoding is to introduce randomness into reconstruction in the feature space, while the latent representation prediction is to enforce the reconstruction in the embedding space. Extensive experiments show that GraphMAE2 can consistently generate top results on various public datasets, including at least 2.45% improvements over state-of-the-art baselines on ogbn-Papers100M with 111M nodes and 1.6B edges."
278,https://arxiv.org/abs/2303.17568,CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X,"Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency for 83.4% of its users. Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX."
279,https://arxiv.org/abs/2303.14655,GOAL: A Challenging Knowledge-grounded Video Captioning Benchmark for Real-time Soccer Commentary Generation,"Despite the recent emergence of video captioning models, how to generate vivid, fine-grained video descriptions based on the background knowledge (i.e., long and informative commentary about the domain-specific scenes with appropriate reasoning) is still far from being solved, which however has great applications such as automatic sports narrative. In this paper, we present GOAL, a benchmark of over 8.9k soccer video clips, 22k sentences, and 42k knowledge triples for proposing a challenging new task setting as Knowledge-grounded Video Captioning (KGVC). Moreover, we conduct experimental adaption of existing methods to show the difficulty and potential directions for solving this valuable and applicable task."
280,https://arxiv.org/abs/2302.11848,"Web-Scale Academic Name Disambiguation: the WhoIsWho Benchmark, Leaderboard, and Toolkit","Name disambiguation -- a fundamental problem in online academic systems -- is now facing greater challenges with the increasing growth of research papers. For example, on AMiner, an online academic search platform, about 10% of names own more than 100 authors. Such real-world hard cases cannot be fully addressed by existing research efforts, because of the small-scale or low-quality datasets that they use to build algorithms. The development of effective algorithms is further hampered by a variety of tasks and evaluation protocols designed on top of diverse datasets. To this end, we present WhoIsWho owning, a large-scale benchmark with over 1,000,000 papers built using an interactive annotation process, a regular leaderboard with comprehensive tasks, and an easy-to-use toolkit encapsulating the entire pipeline as well as the most powerful features and baseline models for tackling the tasks. Our developed strong baseline has already been deployed online in the AMiner system to enable daily arXiv paper assignments. The documentation and regular leaderboards are publicly available at http://whoiswho.biendata.xyz/."
281,https://arxiv.org/abs/2208.07638,Mask and Reason: Pre-Training Knowledge Graph Transformers for Complex Logical Queries,"Knowledge graph (KG) embeddings have been a mainstream approach for reasoning over incomplete KGs. However, limited by their inherently shallow and static architectures, they can hardly deal with the rising focus on complex logical queries, which comprise logical operators, imputed edges, multiple source entities, and unknown intermediate entities. In this work, we present the Knowledge Graph Transformer (kgTransformer) with masked pre-training and fine-tuning strategies. We design a KG triple transformation method to enable Transformer to handle KGs, which is further strengthened by the Mixture-of-Experts (MoE) sparse activation. We then formulate the complex logical queries as masked prediction and introduce a two-stage masked pre-training strategy to improve transferability and generalizability. Extensive experiments on two benchmarks demonstrate that kgTransformer can consistently outperform both KG embedding-based baselines and advanced encoders on nine in-domain and out-of-domain reasoning tasks. Additionally, kgTransformer can reason with explainability via providing the full reasoning paths to interpret given answers."
282,https://arxiv.org/abs/2207.07087,Parameter-Efficient Prompt Tuning Makes Generalized and Calibrated Neural Text Retrievers,"Prompt tuning attempts to update few task-specific parameters in pre-trained models. It has achieved comparable performance to fine-tuning of the full parameter set on both language understanding and generation tasks. In this work, we study the problem of prompt tuning for neural text retrievers. We introduce parameter-efficient prompt tuning for text retrieval across in-domain, cross-domain, and cross-topic settings. Through an extensive analysis, we show that the strategy can mitigate the two issues -- parameter-inefficiency and weak generalizability -- faced by fine-tuning based retrieval methods. Notably, it can significantly improve the out-of-domain zero-shot generalization of the retrieval models. By updating only 0.1% of the model parameters, the prompt tuning strategy can help retrieval models achieve better generalization performance than traditional methods in which all parameters are updated. Finally, to facilitate research on retrievers' cross-topic generalizability, we curate and release an academic retrieval dataset with 18K query-results pairs in 87 topics, making it the largest topic-specific one to date."
283,https://arxiv.org/abs/2205.10803,GraphMAE: Self-Supervised Masked Graph Autoencoders,"Self-supervised learning (SSL) has been extensively explored in recent years. Particularly, generative SSL has seen emerging success in natural language processing and other AI fields, such as the wide adoption of BERT and GPT. Despite this, contrastive learning-which heavily relies on structural data augmentation and complicated training strategies-has been the dominant approach in graph SSL, while the progress of generative SSL on graphs, especially graph autoencoders (GAEs), has thus far not reached the potential as promised in other fields. In this paper, we identify and examine the issues that negatively impact the development of GAEs, including their reconstruction objective, training robustness, and error metric. We present a masked graph autoencoder GraphMAE that mitigates these issues for generative self-supervised graph pretraining. Instead of reconstructing graph structures, we propose to focus on feature reconstruction with both a masking strategy and scaled cosine error that benefit the robust training of GraphMAE. We conduct extensive experiments on 21 public datasets for three different graph learning tasks. The results manifest that GraphMAE-a simple graph autoencoder with careful designs-can consistently generate outperformance over both contrastive and generative state-of-the-art baselines. This study provides an understanding of graph autoencoders and demonstrates the potential of generative self-supervised pre-training on graphs."
284,https://arxiv.org/abs/2204.04497,IDPG: An Instance-Dependent Prompt Generation Method,"Prompt tuning is a new, efficient NLP transfer learning paradigm that adds a task-specific prompt in each input instance during the model training stage. It freezes the pre-trained language model and only optimizes a few task-specific prompts. In this paper, we propose a conditional prompt generation method to generate prompts for each input instance, referred to as the Instance-Dependent Prompt Generation (IDPG). Unlike traditional prompt tuning methods that use a fixed prompt, IDPG introduces a lightweight and trainable component to generate prompts based on each input sentence. Extensive experiments on ten natural language understanding (NLU) tasks show that the proposed strategy consistently outperforms various prompt tuning baselines and is on par with other efficient transfer learning methods such as Compacter while tuning far fewer model parameters."
285,https://arxiv.org/abs/2203.06389,GRAND+: Scalable Graph Random Neural Networks,"Graph neural networks (GNNs) have been widely adopted for semi-supervised learning on graphs. A recent study shows that the graph random neural network (GRAND) model can generate state-of-the-art performance for this problem. However, it is difficult for GRAND to handle large-scale graphs since its effectiveness relies on computationally expensive data augmentation procedures. In this work, we present a scalable and high-performance GNN framework GRAND+ for semi-supervised graph learning. To address the above issue, we develop a generalized forward push (GFPush) algorithm in GRAND+ to pre-compute a general propagation matrix and employ it to perform graph data augmentation in a mini-batch manner. We show that both the low time and space complexities of GFPush enable GRAND+ to efficiently scale to large graphs. Furthermore, we introduce a confidence-aware consistency loss into the model optimization of GRAND+, facilitating GRAND+'s generalization superiority. We conduct extensive experiments on seven public datasets of different sizes. The results demonstrate that GRAND+ 1) is able to scale to large graphs and costs less running time than existing scalable GNNs, and 2) can offer consistent accuracy improvements over both full-batch and scalable GNNs across all datasets."
286,https://arxiv.org/abs/2203.01044,SelfKG: Self-Supervised Entity Alignment in Knowledge Graphs,"Entity alignment, aiming to identify equivalent entities across different knowledge graphs (KGs), is a fundamental problem for constructing Web-scale KGs. Over the course of its development, the label supervision has been considered necessary for accurate alignments. Inspired by the recent progress of self-supervised learning, we explore the extent to which we can get rid of supervision for entity alignment. Commonly, the label information (positive entity pairs) is used to supervise the process of pulling the aligned entities in each positive pair closer. However, our theoretical analysis suggests that the learning of entity alignment can actually benefit more from pushing unlabeled negative pairs far away from each other than pulling labeled positive pairs close. By leveraging this discovery, we develop the self-supervised learning objective for entity alignment. We present SelfKG with efficient strategies to optimize this objective for aligning entities without label supervision. Extensive experiments on benchmark datasets demonstrate that SelfKG without supervision can match or achieve comparable results with state-of-the-art supervised baselines. The performance of SelfKG suggests that self-supervised learning offers great potential for entity alignment in KGs. The code and data are available at https://github.com/THUDM/SelfKG."
287,https://arxiv.org/abs/2202.07648,EvoKG: Jointly Modeling Event Time and Network Structure for Reasoning over Temporal Knowledge Graphs,"How can we perform knowledge reasoning over temporal knowledge graphs (TKGs)? TKGs represent facts about entities and their relations, where each fact is associated with a timestamp. Reasoning over TKGs, i.e., inferring new facts from time-evolving KGs, is crucial for many applications to provide intelligent services. However, despite the prevalence of real-world data that can be represented as TKGs, most methods focus on reasoning over static knowledge graphs, or cannot predict future events. In this paper, we present a problem formulation that unifies the two major problems that need to be addressed for an effective reasoning over TKGs, namely, modeling the event time and the evolving network structure. Our proposed method EvoKG jointly models both tasks in an effective framework, which captures the ever-changing structural and temporal dynamics in TKGs via recurrent event modeling, and models the interactions between entities based on the temporal neighborhood aggregation framework. Further, EvoKG achieves an accurate modeling of event time, using flexible and efficient mechanisms based on neural density estimation. Experiments show that EvoKG outperforms existing methods in terms of effectiveness (up to 77% and 116% more accurate time and link prediction) and efficiency."
288,https://arxiv.org/abs/2112.14936,"Are we really making much progress? Revisiting, benchmarking, and refining heterogeneous graph neural networks","Heterogeneous graph neural networks (HGNNs) have been blossoming in recent years, but the unique data processing and evaluation setups used by each work obstruct a full understanding of their advancements. In this work, we present a systematical reproduction of 12 recent HGNNs by using their official codes, datasets, settings, and hyperparameters, revealing surprising findings about the progress of HGNNs. We find that the simple homogeneous GNNs, e.g., GCN and GAT, are largely underestimated due to improper settings. GAT with proper inputs can generally match or outperform all existing HGNNs across various scenarios. To facilitate robust and reproducible HGNN research, we construct the Heterogeneous Graph Benchmark (HGB), consisting of 11 diverse datasets with three tasks. HGB standardizes the process of heterogeneous graph data splits, feature processing, and performance evaluation. Finally, we introduce a simple but very strong baseline Simple-HGN--which significantly outperforms all previous models on HGB--to accelerate the advancement of HGNNs in the future."
289,https://arxiv.org/abs/2112.04319,SCR: Training Graph Neural Networks with Consistency Regularization,"We present the SCR framework for enhancing the training of graph neural networks (GNNs) with consistency regularization. Regularization is a set of strategies used in Machine Learning to reduce overfitting and improve the generalization ability. However, it is unclear how to best design the generalization strategies in GNNs, as it works in a semi-supervised setting for graph data. The major challenge lies in how to efficiently balance the trade-off between the error from the labeled data and that from the unlabeled data. SCR is a simple yet general framework in which we introduce two strategies of consistency regularization to address the challenge above. One is to minimize the disagreements among the perturbed predictions by different versions of a GNN model. The other is to leverage the Mean Teacher paradigm to estimate a consistency loss between teacher and student models instead of the disagreement of the predictions. We conducted experiments on three large-scale node classification datasets in the Open Graph Benchmark (OGB). Experimental results demonstrate that the proposed SCR framework is a general one that can enhance various GNNs to achieve better performance. Finally, SCR has been the top-1 entry on all three OGB leaderboards as of this submission."
290,https://arxiv.org/abs/2111.04314,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,"Adversarial attacks on graphs have posed a major threat to the robustness of graph machine learning (GML) models. Naturally, there is an ever-escalating arms race between attackers and defenders. However, the strategies behind both sides are often not fairly compared under the same and realistic conditions. To bridge this gap, we present the Graph Robustness Benchmark (GRB) with the goal of providing a scalable, unified, modular, and reproducible evaluation for the adversarial robustness of GML models. GRB standardizes the process of attacks and defenses by 1) developing scalable and diverse datasets, 2) modularizing the attack and defense implementations, and 3) unifying the evaluation protocol in refined scenarios. By leveraging the GRB pipeline, the end-users can focus on the development of robust GML models with automated data processing and experimental evaluations. To support open and reproducible research on graph adversarial learning, GRB also hosts public leaderboards across different scenarios. As a starting point, we conduct extensive experiments to benchmark baseline techniques. GRB is open-source and welcomes contributions from the community. Datasets, codes, leaderboards are available at https://cogdl.ai/grb/home."
291,https://arxiv.org/abs/2108.07516,Graph Contrastive Learning for Anomaly Detection,"Graph-based anomaly detection has been widely used for detecting malicious activities in real-world applications. Existing attempts to address this problem have thus far focused on structural feature engineering or learning in the binary classification regime. In this work, we propose to leverage graph contrastive coding and present the supervised GraphCAD model for contrasting abnormal nodes with normal ones in terms of their distances to the global context (e.g., the average of all nodes). To handle scenarios with scarce labels, we further enable GraphCAD as a self-supervised framework by designing a graph corrupting strategy for generating synthetic node labels. To achieve the contrastive objective, we design a graph neural network encoder that can infer and further remove suspicious links during message passing, as well as learn the global context of the input graph. We conduct extensive experiments on four public datasets, demonstrating that 1) GraphCAD significantly and consistently outperforms various advanced baselines and 2) its self-supervised version without fine-tuning can achieve comparable performance with its fully supervised version."
292,https://arxiv.org/abs/2106.09395,A Self-supervised Method for Entity Alignment,"Entity alignment, aiming to identify equivalent entities across different knowledge graphs (KGs), is a fundamental problem for constructing large-scale KGs. Over the course of its development, supervision has been considered necessary for accurate alignments. Inspired by the recent progress of self-supervised learning, we explore the extent to which we can get rid of supervision for entity alignment. Existing supervised methods for this task focus on pulling each pair of positive (labeled) entities close to each other. However, our analysis suggests that the learning of entity alignment can actually benefit more from pushing sampled (unlabeled) negatives far away than pulling positive aligned pairs close. We present SelfKG by leveraging this discovery to design a contrastive learning strategy across two KGs. Extensive experiments on benchmark datasets demonstrate that SelfKG without supervision can match or achieve comparable results with state-of-the-art supervised baselines. The performance of SelfKG demonstrates self-supervised learning offers great potential for entity alignment in KGs."
293,https://arxiv.org/abs/2106.06663,TDGIA:Effective Injection Attacks on Graph Neural Networks,"Graph Neural Networks (GNNs) have achieved promising performance in various real-world applications. However, recent studies have shown that GNNs are vulnerable to adversarial attacks. In this paper, we study a recently-introduced realistic attack scenario on graphs -- graph injection attack (GIA). In the GIA scenario, the adversary is not able to modify the existing link structure and node attributes of the input graph, instead the attack is performed by injecting adversarial nodes into it. We present an analysis on the topological vulnerability of GNNs under GIA setting, based on which we propose the Topological Defective Graph Injection Attack (TDGIA) for effective injection attacks. TDGIA first introduces the topological defective edge selection strategy to choose the original nodes for connecting with the injected ones. It then designs the smooth feature optimization objective to generate the features for the injected nodes. Extensive experiments on large-scale datasets show that TDGIA can consistently and significantly outperform various attack baselines in attacking dozens of defense GNN models. Notably, the performance drop on target GNNs resultant from TDGIA is more than double the damage brought by the best attack solution among hundreds of submissions on KDD-CUP 2020."
294,https://arxiv.org/abs/2105.00152,Science as a Public Good: Public Use and Funding of Science,"Knowledge of how science is consumed in public domains is essential for a deeper understanding of the role of science in human society. While science is heavily supported by public funding, common depictions suggest that scientific research remains an isolated or 'ivory tower' activity, with weak connectivity to public use, little relationship between the quality of research and its public use, and little correspondence between the funding of science and its public use. This paper introduces a measurement framework to examine public good features of science, allowing us to study public uses of science, the public funding of science, and how use and funding relate. Specifically, we integrate five large-scale datasets that link scientific publications from all scientific fields to their upstream funding support and downstream public uses across three public domains - government documents, the news media, and marketplace invention. We find that the public uses of science are extremely diverse, with different public domains drawing distinctively across scientific fields. Yet amidst these differences, we find key forms of alignment in the interface between science and society. First, despite concerns that the public does not engage high-quality science, we find universal alignment, in each scientific field and public domain, between what the public consumes and what is highly impactful within science. Second, despite myriad factors underpinning the public funding of science, the resulting allocation across fields presents a striking alignment with the field's collective public use. Overall, public uses of science present a rich landscape of specialized consumption, yet collectively science and society interface with remarkable, quantifiable alignment between scientific use, public use, and funding."
295,https://arxiv.org/abs/2103.09430,OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs,"Enabling effective and efficient machine learning (ML) over large-scale graph data (e.g., graphs with billions of edges) can have a great impact on both industrial and scientific applications. However, existing efforts to advance large-scale graph ML have been largely limited by the lack of a suitable public benchmark. Here we present OGB Large-Scale Challenge (OGB-LSC), a collection of three real-world datasets for facilitating the advancements in large-scale graph ML. The OGB-LSC datasets are orders of magnitude larger than existing ones, covering three core graph learning tasks -- link prediction, graph regression, and node classification. Furthermore, we provide dedicated baseline experiments, scaling up expressive graph ML models to the massive datasets. We show that expressive models significantly outperform simple scalable baselines, indicating an opportunity for dedicated efforts to further improve graph ML at scale. Moreover, OGB-LSC datasets were deployed at ACM KDD Cup 2021 and attracted more than 500 team registrations globally, during which significant performance improvements were made by a variety of innovative techniques. We summarize the common techniques used by the winning solutions and highlight the current best practices in large-scale graph ML. Finally, we describe how we have updated the datasets after the KDD Cup to further facilitate research advances. The OGB-LSC datasets, baseline code, and all the information about the KDD Cup are available at https://ogb.stanford.edu/docs/lsc/ ."
296,https://arxiv.org/abs/2103.02930,"Understanding WeChat User Preferences and ""Wow"" Diffusion","WeChat is the largest social instant messaging platform in China, with 1.1 billion monthly active users. ""Top Stories"" is a novel friend-enhanced recommendation engine in WeChat, in which users can read articles based on preferences of both their own and their friends. Specifically, when a user reads an article by opening it, the ""click"" behavior is private. Moreover, if the user clicks the ""wow"" button, (only) her/his direct connections will be aware of this action/preference. Based on the unique WeChat data, we aim to understand user preferences and ""wow"" diffusion in Top Stories at different levels. We have made some interesting discoveries. For instance, the ""wow"" probability of one user is negatively correlated with the number of connected components that are formed by her/his active friends, but the click probability is the opposite. We further study to what extent users' ""wow"" and click behavior can be predicted from their social connections. To address this problem, we present a hierarchical graph representation learning based model DiffuseGNN, which is capable of capturing the structure-based social observations discovered above. Our experiments show that the proposed method can significantly improve the prediction performance compared with alternative methods."
297,https://arxiv.org/abs/2103.02410,OAG-BERT: Towards A Unified Backbone Language Model For Academic Knowledge Services,"Academic knowledge services have substantially facilitated the development of the science enterprise by providing a plenitude of efficient research tools. However, many applications highly depend on ad-hoc models and expensive human labeling to understand scientific contents, hindering deployments into real products. To build a unified backbone language model for different knowledge-intensive academic applications, we pre-train an academic language model OAG-BERT that integrates both the heterogeneous entity knowledge and scientific corpora in the Open Academic Graph (OAG) -- the largest public academic graph to date. In OAG-BERT, we develop strategies for pre-training text and entity data along with zero-shot inference techniques. In OAG-BERT, we develop strategies for pre-training text and entity data along with zero-shot inference techniques. Its zero-shot capability furthers the path to mitigate the need of expensive annotations. OAG-BERT has been deployed for real-world applications, such as the reviewer recommendation function for National Nature Science Foundation of China (NSFC) -- one of the largest funding agencies in China -- and paper tagging in AMiner. All codes and pre-trained models are available via the CogDL toolkit."
298,https://arxiv.org/abs/2103.00959,CogDL: A Comprehensive Library for Graph Deep Learning,"Graph neural networks (GNNs) have attracted tremendous attention from the graph learning community in recent years. It has been widely adopted in various real-world applications from diverse domains, such as social networks and biological graphs. The research and applications of graph deep learning present new challenges, including the sparse nature of graph data, complicated training of GNNs, and non-standard evaluation of graph tasks. To tackle the issues, we present CogDL, a comprehensive library for graph deep learning that allows researchers and practitioners to conduct experiments, compare methods, and build applications with ease and efficiency. In CogDL, we propose a unified design for the training and evaluation of GNN models for various graph tasks, making it unique among existing graph learning libraries. By utilizing this unified trainer, CogDL can optimize the GNN training loop with several training techniques, such as mixed precision training. Moreover, we develop efficient sparse operators for CogDL, enabling it to become the most competitive graph library for efficiency. Another important CogDL feature is its focus on ease of use with the aim of facilitating open and reproducible research of graph learning. We leverage CogDL to report and maintain benchmark results on fundamental graph tasks, which can be reproduced and directly used by the community."
299,https://arxiv.org/abs/2102.07349,MATCH: Metadata-Aware Text Classification in A Large Hierarchy,"Multi-label text classification refers to the problem of assigning each given document its most relevant labels from the label set. Commonly, the metadata of the given documents and the hierarchy of the labels are available in real-world applications. However, most existing studies focus on only modeling the text information, with a few attempts to utilize either metadata or hierarchy signals, but not both of them. In this paper, we bridge the gap by formalizing the problem of metadata-aware text classification in a large label hierarchy (e.g., with tens of thousands of labels). To address this problem, we present the MATCH solution -- an end-to-end framework that leverages both metadata and hierarchy information. To incorporate metadata, we pre-train the embeddings of text and metadata in the same space and also leverage the fully-connected attentions to capture the interrelations between them. To leverage the label hierarchy, we propose different ways to regularize the parameters and output probability of each child label by its parents. Extensive experiments on two massive text datasets with large-scale label hierarchies demonstrate the effectiveness of MATCH over state-of-the-art deep learning baselines."
300,https://arxiv.org/abs/2011.07682,A Large-Scale Database for Graph Representation Learning,"With the rapid emergence of graph representation learning, the construction of new large-scale datasets is necessary to distinguish model capabilities and accurately assess the strengths and weaknesses of each technique. By carefully analyzing existing graph databases, we identify 3 critical components important for advancing the field of graph representation learning: (1) large graphs, (2) many graphs, and (3) class diversity. To date, no single graph database offers all these desired properties. We introduce MalNet, the largest public graph database ever constructed, representing a large-scale ontology of malicious software function call graphs. MalNet contains over 1.2 million graphs, averaging over 15k nodes and 35k edges per graph, across a hierarchy of 47 types and 696 families. Compared to the popular REDDIT-12K database, MalNet offers 105x more graphs, 39x larger graphs on average, and 63x more classes. We provide a detailed analysis of MalNet, discussing its properties and provenance, along with the evaluation of state-of-the-art machine learning and graph neural network techniques. The unprecedented scale and diversity of MalNet offers exciting opportunities to advance the frontiers of graph representation learning--enabling new discoveries and research into imbalanced classification, explainability and the impact of class hardness. The database is publicly available at www.mal-net.org."
301,https://arxiv.org/abs/2006.15437,GPT-GNN: Generative Pre-Training of Graph Neural Networks,"Graph neural networks (GNNs) have been demonstrated to be powerful in modeling graph-structured data. However, training GNNs usually requires abundant task-specific labeled data, which is often arduously expensive to obtain. One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabeled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels. In this paper, we present the GPT-GNN framework to initialize GNNs by generative pre-training. GPT-GNN introduces a self-supervised attributed graph generation task to pre-train a GNN so that it can capture the structural and semantic properties of the graph. We factorize the likelihood of the graph generation into two components: 1) Attribute Generation and 2) Edge Generation. By modeling both components, GPT-GNN captures the inherent dependency between node attributes and graph structure during the generative process. Comprehensive experiments on the billion-scale Open Academic Graph and Amazon recommendation data demonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models without pre-training by up to 9.1% across various downstream tasks."
302,https://arxiv.org/abs/2006.09963,GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training,"Graph representation learning has emerged as a powerful technique for addressing real-world problems. Various downstream graph learning tasks have benefited from its recent developments, such as node classification, similarity search, and graph classification. However, prior arts on graph representation learning focus on domain specific problems and train a dedicated model for each graph dataset, which is usually non-transferable to out-of-domain data. Inspired by the recent advances in pre-training from natural language processing and computer vision, we design Graph Contrastive Coding (GCC) -- a self-supervised graph neural network pre-training framework -- to capture the universal network topological properties across multiple networks. We design GCC's pre-training task as subgraph instance discrimination in and across networks and leverage contrastive learning to empower graph neural networks to learn the intrinsic and transferable structural representations. We conduct extensive experiments on three graph learning tasks and ten graph datasets. The results show that GCC pre-trained on a collection of diverse datasets can achieve competitive or better performance to its task-specific and trained-from-scratch counterparts. This suggests that the pre-training and fine-tuning paradigm presents great potential for graph representation learning."
303,https://arxiv.org/abs/2005.11079,Graph Random Neural Network for Semi-Supervised Learning on Graphs,"We study the problem of semi-supervised learning on graphs, for which graph neural networks (GNNs) have been extensively explored. However, most existing GNNs inherently suffer from the limitations of over-smoothing, non-robustness, and weak-generalization when labeled nodes are scarce. In this paper, we propose a simple yet effective framework -- GRAPH RANDOM NEURAL NETWORKS (GRAND) -- to address these issues. In GRAND, we first design a random propagation strategy to perform graph data augmentation. Then we leverage consistency regularization to optimize the prediction consistency of unlabeled nodes across different data augmentations. Extensive experiments on graph benchmark datasets suggest that GRAND significantly outperforms state-of-the-art GNN baselines on semi-supervised node classification. Finally, we show that GRAND mitigates the issues of over-smoothing and non-robustness, exhibiting better generalization behavior than existing GNNs. The source code of GRAND is publicly available at https://github.com/Grand20/grand."
304,https://arxiv.org/abs/2005.00687,Open Graph Benchmark: Datasets for Machine Learning on Graphs,"We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale, encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at https://ogb.stanford.edu ."
305,https://arxiv.org/abs/2003.01332,Heterogeneous Graph Transformer,"Recent years have witnessed the emerging success of graph neural networks (GNNs) for modeling structured data. However, most GNNs are designed for homogeneous graphs, in which all nodes and edges belong to the same types, making them infeasible to represent heterogeneous structures. In this paper, we present the Heterogeneous Graph Transformer (HGT) architecture for modeling Web-scale heterogeneous graphs. To model heterogeneity, we design node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, empowering HGT to maintain dedicated representations for different types of nodes and edges. To handle dynamic heterogeneous graphs, we introduce the relative temporal encoding technique into HGT, which is able to capture the dynamic structural dependency with arbitrary durations. To handle Web-scale graph data, we design the heterogeneous mini-batch graph sampling algorithm---HGSampling---for efficient and scalable training. Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 9%--21% on various downstream tasks."
306,https://arxiv.org/abs/1906.11156,NetSMF: Large-Scale Network Embedding as Sparse Matrix Factorization,"We study the problem of large-scale network embedding, which aims to learn latent representations for network mining applications. Previous research shows that 1) popular network embedding benchmarks, such as DeepWalk, are in essence implicitly factorizing a matrix with a closed form, and 2)the explicit factorization of such matrix generates more powerful embeddings than existing methods. However, directly constructing and factorizing this matrix---which is dense---is prohibitively expensive in terms of both time and space, making it not scalable for large networks.
  In this work, we present the algorithm of large-scale network embedding as sparse matrix factorization (NetSMF). NetSMF leverages theories from spectral sparsification to efficiently sparsify the aforementioned dense matrix, enabling significantly improved efficiency in embedding learning. The sparsified matrix is spectrally close to the original dense one with a theoretically bounded approximation error, which helps maintain the representation power of the learned embeddings. We conduct experiments on networks of various scales and types. Results show that among both popular benchmarks and factorization based methods, NetSMF is the only method that achieves both high efficiency and effectiveness. We show that NetSMF requires only 24 hours to generate effective embeddings for a large-scale academic collaboration network with tens of millions of nodes, while it would cost DeepWalk months and is computationally infeasible for the dense matrix factorization solution. The source code of NetSMF is publicly available (https://github.com/xptree/NetSMF)."
307,https://arxiv.org/abs/1807.05560,DeepInf: Social Influence Prediction with Deep Learning,"Social and information networking activities such as on Facebook, Twitter, WeChat, and Weibo have become an indispensable part of our everyday life, where we can easily access friends' behaviors and are in turn influenced by them. Consequently, an effective social influence prediction for each user is critical for a variety of applications such as online recommendation and advertising.
  Conventional social influence prediction approaches typically design various hand-crafted rules to extract user- and network-specific features. However, their effectiveness heavily relies on the knowledge of domain experts. As a result, it is usually difficult to generalize them into different domains. Inspired by the recent success of deep neural networks in a wide range of computing applications, we design an end-to-end framework, DeepInf, to learn users' latent feature representation for predicting social influence. In general, DeepInf takes a user's local network as the input to a graph neural network for learning her latent social representation. We design strategies to incorporate both network structures and user-specific features into convolutional neural and attention networks. Extensive experiments on Open Academic Graph, Twitter, Weibo, and Digg, representing different types of social and information networks, demonstrate that the proposed end-to-end model, DeepInf, significantly outperforms traditional feature engineering-based approaches, suggesting the effectiveness of representation learning for social applications."
308,https://arxiv.org/abs/1806.03694,Collaboration Diversity and Scientific Impact,"The shift from individual effort to collaborative output has benefited science, with scientific work pursued collaboratively having increasingly led to more highly impactful research than that pursued individually. However, understanding of how the diversity of a collaborative team influences the production of knowledge and innovation is sorely lacking. Here, we study this question by breaking down the process of scientific collaboration of 32.9 million papers over the last five decades. We find that the probability of producing a top-cited publication increases as a function of the diversity of a team of collaborators---namely, the distinct number of institutions represented by the team. We discover striking phenomena where a smaller, yet more diverse team is more likely to generate highly innovative work than a relatively larger team within one institution. We demonstrate that the synergy of collaboration diversity is universal across different generations, research fields, and tiers of institutions and individual authors. Our findings suggest that collaboration diversity strongly and positively correlates with the production of scientific innovation, giving rise to the potential revolution of the policies used by funding agencies and authorities to fund research projects, and broadly the principles used to organize teams, organizations, and societies."
309,https://arxiv.org/abs/1802.04416,Neural Tensor Factorization,"Neural collaborative filtering (NCF) and recurrent recommender systems (RRN) have been successful in modeling user-item relational data. However, they are also limited in their assumption of static or sequential modeling of relational data as they do not account for evolving users' preference over time as well as changes in the underlying factors that drive the change in user-item relationship over time. We address these limitations by proposing a Neural Tensor Factorization (NTF) model for predictive tasks on dynamic relational data. The NTF model generalizes conventional tensor factorization from two perspectives: First, it leverages the long short-term memory architecture to characterize the multi-dimensional temporal interactions on relational data. Second, it incorporates the multi-layer perceptron structure for learning the non-linearities between different latent factors. Our extensive experiments demonstrate the significant improvement in rating prediction and link prediction on dynamic relational data by our NTF model over both neural network based factorization models and other traditional methods."
310,https://arxiv.org/abs/1710.02971,"Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec","Since the invention of word2vec, the skip-gram model has significantly advanced the research of network embedding, such as the recent emergence of the DeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of the aforementioned models with negative sampling can be unified into the matrix factorization framework with closed forms. Our analysis and proofs reveal that: (1) DeepWalk empirically produces a low-rank transformation of a network's normalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk when the size of vertices' context is set to one; (3) As an extension of LINE, PTE can be viewed as the joint factorization of multiple networks' Laplacians; (4) node2vec is factorizing a matrix related to the stationary distribution and transition probability tensor of a 2nd-order random walk. We further provide the theoretical connections between skip-gram based network embedding algorithms and the theory of graph Laplacian. Finally, we present the NetMF method as well as its approximation algorithm for computing network embedding. Our method offers significant improvements over DeepWalk and LINE for conventional network mining tasks. This work lays the theoretical foundation for skip-gram based network embedding methods, leading to a better understanding of latent network representation learning."
311,https://arxiv.org/abs/1704.05150,"A Century of Science: Globalization of Scientific Collaborations, Citations, and Innovations","Progress in science has advanced the development of human society across history, with dramatic revolutions shaped by information theory, genetic cloning, and artificial intelligence, among the many scientific achievements produced in the 20th century. However, the way that science advances itself is much less well-understood. In this work, we study the evolution of scientific development over the past century by presenting an anatomy of 89 million digitalized papers published between 1900 and 2015. We find that science has benefited from the shift from individual work to collaborative effort, with over 90% of the world-leading innovations generated by collaborations in this century, nearly four times higher than they were in the 1900s. We discover that rather than the frequent myopic- and self-referencing that was common in the early 20th century, modern scientists instead tend to look for literature further back and farther around. Finally, we also observe the globalization of scientific development from 1900 to 2015, including 25-fold and 7-fold increases in international collaborations and citations, respectively, as well as a dramatic decline in the dominant accumulation of citations by the US, the UK, and Germany, from ~95% to ~50% over the same period. Our discoveries are meant to serve as a starter for exploring the visionary ways in which science has developed throughout the past century, generating insight into and an impact upon the current scientific innovations and funding policies."
312,https://arxiv.org/abs/1611.00568,"Analysis of Link Formation, Persistence and Dissolution in NetSense Data","We study a unique behavioral network data set (based on periodic surveys and on electronic logs of dyadic contact via smartphones) collected at the University of Notre Dame.The participants are a sample of members of the entering class of freshmen in the fall of 2011 whose opinions on a wide variety of political and social issues and activities on campus were regularly recorded - at the beginning and end of each semester - for the first three years of their residence on campus. We create a communication activity network implied by call and text data, and a friendship network based on surveys. Both networks are limited to students participating in the NetSense surveys. We aim at finding student traits and activities on which agreements correlate well with formation and persistence of links while disagreements are highly correlated with non-existence or dissolution of links in the two social networks that we created. Using statistical analysis and machine learning, we observe several traits and activities displaying such correlations, thus being of potential use to predict social network evolution."
313,https://arxiv.org/abs/1607.06740,"Gender Differences in Communication Behaviors, Spatial Proximity Patterns, and Mobility Habits","The existence of gender differences in the structure and composition of social networks is a well established finding in the social and behavioral sciences, but researchers continue to debate whether structural, dispositional, or life course factors are the primary driver of these differences. In this paper we extend work on gender differences in social networks to patterns of interaction, propinquity, and connectivity captured via a social sensing platform comprised of an ensemble of individuals' phone calls, text messaging, face-to-face interactions, and traces of their mobility activities. We attempt to isolate dispositional from other factors by focusing on a relatively homogeneous population on a relatively closed setting at the same stage in the life course. Analysis across three different networks along with mobility data reveals systematic gender differences in terms of communicative, distributional, mobility, and spatial proximity tendencies. Most importantly, some patterns observed in the communication network (e.g. stronger same-gender preference for women) are found to be reversed in the spatial proximity context, with men displaying a greater tendency to spend time in a narrower (and thus more predictable) range of spaces with same-gender peers than women. These findings provide robust and novel evidence for the powerful effect of gender in structuring behavior across a wide range of communication and mobility behaviors, validating and extending recent work pointing to differences of socio-cultural and evolutionary origin in the styles of sociability and interaction characteristic of men and women."
314,https://arxiv.org/abs/1606.07556,"Do the Young Live in a ""Smaller World"" Than the Old? Age-Specific Degrees of Separation in Human Communication","In this paper, we investigate the phenomenon of ""age-specific small worlds"" using data from a large-scale mobile communication network approximating interaction patterns at societal scale. Rather than asking whether two random individuals are separated by a small number of links, we ask whether individuals in specific age groups live in a small world in relation to individuals from other age groups. Our analysis shows that there is systematic variation in this age-relative small world effect. Young people live in the ""smallest world,"" being separated from other young people and their parent's generation via a smaller number of intermediaries than older individuals. The oldest people live in the ""least small world,"" being separated from their same age peers and their younger counterparts by a larger number of intermediaries. Variation in the small world effect is specific to age as a node attribute (being absent in the case of gender) and is consistently observed under several data robustness checks. The discovery of age-specific small worlds is consistent with well-known social mechanisms affecting the way age interacts with network connectivity and the relative prevalence of kin ties and non-kin ties observed in this network. This social pattern has significant implications for our understanding of generation-specific dynamics of information cascades, diffusion phenomena, and the spread of fads and fashions."
315,https://arxiv.org/abs/1606.05905,Can Scientific Impact Be Predicted?,"A widely used measure of scientific impact is citations. However, due to their heavy-tailed distribution, citations are fundamentally difficult to predict. Instead, to characterize scientific impact, we address two analogous questions asked by many scientific researchers: ""How will my h-index evolve over time, and which of my previously or newly published papers will contribute to it?"" To answer these questions, we perform two related tasks. First, we develop a model to predict authors' future h-indices based on their current scientific impact. Second, we examine the factors that drive papers---either previously or newly published---to increase their authors' predicted future h-indices. By leveraging relevant factors, we can predict an author's h-index in five years with an R2 value of 0.92 and whether a previously (newly) published paper will contribute to this future h-index with an F1 score of 0.99 (0.77). We find that topical authority and publication venue are crucial to these effective predictions, while topic popularity is surprisingly inconsequential. Further, we develop an online tool that allows users to generate informed h-index predictions. Our work demonstrates the predictability of scientific impact, and can help scholars to effectively leverage their position of ""standing on the shoulders of giants."""
316,https://arxiv.org/abs/1605.08410,Influence Activation Model: A New Perspective in Social Influence Analysis and Social Network Evolution,"What drives the propensity for the social network dynamics? Social influence is believed to drive both off-line and on-line human behavior, however it has not been considered as a driver of social network evolution. Our analysis suggest that, while the network structure affects the spread of influence in social networks, the network is in turn shaped by social influence activity (i.e., the process of social influence wherein one person's attitudes and behaviors affect another's). To that end, we develop a novel model of network evolution where the dynamics of network follow the mechanism of influence propagation, which are not captured by the existing network evolution models. Our experiments confirm the predictions of our model and demonstrate the important role that social influence can play in the process of network evolution. As well exploring the reason of social network evolution, different genres of social influence have been spotted having different effects on the network dynamics. These findings and methods are essential to both our understanding of the mechanisms that drive network evolution and our knowledge of the role of social influence in shaping the network structure."
317,https://arxiv.org/abs/1602.07048,Structural Diversity and Homophily: A Study Across More than One Hundred Big Networks,"A widely recognized organizing principle of networks is structural homophily, which suggests that people with more common neighbors are more likely to connect with each other. However, what influence the diverse structures embedded in common neighbors have on link formation is much less well-understood. To explore this problem, we begin by characterizing the structural diversity of common neighborhoods. Using a collection of 120 large-scale networks, we demonstrate that the impact of the common neighborhood diversity on link existence can vary substantially across networks. We find that its positive effect on Facebook and negative effect on LinkedIn suggest different underlying networking needs in these networks. We also discover striking cases where diversity violates the principle of homophily---that is, where fewer mutual connections may lead to a higher tendency to link with each other. We then leverage structural diversity to develop a common neighborhood signature (CNS), which we apply to a large set of networks to uncover unique network superfamilies not discoverable by conventional methods. Our findings shed light on the pursuit to understand the ways in which network structures are organized and formed, pointing to potential advancement in designing graph generation models and recommender systems."
318,https://arxiv.org/abs/1511.02562,Modeling the Interplay Between Individual Behavior and Network Distributions,"It is well-known that many networks follow a power-law degree distribution; however, the factors that influence the formation of their distributions are still unclear. How can one model the connection between individual actions and network distributions? How can one explain the formation of group phenomena and their evolutionary patterns?
  In this paper, we propose a unified framework, M3D, to model human dynamics in social networks from three perspectives: macro, meso, and micro. At the micro-level, we seek to capture the way in which an individual user decides whether to perform an action. At the meso-level, we study how group behavior develops and evolves over time, based on individual actions. At the macro-level, we try to understand how network distributions such as power-law (or heavy-tailed phenomena) can be explained by group behavior. We provide theoretical analysis for the proposed framework, and discuss the connection of our framework with existing work.
  The framework offers a new, flexible way to explain the interplay between individual user actions and network distributions, and can benefit many applications. To model heavy-tailed distributions from partially observed individual actions and to predict the formation of group behaviors, we apply M3D to three different genres of networks: Tencent Weibo, Citation, and Flickr. We also use information-burst prediction as a particular application to quantitatively evaluate the predictive power of the proposed framework. Our results on the Weibo indicate that M3D's prediction performance exceeds that of several alternative methods by up to 30\%."
319,https://arxiv.org/abs/1504.03643,Inferring Unusual Crowd Events From Mobile Phone Call Detail Records,"The pervasiveness and availability of mobile phone data offer the opportunity of discovering usable knowledge about crowd behaviors in urban environments. Cities can leverage such knowledge in order to provide better services (e.g., public transport planning, optimized resource allocation) and safer cities. Call Detail Record (CDR) data represents a practical data source to detect and monitor unusual events considering the high level of mobile phone penetration, compared with GPS equipped and open devices. In this paper, we provide a methodology that is able to detect unusual events from CDR data that typically has low accuracy in terms of space and time resolution. Moreover, we introduce a concept of unusual event that involves a large amount of people who expose an unusual mobility behavior. Our careful consideration of the issues that come from coarse-grained CDR data ultimately leads to a completely general framework that can detect unusual crowd events from CDR data effectively and efficiently. Through extensive experiments on real-world CDR data for a large city in Africa, we demonstrate that our method can detect unusual events with 16% higher recall and over 10 times higher precision, compared to state-of-the-art methods. We implement a visual analytics prototype system to help end users analyze detected unusual crowd events to best suit different application scenarios. To the best of our knowledge, this is the first work on the detection of unusual events from CDR data with considerations of its temporal and spatial sparseness and distinction between user unusual activities and daily routines."
320,https://arxiv.org/abs/1412.4754,Will This Paper Increase Your h-index? Scientific Impact Prediction,"Scientific impact plays a central role in the evaluation of the output of scholars, departments, and institutions. A widely used measure of scientific impact is citations, with a growing body of literature focused on predicting the number of citations obtained by any given publication. The effectiveness of such predictions, however, is fundamentally limited by the power-law distribution of citations, whereby publications with few citations are extremely common and publications with many citations are relatively rare. Given this limitation, in this work we instead address a related question asked by many academic researchers in the course of writing a paper, namely: ""Will this paper increase my h-index?"" Using a real academic dataset with over 1.7 million authors, 2 million papers, and 8 million citation relationships from the premier online academic service ArnetMiner, we formalize a novel scientific impact prediction problem to examine several factors that can drive a paper to increase the primary author's h-index. We find that the researcher's authority on the publication topic and the venue in which the paper is published are crucial factors to the increase of the primary author's h-index, while the topic popularity and the co-authors' h-indices are of surprisingly little relevance. By leveraging relevant factors, we find a greater than 87.5% potential predictability for whether a paper will contribute to an author's h-index within five years. As a further experiment, we generate a self-prediction for this paper, estimating that there is a 76% probability that it will contribute to the h-index of the co-author with the highest current h-index in five years. We conclude that our findings on the quantification of scientific impact can help researchers to expand their influence and more effectively leverage their position of ""standing on the shoulders of giants."""
321,https://arxiv.org/abs/1412.2269,Predicting Node Degree Centrality with the Node Prominence Profile,"Centrality of a node measures its relative importance within a network. There are a number of applications of centrality, including inferring the influence or success of an individual in a social network, and the resulting social network dynamics. While we can compute the centrality of any node in a given network snapshot, a number of applications are also interested in knowing the potential importance of an individual in the future. However, current centrality is not necessarily an effective predictor of future centrality. While there are different measures of centrality, we focus on degree centrality in this paper. We develop a method that reconciles preferential attachment and triadic closure to capture a node's prominence profile. We show that the proposed node prominence profile method is an effective predictor of degree centrality. Notably, our analysis reveals that individuals in the early stage of evolution display a distinctive and robust signature in degree centrality trend, adequately predicted by their prominence profile. We evaluate our work across four real-world social networks. Our findings have important implications for the applications that require prediction of a node's future degree centrality, as well as the study of social network dynamics."
322,https://arxiv.org/abs/1404.3708,Inferring Social Status and Rich Club Effects in Enterprise Communication Networks,"Social status, defined as the relative rank or position that an individual holds in a social hierarchy, is known to be among the most important motivating forces in social behaviors. In this paper, we consider the notion of status from the perspective of a position or title held by a person in an enterprise. We study the intersection of social status and social networks in an enterprise. We study whether enterprise communication logs can help reveal how social interactions and individual status manifest themselves in social networks. To that end, we use two enterprise datasets with three communication channels --- voice call, short message, and email --- to demonstrate the social-behavioral differences among individuals with different status. We have several interesting findings and based on these findings we also develop a model to predict social status. On the individual level, high-status individuals are more likely to be spanned as structural holes by linking to people in parts of the enterprise networks that are otherwise not well connected to one another. On the community level, the principle of homophily, social balance and clique theory generally indicate a ""rich club"" maintained by high-status individuals, in the sense that this community is much more connected, balanced and dense. Our model can predict social status of individuals with 93% accuracy."
323,https://arxiv.org/abs/1310.1525,Microscopic Evolution of Social Networks by Triad Position Profile,"Disentangling the mechanisms underlying the social network evolution is one of social science's unsolved puzzles. Preferential attachment is a powerful mechanism explaining social network dynamics, yet not able to explain all scaling-laws in social networks. Recent advances in understanding social network dynamics demonstrate that several scaling-laws in social networks follow as natural consequences of triadic closure. Macroscopic comparisons between them are discussed empirically in many works. However the network evolution drives not only the emergence of macroscopic scaling but also the microscopic behaviors. Here we exploit two fundamental aspects of the network microscopic evolution: the individual influence evolution and the process of link formation. First we develop a novel framework for the microscopic evolution, where the mechanisms of preferential attachment and triadic closure are well balanced. Then on four real-world datasets we apply our approach for two microscopic problems: node's prominence prediction and link prediction, where our method yields significant predictive improvement over baseline solutions. Finally to be rigorous and comprehensive, we further observe that our framework has a stronger generalization capacity across different kinds of social networks for two microscopic prediction problems. We unveil the significant factors with a greater degree of precision than has heretofore been possible, and shed new light on networks evolution."
324,https://arxiv.org/abs/2301.08530,Self-Organization Towards $1/f$ Noise in Deep Neural Networks,"Despite $1/f$ noise being ubiquitous in both natural and artificial systems, no general explanations for the phenomenon have received widespread acceptance. One well-known system where $1/f$ noise has been observed in is the human brain, with this 'noise' proposed by some to be important to the healthy function of the brain. As deep neural networks (DNNs) are loosely modelled after the human brain, and as they start to achieve human-level performance in specific tasks, it might be worth investigating if the same $1/f$ noise is present in these artificial networks as well. Indeed, we find the existence of $1/f$ noise in DNNs - specifically Long Short-Term Memory (LSTM) networks modelled on real world dataset - by measuring the Power Spectral Density (PSD) of different activations within the network in response to a sequential input of natural language. This was done in analogy to the measurement of $1/f$ noise in human brains with techniques such as electroencephalography (EEG) and functional Magnetic Resonance Imaging (fMRI). We further examine the exponent values in the $1/f$ noise in ""inner"" and ""outer"" activations in the LSTM cell, finding some resemblance in the variations of the exponents in the fMRI signal. In addition, comparing the values of the exponent at ""rest"" compared to when performing ""tasks"" of the LSTM network, we find a similar trend to that of the human brain where the exponent while performing tasks is less negative."
325,https://arxiv.org/abs/2301.05797,FedSSC: Shared Supervised-Contrastive Federated Learning,"Federated learning is widely used to perform decentralized training of a global model on multiple devices while preserving the data privacy of each device. However, it suffers from heterogeneous local data on each training device which increases the difficulty to reach the same level of accuracy as the centralized training. Supervised Contrastive Learning which outperform cross-entropy tries to minimizes the difference between feature space of points belongs to the same class and pushes away points from different classes. We propose Supervised Contrastive Federated Learning in which devices can share the learned class-wise feature spaces with each other and add the supervised-contrastive learning loss as a regularization term to foster the feature space learning. The loss tries to minimize the cosine similarity distance between the feature map and the averaged feature map from another device in the same class and maximizes the distance between the feature map and that in a different class. This new regularization term when added on top of the moon regularization term is found to outperform the other state-of-the-art regularization terms in solving the heterogeneous data distribution problem."
326,https://arxiv.org/abs/2201.06225,Interactive Contrastive Learning for Self-supervised Entity Alignment,"Self-supervised entity alignment (EA) aims to link equivalent entities across different knowledge graphs (KGs) without seed alignments. The current SOTA self-supervised EA method draws inspiration from contrastive learning, originally designed in computer vision based on instance discrimination and contrastive loss, and suffers from two shortcomings. Firstly, it puts unidirectional emphasis on pushing sampled negative entities far away rather than pulling positively aligned pairs close, as is done in the well-established supervised EA. Secondly, KGs contain rich side information (e.g., entity description), and how to effectively leverage those information has not been adequately investigated in self-supervised EA. In this paper, we propose an interactive contrastive learning model for self-supervised EA. The model encodes not only structures and semantics of entities (including entity name, entity description, and entity neighborhood), but also conducts cross-KG contrastive learning by building pseudo-aligned entity pairs. Experimental results show that our approach outperforms previous best self-supervised results by a large margin (over 9% average improvement) and performs on par with previous SOTA supervised counterparts, demonstrating the effectiveness of the interactive contrastive learning for self-supervised EA."
327,https://arxiv.org/abs/2107.09437,Edge of chaos as a guiding principle for modern neural network training,"The success of deep neural networks in real-world problems has prompted many attempts to explain their training dynamics and generalization performance, but more guiding principles for the training of neural networks are still needed. Motivated by the edge of chaos principle behind the optimal performance of neural networks, we study the role of various hyperparameters in modern neural network training algorithms in terms of the order-chaos phase diagram. In particular, we study a fully analytical feedforward neural network trained on the widely adopted Fashion-MNIST dataset, and study the dynamics associated with the hyperparameters in back-propagation during the training process. We find that for the basic algorithm of stochastic gradient descent with momentum, in the range around the commonly used hyperparameter values, clear scaling relations are present with respect to the training time during the ordered phase in the phase diagram, and the model's optimal generalization power at the edge of chaos is similar across different training parameter combinations. In the chaotic phase, the same scaling no longer exists. The scaling allows us to choose the training parameters to achieve faster training without sacrificing performance. In addition, we find that the commonly used model regularization method - weight decay - effectively pushes the model towards the ordered phase to achieve better performance. Leveraging on this fact and the scaling relations in the other hyperparameters, we derived a principled guideline for hyperparameter determination, such that the model can achieve optimal performance by saturating it at the edge of chaos. Demonstrated on this simple neural network model and training algorithm, our work improves the understanding of neural network training dynamics, and can potentially be extended to guiding principles of more complex model architectures and algorithms."
328,https://arxiv.org/abs/2103.09390,Identify Hidden Spreaders of Pandemic over Contact Tracing Networks,"The COVID-19 infection cases have surged globally, causing devastations to both the society and economy. A key factor contributing to the sustained spreading is the presence of a large number of asymptomatic or hidden spreaders, who mix among the susceptible population without being detected or quarantined. Here we propose an effective non-pharmacological intervention method of detecting the asymptomatic spreaders in contact-tracing networks, and validated it on the empirical COVID-19 spreading network in Singapore. We find that using pure physical spreading equations, the hidden spreaders of COVID-19 can be identified with remarkable accuracy. Specifically, based on the unique characteristics of COVID-19 spreading dynamics, we propose a computational framework capturing the transition probabilities among different infectious states in a network, and extend it to an efficient algorithm to identify asymptotic individuals. Our simulation results indicate that a screening method using our prediction outperforms machine learning algorithms, e.g. graph neural networks, that are designed as baselines in this work, as well as random screening of infection's closest contacts widely used by China in its early outbreak. Furthermore, our method provides high precision even with incomplete information of the contract-tracing networks. Our work can be of critical importance to the non-pharmacological interventions of COVID-19, especially with increasing adoptions of contact tracing measures using various new technologies. Beyond COVID-19, our framework can be useful for other epidemic diseases that also feature asymptomatic spreading"
329,https://arxiv.org/abs/2012.09123,Building and Using Personal Knowledge Graph to Improve Suicidal Ideation Detection on Social Media,"A large number of individuals are suffering from suicidal ideation in the world. There are a number of causes behind why an individual might suffer from suicidal ideation. As the most popular platform for self-expression, emotion release, and personal interaction, individuals may exhibit a number of symptoms of suicidal ideation on social media. Nevertheless, challenges from both data and knowledge aspects remain as obstacles, constraining the social media-based detection performance. Data implicitness and sparsity make it difficult to discover the inner true intentions of individuals based on their posts. Inspired by psychological studies, we build and unify a high-level suicide-oriented knowledge graph with deep neural networks for suicidal ideation detection on social media. We further design a two-layered attention mechanism to explicitly reason and establish key risk factors to individual's suicidal ideation. The performance study on microblog and Reddit shows that: 1) with the constructed personal knowledge graph, the social media-based suicidal ideation detection can achieve over 93% accuracy; and 2) among the six categories of personal factors, post, personality, and experience are the top-3 key indicators. Under these categories, posted text, stress level, stress duration, posted image, and ruminant thinking contribute to one's suicidal ideation detection."
330,https://arxiv.org/abs/2011.14034,Induced Percolation on Networked Systems,"Percolation theory has been widely used to study phase transitions in complex networked systems. It has also successfully explained several macroscopic phenomena across different fields. Yet, the existent theoretical framework for percolation places the focus on the direct interactions among the system's components, while recent empirical observations have shown that indirect interactions are common in many systems like ecological and social networks, among others. Here, we propose a new percolation framework that accounts for indirect interactions, which allows to generalize the current theoretical body and understand the role of the underlying indirect influence of the components of a networked system on its macroscopic behavior. We report a rich phenomenology in which first-order, second-order or hybrid phase transitions are possible depending on whether the links of the substrate network are directed, undirected or a mix, respectively. We also present an analytical framework to characterize the proposed induced percolation, paving the way to further understand network dynamics with indirect interactions."
331,https://arxiv.org/abs/2002.04759,Collaborative Inference for Efficient Remote Monitoring,"While current machine learning models have impressive performance over a wide range of applications, their large size and complexity render them unsuitable for tasks such as remote monitoring on edge devices with limited storage and computational power. A naive approach to resolve this on the model level is to use simpler architectures, but this sacrifices prediction accuracy and is unsuitable for monitoring applications requiring accurate detection of the onset of adverse events. In this paper, we propose an alternative solution to this problem by decomposing the predictive model as the sum of a simple function which serves as a local monitoring tool, and a complex correction term to be evaluated on the server. A sign requirement is imposed on the latter to ensure that the local monitoring function is safe, in the sense that it can effectively serve as an early warning system. Our analysis quantifies the trade-offs between model complexity and performance, and serves as a guidance for architecture design. We validate our proposed framework on a series of monitoring experiments, where we succeed at learning monitoring models with significantly reduced complexity that minimally violate the safety requirement. More broadly, our framework is useful for learning classifiers in applications where false negatives are significantly more costly compared to false positives."
332,https://arxiv.org/abs/1910.12038,Latent Suicide Risk Detection on Microblog via Suicide-Oriented Word Embeddings and Layered Attention,"Despite detection of suicidal ideation on social media has made great progress in recent years, people's implicitly and anti-real contrarily expressed posts still remain as an obstacle, constraining the detectors to acquire higher satisfactory performance. Enlightened by the hidden ""tree holes"" phenomenon on microblog, where people at suicide risk tend to disclose their inner real feelings and thoughts to the microblog space whose authors have committed suicide, we explore the use of tree holes to enhance microblog-based suicide risk detection from the following two perspectives. (1) We build suicide-oriented word embeddings based on tree hole contents to strength the sensibility of suicide-related lexicons and context based on tree hole contents. (2) A two-layered attention mechanism is deployed to grasp intermittently changing points from individual's open blog streams, revealing one's inner emotional world more or less. Our experimental results show that with suicide-oriented word embeddings and attention, microblog-based suicide risk detection can achieve over 91\% accuracy. A large-scale well-labelled suicide data set is also reported in the paper."
333,https://arxiv.org/abs/1909.05176,Optimal Machine Intelligence at the Edge of Chaos,"It has long been suggested that the biological brain operates at some critical point between two different phases, possibly order and chaos. Despite many indirect empirical evidence from the brain and analytical indication on simple neural networks, the foundation of this hypothesis on generic non-linear systems remains unclear. Here we develop a general theory that reveals the exact edge of chaos is the boundary between the chaotic phase and the (pseudo)periodic phase arising from Neimark-Sacker bifurcation. This edge is analytically determined by the asymptotic Jacobian norm values of the non-linear operator and influenced by the dimensionality of the system. The optimality at the edge of chaos is associated with the highest information transfer between input and output at this point similar to that of the logistic map. As empirical validations, our experiments on the various deep learning models in computer vision demonstrate the optimality of the models near the edge of chaos, and we observe that the state-of-art training algorithms push the models towards such edge as they become more accurate. We further establishes the theoretical understanding of deep learning model generalization through asymptotic stability."
334,https://arxiv.org/abs/1611.00212,Non-trivial Resource Amount Requirement in the Early Stage for Containing Fatal Diseases,"During an epidemic control, the containment of the disease is usually achieved through increasing devoted resource to shorten the duration of infectiousness. However, the impact of this resource expenditure has not been studied quantitatively. Using the well-documented cholera data, we observe empirically that the recovery rate which is related to the duration of infectiousness has a strong positive correlation with the average resource devoted to the infected individuals. By incorporating this relation we build a novel model and find that insufficient resource leads to an abrupt increase in the infected population size, which is in marked contrast with the continuous phase transitions believed previously. Counterintuitively, this abrupt phase transition is more pronounced in the less contagious diseases, which usually correspond to the most fatal ones. Furthermore, we find that even for a single infection source, public resource needs to meet a significant amount, which is proportional to the whole population size to ensure epidemic containment. Our findings provide a theoretical foundation for efficient epidemic containment strategies in the early stage."
335,https://arxiv.org/abs/1601.07273,A Method to Support Difficult Re-finding Tasks,"Re-finding electronic documents from a personal computer is a frequent demand to users. In a simple re-finding task, people can use many methods to retrieve a document, such as navigating directly to the document's folder, searching with a desktop search engine, or checking the Recent Files List. However, when encountering a difficult re-finding task, people usually cannot remember the attributes used by conventional re-finding methods, such as file path, file name, keywords etc., the re-finding would fail. We propose a new method to support difficult re-finding tasks. When a user is reading a document, we collect all kinds of possible memory pieces of the user about the document, such as number of pages, number of images, number of math formulas, cumulative reading time, reading frequency, printing experiences etc. If the user wants to re-find a document later, we use these collected attributes to filter out the target document. To alleviate the user's cognitive burden, we use a question and answer wizard interface and provide recommendations to the answers for the user, the recommendations are generated by analyzing the collected attributes of each document and the user's experiences about them."
336,https://arxiv.org/abs/1509.03484,Local structure can identify and quantify influential global spreaders in large scale social networks,"Measuring and optimizing the influence of nodes in big-data online social networks are important for many practical applications, such as the viral marketing and the adoption of new products. As the viral spreading on social network is a global process, it is commonly believed that measuring the influence of nodes inevitably requires the knowledge of the entire network. Employing percolation theory, we show that the spreading process displays a nucleation behavior: once a piece of information spread from the seeds to more than a small characteristic number of nodes, it reaches a point of no return and will quickly reach the percolation cluster, regardless of the entire network structure, otherwise the spreading will be contained locally. Thus, we find that, without the knowledge of entire network, any nodes' global influence can be accurately measured using this characteristic number, which is independent of the network size. This motivates an efficient algorithm with constant time complexity on the long standing problem of best seed spreaders selection, with performance remarkably close to the true optimum."
337,https://arxiv.org/abs/1502.01601,A Simplified Self-Consistent Probabilities Framework to Characterize Percolation Phenomena on Interdependent Networks : An Overview,"Interdependent networks are ubiquitous in our society, ranging from infrastructure to economics, and the study of their cascading behaviors using percolation theory has attracted much attention in the recent years. To analyze the percolation phenomena of these systems, different mathematical frameworks have been proposed including generating functions, eigenvalues among some others. These different frameworks approach the phase transition behaviors from different angles, and have been very successful in shaping the different quantities of interest including critical threshold, size of the giant component, order of phase transition and the dynamics of cascading. These methods also vary in their mathematical complexity in dealing with interdependent networks that have additional complexity in terms of the correlation among different layers of networks or links. In this work, we review a particular approach of simple self-consistent probability equations, and illustrate that it can greatly simplify the mathematical analysis for systems ranging from single layer network to various different interdependent networks. We give an overview on the detailed framework to study the nature of the critical phase transition, value of the critical threshold and size of the giant component for these different systems."
338,https://arxiv.org/abs/1410.1668,Competing for Attention in Social Media under Information Overload Conditions,"Although the many forms of modern social media have become major channels for the dissemination of information, they are becoming overloaded because of the rapidly-expanding number of information feeds. We analyze the expanding user-generated content in Sina Weibo, the largest micro-blog site in China, and find evidence that popular messages often follow a mechanism that differs from that found in the spread of disease, in contrast to common believe. In this mechanism, an individual with more friends needs more repeated exposures to spread further the information. Moreover, our data suggest that in contrast to epidemics, for certain messages the chance of an individual to share the message is proportional to the fraction of its neighbours who shared it with him/her. Thus the greater the number of friends an individual has the greater the number of repeated contacts needed to spread the message, which is a result of competition for attention. We model this process using a fractional susceptible infected recovered (FSIR) model, where the infection probability of a node is proportional to its fraction of infected neighbors. Our findings have dramatic implications for information contagion. For example, using the FSIR model we find that real-world social networks have a finite epidemic threshold. This is in contrast to the zero threshold that conventional wisdom derives from disease epidemic models. This means that when individuals are overloaded with excess information feeds, the information either reaches out the population if it is above the critical epidemic threshold, or it would never be well received, leading to only a handful of information contents that can be widely spread throughout the population."
339,https://arxiv.org/abs/1404.3461,A 2D based Partition Strategy for Solving Ranking under Team Context (RTP),"In this paper, we propose a 2D based partition method for solving the problem of Ranking under Team Context(RTC) on datasets without a priori. We first map the data into 2D space using its minimum and maximum value among all dimensions. Then we construct window queries with consideration of current team context. Besides, during the query mapping procedure, we can pre-prune some tuples which are not top ranked ones. This pre-classified step will defer processing those tuples and can save cost while providing solutions for the problem. Experiments show that our algorithm performs well especially on large datasets with correctness."
340,https://arxiv.org/abs/1401.2018,On the Real-time Prediction Problems of Bursting Hashtags in Twitter,"Hundreds of thousands of hashtags are generated every day on Twitter. Only a few become bursting topics. Among the few, only some can be predicted in real-time. In this paper, we take the initiative to conduct a systematic study of a series of challenging real-time prediction problems of bursting hashtags. Which hashtags will become bursting? If they do, when will the burst happen? How long will they remain active? And how soon will they fade away? Based on empirical analysis of real data from Twitter, we provide insightful statistics to answer these questions, which span over the entire lifecycles of hashtags."
341,https://arxiv.org/abs/1312.5148,Object Selection under Team Context,"Context-aware database has drawn increasing attention from both industry and academia recently by taking users' current situation and environment into consideration. However, most of the literature focus on individual context, overlooking the team users. In this paper, we investigate how to integrate team context into database query process to help the users' get top-ranked database tuples and make the team more competitive. We introduce naive and optimized query algorithm to select the suitable records and show that they output the same results while the latter is more computational efficient. Extensive empirical studies are conducted to evaluate the query approaches and demonstrate their effectiveness and efficiency."
342,https://arxiv.org/abs/1207.2541,A New Weighted Spearman's Footrule as A Measure of Distance between Rankings,"Many applications motivate the distance measure between rankings, such as comparing top-k lists and rank aggregation for voting, and intrigue great interest to researchers. For example, for a search engine, the use of different ranking algorithms may return different ranking lists. The effect of a ranking algorithm can be estimated by computing the distance (similarity) between the result ranking it returns and the appropriate ranking people expect. People may be interested in only the first few items of result ranking, therefore the metric for measuring the distance should emphasize on the items in higher positions. Besides, in an extreme case, if a result ranking is the total reverse of the expected ranking, then it is considered to be the worst ranking with the maximum distance. Therefore, a metric is called for, which can satisfy both of the two intuitions. To address this problem, we present a weighted metric based on the classical Spearman's footrule metric to measure the distance between two permutations of n objects. This metric can be applied in rank aggregation problem with a polynomial time algorithm, and produces a 2-approximation for adopting the weighted Kendall's tau distance proposed by Farnoud et al."
343,https://arxiv.org/abs/2304.10663,Meta Semantics: Towards better natural language understanding and reasoning,"Natural language understanding is one of the most challenging topics in artificial intelligence. Deep neural network methods, particularly large language module (LLM) methods such as ChatGPT and GPT-3, have powerful flexibility to adopt informal text but are weak on logical deduction and suffer from the out-of-vocabulary (OOV) problem. On the other hand, rule-based methods such as Mathematica, Semantic web, and Lean, are excellent in reasoning but cannot handle the complex and changeable informal text. Inspired by pragmatics and structuralism, we propose two strategies to solve the OOV problem and a semantic model for better natural language understanding and reasoning."
344,https://arxiv.org/abs/2303.05758,MIXPGD: Hybrid Adversarial Training for Speech Recognition Systems,"Automatic speech recognition (ASR) systems based on deep neural networks are weak against adversarial perturbations. We propose mixPGD adversarial training method to improve the robustness of the model for ASR systems. In standard adversarial training, adversarial samples are generated by leveraging supervised or unsupervised methods. We merge the capabilities of both supervised and unsupervised approaches in our method to generate new adversarial samples which aid in improving model robustness. Extensive experiments and comparison across various state-of-the-art defense methods and adversarial attacks have been performed to show that mixPGD gains 4.1% WER of better performance than previous best performing models under white-box adversarial attack setting. We tested our proposed defense method against both white-box and transfer based black-box attack settings to ensure that our defense strategy is robust against various types of attacks. Empirical results on several adversarial attacks validate the effectiveness of our proposed approach."
345,https://arxiv.org/abs/2302.06052,CFNet: Cascade Fusion Network for Dense Prediction,"Multi-scale features are essential for dense prediction tasks, including object detection, instance segmentation, and semantic segmentation. Existing state-of-the-art methods usually first extract multi-scale features by a classification backbone and then fuse these features by a lightweight module (e.g. the fusion module in FPN). However, we argue that it may not be sufficient to fuse the multi-scale features through such a paradigm, because the parameters allocated for feature fusion are limited compared with the heavy classification backbone. In order to address this issue, we propose a new architecture named Cascade Fusion Network (CFNet) for dense prediction. Besides the stem and several blocks used to extract initial high-resolution features, we introduce several cascaded stages to generate multi-scale features in CFNet. Each stage includes a sub-backbone for feature extraction and an extremely lightweight transition block for feature integration. This design makes it possible to fuse features more deeply and effectively with a large proportion of parameters of the whole backbone. Extensive experiments on object detection, instance segmentation, and semantic segmentation validated the effectiveness of the proposed CFNet. Codes will be available at https://github.com/zhanggang001/CFNet."
346,https://arxiv.org/abs/2301.13569,NP-Match: Towards a New Probabilistic Model for Semi-Supervised Learning,"Semi-supervised learning (SSL) has been widely explored in recent years, and it is an effective way of leveraging unlabeled data to reduce the reliance on labeled data. In this work, we adjust neural processes (NPs) to the semi-supervised image classification task, resulting in a new method named NP-Match. NP-Match is suited to this task for two reasons. Firstly, NP-Match implicitly compares data points when making predictions, and as a result, the prediction of each unlabeled data point is affected by the labeled data points that are similar to it, which improves the quality of pseudo-labels. Secondly, NP-Match is able to estimate uncertainty that can be used as a tool for selecting unlabeled samples with reliable pseudo-labels. Compared with uncertainty-based SSL methods implemented with Monte-Carlo (MC) dropout, NP-Match estimates uncertainty with much less computational overhead, which can save time at both the training and the testing phases. We conducted extensive experiments on five public datasets under three semi-supervised image classification settings, namely, the standard semi-supervised image classification, the imbalanced semi-supervised image classification, and the multi-label semi-supervised image classification, and NP-Match outperforms state-of-the-art (SOTA) approaches or achieves competitive results on them, which shows the effectiveness of NP-Match and its potential for SSL. The codes are at https://github.com/Jianf-Wang/NP-Match"
347,https://arxiv.org/abs/2301.13096,Language-Driven Anchors for Zero-Shot Adversarial Robustness,"Deep neural networks are known to be susceptible to adversarial attacks. In this work, we focus on improving adversarial robustness in the challenging zero-shot image classification setting. To address this issue, we propose LAAT, a novel Language-driven, Anchor-based Adversarial Training strategy. LAAT utilizes a text encoder to generate fixed anchors (normalized feature embeddings) for each category and then uses these anchors for adversarial training. By leveraging the semantic consistency of the text encoders, LAAT can enhance the adversarial robustness of the image model on novel categories without additional examples. We identify the large cosine similarity problem of recent text encoders and design several effective techniques to address it. The experimental results demonstrate that LAAT significantly improves zero-shot adversarial performance, outperforming previous state-of-the-art adversarially robust one-shot methods. Moreover, our method produces substantial zero-shot adversarial robustness when models are trained on large datasets such as ImageNet-1K and applied to several downstream datasets."
348,https://arxiv.org/abs/2212.10744,An Audio-Visual Speech Separation Model Inspired by Cortico-Thalamo-Cortical Circuits,"Audio-visual approaches involving visual inputs have laid the foundation for recent progress in speech separation. However, the optimization of the concurrent usage of auditory and visual inputs is still an active research area. Inspired by the cortico-thalamo-cortical circuit, in which the sensory processing mechanisms of different modalities modulate one another via the non-lemniscal sensory thalamus, we propose a novel cortico-thalamo-cortical neural network (CTCNet) for audio-visual speech separation (AVSS). First, the CTCNet learns hierarchical auditory and visual representations in a bottom-up manner in separate auditory and visual subnetworks, mimicking the functions of the auditory and visual cortical areas. Then, inspired by the large number of connections between cortical regions and the thalamus, the model fuses the auditory and visual information in a thalamic subnetwork through top-down connections. Finally, the model transmits this fused information back to the auditory and visual subnetworks, and the above process is repeated several times. The results of experiments on three speech separation benchmark datasets show that CTCNet remarkably outperforms existing AVSS methods with considerablely fewer parameters. These results suggest that mimicking the anatomical connectome of the mammalian brain has great potential for advancing the development of deep neural networks. Project repo is https://github.com/JusperLee/CTCNet."
349,https://arxiv.org/abs/2212.01806,Recognizing Object by Components with Human Prior Knowledge Enhances Adversarial Robustness of Deep Neural Networks,"Adversarial attacks can easily fool object recognition systems based on deep neural networks (DNNs). Although many defense methods have been proposed in recent years, most of them can still be adaptively evaded. One reason for the weak adversarial robustness may be that DNNs are only supervised by category labels and do not have part-based inductive bias like the recognition process of humans. Inspired by a well-known theory in cognitive psychology -- recognition-by-components, we propose a novel object recognition model ROCK (Recognizing Object by Components with human prior Knowledge). It first segments parts of objects from images, then scores part segmentation results with predefined human prior knowledge, and finally outputs prediction based on the scores. The first stage of ROCK corresponds to the process of decomposing objects into parts in human vision. The second stage corresponds to the decision process of the human brain. ROCK shows better robustness than classical recognition models across various attack settings. These results encourage researchers to rethink the rationality of currently widely-used DNN-based object recognition models and explore the potential of part-based models, once important but recently ignored, for improving robustness."
350,https://arxiv.org/abs/2211.16710,Extracting Semantic Knowledge from GANs with Unsupervised Learning,"Recently, unsupervised learning has made impressive progress on various tasks. Despite the dominance of discriminative models, increasing attention is drawn to representations learned by generative models and in particular, Generative Adversarial Networks (GANs). Previous works on the interpretation of GANs reveal that GANs encode semantics in feature maps in a linearly separable form. In this work, we further find that GAN's features can be well clustered with the linear separability assumption. We propose a novel clustering algorithm, named KLiSH, which leverages the linear separability to cluster GAN's features. KLiSH succeeds in extracting fine-grained semantics of GANs trained on datasets of various objects, e.g., car, portrait, animals, and so on. With KLiSH, we can sample images from GANs along with their segmentation masks and synthesize paired image-segmentation datasets. Using the synthesized datasets, we enable two downstream applications. First, we train semantic segmentation networks on these datasets and test them on real images, realizing unsupervised semantic segmentation. Second, we train image-to-image translation networks on the synthesized datasets, enabling semantic-conditional image synthesis without human annotations."
351,https://arxiv.org/abs/2210.15271,Drug repositioning for Alzheimer's disease with transfer learning,"Deep Learning and DRUG-seq (Digital RNA with perturbation of genes) have attracted attention in drug discovery. However, the public DRUG-seq dataset is too small to be used for directly training a deep learning neural network from scratch. Inspired by the transfer learning technique, we pretrain a drug efficacy prediction neural network model with the Library of Integrated Network-based Cell-Signature (LINCS) L1000 data and then use human neural cell DRUG-seq data to fine-tune it. After training, the model is used for virtual screening to find potential drugs for Alzheimer's disease (AD) treatment. Finally, we find 27 potential drugs for AD treatment including Irsogladine (PDE4 inhibitor), Tasquinimod (HDAC4 selective inhibitor), Suprofen (dual COX-1/COX-2 inhibitor) et al."
352,https://arxiv.org/abs/2209.15200,An efficient encoder-decoder architecture with top-down attention for speech separation,"Deep neural networks have shown excellent prospects in speech separation tasks. However, obtaining good results while keeping a low model complexity remains challenging in real-world applications. In this paper, we provide a bio-inspired efficient encoder-decoder architecture by mimicking the brain's top-down attention, called TDANet, with decreased model complexity without sacrificing performance. The top-down attention in TDANet is extracted by the global attention (GA) module and the cascaded local attention (LA) layers. The GA module takes multi-scale acoustic features as input to extract global attention signal, which then modulates features of different scales by direct top-down connections. The LA layers use features of adjacent layers as input to extract the local attention signal, which is used to modulate the lateral input in a top-down manner. On three benchmark datasets, TDANet consistently achieved competitive separation performance to previous state-of-the-art (SOTA) methods with higher efficiency. Specifically, TDANet's multiply-accumulate operations (MACs) are only 5\% of Sepformer, one of the previous SOTA models, and CPU inference time is only 10\% of Sepformer. In addition, a large-size version of TDANet obtained SOTA results on three datasets, with MACs still only 10\% of Sepformer and the CPU inference time only 24\% of Sepformer."
353,https://arxiv.org/abs/2208.08270,On the Privacy Effect of Data Enhancement via the Lens of Memorization,"Machine learning poses severe privacy concerns as it has been shown that the learned models can reveal sensitive information about their training data. Many works have investigated the effect of widely-adopted data augmentation (DA) and adversarial training (AT) techniques, termed data enhancement in the paper, on the privacy leakage of machine learning models. Such privacy effects are often measured by membership inference attacks (MIAs), which aim to identify whether a particular example belongs to the training set or not. We propose to investigate privacy from a new perspective called memorization. Through the lens of memorization, we find that previously deployed MIAs produce misleading results as they are less likely to identify samples with higher privacy risks as members compared to samples with low privacy risks. To solve this problem, we deploy a recent attack that can capture individual samples' memorization degrees for evaluation. Through extensive experiments, we unveil non-trivial findings about the connections between three essential properties of machine learning models, including privacy, generalization gap, and adversarial robustness. We demonstrate that, unlike existing results, the generalization gap is shown not highly correlated with privacy leakage. Moreover, stronger adversarial robustness does not necessarily imply that the model is more susceptible to privacy attacks."
354,https://arxiv.org/abs/2207.14227,Visual Recognition by Request,"Humans have the ability of recognizing visual semantics in an unlimited granularity, but existing visual recognition algorithms cannot achieve this goal. In this paper, we establish a new paradigm named visual recognition by request (ViRReq) to bridge the gap. The key lies in decomposing visual recognition into atomic tasks named requests and leveraging a knowledge base, a hierarchical and text-based dictionary, to assist task definition. ViRReq allows for (i) learning complicated whole-part hierarchies from highly incomplete annotations and (ii) inserting new concepts with minimal efforts. We also establish a solid baseline by integrating language-driven recognition into recent semantic and instance segmentation methods, and demonstrate its flexible recognition ability on CPP and ADE20K, two datasets with hierarchical whole-part annotations."
355,https://arxiv.org/abs/2207.11493,Active Pointly-Supervised Instance Segmentation,"The requirement of expensive annotations is a major burden for training a well-performed instance segmentation model. In this paper, we present an economic active learning setting, named active pointly-supervised instance segmentation (APIS), which starts with box-level annotations and iteratively samples a point within the box and asks if it falls on the object. The key of APIS is to find the most desirable points to maximize the segmentation accuracy with limited annotation budgets. We formulate this setting and propose several uncertainty-based sampling strategies. The model developed with these strategies yields consistent performance gain on the challenging MS-COCO dataset, compared against other learning strategies. The results suggest that APIS, integrating the advantages of active learning and point-based supervision, is an effective learning paradigm for label-efficient instance segmentation."
356,https://arxiv.org/abs/2207.01066,NP-Match: When Neural Processes meet Semi-Supervised Learning,"Semi-supervised learning (SSL) has been widely explored in recent years, and it is an effective way of leveraging unlabeled data to reduce the reliance on labeled data. In this work, we adjust neural processes (NPs) to the semi-supervised image classification task, resulting in a new method named NP-Match. NP-Match is suited to this task for two reasons. Firstly, NP-Match implicitly compares data points when making predictions, and as a result, the prediction of each unlabeled data point is affected by the labeled data points that are similar to it, which improves the quality of pseudo-labels. Secondly, NP-Match is able to estimate uncertainty that can be used as a tool for selecting unlabeled samples with reliable pseudo-labels. Compared with uncertainty-based SSL methods implemented with Monte Carlo (MC) dropout, NP-Match estimates uncertainty with much less computational overhead, which can save time at both the training and the testing phases. We conducted extensive experiments on four public datasets, and NP-Match outperforms state-of-the-art (SOTA) results or achieves competitive results on them, which shows the effectiveness of NP-Match and its potential for SSL."
357,https://arxiv.org/abs/2206.07347,On the Use of Deep Mask Estimation Module for Neural Source Separation Systems,"Most of the recent neural source separation systems rely on a masking-based pipeline where a set of multiplicative masks are estimated from and applied to a signal representation of the input mixture. The estimation of such masks, in almost all network architectures, is done by a single layer followed by an optional nonlinear activation function. However, recent literatures have investigated the use of a deep mask estimation module and observed performance improvement compared to a shallow mask estimation module. In this paper, we analyze the role of such deeper mask estimation module by connecting it to a recently proposed unsupervised source separation method, and empirically show that the deep mask estimation module is an efficient approximation of the so-called overseparation-grouping paradigm with the conventional shallow mask estimation layers."
358,https://arxiv.org/abs/2206.05519,Bridging the Gap Between Training and Inference of Bayesian Controllable Language Models,"Large-scale pre-trained language models have achieved great success on natural language generation tasks. However, it is difficult to control the pre-trained language models to generate sentences with the desired attribute such as topic and sentiment, etc. Recently, Bayesian Controllable Language Models (BCLMs) have been shown to be efficient in controllable language generation. Rather than fine-tuning the parameters of pre-trained language models, BCLMs use external discriminators to guide the generation of pre-trained language models. However, the mismatch between training and inference of BCLMs limits the performance of the models. To address the problem, in this work we propose a ""Gemini Discriminator"" for controllable language generation which alleviates the mismatch problem with a small computational cost. We tested our method on two controllable language generation tasks: sentiment control and topic control. On both tasks, our method reached achieved new state-of-the-art results in automatic and human evaluations."
359,https://arxiv.org/abs/2205.05909,Infrared Invisible Clothing:Hiding from Infrared Detectors at Multiple Angles in Real World,"Thermal infrared imaging is widely used in body temperature measurement, security monitoring, and so on, but its safety research attracted attention only in recent years. We proposed the infrared adversarial clothing, which could fool infrared pedestrian detectors at different angles. We simulated the process from cloth to clothing in the digital world and then designed the adversarial ""QR code"" pattern. The core of our method is to design a basic pattern that can be expanded periodically, and make the pattern after random cropping and deformation still have an adversarial effect, then we can process the flat cloth with an adversarial pattern into any 3D clothes. The results showed that the optimized ""QR code"" pattern lowered the Average Precision (AP) of YOLOv3 by 87.7%, while the random ""QR code"" pattern and blank pattern lowered the AP of YOLOv3 by 57.9% and 30.1%, respectively, in the digital world. We then manufactured an adversarial shirt with a new material: aerogel. Physical-world experiments showed that the adversarial ""QR code"" pattern clothing lowered the AP of YOLOv3 by 64.6%, while the random ""QR code"" pattern clothing and fully heat-insulated clothing lowered the AP of YOLOv3 by 28.3% and 22.8%, respectively. We used the model ensemble technique to improve the attack transferability to unseen models."
360,https://arxiv.org/abs/2203.03379,An STDP-Based Supervised Learning Algorithm for Spiking Neural Networks,"Compared with rate-based artificial neural networks, Spiking Neural Networks (SNN) provide a more biological plausible model for the brain. But how they perform supervised learning remains elusive. Inspired by recent works of Bengio et al., we propose a supervised learning algorithm based on Spike-Timing Dependent Plasticity (STDP) for a hierarchical SNN consisting of Leaky Integrate-and-fire (LIF) neurons. A time window is designed for the presynaptic neuron and only the spikes in this window take part in the STDP updating process. The model is trained on the MNIST dataset. The classification accuracy approach that of a Multilayer Perceptron (MLP) with similar architecture trained by the standard back-propagation algorithm."
361,https://arxiv.org/abs/2203.03373,Adversarial Texture for Fooling Person Detectors in the Physical World,"Nowadays, cameras equipped with AI systems can capture and analyze images to detect people automatically. However, the AI system can make mistakes when receiving deliberately designed patterns in the real world, i.e., physical adversarial examples. Prior works have shown that it is possible to print adversarial patches on clothes to evade DNN-based person detectors. However, these adversarial examples could have catastrophic drops in the attack success rate when the viewing angle (i.e., the camera's angle towards the object) changes. To perform a multi-angle attack, we propose Adversarial Texture (AdvTexture). AdvTexture can cover clothes with arbitrary shapes so that people wearing such clothes can hide from person detectors from different viewing angles. We propose a generative method, named Toroidal-Cropping-based Expandable Generative Attack (TC-EGA), to craft AdvTexture with repetitive structures. We printed several pieces of cloth with AdvTexure and then made T-shirts, skirts, and dresses in the physical world. Experiments showed that these clothes could fool person detectors in the physical world."
362,https://arxiv.org/abs/2202.10974,The Winning Solution to the iFLYTEK Challenge 2021 Cultivated Land Extraction from High-Resolution Remote Sensing Image,"Extracting cultivated land accurately from high-resolution remote images is a basic task for precision agriculture. This report introduces our solution to the iFLYTEK challenge 2021 cultivated land extraction from high-resolution remote sensing image. The challenge requires segmenting cultivated land objects in very high-resolution multispectral remote sensing images. We established a highly effective and efficient pipeline to solve this problem. We first divided the original images into small tiles and separately performed instance segmentation on each tile. We explored several instance segmentation algorithms that work well on natural images and developed a set of effective methods that are applicable to remote sensing images. Then we merged the prediction results of all small tiles into seamless, continuous segmentation results through our proposed overlap-tile fusion strategy. We achieved the first place among 486 teams in the challenge."
363,https://arxiv.org/abs/2112.02321,Speech Separation Using an Asynchronous Fully Recurrent Convolutional Neural Network,"Recent advances in the design of neural network architectures, in particular those specialized in modeling sequences, have provided significant improvements in speech separation performance. In this work, we propose to use a bio-inspired architecture called Fully Recurrent Convolutional Neural Network (FRCNN) to solve the separation task. This model contains bottom-up, top-down and lateral connections to fuse information processed at various time-scales represented by \textit{stages}. In contrast to the traditional approach updating stages in parallel, we propose to first update the stages one by one in the bottom-up direction, then fuse information from adjacent stages simultaneously and finally fuse information from all stages to the bottom stage together. Experiments showed that this asynchronous updating scheme achieved significantly better results with much fewer parameters than the traditional synchronous updating scheme. In addition, the proposed model achieved good balance between speech separation accuracy and computational efficiency as compared to other state-of-the-art models on three benchmark datasets."
364,https://arxiv.org/abs/2111.00213,Adjacency constraint for efficient hierarchical reinforcement learning,"Goal-conditioned Hierarchical Reinforcement Learning (HRL) is a promising approach for scaling up reinforcement learning (RL) techniques. However, it often suffers from training inefficiency as the action space of the high-level, i.e., the goal space, is large. Searching in a large goal space poses difficulty for both high-level subgoal generation and low-level policy learning. In this paper, we show that this problem can be effectively alleviated by restricting the high-level action space from the whole goal space to a $k$-step adjacent region of the current state using an adjacency constraint. We theoretically prove that in a deterministic Markov Decision Process (MDP), the proposed adjacency constraint preserves the optimal hierarchical policy, while in a stochastic MDP the adjacency constraint induces a bounded state-value suboptimality determined by the MDP's transition structure. We further show that this constraint can be practically implemented by training an adjacency network that can discriminate between adjacent and non-adjacent subgoals. Experimental results on discrete and continuous control tasks including challenging simulated robot locomotion and manipulation tasks show that incorporating the adjacency constraint significantly boosts the performance of state-of-the-art goal-conditioned HRL approaches."
365,https://arxiv.org/abs/2106.09859,RSG: A Simple but Effective Module for Learning Imbalanced Datasets,"Imbalanced datasets widely exist in practice and area great challenge for training deep neural models with agood generalization on infrequent classes. In this work, wepropose a new rare-class sample generator (RSG) to solvethis problem. RSG aims to generate some new samplesfor rare classes during training, and it has in particularthe following advantages: (1) it is convenient to use andhighly versatile, because it can be easily integrated intoany kind of convolutional neural network, and it works wellwhen combined with different loss functions, and (2) it isonly used during the training phase, and therefore, no ad-ditional burden is imposed on deep neural networks duringthe testing phase. In extensive experimental evaluations, weverify the effectiveness of RSG. Furthermore, by leveragingRSG, we obtain competitive results on Imbalanced CIFARand new state-of-the-art results on Places-LT, ImageNet-LT, and iNaturalist 2018. The source code is available at https://github.com/Jianf-Wang/RSG."
366,https://arxiv.org/abs/2106.02859,Convolutional Neural Networks with Gated Recurrent Connections,"The convolutional neural network (CNN) has become a basic model for solving many computer vision problems. In recent years, a new class of CNNs, recurrent convolution neural network (RCNN), inspired by abundant recurrent connections in the visual systems of animals, was proposed. The critical element of RCNN is the recurrent convolutional layer (RCL), which incorporates recurrent connections between neurons in the standard convolutional layer. With increasing number of recurrent computations, the receptive fields (RFs) of neurons in RCL expand unboundedly, which is inconsistent with biological facts. We propose to modulate the RFs of neurons by introducing gates to the recurrent connections. The gates control the amount of context information inputting to the neurons and the neurons' RFs therefore become adaptive. The resulting layer is called gated recurrent convolution layer (GRCL). Multiple GRCLs constitute a deep model called gated RCNN (GRCNN). The GRCNN was evaluated on several computer vision tasks including object recognition, scene text recognition and object detection, and obtained much better results than the RCNN. In addition, when combined with other adaptive RF techniques, the GRCNN demonstrated competitive performance to the state-of-the-art models on benchmark datasets for these tasks. The codes are released at \href{https://github.com/Jianf-Wang/GRCNN}{https://github.com/Jianf-Wang/GRCNN}."
367,https://arxiv.org/abs/2105.09022,Attack on practical speaker verification system using universal adversarial perturbations,"In authentication scenarios, applications of practical speaker verification systems usually require a person to read a dynamic authentication text. Previous studies played an audio adversarial example as a digital signal to perform physical attacks, which would be easily rejected by audio replay detection modules. This work shows that by playing our crafted adversarial perturbation as a separate source when the adversary is speaking, the practical speaker verification system will misjudge the adversary as a target speaker. A two-step algorithm is proposed to optimize the universal adversarial perturbation to be text-independent and has little effect on the authentication text recognition. We also estimated room impulse response (RIR) in the algorithm which allowed the perturbation to be effective after being played over the air. In the physical experiment, we achieved targeted attacks with success rate of 100%, while the word error rate (WER) on speech recognition was only increased by 3.55%. And recorded audios could pass replay detection for the live person speaking."
368,https://arxiv.org/abs/2104.08569,RefineMask: Towards High-Quality Instance Segmentation with Fine-Grained Features,"The two-stage methods for instance segmentation, e.g. Mask R-CNN, have achieved excellent performance recently. However, the segmented masks are still very coarse due to the downsampling operations in both the feature pyramid and the instance-wise pooling process, especially for large objects. In this work, we propose a new method called RefineMask for high-quality instance segmentation of objects and scenes, which incorporates fine-grained features during the instance-wise segmenting process in a multi-stage manner. Through fusing more detailed information stage by stage, RefineMask is able to refine high-quality masks consistently. RefineMask succeeds in segmenting hard cases such as bent parts of objects that are over-smoothed by most previous methods and outputs accurate boundaries. Without bells and whistles, RefineMask yields significant gains of 2.6, 3.4, 3.8 AP over Mask R-CNN on COCO, LVIS, and Cityscapes benchmarks respectively at a small amount of additional computational cost. Furthermore, our single-model result outperforms the winner of the LVIS Challenge 2020 by 1.3 points on the LVIS test-dev set and establishes a new state-of-the-art. Code will be available at https://github.com/zhanggang001/RefineMask."
369,https://arxiv.org/abs/2104.05239,Look Closer to Segment Better: Boundary Patch Refinement for Instance Segmentation,"Tremendous efforts have been made on instance segmentation but the mask quality is still not satisfactory. The boundaries of predicted instance masks are usually imprecise due to the low spatial resolution of feature maps and the imbalance problem caused by the extremely low proportion of boundary pixels. To address these issues, we propose a conceptually simple yet effective post-processing refinement framework to improve the boundary quality based on the results of any instance segmentation model, termed BPR. Following the idea of looking closer to segment boundaries better, we extract and refine a series of small boundary patches along the predicted instance boundaries. The refinement is accomplished by a boundary patch refinement network at higher resolution. The proposed BPR framework yields significant improvements over the Mask R-CNN baseline on Cityscapes benchmark, especially on the boundary-aware metrics. Moreover, by applying the BPR framework to the PolyTransform + SegFix baseline, we reached 1st place on the Cityscapes leaderboard."
370,https://arxiv.org/abs/2103.01977,CloudAAE: Learning 6D Object Pose Regression with On-line Data Synthesis on Point Clouds,"It is often desired to train 6D pose estimation systems on synthetic data because manual annotation is expensive. However, due to the large domain gap between the synthetic and real images, synthesizing color images is expensive. In contrast, this domain gap is considerably smaller and easier to fill for depth information. In this work, we present a system that regresses 6D object pose from depth information represented by point clouds, and a lightweight data synthesis pipeline that creates synthetic point cloud segments for training. We use an augmented autoencoder (AAE) for learning a latent code that encodes 6D object pose information for pose regression. The data synthesis pipeline only requires texture-less 3D object models and desired viewpoints, and it is cheap in terms of both time and hardware storage. Our data synthesis process is up to three orders of magnitude faster than commonly applied approaches that render RGB image data. We show the effectiveness of our system on the LineMOD, LineMOD Occlusion, and YCB Video datasets. The implementation of our system is available at: https://github.com/GeeeG/CloudAAE."
371,https://arxiv.org/abs/2102.11731,Rethinking Natural Adversarial Examples for Classification Models,"Recently, it was found that many real-world examples without intentional modifications can fool machine learning models, and such examples are called ""natural adversarial examples"". ImageNet-A is a famous dataset of natural adversarial examples. By analyzing this dataset, we hypothesized that large, cluttered and/or unusual background is an important reason why the images in this dataset are difficult to be classified. We validated the hypothesis by reducing the background influence in ImageNet-A examples with object detection techniques. Experiments showed that the object detection models with various classification models as backbones obtained much higher accuracy than their corresponding classification models. A detection model based on the classification model EfficientNet-B7 achieved a top-1 accuracy of 53.95%, surpassing previous state-of-the-art classification models trained on ImageNet, suggesting that accurate localization information can significantly boost the performance of classification models on ImageNet-A. We then manually cropped the objects in images from ImageNet-A and created a new dataset, named ImageNet-A-Plus. A human test on the new dataset showed that the deep learning-based classifiers still performed quite poorly compared with humans. Therefore, the new dataset can be used to study the robustness of classification models to the internal variance of objects without considering the background disturbance."
372,https://arxiv.org/abs/2102.06448,The MSR-Video to Text Dataset with Clean Annotations,"Video captioning automatically generates short descriptions of the video content, usually in form of a single sentence. Many methods have been proposed for solving this task. A large dataset called MSR Video to Text (MSR-VTT) is often used as the benchmark dataset for testing the performance of the methods. However, we found that the human annotations, i.e., the descriptions of video contents in the dataset are quite noisy, e.g., there are many duplicate captions and many captions contain grammatical problems. These problems may pose difficulties to video captioning models for learning underlying patterns. We cleaned the MSR-VTT annotations by removing these problems, then tested several typical video captioning models on the cleaned dataset. Experimental results showed that data cleaning boosted the performances of the models measured by popular quantitative metrics. We recruited subjects to evaluate the results of a model trained on the original and cleaned datasets. The human behavior experiment demonstrated that trained on the cleaned dataset, the model generated captions that were more coherent and more relevant to the contents of the video clips."
373,https://arxiv.org/abs/2102.05822,Frame Difference-Based Temporal Loss for Video Stylization,"Neural style transfer models have been used to stylize an ordinary video to specific styles. To ensure temporal inconsistency between the frames of the stylized video, a common approach is to estimate the optic flow of the pixels in the original video and make the generated pixels match the estimated optical flow. This is achieved by minimizing an optical flow-based (OFB) loss during model training. However, optical flow estimation is itself a challenging task, particularly in complex scenes. In addition, it incurs a high computational cost. We propose a much simpler temporal loss called the frame difference-based (FDB) loss to solve the temporal inconsistency problem. It is defined as the distance between the difference between the stylized frames and the difference between the original frames. The differences between the two frames are measured in both the pixel space and the feature space specified by the convolutional neural networks. A set of human behavior experiments involving 62 subjects with 25,600 votes showed that the performance of the proposed FDB loss matched that of the OFB loss. The performance was measured by subjective evaluation of stability and stylization quality of the generated videos on two typical video stylization models. The results suggest that the proposed FDB loss is a strong alternative to the commonly used OFB loss for video stylization."
374,https://arxiv.org/abs/2101.08154,Fooling thermal infrared pedestrian detectors in real world using small bulbs,"Thermal infrared detection systems play an important role in many areas such as night security, autonomous driving, and body temperature detection. They have the unique advantages of passive imaging, temperature sensitivity and penetration. But the security of these systems themselves has not been fully explored, which poses risks in applying these systems. We propose a physical attack method with small bulbs on a board against the state of-the-art pedestrian detectors. Our goal is to make infrared pedestrian detectors unable to detect real-world pedestrians. Towards this goal, we first showed that it is possible to use two kinds of patches to attack the infrared pedestrian detector based on YOLOv3. The average precision (AP) dropped by 64.12% in the digital world, while a blank board with the same size caused the AP to drop by 29.69% only. After that, we designed and manufactured a physical board and successfully attacked YOLOv3 in the real world. In recorded videos, the physical board caused AP of the target detector to drop by 34.48%, while a blank board with the same size caused the AP to drop by 14.91% only. With the ensemble attack techniques, the designed physical board had good transferability to unseen detectors."
375,https://arxiv.org/abs/2011.12885,Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection,"Localization Quality Estimation (LQE) is crucial and popular in the recent advancement of dense object detectors since it can provide accurate ranking scores that benefit the Non-Maximum Suppression processing and improve detection performance. As a common practice, most existing methods predict LQE scores through vanilla convolutional features shared with object classification or bounding box regression. In this paper, we explore a completely novel and different perspective to perform LQE -- based on the learned distributions of the four parameters of the bounding box. The bounding box distributions are inspired and introduced as ""General Distribution"" in GFLV1, which describes the uncertainty of the predicted bounding boxes well. Such a property makes the distribution statistics of a bounding box highly correlated to its real localization quality. Specifically, a bounding box distribution with a sharp peak usually corresponds to high localization quality, and vice versa. By leveraging the close correlation between distribution statistics and the real localization quality, we develop a considerably lightweight Distribution-Guided Quality Predictor (DGQP) for reliable LQE based on GFLV1, thus producing GFLV2. To our best knowledge, it is the first attempt in object detection to use a highly relevant, statistical representation to facilitate LQE. Extensive experiments demonstrate the effectiveness of our method. Notably, GFLV2 (ResNet-101) achieves 46.2 AP at 14.6 FPS, surpassing the previous state-of-the-art ATSS baseline (43.6 AP at 14.6 FPS) by absolute 2.6 AP on COCO {\tt test-dev}, without sacrificing the efficiency both in training and inference. Code will be available at https://github.com/implus/GFocalV2."
376,https://arxiv.org/abs/2006.11485,Generating Adjacency-Constrained Subgoals in Hierarchical Reinforcement Learning,"Goal-conditioned hierarchical reinforcement learning (HRL) is a promising approach for scaling up reinforcement learning (RL) techniques. However, it often suffers from training inefficiency as the action space of the high-level, i.e., the goal space, is often large. Searching in a large goal space poses difficulties for both high-level subgoal generation and low-level policy learning. In this paper, we show that this problem can be effectively alleviated by restricting the high-level action space from the whole goal space to a $k$-step adjacent region of the current state using an adjacency constraint. We theoretically prove that the proposed adjacency constraint preserves the optimal hierarchical policy in deterministic MDPs, and show that this constraint can be practically implemented by training an adjacency network that can discriminate between adjacent and non-adjacent subgoals. Experimental results on discrete and continuous control tasks show that incorporating the adjacency constraint improves the performance of state-of-the-art HRL approaches in both deterministic and stochastic environments."
377,https://arxiv.org/abs/2006.04388,Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection,"One-stage detector basically formulates object detection as dense classification and localization. The classification is usually optimized by Focal Loss and the box location is commonly learned under Dirac delta distribution. A recent trend for one-stage detectors is to introduce an individual prediction branch to estimate the quality of localization, where the predicted quality facilitates the classification to improve detection performance. This paper delves into the representations of the above three fundamental elements: quality estimation, classification and localization. Two problems are discovered in existing practices, including (1) the inconsistent usage of the quality estimation and classification between training and inference and (2) the inflexible Dirac delta distribution for localization when there is ambiguity and uncertainty in complex scenes. To address the problems, we design new representations for these elements. Specifically, we merge the quality estimation into the class prediction vector to form a joint representation of localization quality and classification, and use a vector to represent arbitrary distribution of box locations. The improved representations eliminate the inconsistency risk and accurately depict the flexible distribution in real data, but contain continuous labels, which is beyond the scope of Focal Loss. We then propose Generalized Focal Loss (GFL) that generalizes Focal Loss from its discrete form to the continuous version for successful optimization. On COCO test-dev, GFL achieves 45.0\% AP using ResNet-101 backbone, surpassing state-of-the-art SAPD (43.5\%) and ATSS (43.6\%) with higher or comparable inference speed, under the same backbone and training settings. Notably, our best model can achieve a single-model single-scale AP of 48.2\%, at 10 FPS on a single 2080Ti GPU. Code and models are available at https://github.com/implus/GFocal."
378,https://arxiv.org/abs/2002.04831,End-to-End Face Parsing via Interlinked Convolutional Neural Networks,"Face parsing is an important computer vision task that requires accurate pixel segmentation of facial parts (such as eyes, nose, mouth, etc.), providing a basis for further face analysis, modification, and other applications. Interlinked Convolutional Neural Networks (iCNN) was proved to be an effective two-stage model for face parsing. However, the original iCNN was trained separately in two stages, limiting its performance. To solve this problem, we introduce a simple, end-to-end face parsing framework: STN-aided iCNN(STN-iCNN), which extends the iCNN by adding a Spatial Transformer Network (STN) between the two isolated stages. The STN-iCNN uses the STN to provide a trainable connection to the original two-stage iCNN pipeline, making end-to-end joint training possible. Moreover, as a by-product, STN also provides more precise cropped parts than the original cropper. Due to these two advantages, our approach significantly improves the accuracy of the original model. Our model achieved competitive performance on the Helen Dataset, the standard face parsing dataset. It also achieved superior performance on CelebAMask-HQ dataset, proving its good generalization. Our code has been released at https://github.com/aod321/STN-iCNN."
379,https://arxiv.org/abs/2001.08942,6D Object Pose Regression via Supervised Learning on Point Clouds,"This paper addresses the task of estimating the 6 degrees of freedom pose of a known 3D object from depth information represented by a point cloud. Deep features learned by convolutional neural networks from color information have been the dominant features to be used for inferring object poses, while depth information receives much less attention. However, depth information contains rich geometric information of the object shape, which is important for inferring the object pose. We use depth information represented by point clouds as the input to both deep networks and geometry-based pose refinement and use separate networks for rotation and translation regression. We argue that the axis-angle representation is a suitable rotation representation for deep learning, and use a geodesic loss function for rotation regression. Ablation studies show that these design choices outperform alternatives such as the quaternion representation and L2 loss, or regressing translation and rotation with the same network. Our simple yet effective approach clearly outperforms state-of-the-art methods on the YCB-video dataset. The implementation and trained model are avaliable at: https://github.com/GeeeG/CloudPose."
380,https://arxiv.org/abs/2001.05614,Delving Deeper into the Decoder for Video Captioning,"Video captioning is an advanced multi-modal task which aims to describe a video clip using a natural language sentence. The encoder-decoder framework is the most popular paradigm for this task in recent years. However, there exist some problems in the decoder of a video captioning model. We make a thorough investigation into the decoder and adopt three techniques to improve the performance of the model. First of all, a combination of variational dropout and layer normalization is embedded into a recurrent unit to alleviate the problem of overfitting. Secondly, a new online method is proposed to evaluate the performance of a model on a validation set so as to select the best checkpoint for testing. Finally, a new training strategy called professional learning is proposed which uses the strengths of a captioning model and bypasses its weaknesses. It is demonstrated in the experiments on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to Text (MSR-VTT) datasets that our model has achieved the best results evaluated by BLEU, CIDEr, METEOR and ROUGE-L metrics with significant gains of up to 18% on MSVD and 3.5% on MSR-VTT compared with the previous state-of-the-art models."
381,https://arxiv.org/abs/1910.04562,Improving Pedestrian Attribute Recognition With Weakly-Supervised Multi-Scale Attribute-Specific Localization,"Pedestrian attribute recognition has been an emerging research topic in the area of video surveillance. To predict the existence of a particular attribute, it is demanded to localize the regions related to the attribute. However, in this task, the region annotations are not available. How to carve out these attribute-related regions remains challenging. Existing methods applied attribute-agnostic visual attention or heuristic body-part localization mechanisms to enhance the local feature representations, while neglecting to employ attributes to define local feature areas. We propose a flexible Attribute Localization Module (ALM) to adaptively discover the most discriminative regions and learns the regional features for each attribute at multiple levels. Moreover, a feature pyramid architecture is also introduced to enhance the attribute-specific localization at low-levels with high-level semantic guidance. The proposed framework does not require additional region annotations and can be trained end-to-end with multi-level deep supervision. Extensive experiments show that the proposed method achieves state-of-the-art results on three pedestrian attribute datasets, including PETA, RAP, and PA-100K."
382,https://arxiv.org/abs/1910.02673,Interpretable Disentanglement of Neural Networks by Extracting Class-Specific Subnetwork,"We propose a novel perspective to understand deep neural networks in an interpretable disentanglement form. For each semantic class, we extract a class-specific functional subnetwork from the original full model, with compressed structure while maintaining comparable prediction performance. The structure representations of extracted subnetworks display a resemblance to their corresponding class semantic similarities. We also apply extracted subnetworks in visual explanation and adversarial example detection tasks by merely replacing the original full model with class-specific subnetworks. Experiments demonstrate that this intuitive operation can effectively improve explanation saliency accuracy for gradient-based explanation methods, and increase the detection rate for confidence score-based adversarial example detection methods."
383,https://arxiv.org/abs/1909.12579,Pruning from Scratch,"Network pruning is an important research field aiming at reducing computational costs of neural networks. Conventional approaches follow a fixed paradigm which first trains a large and redundant network, and then determines which units (e.g., channels) are less important and thus can be removed. In this work, we find that pre-training an over-parameterized model is not necessary for obtaining the target pruned structure. In fact, a fully-trained over-parameterized model will reduce the search space for the pruned structure. We empirically show that more diverse pruned structures can be directly pruned from randomly initialized weights, including potential models with better performance. Therefore, we propose a novel network pruning pipeline which allows pruning from scratch. In the experiments for compressing classification models on CIFAR10 and ImageNet datasets, our approach not only greatly reduces the pre-training burden of traditional pruning methods, but also achieves similar or even higher accuracy under the same computation budgets. Our results facilitate the community to rethink the effectiveness of existing techniques used for network pruning."
384,https://arxiv.org/abs/1909.00121,A Semantics-Assisted Video Captioning Model Trained with Scheduled Sampling,"Given the features of a video, recurrent neural networks can be used to automatically generate a caption for the video. Existing methods for video captioning have at least three limitations. First, semantic information has been widely applied to boost the performance of video captioning models, but existing networks often fail to provide meaningful semantic features. Second, the Teacher Forcing algorithm is often utilized to optimize video captioning models, but during training and inference, different strategies are applied to guide word generation, leading to poor performance. Third, current video captioning models are prone to generate relatively short captions that express video contents inappropriately. Toward resolving these three problems, we suggest three corresponding improvements. First of all, we propose a metric to compare the quality of semantic features, and utilize appropriate features as input for a semantic detection network (SDN) with adequate complexity in order to generate meaningful semantic features for videos. Then, we apply a scheduled sampling strategy that gradually transfers the training phase from a teacher-guided manner toward a more self-teaching manner. Finally, the ordinary logarithm probability loss function is leveraged by sentence length so that the inclination of generating short sentences is alleviated. Our model achieves better results than previous models on the YouTube2Text dataset and is competitive with the previous best model on the MSR-VTT dataset."
385,https://arxiv.org/abs/1905.09646,Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks,"The Convolutional Neural Networks (CNNs) generate the feature representation of complex objects by collecting hierarchical and different parts of semantic sub-features. These sub-features can usually be distributed in grouped form in the feature vector of each layer, representing various semantic entities. However, the activation of these sub-features is often spatially affected by similar patterns and noisy backgrounds, resulting in erroneous localization and identification. We propose a Spatial Group-wise Enhance (SGE) module that can adjust the importance of each sub-feature by generating an attention factor for each spatial location in each semantic group, so that every individual group can autonomously enhance its learnt expression and suppress possible noise. The attention factors are only guided by the similarities between the global and local feature descriptors inside each group, thus the design of SGE module is extremely lightweight with \emph{almost no extra parameters and calculations}. Despite being trained with only category supervisions, the SGE component is extremely effective in highlighting multiple active areas with various high-order semantics (such as the dog's eyes, nose, etc.). When integrated with popular CNN backbones, SGE can significantly boost the performance of image recognition tasks. Specifically, based on ResNet50 backbones, SGE achieves 1.2\% Top-1 accuracy improvement on the ImageNet benchmark and 1.0$\sim$2.0\% AP gain on the COCO benchmark across a wide range of detectors (Faster/Mask/Cascade RCNN and RetinaNet). Codes and pretrained models are available at https://github.com/implus/PytorchInsight."
386,https://arxiv.org/abs/1904.09149,Knowledge Distillation via Route Constrained Optimization,"Distillation-based learning boosts the performance of the miniaturized neural network based on the hypothesis that the representation of a teacher model can be used as structured and relatively weak supervision, and thus would be easily learned by a miniaturized model. However, we find that the representation of a converged heavy model is still a strong constraint for training a small student model, which leads to a high lower bound of congruence loss. In this work, inspired by curriculum learning we consider the knowledge distillation from the perspective of curriculum learning by routing. Instead of supervising the student model with a converged teacher model, we supervised it with some anchor points selected from the route in parameter space that the teacher model passed by, as we called route constrained optimization (RCO). We experimentally demonstrate this simple operation greatly reduces the lower bound of congruence loss for knowledge distillation, hint and mimicking learning. On close-set classification tasks like CIFAR100 and ImageNet, RCO improves knowledge distillation by 2.14% and 1.5% respectively. For the sake of evaluating the generalization, we also test RCO on the open-set face recognition task MegaFace."
387,https://arxiv.org/abs/1903.06586,Selective Kernel Networks,"In standard Convolutional Neural Networks (CNNs), the receptive fields of artificial neurons in each layer are designed to share the same size. It is well-known in the neuroscience community that the receptive field size of visual cortical neurons are modulated by the stimulus, which has been rarely considered in constructing CNNs. We propose a dynamic selection mechanism in CNNs that allows each neuron to adaptively adjust its receptive field size based on multiple scales of input information. A building block called Selective Kernel (SK) unit is designed, in which multiple branches with different kernel sizes are fused using softmax attention that is guided by the information in these branches. Different attentions on these branches yield different sizes of the effective receptive fields of neurons in the fusion layer. Multiple SK units are stacked to a deep network termed Selective Kernel Networks (SKNets). On the ImageNet and CIFAR benchmarks, we empirically show that SKNet outperforms the existing state-of-the-art architectures with lower model complexity. Detailed analyses show that the neurons in SKNet can capture target objects with different scales, which verifies the capability of neurons for adaptively adjusting their receptive field sizes according to the input. The code and models are available at https://github.com/implus/SKNet."
388,https://arxiv.org/abs/1903.06355,Turbo Learning Framework for Human-Object Interactions Recognition and Human Pose Estimation,"Human-object interactions (HOI) recognition and pose estimation are two closely related tasks. Human pose is an essential cue for recognizing actions and localizing the interacted objects. Meanwhile, human action and their interacted objects' localizations provide guidance for pose estimation. In this paper, we propose a turbo learning framework to perform HOI recognition and pose estimation simultaneously. First, two modules are designed to enforce message passing between the tasks, i.e. pose aware HOI recognition module and HOI guided pose estimation module. Then, these two modules form a closed loop to utilize the complementary information iteratively, which can be trained in an end-to-end manner. The proposed method achieves the state-of-the-art performance on two public benchmarks including Verbs in COCO (V-COCO) and HICO-DET datasets."
389,https://arxiv.org/abs/1902.10949,Dynamic Multi-path Neural Network,"Although deeper and larger neural networks have achieved better performance, the complex network structure and increasing computational cost cannot meet the demands of many resource-constrained applications. Existing methods usually choose to execute or skip an entire specific layer, which can only alter the depth of the network. In this paper, we propose a novel method called Dynamic Multi-path Neural Network (DMNN), which provides more path selection choices in terms of network width and depth during inference. The inference path of the network is determined by a controller, which takes into account both previous state and object category information. The proposed method can be easily incorporated into most modern network architectures. Experimental results on ImageNet and CIFAR-100 demonstrate the superiority of our method on both efficiency and overall classification accuracy. To be specific, DMNN-101 significantly outperforms ResNet-101 with an encouraging 45.1% FLOPs reduction, and DMNN-50 performs comparably to ResNet-101 while saving 42.1% parameters."
390,https://arxiv.org/abs/1806.02479,Interlinked Convolutional Neural Networks for Face Parsing,"Face parsing is a basic task in face image analysis. It amounts to labeling each pixel with appropriate facial parts such as eyes and nose. In the paper, we present a interlinked convolutional neural network (iCNN) for solving this problem in an end-to-end fashion. It consists of multiple convolutional neural networks (CNNs) taking input in different scales. A special interlinking layer is designed to allow the CNNs to exchange information, enabling them to integrate local and contextual information efficiently. The hallmark of iCNN is the extensive use of downsampling and upsampling in the interlinking layers, while traditional CNNs usually uses downsampling only. A two-stage pipeline is proposed for face parsing and both stages use iCNN. The first stage localizes facial parts in the size-reduced image and the second stage labels the pixels in the identified facial parts in the original image. On a benchmark dataset we have obtained better results than the state-of-the-art methods."
391,https://arxiv.org/abs/1804.00097,Adversarial Attacks and Defences Competition,"To accelerate research on adversarial examples and robustness of machine learning classifiers, Google Brain organized a NIPS 2017 competition that encouraged researchers to develop new methods to generate adversarial examples as well as to develop new ways to defend against them. In this chapter, we describe the structure and organization of the competition and the solutions developed by several of the top-placing teams."
392,https://arxiv.org/abs/1801.05134,Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift,"This paper first answers the question ""why do the two most powerful techniques Dropout and Batch Normalization (BN) often lead to a worse performance when they are combined together?"" in both theoretical and statistical aspects. Theoretically, we find that Dropout would shift the variance of a specific neural unit when we transfer the state of that network from train to test. However, BN would maintain its statistical variance, which is accumulated from the entire learning procedure, in the test phase. The inconsistency of that variance (we name this scheme as ""variance shift"") causes the unstable numerical behavior in inference that leads to more erroneous predictions finally, when applying Dropout before BN. Thorough experiments on DenseNet, ResNet, ResNeXt and Wide ResNet confirm our findings. According to the uncovered mechanism, we next explore several strategies that modifies Dropout and try to overcome the limitations of their combination by avoiding the variance shift risks."
393,https://arxiv.org/abs/1712.05274,A Hierarchical Recurrent Neural Network for Symbolic Melody Generation,"In recent years, neural networks have been used to generate symbolic melodies. However, the long-term structure in the melody has posed great difficulty for designing a good model. In this paper, we present a hierarchical recurrent neural network for melody generation, which consists of three Long-Short-Term-Memory (LSTM) subnetworks working in a coarse-to-fine manner along time. Specifically, the three subnetworks generate bar profiles, beat profiles and notes in turn, and the output of the high-level subnetworks are fed into the low-level subnetworks, serving as guidance for generating the finer time-scale melody components in low-level subnetworks. Two human behavior experiments demonstrate the advantage of this structure over the single-layer LSTM which attempts to learn all hidden structures in melodies. Compared with the state-of-the-art models MidiNet and MusicVAE, the hierarchical recurrent neural network produces better melodies evaluated by humans."
394,https://arxiv.org/abs/1712.02976,Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser,"Neural networks are vulnerable to adversarial examples, which poses a threat to their application in security sensitive systems. We propose high-level representation guided denoiser (HGD) as a defense for image classification. Standard denoiser suffers from the error amplification effect, in which small residual adversarial noise is progressively amplified and leads to wrong classifications. HGD overcomes this problem by using a loss function defined as the difference between the target model's outputs activated by the clean image and denoised image. Compared with ensemble adversarial training which is the state-of-the-art defending method on large images, HGD has three advantages. First, with HGD as a defense, the target model is more robust to either white-box or black-box adversarial attacks. Second, HGD can be trained on a small subset of the images and generalizes well to other images and unseen classes. Third, HGD can be transferred to defend models other than the one guiding it. In NIPS competition on defense against adversarial attacks, our HGD solution won the first place and outperformed other models by a large margin."
395,https://arxiv.org/abs/1711.08324,Evaluate the Malignancy of Pulmonary Nodules Using the 3D Deep Leaky Noisy-or Network,"Automatic diagnosing lung cancer from Computed Tomography (CT) scans involves two steps: detect all suspicious lesions (pulmonary nodules) and evaluate the whole-lung/pulmonary malignancy. Currently, there are many studies about the first step, but few about the second step. Since the existence of nodule does not definitely indicate cancer, and the morphology of nodule has a complicated relationship with cancer, the diagnosis of lung cancer demands careful investigations on every suspicious nodule and integration of information of all nodules. We propose a 3D deep neural network to solve this problem. The model consists of two modules. The first one is a 3D region proposal network for nodule detection, which outputs all suspicious nodules for a subject. The second one selects the top five nodules based on the detection confidence, evaluates their cancer probabilities and combines them with a leaky noisy-or gate to obtain the probability of lung cancer for the subject. The two modules share the same backbone network, a modified U-net. The over-fitting caused by the shortage of training data is alleviated by training the two modules alternately. The proposed model won the first place in the Data Science Bowl 2017 competition. The code has been made publicly available."
396,https://arxiv.org/abs/1710.06081,Boosting Adversarial Attacks with Momentum,"Deep neural networks are vulnerable to adversarial examples, which poses security concerns on these algorithms due to the potentially severe consequences. Adversarial attacks serve as an important surrogate to evaluate the robustness of deep learning models before they are deployed. However, most of existing adversarial attacks can only fool a black-box model with a low success rate. To address this issue, we propose a broad class of momentum-based iterative algorithms to boost adversarial attacks. By integrating the momentum term into the iterative process for attacks, our methods can stabilize update directions and escape from poor local maxima during the iterations, resulting in more transferable adversarial examples. To further improve the success rates for black-box attacks, we apply momentum iterative algorithms to an ensemble of models, and show that the adversarially trained models with a strong defense ability are also vulnerable to our black-box attacks. We hope that the proposed methods will serve as a benchmark for evaluating the robustness of various deep models and defense methods. With this method, we won the first places in NIPS 2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack competitions."
397,https://arxiv.org/abs/1706.09876,Scale-Aware Face Detection,"Convolutional neural network (CNN) based face detectors are inefficient in handling faces of diverse scales. They rely on either fitting a large single model to faces across a large scale range or multi-scale testing. Both are computationally expensive. We propose Scale-aware Face Detector (SAFD) to handle scale explicitly using CNN, and achieve better performance with less computation cost. Prior to detection, an efficient CNN predicts the scale distribution histogram of the faces. Then the scale histogram guides the zoom-in and zoom-out of the image. Since the faces will be approximately in uniform scale after zoom, they can be detected accurately even with much smaller CNN. Actually, more than 99% of the faces in AFW can be covered with less than two zooms per image. Extensive experiments on FDDB, MALF and AFW show advantages of SAFD."
398,https://arxiv.org/abs/1702.03833,Estimation of the volume of the left ventricle from MRI images using deep neural networks,"Segmenting human left ventricle (LV) in magnetic resonance imaging (MRI) images and calculating its volume are important for diagnosing cardiac diseases. In 2016, Kaggle organized a competition to estimate the volume of LV from MRI images. The dataset consisted of a large number of cases, but only provided systole and diastole volumes as labels. We designed a system based on neural networks to solve this problem. It began with a detector combined with a neural network classifier for detecting regions of interest (ROIs) containing LV chambers. Then a deep neural network named hypercolumns fully convolutional network was used to segment LV in ROIs. The 2D segmentation results were integrated across different images to estimate the volume. With ground-truth volume labels, this model was trained end-to-end. To improve the result, an additional dataset with only segmentation label was used. The model was trained alternately on these two datasets with different types of teaching signals. We also proposed a variance estimation method for the final prediction. Our algorithm ranked the 4th on the test set in this competition."
399,https://arxiv.org/abs/1612.04647,UnrealStereo: Controlling Hazardous Factors to Analyze Stereo Vision,"A reliable stereo algorithm is critical for many robotics applications. But textureless and specular regions can easily cause failure by making feature matching difficult. Understanding whether an algorithm is robust to these hazardous regions is important. Although many stereo benchmarks have been developed to evaluate performance, it is hard to quantify the effect of hazardous regions in real images because the location and severity of these regions are unknown. In this paper, we develop a synthetic image generation tool enabling to control hazardous factors, such as making objects more specular or transparent, to produce hazardous regions at different degrees. The densely controlled sampling strategy in virtual worlds enables to effectively stress test stereo algorithms by varying the types and degrees of the hazard. We generate a large synthetic image dataset with automatically computed hazardous regions and analyze algorithms on these regions. The observations from synthetic images are further validated by annotating hazardous regions in real-world datasets Middlebury and KITTI (which gives a sparse sampling of the hazards). Our synthetic image generation tool is based on a game engine Unreal Engine 4 and will be open-source along with the virtual scenes in our experiments. Many publicly available realistic game contents can be used by our tool to provide an enormous resource for development and evaluation of algorithms."
400,https://arxiv.org/abs/1404.2999,A Reverse Hierarchy Model for Predicting Eye Fixations,"A number of psychological and physiological evidences suggest that early visual attention works in a coarse-to-fine way, which lays a basis for the reverse hierarchy theory (RHT). This theory states that attention propagates from the top level of the visual hierarchy that processes gist and abstract information of input, to the bottom level that processes local details. Inspired by the theory, we develop a computational model for saliency detection in images. First, the original image is downsampled to different scales to constitute a pyramid. Then, saliency on each layer is obtained by image super-resolution reconstruction from the layer above, which is defined as unpredictability from this coarse-to-fine reconstruction. Finally, saliency on each layer of the pyramid is fused into stochastic fixations through a probabilistic model, where attention initiates from the top layer and propagates downward through the pyramid. Extensive experiments on two standard eye-tracking datasets show that the proposed method can achieve competitive results with state-of-the-art models."
401,https://arxiv.org/abs/2305.15294,Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy,"Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation."
402,https://arxiv.org/abs/2305.09137,Pre-Training to Learn in Context,"In-context learning, where pre-trained language models learn to perform tasks from task examples and instructions in their contexts, has attracted much attention in the NLP community. However, the ability of in-context learning is not fully exploited because language models are not explicitly trained to learn in context. To this end, we propose PICL (Pre-training for In-Context Learning), a framework to enhance the language models' in-context learning ability by pre-training the model on a large collection of ""intrinsic tasks"" in the general plain-text corpus using the simple language modeling objective. PICL encourages the model to infer and perform tasks by conditioning on the contexts while maintaining task generalization of pre-trained models. We evaluate the in-context learning performance of the model trained with PICL on seven widely-used text classification datasets and the Super-NaturalInstrctions benchmark, which contains 100+ NLP tasks formulated to text generation. Our experiments show that PICL is more effective and task-generalizable than a range of baselines, outperforming larger language models with nearly 4x parameters. The code is publicly available at https://github.com/thu-coai/PICL."
403,https://arxiv.org/abs/2305.05390,COKE: A Cognitive Knowledge Graph for Machine Theory of Mind,"Theory of mind (ToM) refers to humans' ability to understand and infer the desires, beliefs, and intentions of others. The acquisition of ToM plays a key role in humans' social cognition and interpersonal relations. Though indispensable for social intelligence, ToM is still lacking for modern AI and NLP systems since they cannot access the human mental state and cognitive process beneath the training corpus. To empower AI systems with the ToM ability and narrow the gap between them and humans, in this paper, we propose COKE: the first cognitive knowledge graph for machine theory of mind. Specifically, COKE formalizes ToM as a collection of 45k+ manually verified cognitive chains that characterize human mental activities and subsequent behavioral/affective responses when facing specific social circumstances. Beyond that, we further generalize COKE using pre-trained language models and build a powerful cognitive generation model COKE+. Experimental results in both automatic and human evaluation demonstrate the high quality of COKE and the superior ToM ability of COKE+."
404,https://arxiv.org/abs/2305.02606,"Re$^3$Dial: Retrieve, Reorganize and Rescale Dialogue Corpus for Long-Turn Open-Domain Dialogue Pre-training","Large-scale open-domain dialogue data crawled from public social media has greatly improved the performance of dialogue models. However, long-turn dialogues are still highly scarce. Specifically, most dialogue sessions in existing corpora have less than three turns. To alleviate this issue, we propose the Retrieve, Reorganize and Rescale framework (Re$^3$Dial), which can automatically construct a billion-scale long-turn dialogue corpus from existing short-turn dialogue data. Re$^3$Dial first trains an Unsupervised Dense Session Retriever (UDSR) to capture semantic and discourse relationships within multi-turn dialogues for retrieving relevant and coherent sessions. It then reorganizes the short-turn dialogues into long-turn sessions via recursively retrieving and selecting the consecutive sessions with our proposed diversity sampling strategy. Extensive evaluations on multiple multi-turn dialogue benchmarks demonstrate that Re$^3$Dial consistently and significantly improves the dialogue model's ability to utilize long-term context for modeling multi-turn dialogues across different pre-training settings. Finally, we build a toolkit for efficiently rescaling dialogue corpus with Re$^3$Dial, which enables us to construct a corpus containing 1B Chinese dialogue sessions with 11.3 turns on average (5X longer than the original EVA corpus). We will release our UDSR model, toolkit, and data for public use."
405,https://arxiv.org/abs/2304.11791,Directed Acyclic Transformer Pre-training for High-quality Non-autoregressive Text Generation,"Non-AutoRegressive (NAR) text generation models have drawn much attention because of their significantly faster decoding speed and good generation quality in machine translation. However, in a wider range of text generation tasks, existing NAR models lack proper pre-training, making them still far behind the pre-trained autoregressive models. In this paper, we propose Pre-trained Directed Acyclic Transformer (PreDAT) and a novel pre-training task to promote prediction consistency in NAR generation. Experiments on five text generation tasks show that our PreDAT remarkably outperforms existing pre-trained NAR models (+4.2 scores on average) and even achieves better results than pre-trained autoregressive baselines in n-gram-based metrics, along with 17 times speedup in throughput. Further analysis shows that PreDAT benefits from the unbiased prediction order that alleviates the error accumulation problem in autoregressive generation, which provides new insights into the advantages of NAR generation."
406,https://arxiv.org/abs/2304.10436,Safety Assessment of Chinese Large Language Models,"With the rapid popularity of large language models such as ChatGPT and GPT-4, a growing amount of attention is paid to their safety concerns. These models may generate insulting and discriminatory content, reflect incorrect social values, and may be used for malicious purposes such as fraud and dissemination of misleading information. Evaluating and enhancing their safety is particularly essential for the wide application of large language models (LLMs). To further promote the safe deployment of LLMs, we develop a Chinese LLM safety assessment benchmark. Our benchmark explores the comprehensive safety performance of LLMs from two perspectives: 8 kinds of typical safety scenarios and 6 types of more challenging instruction attacks. Our benchmark is based on a straightforward process in which it provides the test prompts and evaluates the safety of the generated responses from the evaluated model. In evaluation, we utilize the LLM's strong evaluation ability and develop it as a safety evaluator by prompting. On top of this benchmark, we conduct safety assessments and analyze 15 LLMs including the OpenAI GPT series and other well-known Chinese LLMs, where we observe some interesting findings. For example, we find that instruction attacks are more likely to expose safety issues of all LLMs. Moreover, to promote the development and deployment of safe, responsible, and ethical AI, we publicly release SafetyPrompts including 100k augmented prompts and responses by LLMs."
407,https://arxiv.org/abs/2302.13344,Tailoring Language Generation Models under Total Variation Distance,"The standard paradigm of neural language generation adopts maximum likelihood estimation (MLE) as the optimizing method. From a distributional view, MLE in fact minimizes the Kullback-Leibler divergence (KLD) between the distribution of the real data and that of the model. However, this approach forces the model to distribute non-zero (sometimes large) probability mass to all training samples regardless of their quality. Moreover, in the attempt to cover the low-probability regions in the data distribution, the model systematically overestimates the probability of corrupted text sequences, which we conjecture is one of the main reasons for text degeneration during autoregressive decoding. To remedy this problem, we leverage the total variation distance (TVD) with its robustness to outliers, and develop practical bounds to apply it to language generation. Then, we introduce the TaiLr objective that balances the tradeoff of estimating TVD. Intuitively, TaiLr downweights real data samples that have low model probabilities with tunable penalization intensity. Experimental results show that our method alleviates the overestimation of degenerated sequences without sacrificing diversity and improves generation quality on a wide range of text generation tasks."
408,https://arxiv.org/abs/2302.09270,"Recent Advances towards Safe, Responsible, and Moral Dialogue Systems: A Survey","With the development of artificial intelligence, dialogue systems have been endowed with amazing chit-chat capabilities, and there is widespread interest and discussion about whether the generated contents are socially beneficial. In this paper, we present a new perspective of research scope towards building a safe, responsible, and modal dialogue system, including 1) abusive and toxic contents, 2) unfairness and discrimination, 3) ethics and morality issues, and 4) risk of misleading and privacy information. Besides, we review the mainstream methods for evaluating the safety of large models from the perspectives of exposure and detection of safety issues. The recent advances in methodologies for the safety improvement of both end-to-end dialogue systems and pipeline-based models are further introduced. Finally, we discussed six existing challenges towards responsible AI: explainable safety monitoring, continuous learning of safety issues, robustness against malicious attacks, multimodal information processing, unified research framework, and multidisciplinary theory integration. We hope this survey will inspire further research toward safer dialogue systems."
409,https://arxiv.org/abs/2302.00618,Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models,"Large language models can perform various reasoning tasks by using chain-of-thought prompting, which guides them to find answers through step-by-step demonstrations. However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly. We introduce Synthetic prompting, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning. Our method alternates between a backward and forward process to generate new examples. The backward process generates a question that match a sampled reasoning chain, so that the question is solvable and clear. The forward process produces a more detailed reasoning chain for the question, improving the quality of the example. We evaluate our method on numerical, symbolic, and algorithmic reasoning tasks, and show that it outperforms existing prompting techniques."
410,https://arxiv.org/abs/2212.10720,MoralDial: A Framework to Train and Evaluate Moral Dialogue Systems via Moral Discussions,"Morality in dialogue systems has raised great attention in research recently. A moral dialogue system aligned with users' values could enhance conversation engagement and user connections. In this paper, we propose a framework, MoralDial to train and evaluate moral dialogue systems. In our framework, we first explore the communication mechanisms of morality and resolve expressed morality into three parts, which indicate the roadmap for building a moral dialogue system. Based on that, we design a simple yet effective method: constructing moral discussions between simulated specific users and the dialogue system. The constructed discussions consist of expressing, explaining, revising, and inferring moral views in dialogue exchanges, which makes conversational models learn morality well in a natural manner. Furthermore, we propose a novel evaluation method under the framework. We evaluate the multiple aspects of morality by judging the relation between dialogue responses and human values in discussions, where the multifaceted nature of morality is particularly considered. Automatic and manual experiments demonstrate that our framework is promising to train and evaluate moral dialogue systems."
411,https://arxiv.org/abs/2212.09235,PAL: Persona-Augmented Emotional Support Conversation Generation,"Due to the lack of human resources for mental health support, there is an increasing demand for employing conversational agents for support. Recent work has demonstrated the effectiveness of dialogue models in providing emotional support. As previous studies have demonstrated that seekers' persona is an important factor for effective support, we investigate whether there are benefits to modeling such information in dialogue models for support. In this paper, our empirical analysis verifies that persona has an important impact on emotional support. Therefore, we propose a framework for dynamically inferring and modeling seekers' persona. We first train a model for inferring the seeker's persona from the conversation history. Accordingly, we propose PAL, a model that leverages persona information and, in conjunction with our strategy-based controllable generation method, provides personalized emotional support. Automatic and manual evaluations demonstrate that our proposed model, PAL, achieves state-of-the-art results, outperforming the baselines on the studied benchmark. Our code and data are publicly available at https://github.com/chengjl19/PAL."
412,https://arxiv.org/abs/2212.01810,Constructing Highly Inductive Contexts for Dialogue Safety through Controllable Reverse Generation,"Large pretrained language models can easily produce toxic or biased content, which is prohibitive for practical use. In order to detect such toxic generations, existing methods rely on templates, real-world data extraction, crowdsourcing workers, or automatic generation to construct adversarial contexts that are likely to induce toxic generations. However, what type of context is more likely to induce unsafe responses is still under-explored. In this paper, we identify that context toxicity and context category (e.g., \textit{profanity}, \textit{insult}, \textit{drugs}, etc.) are two important factors to cause safety issues in response generation. Hence, we propose a method called \emph{reverse generation} to construct adversarial contexts conditioned on a given response, with the flexibility to control category, toxicity level, and inductivity of the generated contexts. Via reverse generation, we augment the existing BAD dataset and construct a new dataset BAD+ which contains more than 120K diverse and highly inductive contexts in 12 categories. We test three popular pretrained dialogue models (Blender, DialoGPT, and Plato2) and find that BAD+ can largely expose their safety problems. Furthermore, we show that BAD+ can greatly enhance the safety of generation and reveal the key factors of safety improvement. Our code and dataset is available at \url{https://github.com/thu-coai/Reverse_Generation}."
413,https://arxiv.org/abs/2212.01739,KPT: Keyword-guided Pre-training for Grounded Dialog Generation,"Incorporating external knowledge into the response generation process is essential to building more helpful and reliable dialog agents. However, collecting knowledge-grounded conversations is often costly, calling for a better pre-trained model for grounded dialog generation that generalizes well w.r.t. different types of knowledge. In this work, we propose KPT (Keyword-guided Pre-Training), a novel self-supervised pre-training method for grounded dialog generation without relying on extra knowledge annotation. Specifically, we use a pre-trained language model to extract the most uncertain tokens in the dialog as keywords. With these keywords, we construct two kinds of knowledge and pre-train a knowledge-grounded response generation model, aiming at handling two different scenarios: (1) the knowledge should be faithfully grounded; (2) it can be selectively used. For the former, the grounding knowledge consists of keywords extracted from the response. For the latter, the grounding knowledge is additionally augmented with keywords extracted from other utterances in the same dialog. Since the knowledge is extracted from the dialog itself, KPT can be easily performed on a large volume and variety of dialogue data. We considered three data sources (open-domain, task-oriented, conversational QA) with a total of 2.5M dialogues. We conduct extensive experiments on various few-shot knowledge-grounded generation tasks, including grounding on dialog acts, knowledge graphs, persona descriptions, and Wikipedia passages. Our comprehensive experiments and analyses demonstrate that KPT consistently outperforms state-of-the-art methods on these tasks with diverse grounding knowledge."
414,https://arxiv.org/abs/2211.17148,ConvLab-3: A Flexible Dialogue System Toolkit Based on a Unified Data Format,"Diverse data formats and ontologies of task-oriented dialogue (TOD) datasets hinder us from developing general dialogue models that perform well on many datasets and studying knowledge transfer between datasets. To address this issue, we present ConvLab-3, a flexible dialogue system toolkit based on a unified TOD data format. In ConvLab-3, different datasets are transformed into one unified format and loaded by models in the same way. As a result, the cost of adapting a new model or dataset is significantly reduced. Compared to the previous releases of ConvLab (Lee et al., 2019b; Zhu et al., 2020b), ConvLab-3 allows developing dialogue systems with much more datasets and enhances the utility of the reinforcement learning (RL) toolkit for dialogue policies. To showcase the use of ConvLab-3 and inspire future work, we present a comprehensive study with various settings. We show the benefit of pre-training on other datasets for few-shot fine-tuning and RL, and encourage evaluating policy with diverse user simulators."
415,https://arxiv.org/abs/2211.16482,Chaining Simultaneous Thoughts for Numerical Reasoning,"Given that rich information is hidden behind ubiquitous numbers in text, numerical reasoning over text should be an essential skill of AI systems. To derive precise equations to solve numerical reasoning problems, previous work focused on modeling the structures of equations, and has proposed various structured decoders. Though structure modeling proves to be effective, these structured decoders construct a single equation in a pre-defined autoregressive order, potentially placing an unnecessary restriction on how a model should grasp the reasoning process. Intuitively, humans may have numerous pieces of thoughts popping up in no pre-defined order; thoughts are not limited to the problem at hand, and can even be concerned with other related problems. By comparing diverse thoughts and chaining relevant pieces, humans are less prone to errors. In this paper, we take this inspiration and propose CANTOR, a numerical reasoner that models reasoning steps using a directed acyclic graph where we produce diverse reasoning steps simultaneously without pre-defined decoding dependencies, and compare and chain relevant ones to reach a solution. Extensive experiments demonstrated the effectiveness of CANTOR under both fully-supervised and weakly-supervised settings."
416,https://arxiv.org/abs/2211.16202,AutoCAD: Automatically Generating Counterfactuals for Mitigating Shortcut Learning,"Recent studies have shown the impressive efficacy of counterfactually augmented data (CAD) for reducing NLU models' reliance on spurious features and improving their generalizability. However, current methods still heavily rely on human efforts or task-specific designs to generate counterfactuals, thereby impeding CAD's applicability to a broad range of NLU tasks. In this paper, we present AutoCAD, a fully automatic and task-agnostic CAD generation framework. AutoCAD first leverages a classifier to unsupervisedly identify rationales as spans to be intervened, which disentangles spurious and causal features. Then, AutoCAD performs controllable generation enhanced by unlikelihood training to produce diverse counterfactuals. Extensive evaluations on multiple out-of-domain and challenge benchmarks demonstrate that AutoCAD consistently and significantly boosts the out-of-distribution performance of powerful pre-trained models across different NLU tasks, which is comparable or even better than previous state-of-the-art human-in-the-loop or task-specific CAD methods. The code is publicly available at https://github.com/thu-coai/AutoCAD."
417,https://arxiv.org/abs/2211.02848,Aligning Recommendation and Conversation via Dual Imitation,"Human conversations of recommendation naturally involve the shift of interests which can align the recommendation actions and conversation process to make accurate recommendations with rich explanations. However, existing conversational recommendation systems (CRS) ignore the advantage of user interest shift in connecting recommendation and conversation, which leads to an ineffective loose coupling structure of CRS. To address this issue, by modeling the recommendation actions as recommendation paths in a knowledge graph (KG), we propose DICR (Dual Imitation for Conversational Recommendation), which designs a dual imitation to explicitly align the recommendation paths and user interest shift paths in a recommendation module and a conversation module, respectively. By exchanging alignment signals, DICR achieves bidirectional promotion between recommendation and conversation modules and generates high-quality responses with accurate recommendations and coherent explanations. Experiments demonstrate that DICR outperforms the state-of-the-art models on recommendation and conversation performance with automatic, human, and novel explainability metrics."
418,https://arxiv.org/abs/2210.09175,Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization,"Training language models to learn from human instructions for zero-shot cross-task generalization has attracted much attention in NLP communities. Recently, instruction tuning (IT), which fine-tunes a pre-trained language model on a massive collection of tasks described via human-craft instructions, has been shown effective in instruction learning for unseen tasks. However, IT relies on a large amount of human-annotated samples, which restricts its generalization. Unlike labeled data, unlabeled data are often massive and cheap to obtain. In this work, we study how IT can be improved with unlabeled data. We first empirically explore the IT performance trends versus the number of labeled data, instructions, and training tasks. We find it critical to enlarge the number of training instructions, and the instructions can be underutilized due to the scarcity of labeled data. Then, we propose Unlabeled Data Augmented Instruction Tuning (UDIT) to take better advantage of the instructions during IT by constructing pseudo-labeled data from unlabeled plain texts. We conduct extensive experiments to show UDIT's effectiveness in various scenarios of tasks and datasets. We also comprehensively analyze the key factors of UDIT to investigate how to better improve IT with unlabeled data. The code is publicly available at https://github.com/thu-coai/UDIT."
419,https://arxiv.org/abs/2210.08511,CDConv: A Benchmark for Contradiction Detection in Chinese Conversations,"Dialogue contradiction is a critical issue in open-domain dialogue systems. The contextualization nature of conversations makes dialogue contradiction detection rather challenging. In this work, we propose a benchmark for Contradiction Detection in Chinese Conversations, namely CDConv. It contains 12K multi-turn conversations annotated with three typical contradiction categories: Intra-sentence Contradiction, Role Confusion, and History Contradiction. To efficiently construct the CDConv conversations, we devise a series of methods for automatic conversation generation, which simulate common user behaviors that trigger chatbots to make contradictions. We conduct careful manual quality screening of the constructed conversations and show that state-of-the-art Chinese chatbots can be easily goaded into making contradictions. Experiments on CDConv show that properly modeling contextual information is critical for dialogue contradiction detection, but there are still unresolved challenges that require future research."
420,https://arxiv.org/abs/2209.10183,Chatbots for Mental Health Support: Exploring the Impact of Emohaa on Reducing Mental Distress in China,"The growing demand for mental health support has highlighted the importance of conversational agents as human supporters worldwide and in China. These agents could increase availability and reduce the relative costs of mental health support. The provided support can be divided into two main types: cognitive and emotional support. Existing work on this topic mainly focuses on constructing agents that adopt Cognitive Behavioral Therapy (CBT) principles. Such agents operate based on pre-defined templates and exercises to provide cognitive support. However, research on emotional support using such agents is limited. In addition, most of the constructed agents operate in English, highlighting the importance of conducting such studies in China. In this study, we analyze the effectiveness of Emohaa in reducing symptoms of mental distress. Emohaa is a conversational agent that provides cognitive support through CBT-based exercises and guided conversations. It also emotionally supports users by enabling them to vent their desired emotional problems. The study included 134 participants, split into three groups: Emohaa (CBT-based), Emohaa (Full), and control. Experimental results demonstrated that compared to the control group, participants who used Emohaa experienced considerably more significant improvements in symptoms of mental distress. We also found that adding the emotional support agent had a complementary effect on such improvements, mainly depression and insomnia. Based on the obtained results and participants' satisfaction with the platform, we concluded that Emohaa is a practical and effective tool for reducing mental distress."
421,https://arxiv.org/abs/2209.08524,A Benchmark for Understanding and Generating Dialogue between Characters in Stories,"Many classical fairy tales, fiction, and screenplays leverage dialogue to advance story plots and establish characters. We present the first study to explore whether machines can understand and generate dialogue in stories, which requires capturing traits of different characters and the relationships between them. To this end, we propose two new tasks including Masked Dialogue Generation and Dialogue Speaker Recognition, i.e., generating missing dialogue turns and predicting speakers for specified dialogue turns, respectively. We build a new dataset DialStory, which consists of 105k Chinese stories with a large amount of dialogue weaved into the plots to support the evaluation. We show the difficulty of the proposed tasks by testing existing models with automatic and manual evaluation on DialStory. Furthermore, we propose to learn explicit character representations to improve performance on these tasks. Extensive experiments and case studies show that our approach can generate more coherent and informative dialogue, and achieve higher speaker recognition accuracy than strong baselines."
422,https://arxiv.org/abs/2208.13423,StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing,"Non-parallel text style transfer is an important task in natural language generation. However, previous studies concentrate on the token or sentence level, such as sentence sentiment and formality transfer, but neglect long style transfer at the discourse level. Long texts usually involve more complicated author linguistic preferences such as discourse structures than sentences. In this paper, we formulate the task of non-parallel story author-style transfer, which requires transferring an input story into a specified author style while maintaining source semantics. To tackle this problem, we propose a generation model, named StoryTrans, which leverages discourse representations to capture source content information and transfer them to target styles with learnable style embeddings. We use an additional training objective to disentangle stylistic features from the learned discourse representation to prevent the model from degenerating to an auto-encoder. Moreover, to enhance content preservation, we design a mask-and-fill framework to explicitly fuse style-specific keywords of source texts into generation. Furthermore, we constructed new datasets for this task in Chinese and English, respectively. Extensive experiments show that our model outperforms strong baselines in overall performance of style transfer and content preservation."
423,https://arxiv.org/abs/2208.08845,CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic Response Generation,"Empathetic conversation is psychologically supposed to be the result of conscious alignment and interaction between the cognition and affection of empathy. However, existing empathetic dialogue models usually consider only the affective aspect or treat cognition and affection in isolation, which limits the capability of empathetic response generation. In this work, we propose the CASE model for empathetic dialogue generation. It first builds upon a commonsense cognition graph and an emotional concept graph and then aligns the user's cognition and affection at both the coarse-grained and fine-grained levels. Through automatic and manual evaluation, we demonstrate that CASE outperforms state-of-the-art baselines of empathetic dialogues and can generate more empathetic and informative responses."
424,https://arxiv.org/abs/2208.07597,Manual-Guided Dialogue for Flexible Conversational Agents,"How to build and use dialogue data efficiently, and how to deploy models in different domains at scale can be two critical issues in building a task-oriented dialogue system. In this paper, we propose a novel manual-guided dialogue scheme to alleviate these problems, where the agent learns the tasks from both dialogue and manuals. The manual is an unstructured textual document that guides the agent in interacting with users and the database during the conversation. Our proposed scheme reduces the dependence of dialogue models on fine-grained domain ontology, and makes them more flexible to adapt to various domains. We then contribute a fully-annotated multi-domain dataset MagDial to support our scheme. It introduces three dialogue modeling subtasks: instruction matching, argument filling, and response generation. Modeling these subtasks is consistent with the human agent's behavior patterns. Experiments demonstrate that the manual-guided dialogue scheme improves data efficiency and domain scalability in building dialogue systems. The dataset and benchmark will be publicly available for promoting future research."
425,https://arxiv.org/abs/2208.03985,Generating Coherent Narratives by Learning Dynamic and Discrete Entity States with a Contrastive Framework,"Despite advances in generating fluent texts, existing pretraining models tend to attach incoherent event sequences to involved entities when generating narratives such as stories and news. We conjecture that such issues result from representing entities as static embeddings of superficial words, while neglecting to model their ever-changing states, i.e., the information they carry, as the text unfolds. Therefore, we extend the Transformer model to dynamically conduct entity state updates and sentence realization for narrative generation. We propose a contrastive framework to learn the state representations in a discrete space, and insert additional attention layers into the decoder to better exploit these states. Experiments on two narrative datasets show that our model can generate more coherent and diverse narratives than strong baselines with the guidance of meaningful entity states."
426,https://arxiv.org/abs/2206.05975,On the Learning of Non-Autoregressive Transformers,"Non-autoregressive Transformer (NAT) is a family of text generation models, which aims to reduce the decoding latency by predicting the whole sentences in parallel. However, such latency reduction sacrifices the ability to capture left-to-right dependencies, thereby making NAT learning very challenging. In this paper, we present theoretical and empirical analyses to reveal the challenges of NAT learning and propose a unified perspective to understand existing successes. First, we show that simply training NAT by maximizing the likelihood can lead to an approximation of marginal distributions but drops all dependencies between tokens, where the dropped information can be measured by the dataset's conditional total correlation. Second, we formalize many previous objectives in a unified framework and show that their success can be concluded as maximizing the likelihood on a proxy distribution, leading to a reduced information loss. Empirical studies show that our perspective can explain the phenomena in NAT learning and guide the design of new training methods."
427,https://arxiv.org/abs/2206.02712,Curriculum-Based Self-Training Makes Better Few-Shot Learners for Data-to-Text Generation,"Despite the success of text-to-text pre-trained models in various natural language generation (NLG) tasks, the generation performance is largely restricted by the number of labeled data in downstream tasks, particularly in data-to-text generation tasks. Existing works mostly utilize abundant unlabeled structured data to conduct unsupervised pre-training for task adaption, which fail to model the complex relationship between source structured data and target texts. Thus, we introduce self-training as a better few-shot learner than task-adaptive pre-training, which explicitly captures this relationship via pseudo-labeled data generated by the pre-trained model. To alleviate the side-effect of low-quality pseudo-labeled data during self-training, we propose a novel method called Curriculum-Based Self-Training (CBST) to effectively leverage unlabeled data in a rearranged order determined by the difficulty of text generation. Experimental results show that our method can outperform fine-tuning and task-adaptive pre-training methods, and achieve state-of-the-art performance in the few-shot setting of data-to-text generation."
428,https://arxiv.org/abs/2205.14727,CPED: A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI,"Human language expression is based on the subjective construal of the situation instead of the objective truth conditions, which means that speakers' personalities and emotions after cognitive processing have an important influence on conversation. However, most existing datasets for conversational AI ignore human personalities and emotions, or only consider part of them. It's difficult for dialogue systems to understand speakers' personalities and emotions although large-scale pre-training language models have been widely used. In order to consider both personalities and emotions in the process of conversation generation, we propose CPED, a large-scale Chinese personalized and emotional dialogue dataset, which consists of multi-source knowledge related to empathy and personal characteristic. These knowledge covers gender, Big Five personality traits, 13 emotions, 19 dialogue acts and 10 scenes. CPED contains more than 12K dialogues of 392 speakers from 40 TV shows. We release the textual dataset with audio features and video features according to the copyright claims, privacy issues, terms of service of video platforms. We provide detailed description of the CPED construction process and introduce three tasks for conversational AI, including personality recognition, emotion recognition in conversations as well as personalized and emotional conversation generation. Finally, we provide baseline systems for these tasks and consider the function of speakers' personalities and emotions on conversation. Our motivation is to propose a dataset to be widely adopted by the NLP community as a new open benchmark for conversational AI research. The full dataset is available at https://github.com/scutcyr/CPED."
429,https://arxiv.org/abs/2205.11409,Many-Class Text Classification with Matching,"In this work, we formulate \textbf{T}ext \textbf{C}lassification as a \textbf{M}atching problem between the text and the labels, and propose a simple yet effective framework named TCM. Compared with previous text classification approaches, TCM takes advantage of the fine-grained semantic information of the classification labels, which helps distinguish each class better when the class number is large, especially in low-resource scenarios. TCM is also easy to implement and is compatible with various large pretrained language models. We evaluate TCM on 4 text classification datasets (each with 20+ labels) in both few-shot and full-data settings, and this model demonstrates significant improvements over other text classification paradigms. We also conduct extensive experiments with different variants of TCM and discuss the underlying factors of its success. Our method and analyses offer a new perspective on text classification."
430,https://arxiv.org/abs/2205.07459,Directed Acyclic Transformer for Non-Autoregressive Machine Translation,"Non-autoregressive Transformers (NATs) significantly reduce the decoding latency by generating all tokens in parallel. However, such independent predictions prevent NATs from capturing the dependencies between the tokens for generating multiple possible translations. In this paper, we propose Directed Acyclic Transfomer (DA-Transformer), which represents the hidden states in a Directed Acyclic Graph (DAG), where each path of the DAG corresponds to a specific translation. The whole DAG simultaneously captures multiple translations and facilitates fast predictions in a non-autoregressive fashion. Experiments on the raw training data of WMT benchmark show that DA-Transformer substantially outperforms previous NATs by about 3 BLEU on average, which is the first NAT model that achieves competitive results with autoregressive Transformers without relying on knowledge distillation."
431,https://arxiv.org/abs/2204.10703,Persona-Guided Planning for Controlling the Protagonist's Persona in Story Generation,"Endowing the protagonist with a specific personality is essential for writing an engaging story. In this paper, we aim to control the protagonist's persona in story generation, i.e., generating a story from a leading context and a persona description, where the protagonist should exhibit the specified personality through a coherent event sequence. Considering that personas are usually embodied implicitly and sparsely in stories, we propose a planning-based generation model named CONPER to explicitly model the relationship between personas and events. CONPER first plans events of the protagonist's behavior which are motivated by the specified persona through predicting one target sentence, then plans the plot as a sequence of keywords with the guidance of the predicted persona-related events and commonsense knowledge, and finally generates the whole story. Both automatic and manual evaluation results demonstrate that CONPER outperforms state-of-the-art baselines for generating more coherent and persona-controllable stories."
432,https://arxiv.org/abs/2204.09438,A Corpus for Understanding and Generating Moral Stories,"Teaching morals is one of the most important purposes of storytelling. An essential ability for understanding and writing moral stories is bridging story plots and implied morals. Its challenges mainly lie in: (1) grasping knowledge about abstract concepts in morals, (2) capturing inter-event discourse relations in stories, and (3) aligning value preferences of stories and morals concerning good or bad behavior. In this paper, we propose two understanding tasks and two generation tasks to assess these abilities of machines. We present STORAL, a new dataset of Chinese and English human-written moral stories. We show the difficulty of the proposed tasks by testing various models with automatic and manual evaluation on STORAL. Furthermore, we present a retrieval-augmented algorithm that effectively exploits related concepts or events in training sets as additional guidance to improve performance on these tasks."
433,https://arxiv.org/abs/2204.07341,LaMemo: Language Modeling with Look-Ahead Memory,"Although Transformers with fully connected self-attentions are powerful to model long-term dependencies, they are struggling to scale to long texts with thousands of words in language modeling. One of the solutions is to equip the model with a recurrence memory. However, existing approaches directly reuse hidden states from the previous segment that encodes contexts in a uni-directional way. As a result, this prohibits the memory to dynamically interact with the current context that provides up-to-date information for token prediction. To remedy this issue, we propose Look-Ahead Memory (LaMemo) that enhances the recurrence memory by incrementally attending to the right-side tokens, and interpolating with the old memory states to maintain long-term information in the history. LaMemo embraces bi-directional attention and segment recurrence with an additional computation overhead only linearly proportional to the memory length. Experiments on widely used language modeling benchmarks demonstrate its superiority over the baselines equipped with different types of memory."
434,https://arxiv.org/abs/2204.00862,CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation,"Existing reference-free metrics have obvious limitations for evaluating controlled text generation models. Unsupervised metrics can only provide a task-agnostic evaluation result which correlates weakly with human judgments, whereas supervised ones may overfit task-specific data with poor generalization ability to other datasets. In this paper, we propose an unsupervised reference-free metric called CTRLEval, which evaluates controlled text generation from different aspects by formulating each aspect into multiple text infilling tasks. On top of these tasks, the metric assembles the generation probabilities from a pre-trained language model without any model training. Experimental results show that our metric has higher correlations with human judgments than other baselines, while obtaining better generalization of evaluating generated texts from different models and with different qualities."
435,https://arxiv.org/abs/2203.14101,A Roadmap for Big Model,"With the rapid development of deep learning, training Big Models (BMs) for multiple downstream tasks becomes a popular paradigm. Researchers have achieved various outcomes in the construction of BMs and the BM application in many fields. At present, there is a lack of research work that sorts out the overall progress of BMs and guides the follow-up research. In this paper, we cover not only the BM technologies themselves but also the prerequisites for BM training and applications with BMs, dividing the BM review into four parts: Resource, Models, Key Technologies and Application. We introduce 16 specific BM-related topics in those four parts, they are Data, Knowledge, Computing System, Parallel Training System, Language Model, Vision Model, Multi-modal Model, Theory&Interpretability, Commonsense Reasoning, Reliability&Security, Governance, Evaluation, Machine Translation, Text Generation, Dialogue and Protein Research. In each topic, we summarize clearly the current studies and propose some future research directions. At the end of this paper, we conclude the further development of BMs in a more general view."
436,https://arxiv.org/abs/2203.12254,Chat-Capsule: A Hierarchical Capsule for Dialog-level Emotion Analysis,"Many studies on dialog emotion analysis focus on utterance-level emotion only. These models hence are not optimized for dialog-level emotion detection, i.e. to predict the emotion category of a dialog as a whole. More importantly, these models cannot benefit from the context provided by the whole dialog. In real-world applications, annotations to dialog could fine-grained, including both utterance-level tags (e.g. speaker type, intent category, and emotion category), and dialog-level tags (e.g. user satisfaction, and emotion curve category). In this paper, we propose a Context-based Hierarchical Attention Capsule~(Chat-Capsule) model, which models both utterance-level and dialog-level emotions and their interrelations. On a dialog dataset collected from customer support of an e-commerce platform, our model is also able to predict user satisfaction and emotion curve category. Emotion curve refers to the change of emotions along the development of a conversation. Experiments show that the proposed Chat-Capsule outperform state-of-the-art baselines on both benchmark dataset and proprietary dataset. Source code will be released upon acceptance."
437,https://arxiv.org/abs/2203.09313,EVA2.0: Investigating Open-Domain Chinese Dialogue Systems with Large-Scale Pre-Training,"Large-scale pre-training has shown remarkable performance in building open-domain dialogue systems. However, previous works mainly focus on showing and evaluating the conversational performance of the released dialogue model, ignoring the discussion of some key factors towards a powerful human-like chatbot, especially in Chinese scenarios. In this paper, we conduct extensive experiments to investigate these under-explored factors, including data quality control, model architecture designs, training approaches, and decoding strategies. We propose EVA2.0, a large-scale pre-trained open-domain Chinese dialogue model with 2.8 billion parameters, and make our models and code publicly available. To our knowledge, EVA2.0 is the largest open-source Chinese dialogue model. Automatic and human evaluations show that our model significantly outperforms other open-source counterparts. We also discuss the limitations of this work by presenting some failure cases and pose some future directions."
438,https://arxiv.org/abs/2203.06654,Continual Prompt Tuning for Dialog State Tracking,"A desirable dialog system should be able to continually learn new skills without forgetting old ones, and thereby adapt to new domains or tasks in its life cycle. However, continually training a model often leads to a well-known catastrophic forgetting issue. In this paper, we present Continual Prompt Tuning, a parameter-efficient framework that not only avoids forgetting but also enables knowledge transfer between tasks. To avoid forgetting, we only learn and store a few prompt tokens' embeddings for each task while freezing the backbone pre-trained model. To achieve bi-directional knowledge transfer among tasks, we propose several techniques (continual prompt initialization, query fusion, and memory replay) to transfer knowledge from preceding tasks and a memory-guided technique to transfer knowledge from subsequent tasks. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method on continual learning for dialog state tracking, compared with state-of-the-art baselines."
439,https://arxiv.org/abs/2203.02645,Acceleration of Federated Learning with Alleviated Forgetting in Local Training,"Federated learning (FL) enables distributed optimization of machine learning models while protecting privacy by independently training local models on each client and then aggregating parameters on a central server, thereby producing an effective global model. Although a variety of FL algorithms have been proposed, their training efficiency remains low when the data are not independently and identically distributed (non-i.i.d.) across different clients. We observe that the slow convergence rates of the existing methods are (at least partially) caused by the catastrophic forgetting issue during the local training stage on each individual client, which leads to a large increase in the loss function concerning the previous training data at the other clients. Here, we propose FedReg, an algorithm to accelerate FL with alleviated knowledge forgetting in the local training stage by regularizing locally trained parameters with the loss on generated pseudo data, which encode the knowledge of previous training data learned by the global model. Our comprehensive experiments demonstrate that FedReg not only significantly improves the convergence rate of FL, especially when the neural network architecture is deep and the clients' data are extremely non-i.i.d., but is also able to protect privacy better in classification problems and more robust against gradient inversion attacks. The code is available at: https://github.com/Zoesgithub/FedReg."
440,https://arxiv.org/abs/2202.13587,Rethinking and Refining the Distinct Metric,"Distinct-$n$ score\cite{Li2016} is a widely used automatic metric for evaluating diversity in language generation tasks. However, we observed that the original approach for calculating distinct scores has evident biases that tend to assign higher penalties to longer sequences. We refine the calculation of distinct scores by scaling the number of distinct tokens based on their expectations. We provide both empirical and theoretical evidence to show that our method effectively removes the biases existing in the original distinct score. Our experiments show that our proposed metric, \textit{Expectation-Adjusted Distinct (EAD)}, correlates better with human judgment in evaluating response diversity. To foster future research, we provide an example implementation at \url{https://github.com/lsy641/Expectation-Adjusted-Distinct}."
441,https://arxiv.org/abs/2202.13047,AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation,"Crowdsourced dialogue corpora are usually limited in scale and topic coverage due to the expensive cost of data curation. This would hinder the generalization of downstream dialogue models to open-domain topics. In this work, we leverage large language models for dialogue augmentation in the task of emotional support conversation (ESC). By treating dialogue augmentation as a dialogue completion task, we prompt a fine-tuned language model to complete full dialogues from available dialogue posts of various topics, which are then postprocessed based on heuristics. Applying this approach, we construct AugESC, an augmented dataset for the ESC task, which largely extends the scale and topic coverage of the crowdsourced ESConv corpus. Through comprehensive human evaluation, we demonstrate that our approach is superior to strong baselines of dialogue augmentation and that AugESC has comparable dialogue quality to the crowdsourced corpus. We also conduct human interactive evaluation and prove that post-training on AugESC improves downstream dialogue models' generalization ability to open-domain topics. These results suggest the utility of AugESC and highlight the potential of large language models in improving data-scarce dialogue generation tasks."
442,https://arxiv.org/abs/2202.08011,"Towards Identifying Social Bias in Dialog Systems: Frame, Datasets, and Benchmarks","The research of open-domain dialog systems has been greatly prospered by neural models trained on large-scale corpora, however, such corpora often introduce various safety problems (e.g., offensive languages, biases, and toxic behaviors) that significantly hinder the deployment of dialog systems in practice. Among all these unsafe issues, addressing social bias is more complex as its negative impact on marginalized populations is usually expressed implicitly, thus requiring normative reasoning and rigorous analysis. In this paper, we focus our investigation on social bias detection of dialog safety problems. We first propose a novel Dial-Bias Frame for analyzing the social bias in conversations pragmatically, which considers more comprehensive bias-related analyses rather than simple dichotomy annotations. Based on the proposed framework, we further introduce CDail-Bias Dataset that, to our knowledge, is the first well-annotated Chinese social bias dialog dataset. In addition, we establish several dialog bias detection benchmarks at different label granularities and input types (utterance-level and context-level). We show that the proposed in-depth analyses together with these benchmarks in our Dial-Bias Frame are necessary and essential to bias detection tasks and can benefit building safe dialog systems in practice."
443,https://arxiv.org/abs/2201.06724,Youling: an AI-Assisted Lyrics Creation System,"Recently, a variety of neural models have been proposed for lyrics generation. However, most previous work completes the generation process in a single pass with little human intervention. We believe that lyrics creation is a creative process with human intelligence centered. AI should play a role as an assistant in the lyrics creation process, where human interactions are crucial for high-quality creation. This paper demonstrates \textit{Youling}, an AI-assisted lyrics creation system, designed to collaborate with music creators. In the lyrics generation process, \textit{Youling} supports traditional one pass full-text generation mode as well as an interactive generation mode, which allows users to select the satisfactory sentences from generated candidates conditioned on preceding context. The system also provides a revision module which enables users to revise undesired sentences or words of lyrics repeatedly. Besides, \textit{Youling} allows users to use multifaceted attributes to control the content and format of generated lyrics. The demo video of the system is available at https://youtu.be/DFeNpHk0pm4."
444,https://arxiv.org/abs/2201.06025,COLD: A Benchmark for Chinese Offensive Language Detection,"Offensive language detection is increasingly crucial for maintaining a civilized social media platform and deploying pre-trained language models. However, this task in Chinese is still under exploration due to the scarcity of reliable datasets. To this end, we propose a benchmark --COLD for Chinese offensive language analysis, including a Chinese Offensive Language Dataset --COLDATASET and a baseline detector --COLDETECTOR which is trained on the dataset. We show that the COLD benchmark contributes to Chinese offensive language detection which is challenging for existing resources. We then deploy the COLDETECTOR and conduct detailed analyses on popular Chinese pre-trained language models. We first analyze the offensiveness of existing generative models and show that these models inevitably expose varying degrees of offensive issues. Furthermore, we investigate the factors that influence the offensive generations, and we find that anti-bias contents and keywords referring to certain groups or revealing negative attitudes trigger offensive outputs easier."
445,https://arxiv.org/abs/2112.13610,CUGE: A Chinese Language Understanding and Generation Evaluation Benchmark,"Realizing general-purpose language intelligence has been a longstanding goal for natural language processing, where standard evaluation benchmarks play a fundamental and guiding role. We argue that for general-purpose language intelligence evaluation, the benchmark itself needs to be comprehensive and systematic. To this end, we propose CUGE, a Chinese Language Understanding and Generation Evaluation benchmark with the following features: (1) Hierarchical benchmark framework, where datasets are principally selected and organized with a language capability-task-dataset hierarchy. (2) Multi-level scoring strategy, where different levels of model performance are provided based on the hierarchical framework. To facilitate CUGE, we provide a public leaderboard that can be customized to support flexible model judging criteria. Evaluation results on representative pre-trained language models indicate ample room for improvement towards general-purpose language intelligence. CUGE is publicly available at cuge.baai.ac.cn."
446,https://arxiv.org/abs/2111.00667,Unsupervised Domain Adaptation with Adapter,"Unsupervised domain adaptation (UDA) with pre-trained language models (PrLM) has achieved promising results since these pre-trained models embed generic knowledge learned from various domains. However, fine-tuning all the parameters of the PrLM on a small domain-specific corpus distort the learned generic knowledge, and it is also expensive to deployment a whole fine-tuned PrLM for each domain. This paper explores an adapter-based fine-tuning approach for unsupervised domain adaptation. Specifically, several trainable adapter modules are inserted in a PrLM, and the embedded generic knowledge is preserved by fixing the parameters of the original PrLM at fine-tuning. A domain-fusion scheme is introduced to train these adapters using a mix-domain corpus to better capture transferable features. Elaborated experiments on two benchmark datasets are carried out, and the results demonstrate that our approach is effective with different tasks, dataset sizes, and domain similarities."
447,https://arxiv.org/abs/2110.08544,Answering Open-Domain Multi-Answer Questions via a Recall-then-Verify Framework,"Open-domain questions are likely to be open-ended and ambiguous, leading to multiple valid answers. Existing approaches typically adopt the rerank-then-read framework, where a reader reads top-ranking evidence to predict answers. According to our empirical analysis, this framework faces three problems: first, to leverage a large reader under a memory constraint, the reranker should select only a few relevant passages to cover diverse answers, while balancing relevance and diversity is non-trivial; second, the small reading budget prevents the reader from accessing valuable retrieved evidence filtered out by the reranker; third, when using a generative reader to predict answers all at once based on all selected evidence, whether a valid answer will be predicted also pathologically depends on the evidence of some other valid answer(s). To address these issues, we propose to answer open-domain multi-answer questions with a recall-then-verify framework, which separates the reasoning process of each answer so that we can make better use of retrieved evidence while also leveraging large models under the same memory constraint. Our framework achieves state-of-the-art results on two multi-answer datasets, and predicts significantly more gold answers than a rerank-then-read system that uses an oracle reranker."
448,https://arxiv.org/abs/2110.08466,"On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark","Dialogue safety problems severely limit the real-world deployment of neural conversational models and have attracted great research interests recently. However, dialogue safety problems remain under-defined and the corresponding dataset is scarce. We propose a taxonomy for dialogue safety specifically designed to capture unsafe behaviors in human-bot dialogue settings, with focuses on context-sensitive unsafety, which is under-explored in prior works. To spur research in this direction, we compile DiaSafety, a dataset with rich context-sensitive unsafe examples. Experiments show that existing safety guarding tools fail severely on our dataset. As a remedy, we train a dialogue safety classifier to provide a strong baseline for context-sensitive dialogue unsafety detection. With our classifier, we perform safety evaluations on popular conversational models and show that existing dialogue systems still exhibit concerning context-sensitive safety problems."
449,https://arxiv.org/abs/2110.05999,DiscoDVT: Generating Long Text with Discourse-Aware Discrete Variational Transformer,"Despite the recent advances in applying pre-trained language models to generate high-quality texts, generating long passages that maintain long-range coherence is yet challenging for these models. In this paper, we propose DiscoDVT, a discourse-aware discrete variational Transformer to tackle the incoherence issue. DiscoDVT learns a discrete variable sequence that summarizes the global structure of the text and then applies it to guide the generation process at each decoding step. To further embed discourse-aware information into the discrete latent representations, we introduce an auxiliary objective to model the discourse relations within the text. We conduct extensive experiments on two open story generation datasets and demonstrate that the latent codes learn meaningful correspondence to the discourse structures that guide the model to generate long texts with better long-range coherence."
450,https://arxiv.org/abs/2109.07713,Transferable Persona-Grounded Dialogues via Grounded Minimal Edits,"Grounded dialogue models generate responses that are grounded on certain concepts. Limited by the distribution of grounded dialogue data, models trained on such data face the transferability challenges in terms of the data distribution and the type of grounded concepts. To address the challenges, we propose the grounded minimal editing framework, which minimally edits existing responses to be grounded on the given concept. Focusing on personas, we propose Grounded Minimal Editor (GME), which learns to edit by disentangling and recombining persona-related and persona-agnostic parts of the response. To evaluate persona-grounded minimal editing, we present the PersonaMinEdit dataset, and experimental results show that GME outperforms competitive baselines by a large margin. To evaluate the transferability, we experiment on the test set of BlendedSkillTalk and show that GME can edit dialogue models' responses to largely improve their persona consistency while preserving the use of knowledge and empathy."
451,https://arxiv.org/abs/2109.06513,Exploring Prompt-based Few-shot Learning for Grounded Dialog Generation,"Dialog models can be greatly strengthened through grounding on various external information, but grounded dialog corpora are usually not naturally accessible. In this work, we focus on the few-shot learning for grounded dialog generation (GDG). We first propose a simple prompting method for GDG tasks, where different constructs of model input, such as the grounding source and the conversation context, are distinguished through continuous or discrete prompts. On three typical GDG tasks, we empirically demonstrate and analyze in-depth the effectiveness of our method. We then conduct extensive experiments to thoroughly investigate how our prompting method works with different pre-trained models. We show that prompted language models perform superiorly to conversational models, and further analyze various factors that influence the effects of prompting. Overall, our work introduces a prompt-based perspective to the few-shot learning for GDG tasks, and provides valuable findings and insights for future research."
452,https://arxiv.org/abs/2109.05739,CEM: Commonsense-aware Empathetic Response Generation,"A key trait of daily conversations between individuals is the ability to express empathy towards others, and exploring ways to implement empathy is a crucial step towards human-like dialogue systems. Previous approaches on this topic mainly focus on detecting and utilizing the user's emotion for generating empathetic responses. However, since empathy includes both aspects of affection and cognition, we argue that in addition to identifying the user's emotion, cognitive understanding of the user's situation should also be considered. To this end, we propose a novel approach for empathetic response generation, which leverages commonsense to draw more information about the user's situation and uses this additional information to further enhance the empathy expression in generated responses. We evaluate our approach on EmpatheticDialogues, which is a widely-used benchmark dataset for empathetic response generation. Empirical results demonstrate that our approach outperforms the baseline models in both automatic and human evaluations and can generate more informative and empathetic responses."
453,https://arxiv.org/abs/2109.04332,PPT: Pre-trained Prompt Tuning for Few-shot Learning,"Prompts for pre-trained language models (PLMs) have shown remarkable performance by bridging the gap between pre-training tasks and various downstream tasks. Among these methods, prompt tuning, which freezes PLMs and only tunes soft prompts, provides an efficient and effective solution for adapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to be fully explored. In our pilot experiments, we find that prompt tuning performs comparably with conventional full-model fine-tuning when downstream data are sufficient, whereas it performs much worse under few-shot learning settings, which may hinder the application of prompt tuning in practice. We attribute this low performance to the manner of initializing soft prompts. Therefore, in this work, we propose to pre-train prompts by adding soft prompts into the pre-training stage to obtain a better initialization. We name this Pre-trained Prompt Tuning framework ""PPT"". To ensure the generalization of PPT, we formulate similar classification tasks into a unified task form and pre-train soft prompts for this unified task. Extensive experiments show that tuning pre-trained prompts for downstream tasks can reach or even outperform full-model fine-tuning under both full-data and few-shot settings. Our approach is effective and efficient for using large-scale PLMs in practice."
454,https://arxiv.org/abs/2108.12960,LOT: A Story-Centric Benchmark for Evaluating Chinese Long Text Understanding and Generation,"Standard multi-task benchmarks are essential for developing pretraining models that can generalize to various downstream tasks. Existing benchmarks for natural language processing (NLP) usually focus only on understanding or generating short texts. However, long text modeling requires many distinct abilities in contrast to short texts, such as the modeling of long-range discourse and commonsense relations, and the coherence and controllability of generation. The lack of standardized benchmarks makes it difficult to assess these abilities of a model and fairly compare different models, especially Chinese models. Therefore, we propose a story-centric benchmark named LOT for evaluating Chinese long text modeling, which aggregates two understanding tasks and two generation tasks. We construct new datasets for these tasks based on human-written Chinese stories with hundreds of words. Furthermore, we release an encoder-decoder-based Chinese long text pretraining model named LongLM with up to 1 billion parameters. We pretrain LongLM on 120G Chinese novels with two generative tasks including text infilling and conditional continuation. Extensive experiments show that LongLM outperforms similar-sized pretraining models substantially on both the understanding and generation tasks in LOT."
455,https://arxiv.org/abs/2108.12589,Self-training Improves Pre-training for Few-shot Learning in Task-oriented Dialog Systems,"As the labeling cost for different modules in task-oriented dialog (ToD) systems is expensive, a major challenge is to train different modules with the least amount of labeled data. Recently, large-scale pre-trained language models, have shown promising results for few-shot learning in ToD. In this paper, we devise a self-training approach to utilize the abundant unlabeled dialog data to further improve state-of-the-art pre-trained models in few-shot learning scenarios for ToD systems. Specifically, we propose a self-training approach that iteratively labels the most confident unlabeled data to train a stronger Student model. Moreover, a new text augmentation technique (GradAug) is proposed to better train the Student by replacing non-crucial tokens using a masked language model. We conduct extensive experiments and present analyses on four downstream tasks in ToD, including intent classification, dialog state tracking, dialog act prediction, and response selection. Empirical results demonstrate that the proposed self-training approach consistently improves state-of-the-art pre-trained models (BERT, ToD-BERT) when only a small number of labeled data are available."
456,https://arxiv.org/abs/2108.01547,EVA: An Open-Domain Chinese Dialogue System with Large-Scale Generative Pre-Training,"Although pre-trained language models have remarkably enhanced the generation ability of dialogue systems, open-domain Chinese dialogue systems are still limited by the dialogue data and the model size compared with English ones. In this paper, we propose EVA, a Chinese dialogue system that contains the largest Chinese pre-trained dialogue model with 2.8B parameters. To build this model, we collect the largest Chinese dialogue dataset named WDC-Dialogue from various public social media. This dataset contains 1.4B context-response pairs and is used as the pre-training corpus of EVA. Extensive experiments on automatic and human evaluation show that EVA outperforms other Chinese pre-trained dialogue models especially in the multi-turn interaction of human-bot conversations."
457,https://arxiv.org/abs/2106.11796,End-to-End Task-Oriented Dialog Modeling with Semi-Structured Knowledge Management,"Current task-oriented dialog (TOD) systems mostly manage structured knowledge (e.g. databases and tables) to guide the goal-oriented conversations. However, they fall short of handling dialogs which also involve unstructured knowledge (e.g. reviews and documents). In this paper, we formulate a task of modeling TOD grounded on a fusion of structured and unstructured knowledge. To address this task, we propose a TOD system with semi-structured knowledge management, SeKnow, which extends the belief state to manage knowledge with both structured and unstructured contents. Furthermore, we introduce two implementations of SeKnow based on a non-pretrained sequence-to-sequence model and a pretrained language model, respectively. Both implementations use the end-to-end manner to jointly optimize dialog modeling grounded on structured and unstructured knowledge. We conduct experiments on a modified version of MultiWOZ 2.1 dataset, Mod-MultiWOZ 2.1, where dialogs are processed to involve semi-structured knowledge. Experimental results show that SeKnow has strong performances in both end-to-end dialog and intermediate knowledge management, compared to existing TOD systems and their extensions with pipeline knowledge management schemes."
458,https://arxiv.org/abs/2106.10715,CPM-2: Large-scale Cost-effective Pre-trained Language Models,"In recent years, the size of pre-trained language models (PLMs) has grown by leaps and bounds. However, efficiency issues of these large-scale PLMs limit their utilization in real-world scenarios. We present a suite of cost-effective techniques for the use of PLMs to deal with the efficiency issues of pre-training, fine-tuning, and inference. (1) We introduce knowledge inheritance to accelerate the pre-training process by exploiting existing PLMs instead of training models from scratch. (2) We explore the best practice of prompt tuning with large-scale PLMs. Compared with conventional fine-tuning, prompt tuning significantly reduces the number of task-specific parameters. (3) We implement a new inference toolkit, namely InfMoE, for using large-scale PLMs with limited computational resources. Based on our cost-effective pipeline, we pre-train two models: an encoder-decoder bilingual model with 11 billion parameters (CPM-2) and its corresponding MoE version with 198 billion parameters. In our experiments, we compare CPM-2 with mT5 on downstream tasks. Experimental results show that CPM-2 has excellent general language intelligence. Moreover, we validate the efficiency of InfMoE when conducting inference of large-scale models having tens of billions of parameters on a single GPU. All source code and model parameters are available at https://github.com/TsinghuaAI/CPM."
459,https://arxiv.org/abs/2106.10502,JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs,"Existing pre-trained models for knowledge-graph-to-text (KG-to-text) generation simply fine-tune text-to-text pre-trained models such as BART or T5 on KG-to-text datasets, which largely ignore the graph structure during encoding and lack elaborate pre-training tasks to explicitly model graph-text alignments. To tackle these problems, we propose a graph-text joint representation learning model called JointGT. During encoding, we devise a structure-aware semantic aggregation module which is plugged into each Transformer layer to preserve the graph structure. Furthermore, we propose three new pre-training tasks to explicitly enhance the graph-text alignment including respective text / graph reconstruction, and graph-text alignment in the embedding space via Optimal Transport. Experiments show that JointGT obtains new state-of-the-art performance on various KG-to-text datasets."
460,https://arxiv.org/abs/2106.07174,A Mutual Information Maximization Approach for the Spurious Solution Problem in Weakly Supervised Question Answering,"Weakly supervised question answering usually has only the final answers as supervision signals while the correct solutions to derive the answers are not provided. This setting gives rise to the spurious solution problem: there may exist many spurious solutions that coincidentally derive the correct answer, but training on such solutions can hurt model performance (e.g., producing wrong solutions or answers). For example, for discrete reasoning tasks as on DROP, there may exist many equations to derive a numeric answer, and typically only one of them is correct. Previous learning methods mostly filter out spurious solutions with heuristics or using model confidence, but do not explicitly exploit the semantic correlations between a question and its solution. In this paper, to alleviate the spurious solution problem, we propose to explicitly exploit such semantic correlations by maximizing the mutual information between question-answer pairs and predicted solutions. Extensive experiments on four question answering datasets show that our method significantly outperforms previous learning methods in terms of task performance and is more effective in training models to produce correct solutions."
461,https://arxiv.org/abs/2106.07139,"Pre-Trained Models: Past, Present and Future","Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we comprehensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increasing availability of data, towards four important directions: designing effective architectures, utilizing rich contexts, improving computational efficiency, and conducting interpretation and theoretical analysis. Finally, we discuss a series of open problems and research directions of PTMs, and hope our view can inspire and advance the future study of PTMs."
462,https://arxiv.org/abs/2106.03065,Semantic-Enhanced Explainable Finetuning for Open-Domain Dialogues,"This paper propose to combine pretrained language models with the modular dialogue paradigm for open-domain dialogue modeling. Our method, semantic-enhanced finetuning, instantiates conversation understanding, planning, and response generation as a language model finetuning task. At inference, we disentangle semantic and token variations by specifying sampling methods and constraints for each module separately. For training and evaluation, we present X-Weibo, a Chinese multi-turn open-domain dialogue dataset with automatic annotation for emotions, DAs, and topical words. Experiments show that semantic-enhanced finetuning outperforms strong baselines on non-semantic and semantic metrics, improves the human-evaluated relevance, coherence, and informativeness, and exhibits considerable controllability over semantic variables."
463,https://arxiv.org/abs/2106.02210,NAST: A Non-Autoregressive Generator with Word Alignment for Unsupervised Text Style Transfer,"Autoregressive models have been widely used in unsupervised text style transfer. Despite their success, these models still suffer from the content preservation problem that they usually ignore part of the source sentence and generate some irrelevant words with strong styles. In this paper, we propose a Non-Autoregressive generator for unsupervised text Style Transfer (NAST), which alleviates the problem from two aspects. First, we observe that most words in the transferred sentence can be aligned with related words in the source sentence, so we explicitly model word alignments to suppress irrelevant words. Second, existing models trained with the cycle loss align sentences in two stylistic text spaces, which lacks fine-grained control at the word level. The proposed non-autoregressive generator focuses on the connections between aligned words, which learns the word-level transfer between styles. For experiments, we integrate the proposed generator into two base models and evaluate them on two style transfer tasks. The results show that NAST can significantly improve the overall performance and provide explainable word alignments. Moreover, the non-autoregressive generator achieves over 10x speedups at inference. Our codes are available at https://github.com/thu-coai/NAST."
464,https://arxiv.org/abs/2106.01702,PsyQA: A Chinese Dataset for Generating Long Counseling Text for Mental Health Support,"Great research interests have been attracted to devise AI services that are able to provide mental health support. However, the lack of corpora is a main obstacle to this research, particularly in Chinese language. In this paper, we propose PsyQA, a Chinese dataset of psychological health support in the form of question and answer pair. PsyQA is crawled from a Chinese mental health service platform, and contains 22K questions and 56K long and well-structured answers. Based on the psychological counseling theories, we annotate a portion of answer texts with typical strategies for providing support, and further present in-depth analysis of both lexical features and strategy patterns in the counseling answers. We also evaluate the performance of generating counseling answers with the generative pretrained models. Results show that utilizing strategies enhances the fluency and helpfulness of generated answers, but there is still a large space for future research."
465,https://arxiv.org/abs/2106.01144,Towards Emotional Support Dialog Systems,"Emotional support is a crucial ability for many conversation scenarios, including social interactions, mental health support, and customer service chats. Following reasonable procedures and using various support skills can help to effectively provide support. However, due to the lack of a well-designed task and corpora of effective emotional support conversations, research on building emotional support into dialog systems remains untouched. In this paper, we define the Emotional Support Conversation (ESC) task and propose an ESC Framework, which is grounded on the Helping Skills Theory. We construct an Emotion Support Conversation dataset (ESConv) with rich annotation (especially support strategy) in a help-seeker and supporter mode. To ensure a corpus of high-quality conversations that provide examples of effective emotional support, we take extensive effort to design training tutorials for supporters and several mechanisms for quality control during data collection. Finally, we evaluate state-of-the-art dialog models with respect to the ability to provide emotional support. Our results show the importance of support strategies in providing effective emotional support and the utility of ESConv in training more emotional support systems."
466,https://arxiv.org/abs/2105.14781,A Semantic-based Method for Unsupervised Commonsense Question Answering,"Unsupervised commonsense question answering is appealing since it does not rely on any labeled task data. Among existing work, a popular solution is to use pre-trained language models to score candidate choices directly conditioned on the question or context. However, such scores from language models can be easily affected by irrelevant factors, such as word frequencies, sentence structures, etc. These distracting factors may not only mislead the model to choose a wrong answer but also make it oversensitive to lexical perturbations in candidate answers.
  In this paper, we present a novel SEmantic-based Question Answering method (SEQA) for unsupervised commonsense question answering. Instead of directly scoring each answer choice, our method first generates a set of plausible answers with generative models (e.g., GPT-2), and then uses these plausible answers to select the correct choice by considering the semantic similarity between each plausible answer and each choice. We devise a simple, yet sound formalism for this idea and verify its effectiveness and robustness with extensive experiments. We evaluate the proposed method on four benchmark datasets, and our method achieves the best results in unsupervised settings. Moreover, when attacked by TextFooler with synonym replacement, SEQA demonstrates much less performance drops than baselines, thereby indicating stronger robustness."
467,https://arxiv.org/abs/2105.14556,Diversifying Dialog Generation via Adaptive Label Smoothing,"Neural dialogue generation models trained with the one-hot target distribution suffer from the over-confidence issue, which leads to poor generation diversity as widely reported in the literature. Although existing approaches such as label smoothing can alleviate this issue, they fail to adapt to diverse dialog contexts. In this paper, we propose an Adaptive Label Smoothing (AdaLabel) approach that can adaptively estimate a target label distribution at each time step for different contexts. The maximum probability in the predicted distribution is used to modify the soft target distribution produced by a novel light-weight bi-directional decoder module. The resulting target distribution is aware of both previous and future contexts and is adjusted to avoid over-training the dialogue model. Our model can be trained in an end-to-end manner. Extensive experiments on two benchmark datasets show that our approach outperforms various competitive baselines in producing diverse responses."
468,https://arxiv.org/abs/2105.08963,Long Text Generation by Modeling Sentence-Level and Discourse-Level Coherence,"Generating long and coherent text is an important but challenging task, particularly for open-ended language generation tasks such as story generation. Despite the success in modeling intra-sentence coherence, existing generation models (e.g., BART) still struggle to maintain a coherent event sequence throughout the generated text. We conjecture that this is because of the difficulty for the decoder to capture the high-level semantics and discourse structures in the context beyond token-level co-occurrence. In this paper, we propose a long text generation model, which can represent the prefix sentences at sentence level and discourse level in the decoding process. To this end, we propose two pretraining objectives to learn the representations by predicting inter-sentence semantic similarity and distinguishing between normal and shuffled sentence orders. Extensive experiments show that our model can generate more coherent texts than state-of-the-art baselines."
469,https://arxiv.org/abs/2105.08920,OpenMEVA: A Benchmark for Evaluating Open-ended Story Generation Metrics,"Automatic metrics are essential for developing natural language generation (NLG) models, particularly for open-ended language generation tasks such as story generation. However, existing automatic metrics are observed to correlate poorly with human evaluation. The lack of standardized benchmark datasets makes it difficult to fully evaluate the capabilities of a metric and fairly compare different metrics. Therefore, we propose OpenMEVA, a benchmark for evaluating open-ended story generation metrics. OpenMEVA provides a comprehensive test suite to assess the capabilities of metrics, including (a) the correlation with human judgments, (b) the generalization to different model outputs and datasets, (c) the ability to judge story coherence, and (d) the robustness to perturbations. To this end, OpenMEVA includes both manually annotated stories and auto-constructed test examples. We evaluate existing metrics on OpenMEVA and observe that they have poor correlation with human judgments, fail to recognize discourse-level incoherence, and lack inferential knowledge (e.g., causal order between events), the generalization ability and robustness. Our study presents insights for developing NLG models and metrics in further research."
470,https://arxiv.org/abs/2105.08625,Stylized Story Generation with Style-Guided Planning,"Current storytelling systems focus more ongenerating stories with coherent plots regard-less of the narration style, which is impor-tant for controllable text generation. There-fore, we propose a new task, stylized story gen-eration, namely generating stories with speci-fied style given a leading context. To tacklethe problem, we propose a novel generationmodel that first plans the stylized keywordsand then generates the whole story with theguidance of the keywords. Besides, we pro-pose two automatic metrics to evaluate theconsistency between the generated story andthe specified style. Experiments demonstratesthat our model can controllably generateemo-tion-driven orevent-driven stories based onthe ROCStories dataset (Mostafazadeh et al.,2016). Our study presents insights for stylizedstory generation in further research."
471,https://arxiv.org/abs/2105.08316,CoMAE: A Multi-factor Hierarchical Framework for Empathetic Response Generation,"The capacity of empathy is crucial to the success of open-domain dialog systems. Due to its nature of multi-dimensionality, there are various factors that relate to empathy expression, such as communication mechanism, dialog act and emotion. However, existing methods for empathetic response generation usually either consider only one empathy factor or ignore the hierarchical relationships between different factors, leading to a weak ability of empathy modeling. In this paper, we propose a multi-factor hierarchical framework, CoMAE, for empathetic response generation, which models the above three key factors of empathy expression in a hierarchical way. We show experimentally that our CoMAE-based model can generate more empathetic responses than previous methods. We also highlight the importance of hierarchical modeling of different factors through both the empirical analysis on a real-life corpus and the extensive experiments. Our codes and used data are available at https://github.com/chujiezheng/CoMAE."
472,https://arxiv.org/abs/2105.06041,HyKnow: End-to-End Task-Oriented Dialog Modeling with Hybrid Knowledge Management,"Task-oriented dialog (TOD) systems typically manage structured knowledge (e.g. ontologies and databases) to guide the goal-oriented conversations. However, they fall short of handling dialog turns grounded on unstructured knowledge (e.g. reviews and documents). In this paper, we formulate a task of modeling TOD grounded on both structured and unstructured knowledge. To address this task, we propose a TOD system with hybrid knowledge management, HyKnow. It extends the belief state to manage both structured and unstructured knowledge, and is the first end-to-end model that jointly optimizes dialog modeling grounded on these two kinds of knowledge. We conduct experiments on the modified version of MultiWOZ 2.1 dataset, where dialogs are grounded on hybrid knowledge. Experimental results show that HyKnow has strong end-to-end performance compared to existing TOD systems. It also outperforms the pipeline knowledge management schemes, with higher unstructured knowledge retrieval accuracy."
473,https://arxiv.org/abs/2012.15262,Robustness Testing of Language Understanding in Task-Oriented Dialog,"Most language understanding models in task-oriented dialog systems are trained on a small amount of annotated training data, and evaluated in a small set from the same distribution. However, these models can lead to system failure or undesirable output when being exposed to natural language perturbation or variation in practice. In this paper, we conduct comprehensive evaluation and analysis with respect to the robustness of natural language understanding models, and introduce three important aspects related to language understanding in real-world dialog systems, namely, language variety, speech characteristics, and noise perturbation. We propose a model-agnostic toolkit LAUG to approximate natural language perturbations for testing the robustness issues in task-oriented dialog. Four data augmentation approaches covering the three aspects are assembled in LAUG, which reveals critical robustness issues in state-of-the-art models. The augmented dataset through LAUG can be used to facilitate future research on the robustness testing of language understanding in task-oriented dialog."
474,https://arxiv.org/abs/2012.15022,ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning,"Pre-trained Language Models (PLMs) have shown superior performance on various downstream Natural Language Processing (NLP) tasks. However, conventional pre-training objectives do not explicitly model relational facts in text, which are crucial for textual understanding. To address this issue, we propose a novel contrastive learning framework ERICA to obtain a deep understanding of the entities and their relations in text. Specifically, we define two novel pre-training tasks to better understand entities and relations: (1) the entity discrimination task to distinguish which tail entity can be inferred by the given head entity and relation; (2) the relation discrimination task to distinguish whether two relations are close or not semantically, which involves complex relational reasoning. Experimental results demonstrate that ERICA can improve typical PLMs (BERT and RoBERTa) on several language understanding tasks, including relation extraction, entity typing and question answering, especially under low-resource settings."
475,https://arxiv.org/abs/2012.10235,AdvExpander: Generating Natural Language Adversarial Examples by Expanding Text,"Adversarial examples are vital to expose the vulnerability of machine learning models. Despite the success of the most popular substitution-based methods which substitutes some characters or words in the original examples, only substitution is insufficient to uncover all robustness issues of models. In this paper, we present AdvExpander, a method that crafts new adversarial examples by expanding text, which is complementary to previous substitution-based methods. We first utilize linguistic rules to determine which constituents to expand and what types of modifiers to expand with. We then expand each constituent by inserting an adversarial modifier searched from a CVAE-based generative model which is pre-trained on a large scale corpus. To search adversarial modifiers, we directly search adversarial latent codes in the latent space without tuning the pre-trained parameters. To ensure that our adversarial examples are label-preserving for text matching, we also constrain the modifications with a heuristic rule. Experiments on three classification tasks verify the effectiveness of AdvExpander and the validity of our adversarial examples. AdvExpander crafts a new type of adversarial examples by text expansion, thereby promising to reveal new robustness issues."
476,https://arxiv.org/abs/2012.00413,CPM: A Large-scale Generative Chinese Pre-trained Language Model,"Pre-trained Language Models (PLMs) have proven to be beneficial for various downstream NLP tasks. Recently, GPT-3, with 175 billion parameters and 570GB training data, drew a lot of attention due to the capacity of few-shot (even zero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is still challenging, as the training corpus of GPT-3 is primarily English, and the parameters are not publicly available. In this technical report, we release the Chinese Pre-trained Language Model (CPM) with generative pre-training on large-scale Chinese training data. To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language model, which could facilitate several downstream Chinese NLP tasks, such as conversation, essay generation, cloze test, and language understanding. Extensive experiments demonstrate that CPM achieves strong performance on many NLP tasks in the settings of few-shot (even zero-shot) learning. The code and parameters are available at https://github.com/TsinghuaAI/CPM-Generate."
477,https://arxiv.org/abs/2011.07225,Reinforced Molecular Optimization with Neighborhood-Controlled Grammars,"A major challenge in the pharmaceutical industry is to design novel molecules with specific desired properties, especially when the property evaluation is costly. Here, we propose MNCE-RL, a graph convolutional policy network for molecular optimization with molecular neighborhood-controlled embedding grammars through reinforcement learning. We extend the original neighborhood-controlled embedding grammars to make them applicable to molecular graph generation and design an efficient algorithm to infer grammatical production rules from given molecules. The use of grammars guarantees the validity of the generated molecular structures. By transforming molecular graphs to parse trees with the inferred grammars, the molecular structure generation task is modeled as a Markov decision process where a policy gradient strategy is utilized. In a series of experiments, we demonstrate that our approach achieves state-of-the-art performance in a diverse range of molecular optimization tasks and exhibits significant superiority in optimizing molecular properties with a limited number of property evaluations."
478,https://arxiv.org/abs/2011.06486,Overview of the Ninth Dialog System Technology Challenge: DSTC9,"This paper introduces the Ninth Dialog System Technology Challenge (DSTC-9). This edition of the DSTC focuses on applying end-to-end dialog technologies for four distinct tasks in dialog systems, namely, 1. Task-oriented dialog Modeling with unstructured knowledge access, 2. Multi-domain task-oriented dialog, 3. Interactive evaluation of dialog, and 4. Situated interactive multi-modal dialog. This paper describes the task definition, provided datasets, baselines and evaluation set-up for each track. We also summarize the results of the submitted systems to highlight the overall trends of the state-of-the-art technologies for the tasks."
479,https://arxiv.org/abs/2010.10333,CR-Walker: Tree-Structured Graph Reasoning and Dialog Acts for Conversational Recommendation,"Growing interests have been attracted in Conversational Recommender Systems (CRS), which explore user preference through conversational interactions in order to make appropriate recommendation. However, there is still a lack of ability in existing CRS to (1) traverse multiple reasoning paths over background knowledge to introduce relevant items and attributes, and (2) arrange selected entities appropriately under current system intents to control response generation. To address these issues, we propose CR-Walker in this paper, a model that performs tree-structured reasoning on a knowledge graph, and generates informative dialog acts to guide language generation. The unique scheme of tree-structured reasoning views the traversed entity at each hop as part of dialog acts to facilitate language generation, which links how entities are selected and expressed. Automatic and human evaluations show that CR-Walker can arrive at more accurate recommendation, and generate more informative and engaging responses."
480,https://arxiv.org/abs/2010.05594,MultiWOZ 2.3: A multi-domain task-oriented dialogue dataset enhanced with annotation corrections and co-reference annotation,"Task-oriented dialogue systems have made unprecedented progress with multiple state-of-the-art (SOTA) models underpinned by a number of publicly available MultiWOZ datasets. Dialogue state annotations are error-prone, leading to sub-optimal performance. Various efforts have been put in rectifying the annotation errors presented in the original MultiWOZ dataset. In this paper, we introduce MultiWOZ 2.3, in which we differentiate incorrect annotations in dialogue acts from dialogue states, identifying a lack of co-reference when publishing the updated dataset. To ensure consistency between dialogue acts and dialogue states, we implement co-reference features and unify annotations of dialogue acts and dialogue states. We update the state of the art performance of natural language understanding and dialogue state tracking on MultiWOZ 2.3, where the results show significant improvements than on previous versions of MultiWOZ datasets (2.0-2.2)."
481,https://arxiv.org/abs/2010.00910,Continual Learning for Natural Language Generation in Task-oriented Dialog Systems,"Natural language generation (NLG) is an essential component of task-oriented dialog systems. Despite the recent success of neural approaches for NLG, they are typically developed in an offline manner for particular domains. To better fit real-life applications where new data come in a stream, we study NLG in a ""continual learning"" setting to expand its knowledge to new domains or functionalities incrementally. The major challenge towards this goal is catastrophic forgetting, meaning that a continually trained model tends to forget the knowledge it has learned before. To this end, we propose a method called ARPER (Adaptively Regularized Prioritized Exemplar Replay) by replaying prioritized historical exemplars, together with an adaptive regularization technique based on ElasticWeight Consolidation. Extensive experiments to continually learn new domains and intents are conducted on MultiWoZ-2.0 to benchmark ARPER with a wide range of techniques. Empirical results demonstrate that ARPER significantly outperforms other methods by effectively mitigating the detrimental catastrophic forgetting issue."
482,https://arxiv.org/abs/2009.12719,Stylized Dialogue Response Generation Using Stylized Unpaired Texts,"Generating stylized responses is essential to build intelligent and engaging dialogue systems. However, this task is far from well-explored due to the difficulties of rendering a particular style in coherent responses, especially when the target style is embedded only in unpaired texts that cannot be directly used to train the dialogue model. This paper proposes a stylized dialogue generation method that can capture stylistic features embedded in unpaired texts. Specifically, our method can produce dialogue responses that are both coherent to the given context and conform to the target style. In this study, an inverse dialogue model is first introduced to predict possible posts for the input responses, and then this inverse model is used to generate stylized pseudo dialogue pairs based on these stylized unpaired texts. Further, these pseudo pairs are employed to train the stylized dialogue model with a joint training process, and a style routing approach is proposed to intensify stylistic features in the decoder. Automatic and manual evaluations on two datasets demonstrate that our method outperforms competitive baselines in producing coherent and style-intensive dialogue responses."
483,https://arxiv.org/abs/2009.11753,Generating Commonsense Explanation by Extracting Bridge Concepts from Reasoning Paths,"Commonsense explanation generation aims to empower the machine's sense-making capability by generating plausible explanations to statements against commonsense. While this task is easy to human, the machine still struggles to generate reasonable and informative explanations. In this work, we propose a method that first extracts the underlying concepts which are served as \textit{bridges} in the reasoning chain and then integrates these concepts to generate the final explanation. To facilitate the reasoning process, we utilize external commonsense knowledge to build the connection between a statement and the bridge concepts by extracting and pruning multi-hop paths to build a subgraph. We design a bridge concept extraction model that first scores the triples, routes the paths in the subgraph, and further selects bridge concepts with weak supervision at both the triple level and the concept level. We conduct experiments on the commonsense explanation generation task and our model outperforms the state-of-the-art baselines in both automatic and human evaluation."
484,https://arxiv.org/abs/2009.11692,Language Generation with Multi-Hop Reasoning on Commonsense Knowledge Graph,"Despite the success of generative pre-trained language models on a series of text generation tasks, they still suffer in cases where reasoning over underlying commonsense knowledge is required during generation. Existing approaches that integrate commonsense knowledge into generative pre-trained language models simply transfer relational knowledge by post-training on individual knowledge triples while ignoring rich connections within the knowledge graph. We argue that exploiting both the structural and semantic information of the knowledge graph facilitates commonsense-aware text generation. In this paper, we propose Generation with Multi-Hop Reasoning Flow (GRF) that enables pre-trained models with dynamic multi-hop reasoning on multi-relational paths extracted from the external commonsense knowledge graph. We empirically show that our model outperforms existing baselines on three text generation tasks that require reasoning over commonsense knowledge. We also demonstrate the effectiveness of the dynamic multi-hop reasoning module with reasoning paths inferred by the model that provide rationale to the generation."
485,https://arxiv.org/abs/2009.09427,Dialogue Distillation: Open-Domain Dialogue Augmentation Using Unpaired Data,"Recent advances in open-domain dialogue systems rely on the success of neural models that are trained on large-scale data. However, collecting large-scale dialogue data is usually time-consuming and labor-intensive. To address this data dilemma, we propose a novel data augmentation method for training open-domain dialogue models by utilizing unpaired data. Specifically, a data-level distillation process is first proposed to construct augmented dialogues where both post and response are retrieved from the unpaired data. A ranking module is employed to filter out low-quality dialogues. Further, a model-level distillation process is employed to distill a teacher model trained on high-quality paired data to augmented dialogue pairs, thereby preventing dialogue models from being affected by the noise in the augmented data. Automatic and manual evaluation indicates that our method can produce high-quality dialogue pairs with diverse contents, and the proposed data-level and model-level dialogue distillation can improve the performance of competitive baselines."
486,https://arxiv.org/abs/2009.09378,Difference-aware Knowledge Selection for Knowledge-grounded Conversation Generation,"In a multi-turn knowledge-grounded dialog, the difference between the knowledge selected at different turns usually provides potential clues to knowledge selection, which has been largely neglected in previous research. In this paper, we propose a difference-aware knowledge selection method. It first computes the difference between the candidate knowledge sentences provided at the current turn and those chosen in the previous turns. Then, the differential information is fused with or disentangled from the contextual information to facilitate final knowledge selection. Automatic, human observational, and interactive evaluation shows that our method is able to select knowledge more accurately and generate more informative responses, significantly outperforming the state-of-the-art baselines. The codes are available at https://github.com/chujiezheng/DiffKS."
487,https://arxiv.org/abs/2009.07602,UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation,"Despite the success of existing referenced metrics (e.g., BLEU and MoverScore), they correlate poorly with human judgments for open-ended text generation including story or dialog generation because of the notorious one-to-many issue: there are many plausible outputs for the same input, which may differ substantially in literal or semantics from the limited number of given references. To alleviate this issue, we propose UNION, a learnable unreferenced metric for evaluating open-ended story generation, which measures the quality of a generated story without any reference. Built on top of BERT, UNION is trained to distinguish human-written stories from negative samples and recover the perturbation in negative stories. We propose an approach of constructing negative samples by mimicking the errors commonly observed in existing NLG models, including repeated plots, conflicting logic, and long-range incoherence. Experiments on two story datasets demonstrate that UNION is a reliable measure for evaluating the quality of generated stories, which correlates better with human judgments and is more generalizable than existing state-of-the-art metrics."
488,https://arxiv.org/abs/2008.03946,A Large-Scale Chinese Short-Text Conversation Dataset,"The advancements of neural dialogue generation models show promising results on modeling short-text conversations. However, training such models usually needs a large-scale high-quality dialogue corpus, which is hard to access. In this paper, we present a large-scale cleaned Chinese conversation dataset, LCCC, which contains a base version (6.8million dialogues) and a large version (12.0 million dialogues). The quality of our dataset is ensured by a rigorous data cleaning pipeline, which is built based on a set of rules and a classifier that is trained on manually annotated 110K dialogue pairs. We also release pre-training dialogue models which are trained on LCCC-base and LCCC-large respectively. The cleaned dataset and the pre-training models will facilitate the research of short-text conversation modeling. All the models and datasets are available at https://github.com/thu-coai/CDial-GPT."
489,https://arxiv.org/abs/2007.14573,FIVES: Feature Interaction Via Edge Search for Large-Scale Tabular Data,"High-order interactive features capture the correlation between different columns and thus are promising to enhance various learning tasks on ubiquitous tabular data. To automate the generation of interactive features, existing works either explicitly traverse the feature space or implicitly express the interactions via intermediate activations of some designed models. These two kinds of methods show that there is essentially a trade-off between feature interpretability and search efficiency. To possess both of their merits, we propose a novel method named Feature Interaction Via Edge Search (FIVES), which formulates the task of interactive feature generation as searching for edges on the defined feature graph. Specifically, we first present our theoretical evidence that motivates us to search for useful interactive features with increasing order. Then we instantiate this search strategy by optimizing both a dedicated graph neural network (GNN) and the adjacency tensor associated with the defined feature graph. In this way, the proposed FIVES method simplifies the time-consuming traversal as a typical training course of GNN and enables explicit feature generation according to the learned adjacency tensor. Experimental results on both benchmark and real-world datasets show the advantages of FIVES over several state-of-the-art methods. Moreover, the interactive features identified by FIVES are deployed on the recommender system of Taobao, a worldwide leading e-commerce platform. Results of an online A/B testing further verify the effectiveness of the proposed method FIVES, and we further provide FIVES as AI utilities for the customers of Alibaba Cloud."
490,https://arxiv.org/abs/2006.05244,Knowledge-Aided Open-Domain Question Answering,"Open-domain question answering (QA) aims to find the answer to a question from a large collection of documents.Though many models for single-document machine comprehension have achieved strong performance, there is still much room for improving open-domain QA systems since document retrieval and answer reranking are still unsatisfactory. Golden documents that contain the correct answers may not be correctly scored by the retrieval component, and the correct answers that have been extracted may be wrongly ranked after other candidate answers by the reranking component. One of the reasons is derived from the independent principle in which each candidate document (or answer) is scored independently without considering its relationship to other documents (or answers). In this work, we propose a knowledge-aided open-domain QA (KAQA) method which targets at improving relevant document retrieval and candidate answer reranking by considering the relationship between a question and the documents (termed as question-document graph), and the relationship between candidate documents (termed as document-document graph). The graphs are built using knowledge triples from external knowledge resources. During document retrieval, a candidate document is scored by considering its relationship to the question and other documents. During answer reranking, a candidate answer is reranked using not only its own context but also the clues from other documents. The experimental results show that our proposed method improves document retrieval and answer reranking, and thereby enhances the overall performance of open-domain question answering."
491,https://arxiv.org/abs/2005.07362,Is Your Goal-Oriented Dialog Model Performing Really Well? Empirical Analysis of System-wise Evaluation,"There is a growing interest in developing goal-oriented dialog systems which serve users in accomplishing complex tasks through multi-turn conversations. Although many methods are devised to evaluate and improve the performance of individual dialog components, there is a lack of comprehensive empirical study on how different components contribute to the overall performance of a dialog system. In this paper, we perform a system-wise evaluation and present an empirical analysis on different types of dialog systems which are composed of different modules in different settings. Our results show that (1) a pipeline dialog system trained using fine-grained supervision signals at different component levels often obtains better performance than the systems that use joint or end-to-end models trained on coarse-grained labels, (2) component-wise, single-turn evaluation results are not always consistent with the overall performance of a dialog system, and (3) despite the discrepancy between simulators and human users, simulated evaluation is still a valid alternative to the costly human evaluation especially in the early stage of development."
492,https://arxiv.org/abs/2005.05189,A Self-Training Method for Machine Reading Comprehension with Soft Evidence Extraction,"Neural models have achieved great success on machine reading comprehension (MRC), many of which typically consist of two components: an evidence extractor and an answer predictor. The former seeks the most relevant information from a reference text, while the latter is to locate or generate answers from the extracted evidence. Despite the importance of evidence labels for training the evidence extractor, they are not cheaply accessible, particularly in many non-extractive MRC tasks such as YES/NO question answering and multi-choice MRC.
  To address this problem, we present a Self-Training method (STM), which supervises the evidence extractor with auto-generated evidence labels in an iterative process. At each iteration, a base MRC model is trained with golden answers and noisy evidence labels. The trained model will predict pseudo evidence labels as extra supervision in the next iteration. We evaluate STM on seven datasets over three MRC tasks. Experimental results demonstrate the improvement on existing MRC models, and we also analyze how and why such a self-training method works in MRC. The source code can be obtained from https://github.com/SparkJiao/Self-Training-MRC"
493,https://arxiv.org/abs/2004.09731,Learning Goal-oriented Dialogue Policy with Opposite Agent Awareness,"Most existing approaches for goal-oriented dialogue policy learning used reinforcement learning, which focuses on the target agent policy and simply treat the opposite agent policy as part of the environment. While in real-world scenarios, the behavior of an opposite agent often exhibits certain patterns or underlies hidden policies, which can be inferred and utilized by the target agent to facilitate its own decision making. This strategy is common in human mental simulation by first imaging a specific action and the probable results before really acting it. We therefore propose an opposite behavior aware framework for policy learning in goal-oriented dialogues. We estimate the opposite agent's policy from its behavior and use this estimation to improve the target agent by regarding it as part of the target policy. We evaluate our model on both cooperative and competitive dialogue tasks, showing superior performance over state-of-the-art baselines."
494,https://arxiv.org/abs/2004.04100,KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation,"The research of knowledge-driven conversational systems is largely limited due to the lack of dialog data which consist of multi-turn conversations on multiple topics and with knowledge annotations. In this paper, we propose a Chinese multi-domain knowledge-driven conversation dataset, KdConv, which grounds the topics in multi-turn conversations to knowledge graphs. Our corpus contains 4.5K conversations from three domains (film, music, and travel), and 86K utterances with an average turn number of 19.0. These conversations contain in-depth discussions on related topics and natural transition between multiple topics. To facilitate the following research on this corpus, we provide several benchmark models. Comparative results show that the models can be enhanced by introducing background knowledge, yet there is still a large space for leveraging knowledge to model multi-turn conversations for further research. Results also show that there are obvious performance differences between different domains, indicating that it is worth to further explore transfer learning and domain adaptation. The corpus and benchmark models are publicly available."
495,https://arxiv.org/abs/2004.03809,Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition,"Many studies have applied reinforcement learning to train a dialog policy and show great promise these years. One common approach is to employ a user simulator to obtain a large number of simulated user experiences for reinforcement learning algorithms. However, modeling a realistic user simulator is challenging. A rule-based simulator requires heavy domain expertise for complex tasks, and a data-driven simulator requires considerable data and it is even unclear how to evaluate a simulator. To avoid explicitly building a user simulator beforehand, we propose Multi-Agent Dialog Policy Learning, which regards both the system and the user as the dialog agents. Two agents interact with each other and are jointly learned simultaneously. The method uses the actor-critic framework to facilitate pretraining and improve scalability. We also propose Hybrid Value Network for the role-aware reward decomposition to integrate role-specific domain knowledge of each agent in the task-oriented dialog. Results show that our method can successfully build a system policy and a user policy simultaneously, and two agents can achieve a high task success rate through conversational interaction."
496,https://arxiv.org/abs/2003.07490,Recent Advances and Challenges in Task-oriented Dialog System,"Due to the significance and value in human-computer interaction and natural language processing, task-oriented dialog systems are attracting more and more attention in both academic and industrial communities. In this paper, we survey recent advances and challenges in task-oriented dialog systems. We also discuss three critical topics for task-oriented dialog systems: (1) improving data efficiency to facilitate dialog modeling in low-resource settings, (2) modeling multi-turn dynamics for dialog policy learning to achieve better task-completion performance, and (3) integrating domain ontology knowledge into the dialog model. Besides, we review the recent progresses in dialog evaluation and some widely-used corpora. We believe that this survey, though incomplete, can shed a light on future research in task-oriented dialog systems."
497,https://arxiv.org/abs/2002.12920,Automatic Perturbation Analysis for Scalable Certified Robustness and Beyond,"Linear relaxation based perturbation analysis (LiRPA) for neural networks, which computes provable linear bounds of output neurons given a certain amount of input perturbation, has become a core component in robustness verification and certified defense. The majority of LiRPA-based methods focus on simple feed-forward networks and need particular manual derivations and implementations when extended to other architectures. In this paper, we develop an automatic framework to enable perturbation analysis on any neural network structures, by generalizing existing LiRPA algorithms such as CROWN to operate on general computational graphs. The flexibility, differentiability and ease of use of our framework allow us to obtain state-of-the-art results on LiRPA based certified defense on fairly complicated networks like DenseNet, ResNeXt and Transformer that are not supported by prior works. Our framework also enables loss fusion, a technique that significantly reduces the computational complexity of LiRPA for certified defense. For the first time, we demonstrate LiRPA based certified defense on Tiny ImageNet and Downscaled ImageNet where previous approaches cannot scale to due to the relatively large number of classes. Our work also yields an open-source library for the community to apply LiRPA to areas beyond certified defense without much LiRPA expertise, e.g., we create a neural network with a probably flat optimization landscape by applying LiRPA to network parameters. Our opensource library is available at https://github.com/KaidiXu/auto_LiRPA."
498,https://arxiv.org/abs/2002.11893,CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset,"To advance multi-domain (cross-domain) dialogue modeling as well as alleviate the shortage of Chinese task-oriented datasets, we propose CrossWOZ, the first large-scale Chinese Cross-Domain Wizard-of-Oz task-oriented dataset. It contains 6K dialogue sessions and 102K utterances for 5 domains, including hotel, restaurant, attraction, metro, and taxi. Moreover, the corpus contains rich annotation of dialogue states and dialogue acts at both user and system sides. About 60% of the dialogues have cross-domain user goals that favor inter-domain dependency and encourage natural transition across domains in conversation. We also provide a user simulator and several benchmark models for pipelined task-oriented dialogue systems, which will facilitate researchers to compare and evaluate their models on this corpus. The large size and rich annotation of CrossWOZ make it suitable to investigate a variety of tasks in cross-domain dialogue modeling, such as dialogue state tracking, policy learning, user simulation, etc."
499,https://arxiv.org/abs/2002.06622,Robustness Verification for Transformers,"Robustness verification that aims to formally certify the prediction behavior of neural networks has become an important tool for understanding model behavior and obtaining safety guarantees. However, previous methods can usually only handle neural networks with relatively simple architectures. In this paper, we consider the robustness verification problem for Transformers. Transformers have complex self-attention layers that pose many challenges for verification, including cross-nonlinearity and cross-position dependency, which have not been discussed in previous works. We resolve these challenges and develop the first robustness verification algorithm for Transformers. The certified robustness bounds computed by our method are significantly tighter than those by naive Interval Bound Propagation. These bounds also shed light on interpreting Transformers as they consistently reflect the importance of different words in sentiment analysis."
500,https://arxiv.org/abs/2002.04793,"ConvLab-2: An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems","We present ConvLab-2, an open-source toolkit that enables researchers to build task-oriented dialogue systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems. As the successor of ConvLab (Lee et al., 2019b), ConvLab-2 inherits ConvLab's framework but integrates more powerful dialogue models and supports more datasets. Besides, we have developed an analysis tool and an interactive tool to assist researchers in diagnosing dialogue systems. The analysis tool presents rich statistics and summarizes common mistakes from simulated dialogues, which facilitates error analysis and system improvement. The interactive tool provides a user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component."
501,https://arxiv.org/abs/2304.14197,Logarithmic-Regret Quantum Learning Algorithms for Zero-Sum Games,"We propose the first online quantum algorithm for zero-sum games with $\tilde O(1)$ regret under the game setting. Moreover, our quantum algorithm computes an $\varepsilon$-approximate Nash equilibrium of an $m \times n$ matrix zero-sum game in quantum time $\tilde O(\sqrt{m+n}/\varepsilon^{2.5})$, yielding a quadratic improvement over classical algorithms in terms of $m, n$. Our algorithm uses standard quantum inputs and generates classical outputs with succinct descriptions, facilitating end-to-end applications. As an application, we obtain a fast quantum linear programming solver. Technically, our online quantum algorithm ""quantizes"" classical algorithms based on the optimistic multiplicative weight update method. At the heart of our algorithm is a fast quantum multi-sampling procedure for the Gibbs sampling problem, which may be of independent interest."
502,https://arxiv.org/abs/2211.05325,Parameterized Complexity of Weighted Local Hamiltonian Problems and the Quantum Exponential Time Hypothesis,"We study a parameterized version of the local Hamiltonian problem, called the weighted local Hamiltonian problem, where the relevant quantum states are superpositions of computational basis states of Hamming weight $k$. The Hamming weight constraint can have a physical interpretation as a constraint on the number of excitations allowed or particle number in a system. We prove that this problem is in QW[1], the first level of the quantum weft hierarchy and that it is hard for QM[1], the quantum analogue of M[1]. Our results show that this problem cannot be fixed-parameter quantum tractable (FPQT) unless certain natural quantum analogue of the exponential time hypothesis (ETH) is false."
503,https://arxiv.org/abs/2209.14185,Classical and Quantum Iterative Optimization Algorithms Based on Matrix Legendre-Bregman Projections,"We consider Legendre-Bregman projections defined on the Hermitian matrix space and design iterative optimization algorithms based on them. A general duality theorem is established for Bregman divergences on Hermitian matrices, and it plays a crucial role in proving the convergence of the iterative algorithms. We study both exact and approximate Bregman projection algorithms. In the particular case of Kullback-Leibler divergence, our approximate iterative algorithm gives rise to the non-commutative versions of both the generalized iterative scaling (GIS) algorithm for maximum entropy inference and the AdaBoost algorithm in machine learning as special cases. As the Legendre-Bregman projections are simple matrix functions on Hermitian matrices, quantum algorithmic techniques are applicable to achieve potential speedups in each iteration of the algorithm. We discuss several quantum algorithmic design techniques applicable in our setting, including the smooth function evaluation technique, two-phase quantum minimum finding, and NISQ Gibbs state preparation."
504,https://arxiv.org/abs/2203.08002,Quantum Parameterized Complexity,"Parameterized complexity theory was developed in the 1990s to enrich the complexity-theoretic analysis of problems that depend on a range of parameters. In this paper we establish a quantum equivalent of classical parameterized complexity theory, motivated by the need for new tools for the classifications of the complexity of real-world problems. We introduce the quantum analogues of a range of parameterized complexity classes and examine the relationship between these classes, their classical counterparts, and well-studied problems. This framework exposes a rich classification of the complexity of parameterized versions of QMA-hard problems, demonstrating, for example, a clear separation between the Quantum Circuit Satisfiability problem and the Local Hamiltonian problem."
505,https://arxiv.org/abs/2111.08131,Quantum soundness of testing tensor codes,"A locally testable code is an error-correcting code that admits very efficient probabilistic tests of membership. Tensor codes provide a simple family of combinatorial constructions of locally testable codes that generalize the family of Reed-Muller codes. The natural test for tensor codes, the axis-parallel line vs. point test, plays an essential role in constructions of probabilistically checkable proofs.
  We analyze the axis-parallel line vs. point test as a two-prover game and show that the test is sound against quantum provers sharing entanglement. Our result implies the quantum-soundness of the low individual degree test, which is an essential component of the MIP* = RE theorem. Our proof also generalizes to the infinite-dimensional commuting-operator model of quantum provers."
506,https://arxiv.org/abs/2010.03032,Symbolic Verification of Quantum Circuits,"This short note proposes a symbolic approach for representing and reasoning about quantum circuits using complex, vector or matrix-valued Boolean expressions. A major benefit of this approach is that it allows us to directly borrow the existing techniques and tools for verification of classical logic circuits in reasoning about quantum circuits."
507,https://arxiv.org/abs/2009.12982,Quantum soundness of the classical low individual degree test,"Low degree tests play an important role in classical complexity theory, serving as basic ingredients in foundational results such as $\mathsf{MIP} = \mathsf{NEXP}$ [BFL91] and the PCP theorem [AS98,ALM+98]. Over the last ten years, versions of these tests which are sound against quantum provers have found increasing applications to the study of nonlocal games and the complexity class~$\mathsf{MIP}^*$. The culmination of this line of work is the result $\mathsf{MIP}^* = \mathsf{RE}$ [arXiv:2001.04383]. One of the key ingredients in the first reported proof of $\mathsf{MIP}^* = \mathsf{RE}$ is a two-prover variant of the low degree test, initially shown to be sound against multiple quantum provers in [arXiv:1302.1242]. Unfortunately a mistake was recently discovered in the latter result, invalidating the main result of [arXiv:1302.1242] as well as its use in subsequent works, including [arXiv:2001.04383]. We analyze a variant of the low degree test called the low individual degree test. Our main result is that the two-player version of this test is sound against quantum provers. This soundness result is sufficient to re-derive several bounds on~$\mathsf{MIP}^*$ that relied on [arXiv:1302.1242], including $\mathsf{MIP}^* = \mathsf{RE}$."
508,https://arxiv.org/abs/2001.04383,MIP*=RE,"We show that the class MIP* of languages that can be decided by a classical verifier interacting with multiple all-powerful quantum provers sharing entanglement is equal to the class RE of recursively enumerable languages. Our proof builds upon the quantum low-degree test of (Natarajan and Vidick, FOCS 2018) and the classical low-individual degree test of (Ji, et al., 2020) by integrating recent developments from (Natarajan and Wright, FOCS 2019) and combining them with the recursive compression framework of (Fitzsimons et al., STOC 2019).
  An immediate byproduct of our result is that there is an efficient reduction from the Halting Problem to the problem of deciding whether a two-player nonlocal game has entangled value $1$ or at most $1/2$. Using a known connection, undecidability of the entangled value implies a negative answer to Tsirelson's problem: we show, by providing an explicit example, that the closure $C_{qa}$ of the set of quantum tensor product correlations is strictly included in the set $C_{qc}$ of quantum commuting correlations. Following work of (Fritz, Rev. Math. Phys. 2012) and (Junge et al., J. Math. Phys. 2011) our results provide a refutation of Connes' embedding conjecture from the theory of von Neumann algebras."
509,https://arxiv.org/abs/1911.11962,Approximating Permanent of Random Matrices with Vanishing Mean: Made Better and Simpler,"The algorithm and complexity of approximating the permanent of a matrix is an extensively studied topic. Recently, its connection with quantum supremacy and more specifically BosonSampling draws special attention to the average-case approximation problem of the permanent of random matrices with zero or small mean value for each entry. Eldar and Mehraban (FOCS 2018) gave a quasi-polynomial time algorithm for random matrices with mean at least $1/\mathbf{\mathrm{polyloglog}} (n)$. In this paper, we improve the result by designing a deterministic quasi-polynomial time algorithm and a PTAS for random matrices with mean at least $1/\mathbf{\mathrm{polylog}}(n)$. We note that if it can be further improved to $1/\mathbf{\mathrm{poly}}(n)$, it will disprove a central conjecture for quantum supremacy.
  Our algorithm is also much simpler and has a better and flexible trade-off for running time. The running time can be quasi-polynomial in both $n$ and $1/ε$, or PTAS (polynomial in $n$ but exponential in $1/ε$), where $ε$ is the approximation parameter."
510,https://arxiv.org/abs/1906.04330,General Linear Group Action on Tensors: A Candidate for Post-Quantum Cryptography,"Starting from the one-way group action framework of Brassard and Yung (Crypto '90), we revisit building cryptography based on group actions. Several previous candidates for one-way group actions no longer stand, due to progress both on classical algorithms (e.g., graph isomorphism) and quantum algorithms (e.g., discrete logarithm).
  We propose the general linear group action on tensors as a new candidate to build cryptography based on group actions. Recent works (Futorny--Grochow--Sergeichuk, Lin. Alg. Appl., 2019) suggest that the underlying algorithmic problem, the tensor isomorphism problem, is the hardest one among several isomorphism testing problems arising from areas including coding theory, computational group theory, and multivariate cryptography. We present evidence to justify the viability of this proposal from comprehensive study of the state-of-art heuristic algorithms, theoretical algorithms, and hardness results, as well as quantum algorithms.
  We then introduce a new notion called pseudorandom group actions to further develop group-action based cryptography. Briefly speaking, given a group $G$ acting on a set $S$, we assume that it is hard to distinguish two distributions of $(s, t)$ either uniformly chosen from $S\times S$, or where $s$ is randomly chosen from $S$ and $t$ is the result of applying a random group action of $g\in G$ on $s$. This subsumes the classical decisional Diffie-Hellman assumption when specialized to a particular group action. We carefully analyze various attack strategies that support the general linear group action on tensors as a candidate for this assumption.
  Finally, we establish the quantum security of several cryptographic primitives based on the one-way group action assumption and the pseudorandom group action assumption."
511,https://arxiv.org/abs/1805.12166,"Quantum proof systems for iterated exponential time, and beyond","We show that any language in nondeterministic time $\exp(\exp(\cdots \exp(n)))$, where the number of iterated exponentials is an arbitrary function $R(n)$, can be decided by a multiprover interactive proof system with a classical polynomial-time verifier and a constant number of quantum entangled provers, with completeness $1$ and soundness $1 - \exp(-C\exp(\cdots\exp(n)))$, where the number of iterated exponentials is $R(n)-1$ and $C>0$ is a universal constant. The result was previously known for $R=1$ and $R=2$; we obtain it for any time-constructible function $R$.
  The result is based on a compression technique for interactive proof systems with entangled provers that significantly simplifies and strengthens a protocol compression result of Ji (STOC'17). As a separate consequence of this technique we obtain a different proof of Slofstra's recent result (unpublished) on the uncomputability of the entangled value of multiprover games.
  Finally, we show that even minor improvements to our compression result would yield remarkable consequences in computational complexity theory and the foundations of quantum mechanics: first, it would imply that the class MIP* contains all computable languages; second, it would provide a negative resolution to a multipartite version of Tsirelson's problem on the relation between the commuting operator and tensor product models for quantum correlations."
512,https://arxiv.org/abs/1802.04926,A three-player coherent state embezzlement game,"We introduce a three-player nonlocal game, with a finite number of classical questions and answers, such that the optimal success probability of $1$ in the game can only be achieved in the limit of strategies using arbitrarily high-dimensional entangled states. Precisely, there exists a constant $0 <c\leq 1$ such that to succeed with probability $1-\varepsilon$ in the game it is necessary to use an entangled state of at least $Ω(\varepsilon^{-c})$ qubits, and it is sufficient to use a state of at most $O(\varepsilon^{-1})$ qubits.
  The game is based on the coherent state exchange game of Leung et al. (CJTCS 2013). In our game, the task of the quantum verifier is delegated to a third player by a classical referee. Our results complement those of Slofstra (arXiv:1703.08618) and Dykema et al. (arXiv:1709.05032), who obtained two-player games with similar (though quantitatively weaker) properties based on the representation theory of finitely presented groups and $C^*$-algebras respectively."
513,https://arxiv.org/abs/1711.00385,"Pseudorandom States, Non-Cloning Theorems and Quantum Money","We propose the concept of pseudorandom states and study their constructions, properties, and applications. Under the assumption that quantum-secure one-way functions exist, we present concrete and efficient constructions of pseudorandom states. The non-cloning theorem plays a central role in our study---it motivates the proper definition and characterizes one of the important properties of pseudorandom quantum states. Namely, there is no efficient quantum algorithm that can create more copies of the state from a given number of pseudorandom states. As the main application, we prove that any family of pseudorandom states naturally gives rise to a private-key quantum money scheme."
514,https://arxiv.org/abs/1705.01523,A Separability-Entanglement Classifier via Machine Learning,"The problem of determining whether a given quantum state is entangled lies at the heart of quantum information processing, which is known to be an NP-hard problem in general. Despite the proposed many methods such as the positive partial transpose (PPT) criterion and the k-symmetric extendibility criterion to tackle this problem in practice, none of them enables a general, effective solution to the problem even for small dimensions. Explicitly, separable states form a high-dimensional convex set, which exhibits a vastly complicated structure. In this work, we build a new separability-entanglement classifier underpinned by machine learning techniques. Our method outperforms the existing methods in generic cases in terms of both speed and accuracy, opening up the avenues to explore quantum entanglement via the machine learning approach."
515,https://arxiv.org/abs/1610.03133,Compression of Quantum Multi-Prover Interactive Proofs,"We present a protocol that transforms any quantum multi-prover interactive proof into a nonlocal game in which questions consist of logarithmic number of bits and answers of constant number of bits. As a corollary, this proves that the promise problem corresponding to the approximation of the nonlocal value to inverse polynomial accuracy is complete for QMIP*, and therefore NEXP-hard. This establishes that nonlocal games are provably harder than classical games without any complexity theory assumptions. Our result also indicates that gap amplification for nonlocal games may be impossible in general and provides a negative evidence for the possibility of the gap amplification approach to the multi-prover variant of the quantum PCP conjecture."
516,https://arxiv.org/abs/1606.07422,Joint product numerical range and geometry of reduced density matrices,"The reduced density matrices of a many-body quantum system form a convex set, whose three-dimensional projection $Θ$ is convex in $\mathbb{R}^3$. The boundary $\partialΘ$ of $Θ$ may exhibit nontrivial geometry, in particular ruled surfaces. Two physical mechanisms are known for the origins of ruled surfaces: symmetry breaking and gapless. In this work, we study the emergence of ruled surfaces for systems with local Hamiltonians in infinite spatial dimension, where the reduced density matrices are known to be separable as a consequence of the quantum de Finetti's theorem. This allows us to identify the reduced density matrix geometry with joint product numerical range $Π$ of the Hamiltonian interaction terms. We focus on the case where the interaction terms have certain structures, such that ruled surface emerge naturally when taking a convex hull of $Π$. We show that, a ruled surface on $\partialΘ$ sitting in $Π$ has a gapless origin, otherwise it has a symmetry breaking origin. As an example, we demonstrate that a famous ruled surface, known as the oloid, is a possible shape of $Θ$, with two boundary pieces of symmetry breaking origin separated by two gapless lines."
517,https://arxiv.org/abs/1605.06357,Physical origins of ruled surfaces on the reduced density matrices geometry,"The reduced density matrices (RDMs) of many-body quantum states form a convex set. The boundary of low dimensional projections of this convex set may exhibit nontrivial geometry such as ruled surfaces. In this paper, we study the physical origins of these ruled surfaces for bosonic systems. The emergence of ruled surfaces was recently proposed as signatures of symmetry-breaking phase. We show that, apart from being signatures of symmetry-breaking, ruled surfaces can also be the consequence of gapless quantum systems by demonstrating an explicit example in terms of a two-mode Ising model. Our analysis was largely simplified by the quantum de Finetti's theorem---in the limit of large system size, these RDMs are the convex set of all the symmetric separable states. To distinguish ruled surfaces originated from gapless systems from those caused by symmetry-breaking, we propose to use the finite size scaling method for the corresponding geometry. This method is then applied to the two-mode XY model, successfully identifying a ruled surface as the consequence of gapless systems."
518,https://arxiv.org/abs/1604.06277,Quantum State and Process Tomography via Adaptive Measurements,"We investigate quantum state tomography (QST) for pure states and quantum process tomography (QPT) for unitary channels via $adaptive$ measurements. For a quantum system with a $d$-dimensional Hilbert space, we first propose an adaptive protocol where only $2d-1$ measurement outcomes are used to accomplish the QST for $all$ pure states. This idea is then extended to study QPT for unitary channels, where an adaptive unitary process tomography (AUPT) protocol of $d^2+d-1$ measurement outcomes is constructed for any unitary channel. We experimentally implement the AUPT protocol in a 2-qubit nuclear magnetic resonance system. We examine the performance of the AUPT protocol when applied to Hadamard gate, $T$ gate ($π/8$ phase gate), and controlled-NOT gate, respectively, as these gates form the universal gate set for quantum information processing purpose. As a comparison, standard QPT is also implemented for each gate. Our experimental results show that the AUPT protocol that reconstructing unitary channels via adaptive measurements significantly reduce the number of experiments required by standard QPT without considerable loss of fidelity."
519,https://arxiv.org/abs/1604.02804,Zero-knowledge proof systems for QMA,"Prior work has established that all problems in NP admit classical zero-knowledge proof systems, and under reasonable hardness assumptions for quantum computations, these proof systems can be made secure against quantum attacks. We prove a result representing a further quantum generalization of this fact, which is that every problem in the complexity class QMA has a quantum zero-knowledge proof system. More specifically, assuming the existence of an unconditionally binding and quantum computationally concealing commitment scheme, we prove that every problem in the complexity class QMA has a quantum interactive proof system that is zero-knowledge with respect to efficient quantum computations.
  Our QMA proof system is sound against arbitrary quantum provers, but only requires an honest prover to perform polynomial-time quantum computations, provided that it holds a quantum witness for a given instance of the QMA problem under consideration. The proof system relies on a new variant of the QMA-complete local Hamiltonian problem in which the local terms are described by Clifford operations and standard basis measurements. We believe that the QMA-completeness of this problem may have other uses in quantum complexity."
520,https://arxiv.org/abs/1604.02046,Quantum state tomography via reduced density matrices,"Quantum state tomography via local measurements is an efficient tool for characterizing quantum states. However it requires that the original global state be uniquely determined (UD) by its local reduced density matrices (RDMs). In this work we demonstrate for the first time a class of states that are UD by their RDMs under the assumption that the global state is pure, but fail to be UD in the absence of that assumption. This discovery allows us to classify quantum states according to their UD properties, with the requirement that each class be treated distinctly in the practice of simplifying quantum state tomography. Additionally we experimentally test the feasibility and stability of performing quantum state tomography via the measurement of local RDMs for each class. These theoretical and experimental results advance the project of performing efficient and accurate quantum state tomography in practice."
521,https://arxiv.org/abs/1603.03245,Dichotomy of entanglement depth for symmetric states,"Entanglement depth characterizes the minimal number of particles in a system that are mutually entangled. For symmetric states, we show that there is a dichotomy for entanglement depth: an $N$-particle symmetric state is either fully separable, or fully entangled---the entanglement depth is either $1$ or $N$. This property is even stable under non-symmetric noise. We propose an experimentally accessible method to detect entanglement depth in atomic ensembles based on a bound on the particle number population of Dicke states, and demonstrate that the entanglement depth of some Dicke states, for example the twin Fock state, is very stable even under a large arbitrary noise. Our observation can be applied to atomic Bose-Einstein condensates to infer that these systems can be highly entangled with the entanglement depth that is of the order of the system size (i.e. several thousands of atoms)."
522,https://arxiv.org/abs/1602.00401,Quantum Capacities for Entanglement Networks,"We discuss quantum capacities for two types of entanglement networks: $\mathcal{Q}$ for the quantum repeater network with free classical communication, and $\mathcal{R}$ for the tensor network as the rank of the linear operation represented by the tensor network. We find that $\mathcal{Q}$ always equals $\mathcal{R}$ in the regularized case for the samenetwork graph. However, the relationships between the corresponding one-shot capacities $\mathcal{Q}_1$ and $\mathcal{R}_1$ are more complicated, and the min-cut upper bound is in general not achievable. We show that the tensor network can be viewed as a stochastic protocol with the quantum repeater network, such that $\mathcal{R}_1$ is a natural upper bound of $\mathcal{Q}_1$. We analyze the possible gap between $\mathcal{R}_1$ and $\mathcal{Q}_1$ for certain networks, and compare them with the one-shot classical capacity of the corresponding classical network."
523,https://arxiv.org/abs/1601.05379,Pure State Tomography with Pauli Measurements,"We examine the problem of finding the minimum number of Pauli measurements needed to uniquely determine an arbitrary $n$-qubit pure state among all quantum states. We show that only $11$ Pauli measurements are needed to determine an arbitrary two-qubit pure state compared to the full quantum state tomography with $16$ measurements, and only $31$ Pauli measurements are needed to determine an arbitrary three-qubit pure state compared to the full quantum state tomography with $64$ measurements. We demonstrate that our protocol is robust under depolarizing error with simulated random pure states. We experimentally test the protocol on two- and three-qubit systems with nuclear magnetic resonance techniques. We show that the pure state tomography protocol saves us a number of measurements without considerable loss of fidelity. We compare our protocol with same-size sets of randomly selected Pauli operators and find that our selected set of Pauli measurements significantly outperforms those random sampling sets. As a direct application, our scheme can also be used to reduce the number of settings needed for pure-state tomography in quantum optical systems."
524,https://arxiv.org/abs/1511.00581,Tomography is necessary for universal entanglement detection with single-copy observables,"Entanglement, one of the central mysteries of quantum mechanics, plays an essential role in numerous applications of quantum information theory. A natural question of both theoretical and experimental importance is whether universal entanglement detection is possible without full state tomography. In this work, we prove a no-go theorem that rules out this possibility for any non-adaptive schemes that employ single-copy measurements only. We also examine in detail a previously implemented experiment, which claimed to detect entanglement of two-qubit states via adaptive single-copy measurements without full state tomography. By performing the experiment and analyzing the data, we demonstrate that the information gathered is indeed sufficient to reconstruct the state. These results reveal a fundamental limit for single-copy measurements in entanglement detection, and provides a general framework to study the detection of other interesting properties of quantum states, such as the positivity of partial transpose and the $k$-symmetric extendibility."
525,https://arxiv.org/abs/1509.06591,Detecting Consistency of Overlapping Quantum Marginals by Separability,"The quantum marginal problem asks whether a set of given density matrices are consistent, i.e., whether they can be the reduced density matrices of a global quantum state. Not many non-trivial analytic necessary (or sufficient) conditions are known for the problem in general. We propose a method to detect consistency of overlapping quantum marginals by considering the separability of some derived states. Our method works well for the $k$-symmetric extension problem in general, and for the general overlapping marginal problems in some cases. Our work is, in some sense, the converse to the well-known $k$-symmetric extension criterion for separability."
526,https://arxiv.org/abs/1508.03886,Geometry of reduced density matrices for symmetry-protected topological phases,"In this paper, we study the geometry of reduced density matrices for states with symmetry-protected topological (SPT) order. We observe ruled surface structures on the boundary of the convex set of low dimension projections of the reduced density matrices. In order to signal the SPT order using ruled surfaces, it is important that we add a symmetry-breaking term to the boundary of the system---no ruled surface emerges in systems without boundary or when we add a symmetry-breaking term representing a thermodynamic quantity. Although the ruled surfaces only appear in the thermodynamic limit where the ground-state degeneracy is exact, we analyze the precision of our numerical algorithm and show that a finite system calculation suffices to reveal the ruled surface structures."
527,https://arxiv.org/abs/1508.01797,Sample-optimal tomography of quantum states,"It is a fundamental problem to decide how many copies of an unknown mixed quantum state are necessary and sufficient to determine the state. Previously, it was known only that estimating states to error $ε$ in trace distance required $O(dr^2/ε^2)$ copies for a $d$-dimensional density matrix of rank $r$. Here, we give a theoretical measurement scheme (POVM) that requires $O (dr/ δ) \ln (d/δ) $ copies of $ρ$ to error $δ$ in infidelity, and a matching lower bound up to logarithmic factors. This implies $O( (dr / ε^2) \ln (d/ε) )$ copies suffice to achieve error $ε$ in trace distance. We also prove that for independent (product) measurements, $Ω(dr^2/δ^2) / \ln(1/δ)$ copies are necessary in order to achieve error $δ$ in infidelity. For fixed $d$, our measurement can be implemented on a quantum computer in time polynomial in $n$."
528,https://arxiv.org/abs/1505.07432,Classical Verification of Quantum Proofs,We present a classical interactive protocol that verifies the validity of a quantum witness state for the local Hamiltonian problem. It follows from this protocol that approximating the non-local value of a multi-player one-round game to inverse polynomial precision is QMA-hard. Our work makes an interesting connection between the theory of QMA-completeness and Hamiltonian complexity on one hand and the study of non-local games and Bell inequalities on the other.
529,https://arxiv.org/abs/1406.5046,Discontinuity of Maximum Entropy Inference and Quantum Phase Transitions,"In this paper, we discuss the connection between two genuinely quantum phenomena --- the discontinuity of quantum maximum entropy inference and quantum phase transitions at zero temperature. It is shown that the discontinuity of the maximum entropy inference of local observable measurements signals the non-local type of transitions, where local density matrices of the ground state change smoothly at the transition point. We then propose to use the quantum conditional mutual information of the ground state as an indicator to detect the discontinuity and the non-local type of quantum phase transitions in the thermodynamic limit."
530,https://arxiv.org/abs/1310.3794,Binary Constraint System Games and Locally Commutative Reductions,"A binary constraint system game is a two-player one-round non-local game defined by a system of Boolean constraints. The game has a perfect quantum strategy if and only if the constraint system has a quantum satisfying assignment [R. Cleve and R. Mittal, arXiv:1209.2729]. We show that several concepts including the quantum chromatic number and the Kochen-Specker sets that arose from different contexts fit naturally in the binary constraint system framework. The structure and complexity of the quantum satisfiability problems for these constraint systems are investigated. Combined with a new construct called the commutativity gadget for each problem, several classic NP-hardness reductions are lifted to their corresponding quantum versions. We also provide a simple parity constraint game that requires $Ω(\sqrt{n})$ EPR pairs in perfect strategies where $n$ is the number of variables in the constraint system."
531,https://arxiv.org/abs/1310.3530,Symmetric Extension of Two-Qubit States,"Quantum key distribution uses public discussion protocols to establish shared secret keys. In the exploration of ultimate limits to such protocols, the property of symmetric extendibility of underlying bipartite states $ρ_{AB}$ plays an important role. A bipartite state $ρ_{AB}$ is symmetric extendible if there exits a tripartite state $ρ_{ABB'}$, such that the $AB$ marginal state is identical to the $AB'$ marginal state, i.e. $ρ_{AB'}=ρ_{AB}$. For a symmetric extendible state $ρ_{AB}$, the first task of the public discussion protocol is to break this symmetric extendibility. Therefore to characterize all bi-partite quantum states that possess symmetric extensions is of vital importance. We prove a simple analytical formula that a two-qubit state $ρ_{AB}$ admits a symmetric extension if and only if $\tr(ρ_B^2)\geq \tr(ρ_{AB}^2)-4\sqrt{\det{ρ_{AB}}}$. Given the intimate relationship between the symmetric extension problem and the quantum marginal problem, our result also provides the first analytical necessary and sufficient condition for the quantum marginal problem with overlapping marginals."
532,https://arxiv.org/abs/1303.7020,Symmetries of Codeword Stabilized Quantum Codes,"Symmetry is at the heart of coding theory. Codes with symmetry, especially cyclic codes, play an essential role in both theory and practical applications of classical error-correcting codes. Here we examine symmetry properties for codeword stabilized (CWS) quantum codes, which is the most general framework for constructing quantum error-correcting codes known to date. A CWS code Q can be represented by a self-dual additive code S and a classical code C, i.,e., Q=(S,C), however this representation is in general not unique. We show that for any CWS code Q with certain permutation symmetry, one can always find a self-dual additive code S with the same permutation symmetry as Q such that Q=(S,C). As many good CWS codes have been found by starting from a chosen S, this ensures that when trying to find CWS codes with certain permutation symmetry, the choice of S with the same symmetry will suffice. A key step for this result is a new canonical representation for CWS codes, which is given in terms of a unique decomposition as union stabilizer codes. For CWS codes, so far mainly the standard form (G,C) has been considered, where G is a graph state. We analyze the symmetry of the corresponding graph of G, which in general cannot possess the same permutation symmetry as Q. We show that it is indeed the case for the toric code on a square lattice with translational symmetry, even if its encoding graph can be chosen to be translational invariant."
533,https://arxiv.org/abs/1212.3503,Uniqueness of Quantum States Compatible with Given Measurement Results,"We discuss the uniqueness of quantum states compatible with given results for measuring a set of observables. For a given pure state, we consider two different types of uniqueness: (1) no other pure state is compatible with the same measurement results and (2) no other state, pure or mixed, is compatible with the same measurement results. For case (1), it is known that for a d-dimensional Hilbert space, there exists a set of 4d-5 observables that uniquely determines any pure state. We show that for case (2), 5d-7 observables suffice to uniquely determine any pure state. Thus there is a gap between the results for (1) and (2), and we give some examples to illustrate this. The case of observables corresponding to reduced density matrices (RDMs) of a multipartite system is also discussed, where we improve known bounds on local dimensions for case (2) in which almost all pure states are uniquely determined by their RDMs. We further discuss circumstances where (1) can imply (2). We use convexity of the numerical range of operators to show that when only two observables are measured, (1) always implies (2). More generally, if there is a compact group of symmetries of the state space which has the span of the observables measured as the set of fixed points, then (1) implies (2). We analyze the possible dimensions for the span of such observables. Our results extend naturally to the case of low rank quantum states."
534,https://arxiv.org/abs/1210.1296,Minimum Entangling Power is Close to Its Maximum,"Given a quantum gate $U$ acting on a bipartite quantum system, its maximum (average, minimum) entangling power is the maximum (average, minimum) entanglement generation with respect to certain entanglement measure when the inputs are restricted to be product states. In this paper, we mainly focus on the 'weakest' one, i.e., the minimum entangling power, among all these entangling powers. We show that, by choosing von Neumann entropy of reduced density operator or Schmidt rank as entanglement measure, even the 'weakest' entangling power is generically very close to its maximal possible entanglement generation. In other words, maximum, average and minimum entangling powers are generically close. We then study minimum entangling power with respect to other Lipschitiz-continuous entanglement measures and generalize our results to multipartite quantum systems.
  As a straightforward application, a random quantum gate will almost surely be an intrinsically fault-tolerant entangling device that will always transform every low-entangled state to near-maximally entangled state."
535,https://arxiv.org/abs/1205.3682,Comment on some results of Erdahl and the convex structure of reduced density matrices,"In J. Math. Phys. 13, 1608-1621 (1972), Erdahl considered the convex structure of the set of $N$-representable 2-body reduced density matrices in the case of fermions. Some of these results have a straightforward extension to the $m$-body setting and to the more general quantum marginal problem. We describe these extensions, but can not resolve a problem in the proof of Erdahl's claim that every extreme point is exposed in finite dimensions. Nevertheless, we can show that when $2m \geq N$ every extreme point of the set of $N$-representable $m$-body reduced density matrices has a unique pre-image in both the symmetric and anti-symmetric setting. Moreover, this extends to the quantum marginal setting for a pair of complementary $m$-body and $(N-m)$-body reduced density matrices."
536,https://arxiv.org/abs/1112.0762,Ground-State Spaces of Frustration-Free Hamiltonians,"We study the ground-state space properties for frustration-free Hamiltonians. We introduce a concept of `reduced spaces' to characterize local structures of ground-state spaces. For a many-body system, we characterize mathematical structures for the set $Θ_k$ of all the $k$-particle reduced spaces, which with a binary operation called join forms a semilattice that can be interpreted as an abstract convex structure. The smallest nonzero elements in $Θ_k$, called atoms, are analogs of extreme points. We study the properties of atoms in $Θ_k$ and discuss its relationship with ground states of $k$-local frustration-free Hamiltonians. For spin-1/2 systems, we show that all the atoms in $Θ_2$ are unique ground states of some 2-local frustration-free Hamiltonians. Moreover, we show that the elements in $Θ_k$ may not be the join of atoms, indicating a richer structure for $Θ_k$ beyond the convex structure. Our study of $Θ_k$ deepens the understanding of ground-state space properties for frustration-free Hamiltonians, from a new angle of reduced spaces."
537,https://arxiv.org/abs/1110.6583,From Ground States to Local Hamiltonians,"Traditional quantum physics solves ground states for a given Hamiltonian, while quantum information science asks for the existence and construction of certain Hamiltonians for given ground states. In practical situations, one would be mainly interested in local Hamiltonians with certain interaction patterns, such as nearest neighbour interactions on some type of lattices. A necessary condition for a space $V$ to be the ground-state space of some local Hamiltonian with a given interaction pattern, is that the maximally mixed state supported on $V$ is uniquely determined by its reduced density matrices associated with the given pattern, based on the principle of maximum entropy. However, it is unclear whether this condition is in general also sufficient. We examine the situations for the existence of such a local Hamiltonian to have $V$ satisfying the necessary condition mentioned above as its ground-state space, by linking to faces of the convex body of the local reduced states. We further discuss some methods for constructing the corresponding local Hamiltonians with given interaction patterns, mainly from physical points of view, including constructions related to perturbation methods, local frustration-free Hamiltonians, as well as thermodynamical ensembles."
538,https://arxiv.org/abs/1106.3235,Rank Reduction for the Local Consistency Problem,"We address the problem of how simple a solution can be for a given quantum local consistency instance. More specifically, we investigate how small the rank of the global density operator can be if the local constraints are known to be compatible. We prove that any compatible local density operators can be satisfied by a low rank global density operator. Then we study both fermionic and bosonic versions of the N-representability problem as applications. After applying the channel-state duality, we prove that any compatible local channels can be obtained through a global quantum channel with small Kraus rank."
539,https://arxiv.org/abs/1106.1373,Correlations in excited states of local Hamiltonians,"Physical properties of the ground and excited states of a $k$-local Hamiltonian are largely determined by the $k$-particle reduced density matrices ($k$-RDMs), or simply the $k$-matrix for fermionic systems---they are at least enough for the calculation of the ground state and excited state energies. Moreover, for a non-degenerate ground state of a $k$-local Hamiltonian, even the state itself is completely determined by its $k$-RDMs, and therefore contains no genuine ${>}k$-particle correlations, as they can be inferred from $k$-particle correlation functions. It is natural to ask whether a similar result holds for non-degenerate excited states. In fact, for fermionic systems, it has been conjectured that any non-degenerate excited state of a 2-local Hamiltonian is simultaneously a unique ground state of another 2-local Hamiltonian, hence is uniquely determined by its 2-matrix. And a weaker version of this conjecture states that any non-degenerate excited state of a 2-local Hamiltonian is uniquely determined by its 2-matrix among all the pure $n$-particle states. We construct explicit counterexamples to show that both conjectures are false. It means that correlations in excited states of local Hamiltonians could be dramatically different from those in ground states. We further show that any non-degenerate excited state of a $k$-local Hamiltonian is a unique ground state of another $2k$-local Hamiltonian, hence is uniquely determined by its $2k$-RDMs (or $2k$-matrix)."
540,https://arxiv.org/abs/1010.2739,Principle of Maximum Entropy and Ground Spaces of Local Hamiltonians,"The structure of the ground spaces of quantum systems consisting of local interactions is of fundamental importance to different areas of physics. In this Letter, we present a necessary and sufficient condition for a subspace to be the ground space of a k-local Hamiltonian. Our analysis are motivated by the concept of irreducible correlations studied by [Linden et al., PRL 89, 277906] and [Zhou, PRL 101, 180505], which is in turn based on the principle of maximum entropy. It establishes a better understanding of the ground spaces of local Hamiltonians and builds an intimate link of ground spaces to the correlations of quantum states."
541,https://arxiv.org/abs/1010.2717,Quantum codes give counterexamples to the unique pre-image conjecture of the N-representability problem,"It is well known that the ground state energy of many-particle Hamiltonians involving only 2-body interactions can be obtained using constrained optimizations over density matrices which arise from reducing an N-particle state. While determining which 2-particle density matrices are ""N- representable"" is a computationally hard problem, all known extreme N-representable 2-particle reduced density matrices arise from a unique N-particle pre-image, satisfying a conjecture established in 1972. We present explicit counterexamples to this conjecture through giving Hamiltonians with 2-body interactions which have degenerate ground states that cannot be distinguished by any 2-body operator. We relate the existence of such counterexamples to quantum error correction codes and topologically ordered spin systems."
542,https://arxiv.org/abs/1010.2480,Complete Characterization of the Ground Space Structure of Two-Body Frustration-Free Hamiltonians for Qubits,"The problem of finding the ground state of a frustration-free Hamiltonian carrying only two-body interactions between qubits is known to be solvable in polynomial time. It is also shown recently that, for any such Hamiltonian, there is always a ground state that is a product of single- or two-qubit states. However, it remains unclear whether the whole ground space is of any succinct structure. Here, we give a complete characterization of the ground space of any two-body frustration-free Hamiltonian of qubits. Namely, it is a span of tree tensor network states of the same tree structure. This characterization allows us to show that the problem of determining the ground state degeneracy is as hard as, but no harder than, its classical analog."
543,https://arxiv.org/abs/1008.3350,Quantum Capacity Approaching Codes for the Detected-Jump Channel,"The quantum channel capacity gives the ultimate limit for the rate at which quantum data can be reliably transmitted through a noisy quantum channel. Degradable quantum channels are among the few channels whose quantum capacities are known. Given the quantum capacity of a degradable channel, it remains challenging to find a practical coding scheme which approaches capacity. Here we discuss code designs for the detected-jump channel, a degradable channel with practical relevance describing the physics of spontaneous decay of atoms with detected photon emission. We show that this channel can be used to simulate a binary classical channel with both erasures and bit-flips. The capacity of the simulated classical channel gives a lower bound on the quantum capacity of the detected-jump channel. When the jump probability is small, it almost equals the quantum capacity. Hence using a classical capacity approaching code for the simulated classical channel yields a quantum code which approaches the quantum capacity of the detected-jump channel."
544,https://arxiv.org/abs/1004.3787,No-go Theorem for One-way Quantum Computing on Naturally Occurring Two-level Systems,"One-way quantum computing achieves the full power of quantum computation by performing single particle measurements on some many-body entangled state, known as the resource state. As single particle measurements are relatively easy to implement, the preparation of the resource state becomes a crucial task. An appealing approach is simply to cool a strongly correlated quantum many-body system to its ground state. In addition to requiring the ground state of the system to be universal for one-way quantum computing, we also want the Hamiltonian to have non-degenerate ground state protected by a fixed energy gap, to involve only two-body interactions, and to be frustration-free so that measurements in the course of the computation leave the remaining particles in the ground space. Recently, significant efforts have been made to the search of resource states that appear naturally as ground states in spin lattice systems. The approach is proved to be successful in spin-5/2 and spin-3/2 systems. Yet, it remains an open question whether there could be such a natural resource state in a spin-1/2, i.e., qubit system. Here, we give a negative answer to this question by proving that it is impossible for a genuinely entangled qubit states to be a non-degenerate ground state of any two-body frustration-free Hamiltonian. What is more, we prove that every spin-1/2 frustration-free Hamiltonian with two-body interaction always has a ground state that is a product of single- or two-qubit states, a stronger result that is interesting independent of the context of one-way quantum computing."
545,https://arxiv.org/abs/1003.3059,Tensor Rank and Stochastic Entanglement Catalysis for Multipartite Pure States,"The tensor rank (also known as generalized Schmidt rank) of multipartite pure states plays an important role in the study of entanglement classifications and transformations. We employ powerful tools from the theory of homogeneous polynomials to investigate the tensor rank of symmetric states such as the tripartite state $\ket{W_3}=\tfrac{1}{\sqrt{3}}(\ket{100}+\ket{010}+\ket{001})$ and its $N$-partite generalization $\ket{W_N}$. Previous tensor rank estimates are dramatically improved and we show that (i) three copies of $\ket{W_3}$ has rank either 15 or 16, (ii) two copies of $\ket{W_N}$ has rank $3N-2$, and (iii) $n$ copies of $\ket{W_N}$ has rank O(N). A remarkable consequence of these results is that certain multipartite transformations, impossible even probabilistically, can become possible when performed in multiple copy bunches or when assisted by some catalyzing state. This effect is impossible for bipartite pure states."
546,https://arxiv.org/abs/1002.1567,Quantum state reduction for universal measurement based computation,"Measurement based quantum computation (MBQC), which requires only single particle measurements on a universal resource state to achieve the full power of quantum computing, has been recognized as one of the most promising models for the physical realization of quantum computers. Despite considerable progress in the last decade, it remains a great challenge to search for new universal resource states with naturally occurring Hamiltonians, and to better understand the entanglement structure of these kinds of states. Here we show that most of the resource states currently known can be reduced to the cluster state, the first known universal resource state, via adaptive local measurements at a constant cost. This new quantum state reduction scheme provides simpler proofs of universality of resource states and opens up plenty of space to the search of new resource states, including an example based on the one-parameter deformation of the AKLT state studied in [Commun. Math. Phys. 144, 443 (1992)] by M. Fannes et al. about twenty years ago."
547,https://arxiv.org/abs/1001.2356,Multi-Error-Correcting Amplitude Damping Codes,"We construct new families of multi-error-correcting quantum codes for the amplitude damping channel. Our key observation is that, with proper encoding, two uses of the amplitude damping channel simulate a quantum erasure channel. This allows us to use concatenated codes with quantum erasure-correcting codes as outer codes for correcting multiple amplitude damping errors. Our new codes are degenerate stabilizer codes and have parameters which are better than the amplitude damping codes obtained by any previously known construction."
548,https://arxiv.org/abs/0907.4737,QIP = PSPACE,"We prove that the complexity class QIP, which consists of all problems having quantum interactive proof systems, is contained in PSPACE. This containment is proved by applying a parallelized form of the matrix multiplicative weights update method to a class of semidefinite programs that captures the computational power of quantum interactive proofs. As the containment of PSPACE in QIP follows immediately from the well-known equality IP = PSPACE, the equality QIP = PSPACE follows."
549,https://arxiv.org/abs/0906.5416,Non-Identity Check Remains QMA-Complete for Short Circuits,"The Non-Identity Check problem asks whether a given a quantum circuit is far away from the identity or not. It is well known that this problem is QMA-Complete \cite{JWB05}. In this note, it is shown that the Non-Identity Check problem remains QMA-Complete for circuits of short depth. Specifically, we prove that for constant depth quantum circuit in which each gate is given to at least $Ω(\log n)$ bits of precision, the Non-Identity Check problem is QMA-Complete. It also follows that the hardness of the problem remains for polylogarithmic depth circuit consisting of only gates from any universal gate set and for logarithmic depth circuit using some specific universal gate set."
550,https://arxiv.org/abs/0709.1266,The LU-LC conjecture is false,"The LU-LC conjecture is an important open problem concerning the structure of entanglement of states described in the stabilizer formalism. It states that two local unitary equivalent stabilizer states are also local Clifford equivalent. If this conjecture were true, the local equivalence of stabilizer states would be extremely easy to characterize. Unfortunately, however, based on the recent progress made by Gross and Van den Nest, we find that the conjecture is false."
551,https://arxiv.org/abs/0707.0330,An Algebra of Quantum Processes,"We introduce an algebra qCCS of pure quantum processes in which no classical data is involved, communications by moving quantum states physically are allowed, and computations is modeled by super-operators. An operational semantics of qCCS is presented in terms of (non-probabilistic) labeled transition systems. Strong bisimulation between processes modeled in qCCS is defined, and its fundamental algebraic properties are established, including uniqueness of the solutions of recursive equations. To model sequential computation in qCCS, a reduction relation between processes is defined. By combining reduction relation and strong bisimulation we introduce the notion of strong reduction-bisimulation, which is a device for observing interaction of computation and communication in quantum systems. Finally, a notion of strong approximate bisimulation (equivalently, strong bisimulation distance) and its reduction counterpart are introduced. It is proved that both approximate bisimilarity and approximate reduction-bisimilarity are preserved by various constructors of quantum processes. This provides us with a formal tool for observing robustness of quantum processes against inaccuracy in the implementation of its elementary gates."
552,https://arxiv.org/abs/0704.1473,Existence of Universal Entangler,"A gate is called entangler if it transforms some (pure) product states to entangled states. A universal entangler is a gate which transforms all product states to entangled states. In practice, a universal entangler is a very powerful device for generating entanglements, and thus provides important physical resources for accomplishing many tasks in quantum computing and quantum information. This Letter demonstrates that a universal entangler always exists except for a degenerated case. Nevertheless, the problem how to find a universal entangler remains open."
553,https://arxiv.org/abs/quant-ph/0612034,Distinguishing Arbitrary Multipartite Basis Unambiguously Using Local Operations and Classical Communication,"We show that an arbitrary basis of a multipartite quantum state space consisting of $K$ distant parties such that the $k$th party has local dimension $d_k$ always contains at least $N=\sum_{k=1}^K (d_k-1)+1$ members that are unambiguously distinguishable using local operations and classical communication (LOCC). We further show this lower bound is optimal by analytically constructing a special product basis having only $N$ members unambiguously distinguishable by LOCC. Interestingly, such a special product basis not only gives a stronger form of the weird phenomenon ``nonlocality without entanglement"", but also implies the existence of locally distinguishable entangled basis."
554,https://arxiv.org/abs/quant-ph/0610060,Parameter estimation of quantum channels,The efficiency of parameter estimation of quantum channels is studied in this paper. We introduce the concept of programmable parameters to the theory of estimation. It is found that programmable parameters obey the standard quantum limit strictly; hence no speedup is possible in its estimation. We also construct a class of non-unitary quantum channels whose parameter can be estimated in a way that the standard quantum limit is broken. The study of estimation of general quantum channels also enables an investigation of the effect of noises on quantum estimation.
555,https://arxiv.org/abs/quant-ph/0606015,Majorization in Quantum Adiabatic Algorithms,"The majorization theory has been applied to analyze the mathematical structure of quantum algorithms. An empirical conclusion by numerical simulations obtained in the previous literature indicates that step-by-step majorization seems to appear universally in quantum adiabatic algorithms. In this paper, a rigorous analysis of the majorization arrow in a special class of quantum adiabatic algorithms is carried out. In particular, we prove that for any adiabatic algorithm of this class, step-by-step majorization of the ground state holds exactly. For the actual state, we show that step-by-step majorization holds approximately, and furthermore that the longer the running time of the algorithm, the better the approximation."
556,https://arxiv.org/abs/quant-ph/0604149,Optimal dense coding with arbitrary pure entangled states,We examine dense coding with an arbitrary pure entangled state sharing between the sender and the receiver. Upper bounds on the average success probability in approximate dense coding and on the probability of conclusive results in unambiguous dense coding are derived. We also construct the optimal protocol which saturates the upper bound in each case.
557,https://arxiv.org/abs/quant-ph/0601136,Boundary effect of deterministic dense coding,"We present a rigorous proof of an interesting boundary effect of deterministic dense coding first observed by Mozes et al. [Phys. Rev. A 71, 012311 (2005)]. Namely, it is shown that $d^2-1$ cannot be the maximal alphabet size of any isometric deterministic dense coding schemes utilizing $d$-level partial entanglement."
558,https://arxiv.org/abs/quant-ph/0601090,Identification and Distance Measures of Measurement Apparatus,We propose simple schemes that can perfectly identify projective measurement apparatus secretly chosen from a finite set. Entanglements are used in these schemes both to make possible the perfect identification and to improve the efficiency significantly. A brief discussion on the problem of how to appropriately define distance measures of measurements is also provided based on the results of identification.
559,https://arxiv.org/abs/cs/0601014,Probabilistic bisimilarities between quantum processes,"Modeling and reasoning about concurrent quantum systems is very important both for distributed quantum computing and for quantum protocol verification. As a consequence, a general framework describing formally the communication and concurrency in complex quantum systems is necessary. For this purpose, we propose a model qCCS which is a natural quantum extension of classical value-passing CCS with the input and output of quantum states, and unitary transformations and measurements on quantum systems. The operational semantics of qCCS is given based on probabilistic labeled transition system. This semantics has many different features compared with the proposals in literature in order to describe input and output of quantum systems which are possibly correlated with other components. Based on this operational semantics, we introduce the notions of strong probabilistic bisimilarity and weak probabilistic bisimilarity between quantum processes and discuss some properties of them, such as congruence under various combinators."
560,https://arxiv.org/abs/cs/0507043,Proof rules for purely quantum programs,"We apply the notion of quantum predicate proposed by D'Hondt and Panangaden to analyze a purely quantum language fragment which describes the quantum part of a future quantum computer in Knill's architecture. The denotational semantics, weakest precondition semantics, and weakest liberal precondition semantics of this language fragment are introduced. To help reasoning about quantum programs involving quantum loops, we extend proof rules for classical probabilistic programs to our purely quantum programs."
561,https://arxiv.org/abs/quant-ph/0506021,Condition and capability of quantum state separation,"The linearity of quantum operations puts many fundamental constraints on the information processing tasks we can achieve on a quantum system whose state is not exactly known, just as we observe in quantum cloning and quantum discrimination. In this paper we show that in probabilistic manner, linearity is in fact the only one that restricts the physically realizable tasks. To be specific, if a system is prepared in a state secretly chosen from a linearly independent pure state set, then any quantum state separation can be physically realized with a positive probability. Furthermore, we derive a lower bound on the average failure probability of any quantum state separation."
562,https://arxiv.org/abs/quant-ph/0501089,Local cloning of two product states,"Local quantum operations and classical communication (LOCC) put considerable constraints on many quantum information processing tasks such as cloning and discrimination. Surprisingly however, discrimination of any two pure states survives such constraints in some sense. In this paper, we show that cloning is not that lucky; namely, conclusive LOCC cloning of two product states is strictly less efficient than global cloning."
563,https://arxiv.org/abs/quant-ph/0410046,Efficiency of Deterministic Entanglement Transformation,"We prove that sufficiently many copies of a bipartite entangled pure state can always be transformed into some copies of another one with certainty by local quantum operations and classical communication. The efficiency of such a transformation is characterized by deterministic entanglement exchange rate, and it is proved to be always positive and bounded from top by the infimum of the ratios of Renyi's entropies of source state and target state. A careful analysis shows that the deterministic entanglement exchange rate cannot be increased even in the presence of catalysts. As an application, we show that there can be two incomparable states with deterministic entanglement exchange rate strictly exceeding 1."
564,https://arxiv.org/abs/quant-ph/0407120,Optimal conclusive discrimination of two states can be achieved locally,This paper constructs a LOCC protocol that achieves the global optimality in conclusive discrimination of any two states with arbitrary a priori probability. This can be interpreted that there is no ``non-locality'' in the conclusive discrimination of two multipartite states.
565,https://arxiv.org/abs/quant-ph/0407001,Comparability of multipartite entanglement,"We prove, in a multipartite setting, that it's always feasible to exactly transform a genuinely $m$-partite entangled state with sufficient many copies to any other $m$-partite state via local quantum operation and classical communication. This result affirms the comparability of multipartite pure entangled states."
566,https://arxiv.org/abs/quant-ph/0304145,"Quantum operation, quantum Fourier transform and semi-definite programming","We analyze a class of quantum operations based on a geometrical representation of $d-$level quantum system (or qudit for short). A sufficient and necessary condition of complete positivity, expressed in terms of the quantum Fourier transform, is found for this class of operations. A more general class of operations on qudits is also considered and its completely positive condition is reduced to the well-known semi-definite programming problem."
567,https://arxiv.org/abs/2305.16652,Faster Ray Tracing through Hierarchy Cut Code,"We propose a novel ray reordering technique to accelerate the ray tracing process by encoding and sorting rays prior to traversal. Instead of spatial coordinates, our method encodes rays according to the cuts of the hierarchical acceleration structure, which is called the \textbf{hierarchy cut code}. This approach can better adapt to the acceleration structure and obtain a more reliable encoding result. We also propose a compression scheme to decrease the sorting overhead by a shorter sorting key. In addition, based on the phenomenon of boundary drift, we theoretically explain the reason why existing reordering methods cannot achieve better performance by using longer sorting keys. The experiment demonstrates that our method can accelerate secondary ray tracing by up to 1.81 times, outperforming the existing methods. Such result proves the effectiveness of hierarchy cut code, and indicate that the reordering technique can achieve greater performance improvement, which worth further research."
568,https://arxiv.org/abs/2305.08736,Generalized weak Galerkin finite element methods for second order elliptic problems,"This article proposes and analyzes the generalized weak Galerkin ({\rm g}WG) finite element method for the second order elliptic problem. A generalized discrete weak gradient operator is introduced in the weak Galerkin framework so that the {\rm g}WG methods would not only allow arbitrary combinations of piecewise polynomials defined in the interior and on the boundary of each local finite element, but also work on general polytopal partitions. Error estimates are established for the corresponding numerical functions in the energy norm and the usual $L^2$ norm. A series of numerical experiments are presented to demonstrate the performance of the newly proposed {\rm g}WG method."
569,https://arxiv.org/abs/2304.07673,GLHAD: A Group Lasso-based Hybrid Attack Detection and Localization Framework for Multistage Manufacturing Systems,"As Industry 4.0 and digitalization continue to advance, the reliance on information technology increases, making the world more vulnerable to cyber-attacks, especially cyber-physical attacks that can manipulate physical systems and compromise operational data integrity. Detecting cyber-attacks in multistage manufacturing systems (MMS) is crucial due to the growing sophistication of attacks and the complexity of MMS. Attacks can propagate throughout the system, affecting subsequent stages and making detection more challenging than in single-stage systems. Localization is also critical due to the complex interactions in MMS. To address these challenges, a group lasso regression-based framework is proposed to detect and localize attacks in MMS. The proposed algorithm outperforms traditional hypothesis testing-based methods in expected detection delay and localization accuracy, as demonstrated in a linear multistage manufacturing system."
570,https://arxiv.org/abs/2304.07363,An Optimization Framework for Cyber-Physical Vulnerability Analysis in Industrial Cyber-Physical Systems,"Industrial Control Systems (ICSs) are widely used in critical infrastructures. These industrial facilities face various cyberattacks that may cause physical damage. With the increasing integration of the ICSs and information technology (IT) components, ensuring the security of ICSs is of paramount importance. Typically, a cyberattack in ICS aims to impose physical damage on the ICSs by exploiting a sequence of vulnerabilities that leads to compromising the system's sensors and/or controllers. Due to the physics of the ICSs operation, maliciously accessing different sensors or components may cause varying levels of risk. Therefore, identifying cyberattacks with the highest risks is essential for effectively designing detection schemes and mitigation strategies. In this paper, we develop an optimization-based holistic cybersecurity risk assessment framework by integrating the cyber and physical systems of ICSs to understand 1) the vulnerabilities of the ICS network with holistic consideration of cyber and physical components and their interactions, and 2) the attack strategies that can lead to significant physical damage to the critical assets in ICS. We formulate a novel optimization problem that accounts for the attacker's objective and limitation in both the physical and cyber systems of ICSs. This allows us to jointly model the interactions between the cyber and physical components and study the critical cyberattacks that cause the highest impact on the physical components under certain resource constraints. We verified the effectiveness of our proposed method in a numerical study, and the results show that a strategic attacker causes almost 19% further acceleration in the failure time of the physical system compared to a random attacker."
571,https://arxiv.org/abs/2302.11931,A high-fidelity quantum state transfer algorithm on the complete bipartite graph,"High-fidelity quantum state transfer is critical for quantum communication and scalable quantum computation. Current quantum state transfer algorithms on the complete bipartite graph, which are based on discrete-time quantum walk search algorithms, suffer from low fidelity in some cases. To solve this problem, in this paper we propose a two-stage quantum state transfer algorithm on the complete bipartite graph. The algorithm is achieved by the generalized Grover walk with one marked vertex. The generalized Grover walk's coin operators and the query oracles are both parametric unitary matrices, which are designed flexibly based on the positions of the sender and receiver and the size of the complete bipartite graph. We prove that the fidelity of the algorithm is greater than $1-2ε_{1}-ε_{2}-2\sqrt{2}\sqrt{ε_{1}ε_{2}}$ or $1-(2+2\sqrt{2})ε_{1}-ε_{2}-(2+2\sqrt{2})\sqrt{ε_{1}ε_{2}}$ for any adjustable parameters $ε_{1}$ and $ε_{2}$ when the sender and receiver are in the same partition or different partitions of the complete bipartite graph. The algorithm provides a novel approach to achieve high-fidelity quantum state transfer on the complete bipartite graph in any case, which will offer potential applications for quantum information processing."
572,https://arxiv.org/abs/2302.06531,Generalized Weak Galerkin Finite Element Methods for Biharmonic Equations,The generalized weak Galerkin (gWG) finite element method is proposed and analyzed for the biharmonic equation. A new generalized discrete weak second order partial derivative is introduced in the gWG scheme to allow arbitrary combinations of piecewise polynomial functions defined in the interior and on the boundary of general polygonal or polyhedral elements. The error estimates are established for the numerical approximation in a discrete H^2 norm and a L^2 norm. The numerical results are reported to demonstrate the accuracy and flexibility of our proposed gWG method for the biharmonic equation.
573,https://arxiv.org/abs/2212.12783,An $L^p$-primal-dual finite element method for first-order transport problems,"A new $L^p$-primal-dual weak Galerkin method ($L^p$-PDWG) with $p>1$ is proposed for the first-order transport problems. The existence and uniqueness of the $L^p$-PDWG numerical solutions is established. In addition, the $L^p$-PDWG method offers a numerical solution which retains mass conservation locally on each element. An optimal order error estimate is established for the primal variable. A series of numerical results are presented to verify the efficiency and accuracy of the proposed $L^p$-PDWG scheme."
574,https://arxiv.org/abs/2210.17518,Weak Galerkin methods based Morley elements on general polytopal partitions,A new weak Galerkin method based on the weak tangential derivative and weak second order partial derivative is proposed to extend the well-known Morley element for the biharmonic equation from triangular elements to general polytopal elements. The Schur complement of the weak Galerkin scheme not only enjoys the same degrees of freedom as the Morley element on the triangular element but also extends the Morley element to any general polytopal element. The error estimates for the numerical approximation are established in the energy norm and the usual $L^2$ norms. Several numerical experiments are demonstrated to validate the theory developed in this article.
575,https://arxiv.org/abs/2210.16907,Curved Elements in Weak Galerkin Finite Element Methods,A mathematical analysis is established for the weak Galerkin finite element methods for the Poisson equation with Dirichlet boundary value when the curved elements are involved on the interior edges of the finite element partition or/and on the boundary of the whole domain in two dimensions. The optimal orders of error estimates for the weak Galerkin approximations in both the $H^1$-norm and the $L^2$-norm are established. Numerical results are reported to demonstrate the performance of the weak Galerkin methods on general curved polygonal partitions.
576,https://arxiv.org/abs/2208.01985,The primitive equations with magnetic field approximation of the 3D MHD equations,"In our earlier work \cite{DLL}, we have shown the global well-posedness of strong solutions to the three-dimensional primitive equations with the magnetic field (PEM) on a thin domain. The heart of this paper is to provide a rigorous justification of the derivation of the PEM as the small aspect ratio limit of the incompressible three-dimensional scaled magnetohydrodynamics (SMHD) equations in the anisotropic horizontal viscosity and magnetic field regime. For the case of $H^1$-initial data case, we prove that global Leray-Hopf weak solutions of the three-dimensional SMHD equation strongly converge to the global strong solutions of the PEM. In the $H^2$-initial data case, the strong solution of the SMHD can be extended to be a global one for small $\v$. As a consequence, we observe that the global strong solutions of the SMHD strong converge to the global strong solutions of the PEM. As a byproduct, the convergence rate is of the same order as the aspect ratio parameter."
577,https://arxiv.org/abs/2207.14047,Chaotic diffusion of asteroids in the exterior 1:2 mean motion resonance with Mars,"The inner asteroid belt between 2.1 and 2.5 au is of particular dynamical significance because it is the dominant source of both chondritic meteorites and near-Earth asteroids. This inner belt is bounded by an eccentricity-type secular resonance and by the 1:3 mean motion resonance with Jupiter. Unless asteroid perihelia are low enough to allow scattering by Mars, escape requires transport to one of the bounding resonances. In addition Yarkovsky forces are generally ineffective in changing either the eccentricity and/or inclination for asteroids with diameter $\gtrsim$30 km. Thus, large asteroids with pericentres far from Mars may only escape from the inner belt through large changes in their eccentricities. In this paper we study chaotic diffusion of orbits near the 1:2 mean motion resonance with Mars in a systematic way. We show that, while chaotic orbital evolution in both resonant and non-resonant orbits increase the dispersion of the inclinations and eccentricities, it does not significantly change their mean values. We show further that, while the dispersive growth is greatest for resonant orbits, at high $e$ the resonance acts to mitigate asteroid scattering by Mars - making the asteroid lifetime in the belt longer than it would have been for a non-resonant orbit. For asteroids of all sizes in both resonant and non-resonant orbits, the changes in eccentricity needed to account for the observations cannot be achieved by gravitational forces alone. The role of resonant trapping in protecting asteroids from encounters with Mars is also analysed."
578,https://arxiv.org/abs/2207.10672,"GJ 3929: High Precision Photometric and Doppler Characterization of an Exo-Venus and its Hot, Mini-Neptune-mass Companion","We detail the follow up and characterization of a transiting exo-Venus identified by TESS, GJ 3929b, (TOI-2013b) and its non-transiting companion planet, GJ 3929c (TOI-2013c). GJ 3929b is an Earth-sized exoplanet in its star's Venus-zone (P$_{b}$ = 2.616272 $\pm$ 0.000005 days; S$_{b}$ = 17.3$^{+0.8}_{-0.7}$ S$_{\oplus}$) orbiting a nearby M dwarf. GJ 3929c is most likely a non-transiting sub-Neptune. Using the new, ultra-precise NEID spectrometer on the WIYN 3.5 m Telescope at Kitt Peak National Observatory, we are able to modify the mass constraints of planet b reported in previous works and consequently improve the significance of the mass measurement to almost 4$σ$ confidence (M$_{b}$ = 1.75 $\pm$ 0.45 M$_{\oplus}$). We further adjust the orbital period of planet c from its alias at 14.30 $\pm$ 0.03 days to the likely true period of 15.04 $\pm$ 0.03 days, and adjust its minimum mass to m$\sin i$ = 5.71 $\pm$ 0.92 M$_{\oplus}$. Using the diffuser-assisted ARCTIC imager on the ARC 3.5 m telescope at Apache Point Observatory, in addition to publicly available TESS and LCOGT photometry, we are able to constrain the radius of planet b to R$_{p}$ = 1.09 $\pm$ 0.04 R$_{\oplus}$. GJ 3929b is a top candidate for transmission spectroscopy in its size regime (TSM = 14 $\pm$ 4), and future atmospheric studies of GJ 3929b stand to shed light on the nature of small planets orbiting M dwarfs."
579,https://arxiv.org/abs/2206.12065,Eco-driving for Electric Connected Vehicles at Signalized Intersections: A Parameterized Reinforcement Learning approach,"This paper proposes an eco-driving framework for electric connected vehicles (CVs) based on reinforcement learning (RL) to improve vehicle energy efficiency at signalized intersections. The vehicle agent is specified by integrating the model-based car-following policy, lane-changing policy, and the RL policy, to ensure safe operation of a CV. Subsequently, a Markov Decision Process (MDP) is formulated, which enables the vehicle to perform longitudinal control and lateral decisions, jointly optimizing the car-following and lane-changing behaviors of the CVs in the vicinity of intersections. Then, the hybrid action space is parameterized as a hierarchical structure and thereby trains the agents with two-dimensional motion patterns in a dynamic traffic environment. Finally, our proposed methods are evaluated in SUMO software from both a single-vehicle-based perspective and a flow-based perspective. The results show that our strategy can significantly reduce energy consumption by learning proper action schemes without any interruption of other human-driven vehicles (HDVs)."
580,https://arxiv.org/abs/2206.06005,Global well-posedness of the 3D Primitive Equations with magnetic field,"In this paper, the three-dimensional primitive equations with magnetic field (PEM) are considered on a thin domain. We showed the global existence and uniqueness (regularity) of strong solutions to the three-dimensional incompressible PEM without any small assumption on the initial data. More precisely, there exists a unique strong solution globally in time for any given $H^2$-smooth initial data."
581,https://arxiv.org/abs/2206.05656,On the initiation of fiber fuse damage in high-power ytterbium-doped fiber lasers,"Fiber fuse effect can occur spontaneously and propagate along optical fibers to cause wide-spread damage; it threatens all applications involving optical fibers. This paper presents two results. First, it establishes that the initiation of fiber fuse (IFF) in silica fibers is caused by defect-induced absorption. Critical temperatures and critical optical powers for IFF are simulated for the first time using a 3D solid-state heat transfer model with heat source generated by defect-induced absorption. In this method, formation energies of the defects can be uniquely determined, which offers critical information on the chemical reasons for fiber fuse. Second, this paper offers a method to evaluate operating temperatures of fiber lasers. General analytical solutions of the operating temperatures along gain fibers are deduced. Results of 976-nm laser-diode-(LD)-pumped and 1018-nm tandem-pumped ytterbium-doped fiber (YDF) amplifiers using 10/130-μm YDFs are calculated. Potential limits caused by fiber fuse are discussed."
582,https://arxiv.org/abs/2205.07855,Decentral and Incentivized Federated Learning Frameworks: A Systematic Literature Review,"The advent of Federated Learning (FL) has ignited a new paradigm for parallel and confidential decentralized Machine Learning (ML) with the potential of utilizing the computational power of a vast number of IoT, mobile and edge devices without data leaving the respective device, ensuring privacy by design. Yet, in order to scale this new paradigm beyond small groups of already entrusted entities towards mass adoption, the Federated Learning Framework (FLF) has to become (i) truly decentralized and (ii) participants have to be incentivized. This is the first systematic literature review analyzing holistic FLFs in the domain of both, decentralized and incentivized federated learning. 422 publications were retrieved, by querying 12 major scientific databases. Finally, 40 articles remained after a systematic review and filtering process for in-depth examination. Although having massive potential to direct the future of a more distributed and secure AI, none of the analyzed FLF is production-ready. The approaches vary heavily in terms of use-cases, system design, solved issues and thoroughness. We are the first to provide a systematic approach to classify and quantify differences between FLF, exposing limitations of current works and derive future directions for research in this novel domain."
583,https://arxiv.org/abs/2205.05983,Controlled Alternate Quantum Walk based Block Hash Function,"The hash function is an important branch of cryptology. Controlled quantum walk based hash function is a kind of novel hash function, which is safe, flexible, high-efficient, and compatible. All existing controlled quantum walk based hash functions are controlled by one bit message in each step. To process message in batch amounts, in this paper, controlled alternate quantum walk based block hash function is presented by using the time-position-dependent controlled quantum walks on complete graphs with self-loops. The presented hash function accelerate the hash processing dramatically, so it is more high-efficient."
584,https://arxiv.org/abs/2204.11011,A Novel Splitting Criterion Inspired by Geometric Mean Metric Learning for Decision Tree,"Decision tree (DT) attracts persistent research attention due to its impressive empirical performance and interpretability in numerous applications. However, the growth of traditional yet widely-used univariate decision trees (UDTs) is quite time-consuming as they need to traverse all the features to find the splitting value with the maximal reduction of the impurity at each internal node. In this paper, we newly design a splitting criterion to speed up the growth. The criterion is induced from Geometric Mean Metric Learning (GMML) and then optimized under its diagonalized metric matrix constraint, consequently, a closed-form rank of feature discriminant abilities can at once be obtained and the top 1 feature at each node used to grow an intent DT (called as dGMML-DT, where d is an abbreviation for diagonalization). We evaluated the performance of the proposed methods and their corresponding ensembles on benchmark datasets. The experiment shows that dGMML-DT achieves comparable or better classification results more efficiently than the UDTs with 10x average speedup. Furthermore, dGMML-DT can straightforwardly be extended to its multivariable counterpart (dGMML-MDT) without needing laborious operations."
585,https://arxiv.org/abs/2202.11418,The Astrometric Performance Test of 80-cm Telescope at Yaoan Station and Precise CCD Positions of Apophis,"The 80-cm azimuthal telescope is newly mounted at Yaoan Station, Purple Mountain Observatory in 2018. The astrometric performance of the telescope is tested in the following three aspects. (a) The geometric distortion of its CCD attached. It is stable in both a single epoch and multi epochs. Eight distortion solutions are derived over about one year. The maximum values range from 0.75 to 0.79 pixel and the median values range from 0.14 to 0.16 pixel. (b) The limit magnitude of stars. About 20.5 magnitude (Gaia-G) stars can be detected with Johnson-V filter exposured in 300 seconds. The astrometric error of about 20.5 magnitude stars is estimated at 0.14 arcsec using the fitted sigmoidal function. (c) The astrometric accuracy and the precision of stacked fast-moving faint object. 24 stacked frames of the potentially hazardous asteroid (PHA) (99942) Apophis are derived on April 14 and 15, 2021 (fainter than 18 mag) based on the ephemeris shifts. During data reduction, the newest Gaia EDR3 Catalog and Jet Propulsion Laboratory Horizons ephemeris are referenced as theoretical positions of stars and Apophis, respectively. Our results show that the mean (O-C)s (observed minus computed) of Apophis are -0.018 and 0.020 arcsec in right ascention and declination, and the dispersions are estimated at 0.094 and 0.085 arcsec, respectively, which show the consistency of the stacked results by Astrometrica."
586,https://arxiv.org/abs/2202.06266,Improve Deep Image Inpainting by Emphasizing the Complexity of Missing Regions,"Deep image inpainting research mainly focuses on constructing various neural network architectures or imposing novel optimization objectives. However, on the one hand, building a state-of-the-art deep inpainting model is an extremely complex task, and on the other hand, the resulting performance gains are sometimes very limited. We believe that besides the frameworks of inpainting models, lightweight traditional image processing techniques, which are often overlooked, can actually be helpful to these deep models. In this paper, we enhance the deep image inpainting models with the help of classical image complexity metrics. A knowledge-assisted index composed of missingness complexity and forward loss is presented to guide the batch selection in the training procedure. This index helps find samples that are more conducive to optimization in each iteration and ultimately boost the overall inpainting performance. The proposed approach is simple and can be plugged into many deep inpainting models by changing only a few lines of code. We experimentally demonstrate the improvements for several recently developed image inpainting models on various datasets."
587,https://arxiv.org/abs/2201.06729,Eigenvalues of signed graphs,"Signed graphs have their edges labeled either as positive or negative. $ρ(M)$ denote the $M$-spectral radius of $Σ$, where $M=M(Σ)$ is a real symmetric graph matrix of $Σ$. Obviously, $ρ(M)=\mbox{max}\{λ_1(M),-λ_n(M)\}$. Let $A(Σ)$ be the adjacency matrix of $Σ$ and $(K_n,H^-)$ be a signed complete graph whose negative edges induce a subgraph $H$. In this paper, we first focus on a central problem in spectral extremal graph theory as follows: Which signed graph with maximum $ρ(A(Σ))$ among $(K_n,T^-)$ where $T$ is a spanning tree? To answer the problem, we characterize the extremal signed graph with maximum $λ_1(A(Σ))$ and minimum $λ_n(A(Σ))$ among $(K_n,T^-)$, respectively. Another interesting graph matrix of a signed graph is distance matrix, i.e. $D(Σ)$ which was defined by Hameed, Shijin, Soorya, Germina and Zaslavsky [8]. Note that $A(Σ)=D(Σ)$ when $Σ\in (K_n,T^-)$. In this paper, we give upper bounds on the least distance eigenvalue of a signed graph $Σ$ with diameter at least 2. This result implies a result proved by Lin [11] was originally conjectured by Aouchiche and Hansen [1]."
588,https://arxiv.org/abs/2112.08541,BGL: GPU-Efficient GNN Training by Optimizing Graph Data I/O and Preprocessing,"Graph neural networks (GNNs) have extended the success of deep neural networks (DNNs) to non-Euclidean graph data, achieving ground-breaking performance on various tasks such as node classification and graph property prediction. Nonetheless, existing systems are inefficient to train large graphs with billions of nodes and edges with GPUs. The main bottlenecks are the process of preparing data for GPUs - subgraph sampling and feature retrieving. This paper proposes BGL, a distributed GNN training system designed to address the bottlenecks with a few key ideas. First, we propose a dynamic cache engine to minimize feature retrieving traffic. By a co-design of caching policy and the order of sampling, we find a sweet spot of low overhead and high cache hit ratio. Second, we improve the graph partition algorithm to reduce cross-partition communication during subgraph sampling. Finally, careful resource isolation reduces contention between different data preprocessing stages. Extensive experiments on various GNN models and large graph datasets show that BGL significantly outperforms existing GNN training systems by 20.68x on average."
589,https://arxiv.org/abs/2112.04195,VIRT: Improving Representation-based Models for Text Matching through Virtual Interaction,"With the booming of pre-trained transformers, representation-based models based on Siamese transformer encoders have become mainstream techniques for efficient text matching. However, these models suffer from severe performance degradation due to the lack of interaction between the text pair, compared with interaction-based models. Prior arts attempt to address this through performing extra interaction for Siamese encoded representations, while the interaction during encoding is still ignored. To remedy this, we propose a \textit{Virtual} InteRacTion mechanism (VIRT) to transfer interactive knowledge from interaction-based models into Siamese encoders through attention map distillation. As a train-time-only component, VIRT could completely maintain the high efficiency of the Siamese structure and brings no extra computation cost during inference. To fully utilize the learned interactive knowledge, we further design a VIRT-adapted interaction strategy. Experimental results on multiple text matching datasets demonstrate that our method outperforms state-of-the-art representation-based models. What's more, VIRT can be easily integrated into existing representation-based methods to achieve further improvements."
590,https://arxiv.org/abs/2111.07282,Soft magnetic microrobot doped with porous silica for stability-enhanced multimodal locomotion in nonideal environment,"As an emerging field of robotics, magnetic-field-controlled soft microrobot has broad application prospects for its flexibility, locomotion diversity as well as remote controllability. Magnetic soft microrobots can perform multimodal locomotion under the control of a magnetic field, which may have potential applications in precision medicine. However, previous researches mainly focus on new locomotion in a relatively ideal environment, lacking exploration on the ability of magnetic microrobot locomotion to resist external disturbances and proceed in a nonideal environment. Here, a porous silica-doped soft magnetic microrobot is constructed for enhanced stability of multimodal locomotion in the nonideal biological environment. Porous silica spheres are doped into NdFeB-silicone elastomer base, improving adhesion properties as well as refining the comprehensive mechanical properties of the microrobot. Multimodal locomotions are achieved, and the influence of porous silica doping on the stability of each locomotion in nonideal environment is explored in depth. Motions in nonideal circumstances such as climbing, loading, current rushing, wind blowing, and obstacle hindering are conducted successfully with porous silica doping. Such a stability-enhanced multimodal locomotion system can be used in biocatalysis as well as thrombus removal, and its prospect for precision medicine is highlighted by in vivo demonstration of multimodal locomotion with nonideal disturbance."
591,https://arxiv.org/abs/2109.11682,Paint4Poem: A Dataset for Artistic Visualization of Classical Chinese Poems,"In this work we propose a new task: artistic visualization of classical Chinese poems, where the goal is to generatepaintings of a certain artistic style for classical Chinese poems. For this purpose, we construct a new dataset called Paint4Poem. Thefirst part of Paint4Poem consists of 301 high-quality poem-painting pairs collected manually from an influential modern Chinese artistFeng Zikai. As its small scale poses challenges for effectively training poem-to-painting generation models, we introduce the secondpart of Paint4Poem, which consists of 3,648 caption-painting pairs collected manually from Feng Zikai's paintings and 89,204 poem-painting pairs collected automatically from the web. We expect the former to help learning the artist painting style as it containshis most paintings, and the latter to help learning the semantic relevance between poems and paintings. Further, we analyze Paint4Poem regarding poem diversity, painting style, and the semantic relevance between poems and paintings. We create abenchmark for Paint4Poem: we train two representative text-to-image generation models: AttnGAN and MirrorGAN, and evaluate theirperformance regarding painting pictorial quality, painting stylistic relevance, and semantic relevance between poems and paintings.The results indicate that the models are able to generate paintings that have good pictorial quality and mimic Feng Zikai's style, but thereflection of poem semantics is limited. The dataset also poses many interesting research directions on this task, including transferlearning, few-shot learning, text-to-image generation for low-resource data etc. The dataset is publicly available.(https://github.com/paint4poem/paint4poem)"
592,https://arxiv.org/abs/2108.02566,Missingness Augmentation: A General Approach for Improving Generative Imputation Models,"Missing data imputation is a fundamental problem in data analysis, and many studies have been conducted to improve its performance by exploring model structures and learning procedures. However, data augmentation, as a simple yet effective method, has not received enough attention in this area. In this paper, we propose a novel data augmentation method called Missingness Augmentation (MisA) for generative imputation models. Our approach dynamically produces incomplete samples at each epoch by utilizing the generator's output, constraining the augmented samples using a simple reconstruction loss, and combining this loss with the original loss to form the final optimization objective. As a general augmentation technique, MisA can be easily integrated into generative imputation frameworks, providing a simple yet effective way to enhance their performance. Experimental results demonstrate that MisA significantly improves the performance of many recently proposed generative imputation models on a variety of tabular and image datasets. The code is available at \url{https://github.com/WYu-Feng/Missingness-Augmentation}."
593,https://arxiv.org/abs/2106.14265,Reward-Based 1-bit Compressed Federated Distillation on Blockchain,"The recent advent of various forms of Federated Knowledge Distillation (FD) paves the way for a new generation of robust and communication-efficient Federated Learning (FL), where mere soft-labels are aggregated, rather than whole gradients of Deep Neural Networks (DNN) as done in previous FL schemes. This security-per-design approach in combination with increasingly performant Internet of Things (IoT) and mobile devices opens up a new realm of possibilities to utilize private data from industries as well as from individuals as input for artificial intelligence model training. Yet in previous FL systems, lack of trust due to the imbalance of power between workers and a central authority, the assumption of altruistic worker participation and the inability to correctly measure and compare contributions of workers hinder this technology from scaling beyond small groups of already entrusted entities towards mass adoption. This work aims to mitigate the aforementioned issues by introducing a novel decentralized federated learning framework where heavily compressed 1-bit soft-labels, resembling 1-hot label predictions, are aggregated on a smart contract. In a context where workers' contributions are now easily comparable, we modify the Peer Truth Serum for Crowdsourcing mechanism (PTSC) for FD to reward honest participation based on peer consistency in an incentive compatible fashion. Due to heavy reductions of both computational complexity and storage, our framework is a fully on-blockchain FL system that is feasible on simple smart contracts and therefore blockchain agnostic. We experimentally test our new framework and validate its theoretical properties."
594,https://arxiv.org/abs/2106.06940,Multi-spectral programmable absorbers,"We designed and demonstrated a multi-spectral programmable perfect absorber that exploits two different phase-change materials. This programmability is possible by resonantly coupling two phase change materials, a Ge2Sb2Te5 layer to vanadium dioxide nanoparticles (VO2 NPs). The perfect absorption is attributed to the coalescence of gap plasmon modes excited between the NPs and waveguide cavity-like modes excited between the film and the NPs. The absorptance peak (>90%) can be tuned to four different infrared (IR) wavelengths from 1906 to 2960 nm by heating the structure to different temperatures. The perfect absorber is reconfigurable, lithography-free, large-scale, polarization-insensitive omnidirectional. Our strategy opens a new path for programmable infrared photonics."
595,https://arxiv.org/abs/2106.03601,Analyzing Open-Source Serverless Platforms: Characteristics and Performance,"Serverless computing is increasingly popular because of its lower cost and easier deployment. Several cloud service providers (CSPs) offer serverless computing on their public clouds, but it may bring the vendor lock-in risk. To avoid this limitation, many open-source serverless platforms come out to allow developers to freely deploy and manage functions on self-hosted clouds. However, building effective functions requires much expertise and thorough comprehension of platform frameworks and features that affect performance. It is a challenge for a service developer to differentiate and select the appropriate serverless platform for different demands and scenarios. Thus, we elaborate the frameworks and event processing models of four popular open-source serverless platforms and identify their salient idiosyncrasies. We analyze the root causes of performance differences between different service exporting and auto-scaling modes on those platforms. Further, we provide several insights for future work, such as auto-scaling and metric collection."
596,https://arxiv.org/abs/2105.13603,High Performance and Scalable NAT System on Commodity Platforms,"Quick network address translation (NAT) is proposed to improve the network performance of the NAT system on the commodity server by three ways. First, the quick NAT search algorithm is designed to use the Hash search instead of the sequential search to reduce latency when looking up the NAT rule table. Second, to leverage the power of the multi-core central processing unit (CPU) and the multi-queue network interface card, Quick NAT enables multiple CPU cores to process in parallel. The localized connection tracking table and the compare-and-swap based lock-free NAT Hash tables are designed to eliminate the lock overhead. Third, Quick NAT uses the polling and zero-copy delivery to reduce the cost of interrupt and packet copies. The evaluation results show that Quick NAT obtains high scalability and line-rate throughput on the commodity server."
597,https://arxiv.org/abs/2105.03864,Quick NAT: High performance NAT system on commodity platforms,"NAT gateway is an important network system in today's IPv4 network when translating a private IPv4 address to a public address. However, traditional NAT system based on Linux Netfilter cannot achieve high network throughput to meet modern requirements such as data centers. To address this challenge, we improve the network performance of NAT system by three ways. First, we leverage DPDK to enable polling and zero-copy delivery, so as to reduce the cost of interrupt and packet copies. Second, we enable multiple CPU cores to process in parallel and use lock-free hash table to minimize the contention between CPU cores. Third, we use hash search instead of sequential search when looking up the NAT rule table. Evaluation shows that our Quick NAT system significantly improves the performance of NAT on commodity platforms."
598,https://arxiv.org/abs/2103.02738,Exciton emissions in bilayer WSe2 tuned by the ferroelectric polymer,"In this work, a hybrid integration of few-layer transition metal dichalcogenides (TMDCs) and ferroelectric polymer is designed to achieve passive control of optical properties in-situ. The electrical polarization in ferroelectric P(VDF-TrFE) polymer can regulate the photoluminescence (PL) in few-layer TMDCs. The total PL intensity is substantially suppressed or enhanced under opposite polarization in bilayer WSe2. This is because electrons transfer between valley K and Λ in the conduction band induced by the built-in electric field in P(VDF-TrFE) polymer. This charge transfer further changes the competing dynamics between direct and indirect exciton recombination path and overall optical radiation efficiency. We also illustrate that the engineered PL originates from external electric field dependent transferred electron effect. The theoretical result matches the experimental data well. This work demonstrates a device platform in which passive regulation is achieved using 2D TMDCs modulated by polarized ferroelectric materials."
599,https://arxiv.org/abs/2102.11401,An Online Approach to Cyberattack Detection and Localization in Smart Grid,"Complex interconnections between information technology and digital control systems have significantly increased cybersecurity vulnerabilities in smart grids. Cyberattacks involving data integrity can be very disruptive because of their potential to compromise physical control by manipulating measurement data. This is especially true in large and complex electric networks that often rely on traditional intrusion detection systems focused on monitoring network traffic. In this paper, we develop an online detection algorithm to detect and localize covert attacks on smart grids. Using a network system model, we develop a theoretical framework by characterizing a covert attack on a generator bus in the network as sparse features in the state-estimation residuals. We leverage such sparsity via a regularized linear regression method to detect and localize covert attacks based on the regression coefficients. We conduct a comprehensive numerical study on both linear and nonlinear system models to validate our proposed method. The results show that our method outperforms conventional methods in both detection delay and localization accuracy."
600,https://arxiv.org/abs/2102.09111,Online Optimization and Learning in Uncertain Dynamical Environments with Performance Guarantees,"We propose a new framework to solve online optimization and learning problems in unknown and uncertain dynamical environments. This framework enables us to simultaneously learn the uncertain dynamical environment while making online decisions in a quantifiably robust manner. The main technical approach relies on the theory of distributional robust optimization that leverages adaptive probabilistic ambiguity sets. However, as defined, the ambiguity set usually leads to online intractable problems, and the first part of our work is directed to find reformulations in the form of online convex problems for two sub-classes of objective functions. To solve the resulting problems in the proposed framework, we further introduce an online version of the Nesterov accelerated-gradient algorithm. We determine how the proposed solution system achieves a probabilistic regret bound under certain conditions. Two applications illustrate the applicability of the proposed framework."
601,https://arxiv.org/abs/2101.07535,Electrocardiogram Classification and Visual Diagnosis of Atrial Fibrillation with DenseECG,"Atrial Fibrillation (AF) is a common cardiac arrhythmia affecting a large number of people around the world. If left undetected, it will develop into chronic disability or even early mortality. However, patients who have this problem can barely feel its presence, especially in its early stage. A non-invasive, automatic, and effective detection method is therefore needed to help early detection so that medical intervention can be implemented in time to prevent its progression.
  Electrocardiogram (ECG), which records the electrical activities of the heart, has been widely used for detecting the presence of AF. However, due to the subtle patterns of AF, the performance of detection models have largely depended on complicated data pre-processing and expertly engineered features. In our work, we developed DenseECG, an end-to-end model based on 5 layers 1D densely connected convolutional neural network. We trained our model using the publicly available dataset from 2017 PhysioNet Computing in Cardiology(CinC) Challenge containing 8528 single-lead ECG recordings of short-term heart rhythms (9-61s). Our trained model was able to outperform the other state-of-the-art AF detection models on this dataset without complicated data pre-processing and expert-supervised feature engineering."
602,https://arxiv.org/abs/2012.13154,Adversarial Momentum-Contrastive Pre-Training,"Recently proposed adversarial self-supervised learning methods usually require big batches and long training epochs to extract robust features, which will bring heavy computational overhead on platforms with limited resources. In order to help the network learn more powerful feature representations in smaller batches and fewer epochs, this paper proposes a novel adversarial momentum contrastive learning method, which introduces two memory banks corresponding to clean samples and adversarial samples, respectively. These memory banks can be dynamically incorporated into the training process to track invariant features among historical mini-batches. Compared with the previous adversarial pre-training model, our method achieves superior performance with smaller batch size and less training epochs. In addition, the model outperforms some state-of-the-art supervised defensive methods on multiple benchmark datasets after being fine-tuned on downstream classification tasks."
603,https://arxiv.org/abs/2011.07770,PC-GAIN: Pseudo-label Conditional Generative Adversarial Imputation Networks for Incomplete Data,"Datasets with missing values are very common in real world applications. GAIN, a recently proposed deep generative model for missing data imputation, has been proved to outperform many state-of-the-art methods. But GAIN only uses a reconstruction loss in the generator to minimize the imputation error of the non-missing part, ignoring the potential category information which can reflect the relationship between samples. In this paper, we propose a novel unsupervised missing data imputation method named PC-GAIN, which utilizes potential category information to further enhance the imputation power. Specifically, we first propose a pre-training procedure to learn potential category information contained in a subset of low-missing-rate data. Then an auxiliary classifier is determined using the synthetic pseudo-labels. Further, this classifier is incorporated into the generative adversarial framework to help the generator to yield higher quality imputation results. The proposed method can improve the imputation quality of GAIN significantly. Experimental results on various benchmark datasets show that our method is also superior to other baseline approaches. Our code is available at \url{https://github.com/WYu-Feng/pc-gain}."
604,https://arxiv.org/abs/2010.13218,The common origin of family and non-family asteroids,"All asteroids are currently classified as either family, originating from the disruption of known bodies, or non-family. An outstanding question is the origin of these non-family asteroids. Were they formed individually, or as members of known families but with chaotically evolving orbits, or are they members of old ghost families, that is, asteroids with a common parent body but with orbits that no longer cluster in orbital element space? Here, we show that the sizes of the non-family asteroids in the inner belt are correlated with their orbital eccentricities and anticorrelated with their inclinations, suggesting that both non-family and family asteroids originate from a small number of large primordial planetesimals. We estimate that ~85% of the asteroids in the inner main belt originate from the Flora, Vesta, Nysa, Polana and Eulalia families, with the remaining ~15% originating from either the same families or, more likely, a few ghost families. These new results imply that we must seek explanations for the differing characteristics of the various meteorite groups in the evolutionary histories of a few, large, precursor bodies. Our findings also support the model that asteroids formed big through the gravitational collapse of material in a protoplanetary disk."
605,https://arxiv.org/abs/2010.09086,Blockchain Based Decentralized Replay Attack Detection for Large Scale Power Systems,"Large scale power systems are comprised of regional utilities with assets that stream sensor readings in real time. In order to detect cyberattacks, the globally acquired, real time sensor data needs to be analyzed in a centralized fashion. However, owing to operational constraints, such a centralized sharing mechanism turns out to be a major obstacle. In this paper, we propose a blockchain based decentralized framework for detecting coordinated replay attacks with full privacy of sensor data. We develop a Bayesian inference mechanism employing locally reported attack probabilities that is tailor made for a blockchain framework. We compare our framework to a traditional decentralized algorithm based on the broadcast gossip framework both theoretically as well as empirically. With the help of experiments on a private Ethereum blockchain, we show that our approach achieves good detection quality and significantly outperforms gossip driven approaches in terms of accuracy, timeliness and scalability."
606,https://arxiv.org/abs/2010.08151,Multiscale studies of nanoconfined charging dynamics in supercapacitors bridged by machine learning,"The energy-delivery performance of supercapacitors is fundamentally determined by the dynamics of ions confined in nanoporous materials, which has attracted intensive interest in nanoscopic research. Many nanoscopic understandings, including changes in in-pore ion concentration and mobility during dynamical processes, are continuously reported. However, quantitative scale-up of these nanoscopic understandings for evaluation of the macroscopic performance of supercapacitors is difficult due to the absence of links between these scales. Here we demonstrate that machine learning can be used to establish such links. Starting from nanoscale, we first reveal a diffusion-enhanced migration of ions in nanopores using primarily modified Poisson-Nernst-Planck model, unlike in bulk electrolyte where diffusion counteracts migration. Using machine learning, we discover a dynamically varying ionic resistance and its equation, resulting from the in-pore ion concentration change contributed by diffusion-enhanced migration. The obtained equation is used to construct a nano-circuitry model (NCM), which describes both the macroscopic performance of supercapacitors and nanometre-resolved ionic behaviour. We demonstrate that NCM can provide additional perspectives to understand cyclic voltammograms. A Faradaic-like current peak can show in non-Faradaic processes, and an asymmetric charging/discharging can occur without ion desolvation. These is because the dynamically varying resistance delivers ions effectively for storage. The demonstrated use of machine learning could extend to other ionic systems including batteries and desalination, paving the route towards rational design."
607,https://arxiv.org/abs/2010.03844,Improve Adversarial Robustness via Weight Penalization on Classification Layer,"It is well-known that deep neural networks are vulnerable to adversarial attacks. Recent studies show that well-designed classification parts can lead to better robustness. However, there is still much space for improvement along this line. In this paper, we first prove that, from a geometric point of view, the robustness of a neural network is equivalent to some angular margin condition of the classifier weights. We then explain why ReLU type function is not a good choice for activation under this framework. These findings reveal the limitations of the existing approaches and lead us to develop a novel light-weight-penalized defensive method, which is simple and has a good scalability. Empirical results on multiple benchmark datasets demonstrate that our method can effectively improve the robustness of the network without requiring too much additional computation, while maintaining a high classification precision for clean data."
608,https://arxiv.org/abs/2009.12360,Deep Learning based Covert Attack Identification for Industrial Control Systems,"Cybersecurity of Industrial Control Systems (ICS) is drawing significant concerns as data communication increasingly leverages wireless networks. A lot of data-driven methods were developed for detecting cyberattacks, but few are focused on distinguishing them from equipment faults. In this paper, we develop a data-driven framework that can be used to detect, diagnose, and localize a type of cyberattack called covert attacks on smart grids. The framework has a hybrid design that combines an autoencoder, a recurrent neural network (RNN) with a Long-Short-Term-Memory (LSTM) layer, and a Deep Neural Network (DNN). This data-driven framework considers the temporal behavior of a generic physical system that extracts features from the time series of the sensor measurements that can be used for detecting covert attacks, distinguishing them from equipment faults, as well as localize the attack/fault. We evaluate the performance of the proposed method through a realistic simulation study on the IEEE 14-bus model as a typical example of ICS. We compare the performance of the proposed method with the traditional model-based method to show its applicability and efficacy."
609,https://arxiv.org/abs/2009.02390,Online Learning of Parameterized Uncertain Dynamical Environments with Finite-sample Guarantees,"We present a novel online learning algorithm for a class of unknown and uncertain dynamical environments that are fully observable. First, we obtain a novel probabilistic characterization of systems whose mean behavior is known but which are subject to additive, unknown subGaussian disturbances. This characterization relies on recent concentration of measure results and is given in terms of ambiguity sets. Second, we extend the results to environments whose mean behavior is also unknown but described by a parameterized class of possible mean behaviors. Our algorithm adapts the ambiguity set dynamically by learning the parametric dependence online, and retaining similar probabilistic guarantees with respect to the additive, unknown disturbance. We illustrate the results on a differential-drive robot subject to environmental uncertainty."
610,https://arxiv.org/abs/2007.03526,Grover search with smaller oracles,"Grover search is one of the most important quantum algorithms. In this paper, we consider a kind of search that the conditions of satisfaction $T$ can be rewritten as $T=T_1\bigcap T_2$. Then we present a new Grover search with smaller oracles. The time complexity of this algorithm $O(\fracπ{4}\sqrt{\frac{N}{bλ}}+\fracπ{4}\sqrt{\frac{b}τ})$, which is smaller than the time complexity of original Grover search, i.e. $O(\fracπ{4}\sqrt{\frac{N}{M}})$."
611,https://arxiv.org/abs/2006.15170,Dynamical evolution of the inner asteroid belt,"A determination of the dynamical evolution of the asteroid belt is difficult because the asteroid belt has evolved since the time of asteroid formation through mechanisms that include: (1) catastrophic collisions, (2) rotational disruption, (3) chaotic orbital evolution and (4) orbital evolution driven by Yarkovsky radiation forces. The timescales of these loss mechanisms are uncertain and there is a need for more observational constraints. In the inner main belt, the mean size of the non-family asteroids increases with increasing inclination. Here, we use that observation to show that all inner main belt asteroids originate from either the known families or from ghost families, that is, old families with dispersed orbital elements. We estimate that the average age of the asteroids in the ghost families is a factor of 1/3 less than the Yarkovsky orbital evolution timescale. However, this orbital evolution timescale is a long-term average that must allow for the collisional evolution of the asteroids and for stochastic changes in their spin directions. By applying these constraints on the orbital evolution timescales to the evolution of the size-frequency distribution of the Vesta asteroid family, we estimate that the age of this family is greater than 1.3 $Gyr$ and could be comparable with the age of the solar system. By estimating the number of ghost families, we calculate that the number of asteroids that are the root sources of the meteorites and the near-Earth asteroids that originate from the inner main belt is about 20."
612,https://arxiv.org/abs/2006.14669,A simplified primal-dual weak Galerkin finite element method for Fokker-Planck type equations,"A simplified primal-dual weak Galerkin (S-PDWG) finite element method is designed for the Fokker-Planck type equation with non-smooth diffusion tensor and drift vector. The discrete system resulting from S-PDWG method has significantly fewer degrees of freedom compared with the one resulting from the PDWG method proposed by Wang-Wang \cite{WW-fp-2018}. Furthermore, the condition number of the S-PDWG method is smaller than the PDWG method \cite{WW-fp-2018} due to the introduction of a new stabilizer, which provides a potential for designing fast algorithms. Optimal order error estimates for the S-PDWG approximation are established in the $L^2$ norm. A series of numerical results are demonstrated to validate the effectiveness of the S-PDWG method."
613,https://arxiv.org/abs/2006.08992,Three-state quantum walk on the Cayley Graph of the Dihedral Group,"The finite dihedral group generated by one rotation and one reflection is the simplest case of the non-abelian group. Cayley graphs are diagrammatic counterparts of groups. In this paper, much attention is given to the Cayley graph of the dihedral group. Considering the characteristics of the elements in the dihedral group, we propose a model of three-state discrete-time quantum walk (DTQW) on the Caylay graph of the dihedral group with Grover coin. We derive analytic expressions for the the position probability distribution and the long-time limit of the return probability starting from the origin. It is shown that the localization effect is governed by the size of the underlying dihedral group, coin operator and initial state. We also numerically investigate the properties of the proposed model via the probability distribution and the time-averaged probability at the designated position. The abundant phenomena of three-state Grover DTQW on the Caylay graph of the dihedral group can help the community to better understand and to develop new quantum algorithms."
614,https://arxiv.org/abs/2005.11723,Query Resolution for Conversational Search with Limited Supervision,"In this work we focus on multi-turn passage retrieval as a crucial component of conversational search. One of the key challenges in multi-turn passage retrieval comes from the fact that the current turn query is often underspecified due to zero anaphora, topic change, or topic return. Context from the conversational history can be used to arrive at a better expression of the current turn query, defined as the task of query resolution. In this paper, we model the query resolution task as a binary term classification problem: for each term appearing in the previous turns of the conversation decide whether to add it to the current turn query or not. We propose QuReTeC (Query Resolution by Term Classification), a neural query resolution model based on bidirectional transformers. We propose a distant supervision method to automatically generate training data by using query-passage relevance labels. Such labels are often readily available in a collection either as human annotations or inferred from user interactions. We show that QuReTeC outperforms state-of-the-art models, and furthermore, that our distant supervision method can be used to substantially reduce the amount of human-curated data required to train QuReTeC. We incorporate QuReTeC in a multi-turn, multi-stage passage retrieval architecture and demonstrate its effectiveness on the TREC CAsT dataset."
615,https://arxiv.org/abs/2004.09820,Improving Positive Unlabeled Learning: Practical AUL Estimation and New Training Method for Extremely Imbalanced Data Sets,"Positive Unlabeled (PU) learning is widely used in many applications, where a binary classifier is trained on the datasets consisting of only positive and unlabeled samples. In this paper, we improve PU learning over state-of-the-art from two aspects. Firstly, existing model evaluation methods for PU learning requires ground truth of unlabeled samples, which is unlikely to be obtained in practice. In order to release this restriction, we propose an asymptotic unbiased practical AUL (area under the lift) estimation method, which makes use of raw PU data without prior knowledge of unlabeled samples.
  Secondly, we propose ProbTagging, a new training method for extremely imbalanced data sets, where the number of unlabeled samples is hundreds or thousands of times that of positive samples. ProbTagging introduces probability into the aggregation method. Specifically, each unlabeled sample is tagged positive or negative with the probability calculated based on the similarity to its positive neighbors. Based on this, multiple data sets are generated to train different models, which are then combined into an ensemble model. Compared to state-of-the-art work, the experimental results show that ProbTagging can increase the AUC by up to 10%, based on three industrial and two artificial PU data sets."
616,https://arxiv.org/abs/2003.07880,High-Confidence Attack Detection via Wasserstein-Metric Computations,"This paper considers a sensor attack and fault detection problem for linear cyber-physical systems, which are subject to system noise that can obey an unknown light-tailed distribution. We propose a new threshold-based detection mechanism that employs the Wasserstein metric, and which guarantees system performance with high confidence employing a finite number of measurements. The proposed detector may generate false alarms with a rate $Δ$ in normal operation, where $Δ$ can be tuned to be arbitrarily small by means of a benchmark distribution which is part of our mechanism. Thus, the proposed detector is sensitive to sensor attacks and faults which have a statistical behavior that is different from that of the system's noise. We quantify the impact of stealthy attacks---which aim to perturb the system operation while producing false alarms that are consistent with the natural system's noise---via a probabilistic reachable set. To enable tractable implementation of our methods, we propose a linear optimization problem that computes the proposed detection measure and a semidefinite program that produces the proposed reachable set."
617,https://arxiv.org/abs/2002.03419,The Alzheimer's Disease Prediction Of Longitudinal Evolution (TADPOLE) Challenge: Results after 1 Year Follow-up,"We present the findings of ""The Alzheimer's Disease Prediction Of Longitudinal Evolution"" (TADPOLE) Challenge, which compared the performance of 92 algorithms from 33 international teams at predicting the future trajectory of 219 individuals at risk of Alzheimer's disease. Challenge participants were required to make a prediction, for each month of a 5-year future time period, of three key outcomes: clinical diagnosis, Alzheimer's Disease Assessment Scale Cognitive Subdomain (ADAS-Cog13), and total volume of the ventricles. The methods used by challenge participants included multivariate linear regression, machine learning methods such as support vector machines and deep neural networks, as well as disease progression models. No single submission was best at predicting all three outcomes. For clinical diagnosis and ventricle volume prediction, the best algorithms strongly outperform simple baselines in predictive ability. However, for ADAS-Cog13 no single submitted prediction method was significantly better than random guesswork. Two ensemble methods based on taking the mean and median over all predictions, obtained top scores on almost all tasks. Better than average performance at diagnosis prediction was generally associated with the additional inclusion of features from cerebrospinal fluid (CSF) samples and diffusion tensor imaging (DTI). On the other hand, better performance at ventricle volume prediction was associated with inclusion of summary statistics, such as the slope or maxima/minima of biomarkers. TADPOLE's unique results suggest that current prediction algorithms provide sufficient accuracy to exploit biomarkers related to clinical diagnosis and ventricle volume, for cohort refinement in clinical trials for Alzheimer's disease. However, results call into question the usage of cognitive test scores for patient selection and as a primary endpoint in clinical trials."
618,https://arxiv.org/abs/1912.09688,An Upbound of Hausdorff's Dimension of the Divergence Set of the fractional Schrödinger Operator on $H^s(\mathbb R^n),"This paper shows $$ \sup_{f\in H^s(\mathbb{R}^n)}\dim _H\left\{x\in\mathbb{R}^n:\ \lim_{t\rightarrow0}e^{it(-Δ)^α}f(x)\neq f(x)\right\}\leq n+1-\frac{2(n+1)s}{n}\ \ \text{under}\ \ \begin{cases} n\geq2;\\ α>\frac12;
  \frac{n}{2(n+1)}<s\leq\frac{n}{2} . \end{cases} $$"
619,https://arxiv.org/abs/1912.09636,A Carleson problem for the Boussinesq operator,"In this paper, Theorems 1.1- 1.2 show that the Boussinesq operator $\mathcal{B}_tf$ converges pointwise to its initial data $f\in H^s(\mathbb{R})$ as $t\to 0$ provided $s\geq\frac{1}{4}$ -- more precisely -- on the one hand, by constructing a counterexample in $\mathbb{R}$ we discover that the optimal convergence index $s_{c,1}=\frac14$; on the other hand, we find that the Hausdorff dimension of the disconvergence set for $\mathcal{B}_tf$ is \begin{align*} α_{1,\mathcal{B}}(s)&=\begin{cases} 1-2s&\ \ \text{as}\ \ \frac{1}{4}\leq s\leq\frac{1}{2};\\ 1 &\ \ \text{as}\ \ 0<s<\frac{1}{4}. \end{cases} \end{align*} Moreover, Theorem 1.3 presents a higher dimensional lift of Theorems 1.1- 1.2 under $f$ being radial."
620,https://arxiv.org/abs/1911.12948,"Nanoconfined, dynamic electrolyte gating and memory effects in multilayered graphene-based membranes","Multilayered graphene-based nanoporous membranes with electrolyte incorporated between individual sheets is a unique nano-heterostructure system in which nanoconfined electrons in graphene and ions confined in between sheets are intimately coupled throughout the entire membrane. In contrast to the general notion that the electrolyte gating is unlikely to appear in multilayered graphene stacks, it is demonstrated in this work that the electrolyte gating effect in monolayer graphene can be transferred to its corresponding multilayered porous membranes. This gating effect presented on each individual graphene sheets through electrolyte confined in nanopores provides a real-time, electrical approach for probing the complex dynamics of nanoconfined electrical double layer. This has enabled the observation of the ionic memory effect in supercapacitors and produces new insights into the charging dynamics of supercapacitors. Such discoveries may stimulate the design of novel nanoionic devices."
621,https://arxiv.org/abs/1911.11365,ATCSpeech: a multilingual pilot-controller speech corpus from real Air Traffic Control environment,"Automatic Speech Recognition (ASR) is greatly developed in recent years, which expedites many applications on other fields. For the ASR research, speech corpus is always an essential foundation, especially for the vertical industry, such as Air Traffic Control (ATC). There are some speech corpora for common applications, public or paid. However, for the ATC, it is difficult to collect raw speeches from real systems due to safety issues. More importantly, for a supervised learning task like ASR, annotating the transcription is a more laborious work, which hugely restricts the prospect of ASR application. In this paper, a multilingual speech corpus (ATCSpeech) from real ATC systems, including accented Mandarin Chinese and English, is built and released to encourage the non-commercial ASR research in ATC domain. The corpus is detailly introduced from the perspective of data amount, speaker gender and role, speech quality and other attributions. In addition, the performance of our baseline ASR models is also reported. A community edition for our speech database can be applied and used under a special contrast. To our best knowledge, this is the first work that aims at building a real and multilingual ASR corpus for the air traffic related research."
622,https://arxiv.org/abs/1911.10184,Data-driven Predictive Control for a Class of Uncertain Control-Affine Systems,"This paper studies a data-driven predictive control for a class of control-affine systems which is subject to uncertainty. With the accessibility to finite sample measurements of the uncertain variables, we aim to find controls which are feasible and provide superior performance guarantees with high probability. This results into the formulation of a stochastic optimization problem (P), which is intractable due to the unknown distribution of the uncertainty variables. By developing a distributionally robust optimization framework, we present an equivalent and yet tractable reformulation of (P). Further, we propose an efficient algorithm that provides online suboptimal data-driven solutions and guarantees performance with high probability. To illustrate the effectiveness of the proposed approach, we consider a highway speed-limit control problem. We then develop a set of data-driven speed controls that allow us to prevent traffic congestion with high probability. Finally, we employ the resulting control method on a traffic simulator to illustrate the effectiveness of this approach numerically."
623,https://arxiv.org/abs/1911.07449,Understanding Open Source Serverless Platforms: Design Considerations and Performance,"Serverless computing is increasingly popular because of the promise of lower cost and the convenience it provides to users who do not need to focus on server management. This has resulted in the availability of a number of proprietary and open-source serverless solutions. We seek to understand how the performance of serverless computing depends on a number of design issues using several popular open-source serverless platforms. We identify the idiosyncrasies affecting performance (throughput and latency) for different open-source serverless platforms. Further, we observe that just having either resource-based (CPU and memory) or workload-based (request per second (RPS) or concurrent requests) auto-scaling is inadequate to address the needs of the serverless platforms."
624,https://arxiv.org/abs/1910.14073,A Primal-dual weak Galerkin finite element method for linear convection equations in non-divergence form,"A new primal-dual weak Galerkin (PD-WG) finite element method was developed and analyzed in this article for first-order linear convection equations in non-divergence form. The PD-WG method results in a symmetric discrete system involving not only the original equation for the primal variable, but also the dual/adjoint equation for the dual variable (also known as Lagrangian multiplier). Optimal order of error estimates in various discrete Sobolev norms are derived for the numerical solutions arising from the PD-WG scheme. Numerical results are produced and reported to illustrate the accuracy and effectiveness of the new PD-WG method."
625,https://arxiv.org/abs/1910.06895,CRISLoc: Reconstructable CSI Fingerprintingfor Indoor Smartphone Localization,"Channel state information (CSI) based fingerprinting for WIFI indoor localization has attracted lots of attention very recently.The frequency diverse and temporally stable CSI better represents the location dependent channel characteristics than the coarsereceived signal strength (RSS). However, the acquisition of CSI requires the cooperation of access points (APs) and involves only dataframes, which imposes restrictions on real-world deployment. In this paper, we present CRISLoc, the first CSI fingerprinting basedlocalization prototype system using ubiquitous smartphones. CRISLoc operates in a completely passive mode, overhearing thepackets on-the-fly for his own CSI acquisition. The smartphone CSI is sanitized via calibrating the distortion enforced by WiFi amplifiercircuits. CRISLoc tackles the challenge of altered APs with a joint clustering and outlier detection method to find them. A novel transferlearning approach is proposed to reconstruct the high-dimensional CSI fingerprint database on the basis of the outdated fingerprintsand a few fresh measurements, and an enhanced KNN approach is proposed to pinpoint the location of a smartphone. Our studyreveals important properties about the stability and sensitivity of smartphone CSI that has not been reported previously. Experimentalresults show that CRISLoc can achieve a mean error of around 0.29m in a6m times 8mresearch laboratory. The mean error increases by 5.4 cm and 8.6 cm upon the movement of one and two APs, which validates the robustness of CRISLoc against environment changes."
626,https://arxiv.org/abs/1909.10438,Improving the Thermal Stability of a CCD Through Clocking,"Modern precise radial velocity spectrometers are designed to infer the existence of planets orbiting other stars by measuring few-nm shifts in the positions of stellar spectral lines recorded at high spectral resolution on a large-area digital detector. While the spectrometer may be highly stabilized in terms of temperature, the detector itself may undergo changes in temperature during readout that are an order of magnitude or more larger than the other opto-mechanical components within the instrument. These variations in detector temperature can translate directly into systematic measurement errors. We explore a technique for reducing the amplitude of CCD temperature variations by shuffling charge within a pixel in the parallel direction during integration. We find that this ""dither clocking"" mode greatly reduces temperature variations in the CCDs being tested for the NEID spectrometer. We investigate several potential negative effects this clocking scheme could have on the underlying spectral data."
627,https://arxiv.org/abs/1908.06553,LabelECG: A Web-based Tool for Distributed Electrocardiogram Annotation,"Electrocardiography plays an essential role in diagnosing and screening cardiovascular diseases in daily healthcare. Deep neural networks have shown the potentials to improve the accuracies of arrhythmia detection based on electrocardiograms (ECGs). However, more ECG records with ground truth are needed to promote the development and progression of deep learning techniques in automatic ECG analysis. Here we propose a web-based tool for ECG viewing and annotating, LabelECG. With the facilitation of unified data management, LabelECG is able to distribute large cohorts of ECGs to dozens of technicians and physicians, who can simultaneously make annotations through web-browsers on PCs, tablets and cell phones. Along with the doctors from four hospitals in China, we applied LabelECG to support the annotations of about 15,000 12-lead resting ECG records in three months. These annotated ECGs have successfully supported the First China ECG intelligent Competition. La-belECG will be freely accessible on the Internet to support similar researches, and will also be upgraded through future works."
628,https://arxiv.org/abs/1906.05145,Convergence of a Class of Schrödinger Equations,"In this paper, we set up the selection conditions for time series $\{t_k\}_{k=1}^\infty$ which converge to 0 as $k\rightarrow\infty$ such that the solutions of a class of generalized Schrödinger equations almost everywhere pointwise converge to their initial data in $H^s(\mathbb{R}^n)$ for $s>0$. As it is known that the pointwise convergence can not be true for Schrödinger equation when $s<\frac{n}{2(n+1)}$ as $t\rightarrow0$."
629,https://arxiv.org/abs/1906.03828,Efficient Bayesian estimation for GARCH-type models via Sequential Monte Carlo,"The advantages of sequential Monte Carlo (SMC) are exploited to develop parameter estimation and model selection methods for GARCH (Generalized AutoRegressive Conditional Heteroskedasticity) style models. It provides an alternative method for quantifying estimation uncertainty relative to classical inference. Even with long time series, it is demonstrated that the posterior distribution of model parameters are non-normal, highlighting the need for a Bayesian approach and an efficient posterior sampling method. Efficient approaches for both constructing the sequence of distributions in SMC, and leave-one-out cross-validation, for long time series data are also proposed. Finally, an unbiased estimator of the likelihood is developed for the Bad Environment-Good Environment model, a complex GARCH-type model, which permits exact Bayesian inference not previously available in the literature."
630,https://arxiv.org/abs/1905.11105,The equations of motion for consistency of a post-Newtonian Lagrangian formulation,"Equations of motion for a general relativistic post-Newtonian Lagrangian approach mainly refer to acceleration equations, i.e. differential equations of velocities. They are directly from the Euler-Lagrangian equations, and usually have higher-order terms truncated when they remain at the same post-Newtonian order of the Lagrangian. In this sense, they are incoherent equations of the Lagrangian and approximately conserve constants of motion in this system. In this paper, we show that the Euler-Lagrangian equations can also yield the equations of motion for consistency of the Lagrangian in the general case. The coherent equations are the \emph{differential} equations of generalized momenta rather than those of the velocities, and have no terms truncated. The velocities are not integration variables, but they can be solved from the \emph{algebraic} equations of the generalized momenta with an iterative method. Taking weak relativistic fields in the Solar System and strong relativistic fields of compact objects as examples, we numerically evaluate the accuracies of the constants of motion in the two sets of equations of motion. It is confirmed that these accuracies well satisfy the theoretical need if the chosen integrator can provide a high enough precision. The differences in the dynamical behavior of order and chaos between the two sets of equations are also compared. Unlike the incoherent post-Newtonian Lagrangian equations of motion, the coherent ones can theoretically, strictly conserve all integrals in some post-Newtonian Lagrangian problems, and therefore are worth recommending."
631,https://arxiv.org/abs/1905.00625,Szegedy Quantum Walks with Memory on Regular Graphs,"Quantum walks with memory(QWM) are a type of modified quantum walks that record the walker's latest path. The general model of coined QWM is presented in Phys. Rev. A 93, 042323 (2016). In this paper, we present general model of Szegedy QWM. Importantly, the relation of coined QWM and Szegedy QWM is revealed. By transforming coined QWM to Szegedy QWM, some amazing results about QWM are founded."
632,https://arxiv.org/abs/1903.05478,Polarization in Disks,"Polarized dust emission outside of disks reveal the magnetic field morphology of molecular clouds. Within disks, however, polarized dust emission can arise from very different mechanisms (e.g., self-scattering), and each of them are useful for constraining physical properties in the disk. For example, these mechanisms allow us to constrain the disk grain size distributions and grain/disk geometries, independent from current methods of measuring these parameters. To accurately model these features and disentangle the various polarization mechanisms, multiwavelength observations at very high resolution and sensitivity are required. With significant upgrades to current interferometric facilities, we can understand how grains evolve in disks during the planet formation process."
633,https://arxiv.org/abs/1902.07729,Ultra-Stable Environment Control for the NEID Spectrometer: Design and Performance Demonstration,"Two key areas of emphasis in contemporary experimental exoplanet science are the detailed characterization of transiting terrestrial planets, and the search for Earth analog planets to be targeted by future imaging missions. Both of these pursuits are dependent on an order-of-magnitude improvement in the measurement of stellar radial velocities (RV), setting a requirement on single-measurement instrumental uncertainty of order 10 cm/s. Achieving such extraordinary precision on a high-resolution spectrometer requires thermo-mechanically stabilizing the instrument to unprecedented levels. Here, we describe the Environment Control System (ECS) of the NEID Spectrometer, which will be commissioned on the 3.5 m WIYN Telescope at Kitt Peak National Observatory in 2019, and has a performance specification of on-sky RV precision < 50 cm/s. Because NEID's optical table and mounts are made from aluminum, which has a high coefficient of thermal expansion, sub-milliKelvin temperature control is especially critical. NEID inherits its ECS from that of the Habitable-zone Planet Finder (HPF), but with modifications for improved performance and operation near room temperature. Our full-system stability test shows the NEID system exceeds the already impressive performance of HPF, maintaining vacuum pressures below $10^{-6}$ Torr and an RMS temperature stability better than 0.4 mK over 30 days. Our ECS design is fully open-source; the design of our temperature-controlled vacuum chamber has already been made public, and here we release the electrical schematics for our custom Temperature Monitoring and Control (TMC) system."
634,https://arxiv.org/abs/1902.06366,Detecting and Diagnosing Incipient Building Faults Using Uncertainty Information from Deep Neural Networks,"Early detection of incipient faults is of vital importance to reducing maintenance costs, saving energy, and enhancing occupant comfort in buildings. Popular supervised learning models such as deep neural networks are considered promising due to their ability to directly learn from labeled fault data; however, it is known that the performance of supervised learning approaches highly relies on the availability and quality of labeled training data. In Fault Detection and Diagnosis (FDD) applications, the lack of labeled incipient fault data has posed a major challenge to applying these supervised learning techniques to commercial buildings. To overcome this challenge, this paper proposes using Monte Carlo dropout (MC-dropout) to enhance the supervised learning pipeline, so that the resulting neural network is able to detect and diagnose unseen incipient fault examples. We also examine the proposed MC-dropout method on the RP-1043 dataset to demonstrate its effectiveness in indicating the most likely incipient fault types."
635,https://arxiv.org/abs/1902.06361,A One-Class Support Vector Machine Calibration Method for Time Series Change Point Detection,"It is important to identify the change point of a system's health status, which usually signifies an incipient fault under development. The One-Class Support Vector Machine (OC-SVM) is a popular machine learning model for anomaly detection and hence could be used for identifying change points; however, it is sometimes difficult to obtain a good OC-SVM model that can be used on sensor measurement time series to identify the change points in system health status. In this paper, we propose a novel approach for calibrating OC-SVM models. The approach uses a heuristic search method to find a good set of input data and hyperparameters that yield a well-performing model. Our results on the C-MAPSS dataset demonstrate that OC-SVM can also achieve satisfactory accuracy in detecting change point in time series with fewer training data, compared to state-of-the-art deep learning approaches. In our case study, the OC-SVM calibrated by the proposed model is shown to be useful especially in scenarios with limited amount of training data."
636,https://arxiv.org/abs/1902.02026,The relative efficiency of time-to-progression and continuous measures of cognition in pre-symptomatic Alzheimer's,"Pre-symptomatic (or Preclinical) Alzheimer's Disease is defined by biomarker evidence of fibrillar amyloid beta pathology in the absence of clinical symptoms. Clinical trials in this early phase of disease are challenging due to the slow rate of disease progression as measured by periodic cognitive performance tests or by transition to a diagnosis of Mild Cognitive Impairment. In a multisite study, experts provide diagnoses by central chart review without the benefit of in-person assessment. We use a simulation study to demonstrate that models of repeated cognitive assessments detect treatment effects more efficiently compared to models of time-to-progression to an endpoint such as change in diagnosis. Multivariate continuous data are simulated from a Bayesian joint mixed effects model fit to data from the Alzheimer's Disease Neuroimaging Initiative. Simulated progression events are algorithmically derived from the continuous assessments using a random forest model fit to the same data. We find that power is approximately doubled with models of repeated continuous outcomes compared to the time-to-progression analysis. The simulations also demonstrate that a plausible informative missing data pattern can induce a bias which inflates treatment effects, yet 5% Type I error is maintained."
637,https://arxiv.org/abs/1901.07377,Data assimilation and online optimization with performance guarantees,"This paper considers a class of real-time stochastic optimization problems dependent on an unknown probability distribution. In the considered scenario, data is streaming frequently while trying to reach a decision. Thus, we aim to devise a procedure that incorporates samples (data) of the distribution sequentially and adjusts decisions accordingly. We approach this problem in a distributionally robust optimization framework and propose a novel Online Data Assimilation Algorithm (ONDA Algorithm) for this purpose. This algorithm guarantees out-of-sample performance of decisions with high probability, and gradually improves the quality of the decisions by incorporating the streaming data. We show that the ONDA Algorithm converges under a sufficiently slow data streaming rate, and provide a criteria for its termination after certain number of data have been collected. Simulations illustrate the results."
638,https://arxiv.org/abs/1901.04997,MAD-GAN: Multivariate Anomaly Detection for Time Series Data with Generative Adversarial Networks,"The prevalence of networked sensors and actuators in many real-world systems such as smart buildings, factories, power plants, and data centers generate substantial amounts of multivariate time series data for these systems. The rich sensor data can be continuously monitored for intrusion events through anomaly detection. However, conventional threshold-based anomaly detection methods are inadequate due to the dynamic complexities of these systems, while supervised machine learning methods are unable to exploit the large amounts of data due to the lack of labeled data. On the other hand, current unsupervised machine learning approaches have not fully exploited the spatial-temporal correlation and other dependencies amongst the multiple variables (sensors/actuators) in the system for detecting anomalies. In this work, we propose an unsupervised multivariate anomaly detection method based on Generative Adversarial Networks (GANs). Instead of treating each data stream independently, our proposed MAD-GAN framework considers the entire variable set concurrently to capture the latent interactions amongst the variables. We also fully exploit both the generator and discriminator produced by the GAN, using a novel anomaly score called DR-score to detect anomalies by discrimination and reconstruction. We have tested our proposed MAD-GAN using two recent datasets collected from real-world CPS: the Secure Water Treatment (SWaT) and the Water Distribution (WADI) datasets. Our experimental results showed that the proposed MAD-GAN is effective in reporting anomalies caused by various cyber-intrusions compared in these complex real-world systems."
639,https://arxiv.org/abs/1812.09782,Quantum algorithm and quantum circuit for A-Optimal Projection: dimensionality reduction,"Learning low dimensional representation is a crucial issue for many machine learning tasks such as pattern recognition and image retrieval. In this article, we present a quantum algorithm and a quantum circuit to efficiently perform A-Optimal Projection for dimensionality reduction. Compared with the best-know classical algorithms, the quantum A-Optimal Projection (QAOP) algorithm shows an exponential speedup in both the original feature space dimension $n$ and the reduced feature space dimension $k$. We show that the space and time complexity of the QAOP circuit are $O\left[ {{{\log }_2}\left( {nk} /ε \right)} \right]$ and $O[ {\log_2(nk)} {poly}\left({{\log }_2}ε^{-1} \right)]$ respectively, with fidelity at least $1-ε$. Firstly, a reformation of the original QAOP algorithm is proposed to help omit the quantum-classical interactions during the QAOP algorithm. Then the quantum algorithm and quantum circuit with performance guarantees are proposed. Specifically, the quantum circuit modules for preparing the initial quantum state and implementing the controlled rotation can be also used for other quantum machine learning algorithms."
640,https://arxiv.org/abs/1810.12726,Index theory and noncommutative geometry of topological insulator,"In this chapter, we report the recent progress in the understanding of the rich mathematical structures of topological insulators in the framework of index theory and noncommutative geometry."
641,https://arxiv.org/abs/1810.11385,Data-driven Variable Speed Limit Design for Highways via Distributionally Robust Optimization,"This paper introduces an optimization problem (P) and a solution strategy to design variable-speed-limit controls for a highway that is subject to traffic congestion and uncertain vehicle arrival and departure. By employing a finite data-set of samples of the uncertain variables, we aim to find a data-driven solution that has a guaranteed out-of-sample performance. In principle, such formulation leads to an intractable problem (P) as the distribution of the uncertainty variable is unknown. By adopting a distributionally robust optimization approach, this work presents a tractable reformulation of (P) and an efficient algorithm that provides a suboptimal solution that retains the out-of-sample performance guarantee. A simulation illustrates the effectiveness of this method."
642,https://arxiv.org/abs/1810.08084,Superconvergence of Numerical Gradient for Weak Galerkin Finite Element Methods on Nonuniform Cartesian Partitions in Three Dimensions,"A superconvergence error estimate for the gradient approximation of the second order elliptic problem in three dimensions is analyzed by using weak Galerkin finite element scheme on the uniform and non-uniform cubic partitions. Due to the loss of the symmetric property from two dimensions to three dimensions, this superconvergence result in three dimensions is not a trivial extension of the recent superconvergence result in two dimensions \cite{sup_LWW2018} from rectangular partitions to cubic partitions. The error estimate for the numerical gradient in the $L^{2}$-norm arrives at a superconvergence order of ${\cal O}(h^r) (1.5 \leq r\leq 2)$ when the lowest order weak Galerkin finite elements consisting of piecewise linear polynomials in the interior of the elements and piecewise constants on the faces of the elements are employed. A series of numerical experiments are illustrated to confirm the established superconvergence theory in three dimensions."
643,https://arxiv.org/abs/1810.05414,Technology Assisted Reviews: Finding the Last Few Relevant Documents by Asking Yes/No Questions to Reviewers,"The goal of a technology-assisted review is to achieve high recall with low human effort. Continuous active learning algorithms have demonstrated good performance in locating the majority of relevant documents in a collection, however their performance is reaching a plateau when 80\%-90\% of them has been found. Finding the last few relevant documents typically requires exhaustively reviewing the collection. In this paper, we propose a novel method to identify these last few, but significant, documents efficiently. Our method makes the hypothesis that entities carry vital information in documents, and that reviewers can answer questions about the presence or absence of an entity in the missing relevance documents. Based on this we devise a sequential Bayesian search method that selects the optimal sequence of questions to ask. The experimental results show that our proposed method can greatly improve performance requiring less reviewing effort."
644,https://arxiv.org/abs/1810.04693,"Gemini, SOFIA, and ATCA Reveal Very Young, Massive Protostars in the Collapsing Molecular Cloud BYF 73","We present multi-wavelength data on the globally infalling molecular cloud/protostellar cluster BYF 73. These include new far-IR spectral line and continuum data from SOFIA's Far Infrared Field-Imaging Line Spectrometer (FIFI-LS), mid-infrared (MIR) observations with the Thermal-Region Camera Spectrograph (T-ReCS) on Gemini-South, and 3 mm continuum data from the Australia Telescope Compact Array (ATCA), plus archival data from Spitzer/IRAC, and Herschel/PACS and SPIRE. The FIFI-LS spectroscopy in [OI]$\lambda63 μ$m, [OIII]$\lambda88 μ$m, [OI]$\lambda145 μ$m, and [CII]$\lambda158 μ$m highlights different gas environments in and between the dense molecular cloud and HII region. The photo-dissociation region (PDR) between the cloud and HII region is best traced by [OI]$\lambda145 μ$m and may have density $>$10$^{10}$ m$^{-3}$, but the observed $\lambda145μ$m/$\lambda63μ$m and $\lambda63μ$m/$\lambda158μ$m line ratios in the densest gas are well outside model values. The HII region is well-traced by [CII], with the $\lambda158μ$m/$\lambda145μ$m line ratio indicating a density of 10$^{8.5}$ m$^{-3}$ and a relatively weak ionizing radiation field, 1.5 $\lesssim$ log$(G/G_0)\lesssim$ 2. The T-ReCS data reveal eight protostellar objects in the cloud, of which six appear deeply embedded ($A_V$ $>$ 30$^m$ or more) near the cloud's center. MIR 2 has the most massive core at $\sim$240 M$_{\odot}$, more massive than all the others combined by up to tenfold, with no obvious gas outflow, negligible cooling line emission, and $\sim3-8$% of its 4.7$\times$10$^3$ L$_{\odot}$ luminosity originating from the release of gravitational potential energy. MIR 2's dynamical age may be as little as 7000 yr. This fact, and the cloud's total embedded stellar mass being far less than its gas mass, confirm BYF 73's relatively early stage of evolution."
645,https://arxiv.org/abs/1810.00158,Discrete-time Quantum Walk on the Cayley Graph of the Dihedral Group,"The finite dihedral group generated by one rotation and one flip is the simplest case of the non-abelian group. Cayley graphs are diagrammatic counterparts of groups. In this paper, much attention is given to the Cayley graph of the dihedral group. Considering the characteristics of the elements in the dihedral group, we conduct the model of discrete-time quantum walk on the Cayley graph of the dihedral group by special coding mode. This construction makes Fourier transformation can be used to carry out spectral analysis of the dihedral quantum walk, i.e. the non-abelian case. Furthermore, the relation between quantum walk without memory on the Cayley graph of the dihedral group and quantum walk with memory on a cycle is discussed, so that we can explore the potential of quantum walks without and with memory. Here, the numerical simulation is carried out to verify the theoretical analysis results and other properties of the proposed model are further studied."
646,https://arxiv.org/abs/1809.04758,Anomaly Detection with Generative Adversarial Networks for Multivariate Time Series,"Today's Cyber-Physical Systems (CPSs) are large, complex, and affixed with networked sensors and actuators that are targets for cyber-attacks. Conventional detection techniques are unable to deal with the increasingly dynamic and complex nature of the CPSs. On the other hand, the networked sensors and actuators generate large amounts of data streams that can be continuously monitored for intrusion events. Unsupervised machine learning techniques can be used to model the system behaviour and classify deviant behaviours as possible attacks. In this work, we proposed a novel Generative Adversarial Networks-based Anomaly Detection (GAN-AD) method for such complex networked CPSs. We used LSTM-RNN in our GAN to capture the distribution of the multivariate time series of the sensors and actuators under normal working conditions of a CPS. Instead of treating each sensor's and actuator's time series independently, we model the time series of multiple sensors and actuators in the CPS concurrently to take into account of potential latent interactions between them. To exploit both the generator and the discriminator of our GAN, we deployed the GAN-trained discriminator together with the residuals between generator-reconstructed data and the actual samples to detect possible anomalies in the complex CPS. We used our GAN-AD to distinguish abnormal attacked situations from normal working conditions for a complex six-stage Secure Water Treatment (SWaT) system. Experimental results showed that the proposed strategy is effective in identifying anomalies caused by various attacks with high detection rate and low false positive rate as compared to existing methods."
647,https://arxiv.org/abs/1807.03773,EAST Real-Time VOD System Based on MDSplus,"As with EAST (Experimental Advanced Superconducting Tokamak) experimental data analyzed by more and more collaborators, the experimental videos which directly reflect the real status of vacuum attract more and more researchers' attention. The real time VOD (Video On Demand) system based on MDSplus allows users reading the video frames in real time as same as the signal data which is also stored in the MDSplus database. User can display the plasma discharge videos and analyze videos frame by frame through jScope or our VOD web station. The system mainly includes the frames storing and frames displaying. The frames storing application accepts shot information by using socket TCP communication firstly, then reads video frames through disk mapping, finally stores them into MDSplus. The displaying process is implemented through B/S (Browser/Server) framework, it uses PHP and JavaScript to realize VOD function and read frames information from MDSplus. The system offers a unit way to access and backup experimental data and video during the EAST experiment, which is of great benefit to EAST experimenter than the formal VOD system in VOD function and real time performance."
648,https://arxiv.org/abs/1807.00327,Performance Analysis of Indoor THz Communications with One-Bit Precoding,"In this paper, the performance of indoor Terahertz (THz) communication systems with one-bit digital-to-analog converters (DACs) is investigated. Array-of-subarrays architecture is assumed for the antennas at the access points, where each RF chain uniquely activates a disjoint subset of antennas, each of which is connected to an exclusive phase shifter. Hybrid precoding, including maximum ratio transmission (MRT) and zero-forcing (ZF) precoding, is considered. The best beamsteering direction for the phase shifter in the large subarray antenna regime is first proved to be the direction of the line-of-sight (LOS) path. Subsequently, the closed-form expression of the lower-bound of the achievable rate in the large subarray antenna regime is derived, which is the same for both MRT and ZF and is independent of the transmit power. Numerical results validating the analysis are provided as well."
649,https://arxiv.org/abs/1806.02603,The $A_α$-spectral radius of graphs with given degree sequence,"Let $G$ be a graph with adjacency matrix $A(G)$, and let $D(G)$ be the diagonal matrix of the degrees of $G$. For any real $α\in[0,1]$, write $A_α(G)$ for the matrix $$A_α(G)=αD(G)+(1-α)A(G).$$ This paper presents some extremal results about the spectral radius $ρ(A_α(G))$ of $A_α(G)$ that generalize previous results about $ρ(A_0(G))$ and $ρ(A_{\frac{1}{2}}(G))$. In this paper, we give some results on graph perturbation for $A_α$-matrix with $α\in [0,1)$. As applications, we characterize all extremal trees with the maximum $A_α$-spectral radius in the set of all trees with prescribed degree sequence firstly. Furthermore, we characterize the unicyclic graphs that have the largest $A_α$-spectral radius for a given unicycilc degree sequence."
650,https://arxiv.org/abs/1804.06377,Herschel PACS observations of 4-10 Myr old Classical T Tauri stars in Orion OB1,"We present \emph{Herschel} PACS observations of 8 Classical T Tauri Stars in the $\sim 7-10$ Myr old OB1a and the $\sim 4-5$ Myr old OB1b Orion sub-asscociations. Detailed modeling of the broadband spectral energy distributions, particularly the strong silicate emission at 10 $μ$m, shows that these objects are (pre)transitional disks with some amount of small optically thin dust inside their cavities, ranging from $\sim 4$ AU to $\sim 90$ AU in size. We analyzed \emph{Spitzer} IRS spectra for two objects in the sample: CVSO-107 and CVSO-109. The IRS spectrum of CVSO-107 indicates the presence of crystalline material inside its gap while the silicate feature of CVSO-109 is characterized by a pristine profile produced by amorphous silicates; the mechanisms creating the optically thin dust seem to depend on disk local conditions. Using millimeter photometry we estimated dust disk masses for CVSO-107 and CVSO-109 lower than the minimum mass of solids needed to form the planets in our Solar System, which suggests that giant planet formation should be over in these disks. We speculate that the presence and maintenance of optically thick material in the inner regions of these pre-transitional disks might point to low-mass planet formation."
651,https://arxiv.org/abs/1804.03998,Superconvergence of the Gradient Approximation for Weak Galerkin Finite Element Methods on Nonuniform Rectangular Partitions,"This article presents a superconvergence for the gradient approximation of the second order elliptic equation discretized by the weak Galerkin finite element methods on nonuniform rectangular partitions. The result shows a convergence of ${\cal O}(h^r)$, $1.5\leq r \leq 2$, for the numerical gradient obtained from the lowest order weak Galerkin element consisting of piecewise linear and constant functions. For this numerical scheme, the optimal order of error estimate is ${\cal O}(h)$ for the gradient approximation. The superconvergence reveals a superior performance of the weak Galerkin finite element methods. Some computational results are included to numerically validate the superconvergence theory."
652,https://arxiv.org/abs/1803.07984,Online data assimilation in distributionally robust optimization,"This paper considers a class of real-time decision making problems to minimize the expected value of a function that depends on a random variable $ξ$ under an unknown distribution $\mathbb{P}$. In this process, samples of $ξ$ are collected sequentially in real time, and the decisions are made, using the real-time data, to guarantee out-of-sample performance. We approach this problem in a distributionally robust optimization framework and propose a novel Online Data Assimilation Algorithm for this purpose. This algorithm guarantees the out-of-sample performance in high probability, and gradually improves the quality of the data-driven decisions by incorporating the streaming data. We show that the Online Data Assimilation Algorithm guarantees convergence under the streaming data, and a criteria for termination of the algorithm after certain number of data has been collected."
653,https://arxiv.org/abs/1803.02119,Chaotic motion of neutral and charged particles in the magnetized Ernst-Schwarzschild spacetime,"Neutral test particles around a Schwarzschild black hole immersed in an external uniform magnetic field have no interactions of electromagnetic forces, but their motions can be chaotic. This chaotic behavior is induced owing to the gravitational effect of the magnetic field leading to the nonintegrability of the magnetized Ernst-Schwarzschild spacetime geometry. In fact, chaos is strengthened typically with an increase of the energy or the magnetic field under appropriate circumstances. When these test particles have charges, the electromagnetic forces are included. As a result, the electromagnetic forces have an effect on strengthening or weakening the extent of chaos caused by the gravitational effect of the magnetic field."
654,https://arxiv.org/abs/1801.10603,ILPS at TREC 2017 Common Core Track,"The TREC 2017 Common Core Track aimed at gathering a diverse set of participating runs and building a new test collection using advanced pooling methods.
  In this paper, we describe the participation of the IlpsUvA team at the TREC 2017 Common Core Track. We submitted runs created using two methods to the track: (1) BOIR uses Bayesian optimization to automatically optimize retrieval model hyperparameters. (2) NVSM is a latent vector space model where representations of documents and query terms are learned from scratch in an unsupervised manner.
  We find that BOIR is able to optimize hyperparameters as to find a system that performs competitively amongst track participants. NVSM provides rankings that are diverse, as it was amongst the top automated unsupervised runs that provided the most unique relevant documents."
655,https://arxiv.org/abs/1711.08896,Efficient quantum circuit for singular value thresholding,"Singular value thresholding (SVT) operation is a fundamental core module in many mathematical models in computer vision and machine learning, particularly for many nuclear norm minimizing-based problems. We presented a quantum SVT (QSVT) algorithm which was used as a subroutine to address an image classification problem. This algorithm runs in $O\left[\log\left(pq\right)\right]$, an exponential speed improvement over the classical algorithm which runs in $O\left[poly\left(pq\right)\right]$. In this study, we investigate this algorithm and design a scalable quantum circuit for QSVT. In the circuit design, we introduce an adjustable parameter $α$ to ensure the high probability of obtaining the final result and the high fidelity of actual and ideal final states. We also show that the value of $α$ can be computed ahead of implementing the quantum circuit when the inputs of the QSVT algorithm, i.e. matrix ${\bf{A}}$ and constant $τ$, are given. In addition, we propose a small-scale quantum circuit for QSVT. We numerically simulate and demonstrate the performance of this circuit, verifying its capability to solve the intended SVT. The quantum circuit for QSVT implies a tempting possibility for experimental realization on a quantum computer."
656,https://arxiv.org/abs/1711.00510,Temporal Variations of Telluric Water Vapor Absorption at Apache Point Observatory,"Time-variable absorption by water vapor in Earth's atmosphere presents an important source of systematic error for a wide range of ground-based astronomical measurements, particularly at near-infrared wavelengths. We present results from the first study on the temporal and spatial variability of water vapor absorption at Apache Point Observatory (APO). We analyze $\sim$400,000 high-resolution, near-infrared ($H$-band) spectra of hot stars collected as calibration data for the APO Galactic Evolution Explorer (APOGEE) survey. We fit for the optical depths of telluric water vapor absorption features in APOGEE spectra and convert these optical depths to Precipitable Water Vapor (PWV) using contemporaneous data from a GPS-based PWV monitoring station at APO. Based on simultaneous measurements obtained over a 3$^{\circ}$ field of view, we estimate that our PWV measurement precision is $\pm0.11$ mm. We explore the statistics of PWV variations over a range of timescales from less than an hour to days. We find that the amplitude of PWV variations within an hour is less than 1 mm for most (96.5%) APOGEE field visits. By considering APOGEE observations that are close in time but separated by large distances on the sky, we find that PWV is homogeneous across the sky at a given epoch, with 90% of measurements taken up to 70$^{\circ}$ apart within 1.5 hr having $Δ\,\rm{PWV}<1.0$ mm. Our results can be used to help simulate the impact of water vapor absorption on upcoming surveys at continental observing sites like APO, and also to help plan for simultaneous water vapor metrology that may be carried out in support of upcoming photometric and spectroscopic surveys."
657,https://arxiv.org/abs/1710.01426,Bott--Kitaev periodic table and index theory,"We consider topological insulators and superconductors with discrete symmetries and clarify the relevant index theory behind the periodic table proposed by Kitaev.
  An effective Hamiltonian determines the analytical index, which can be computed by a topological index. We focus on the spatial dimensions one, two and three, and only consider the bulk theory.
  In two dimensions, the $\mathbb{Z}$-valued invariants are given by the first Chern number. Meanwhile, $\mathbb{Z}_2$-valued invariants can be computed by the odd topological index and its variations.
  The Bott-Kitaev periodic table is well-known in the physics literature, we organize the topological invariants in the framework of KR-theory."
658,https://arxiv.org/abs/1710.01120,Oscillating scalar fields in extended quintessence,"We study a rapidly-oscillating scalar field with potential $V(φ) = k|φ|^n$ nonminally coupled to the Ricci scalar $R$ via a term of the form $(1- 8 πG_0 ξφ^2) R$ in the action. In the weak coupling limit, we calculate the effect of the nonminimal coupling on the time-averaged equation of state parameter $γ= (p + ρ)/ρ$. The change in $\langle γ\rangle$ is always negative for $n \ge 2$ and always positive for $n < 0.71$ (which includes the case where the oscillating scalar field could serve as dark energy), while it can be either positive or negative for intermediate values of $n$. Constraints on the time-variation of $G$ force this change to be infinitesimally small at the present time whenever the scalar field dominates the expansion, but constraints in the early universe are not as stringent. The rapid oscillation induced in $G$ also produces an additional contribution to the Friedman equation that behaves like an effective energy density with a stiff equation of state, but we show that, under reasonable assumptions, this effective energy density is always smaller than the density of the scalar field itself."
659,https://arxiv.org/abs/1709.09454,Release Connection Fingerprints in Social Networks Using Personalized Differential Privacy,"In social networks, different users may have different privacy preferences and there are many users with public identities. Most work on differentially private social network data publication neglects this fact. We aim to release the number of public users that a private user connects to within n hops, called n-range Connection fingerprints(CFPs), under user-level personalized privacy preferences. We proposed two schemes, Distance-based exponential budget absorption (DEBA) and Distance-based uniformly budget absorption using Ladder function (DUBA-LF), for privacy-preserving publication of the CFPs based on Personalized differential privacy(PDP), and we conducted a theoretical analysis of the privacy guarantees provided within the proposed schemes. The implementation showed that the proposed schemes are superior in publication errors on real datasets."
660,https://arxiv.org/abs/1709.05015,Quantum walks on regular uniform hypergraphs,"Quantum walks on graphs have shown prioritized benefits and applications in wide areas. In some scenarios, however, it may be more natural and accurate to mandate high-order relationships for hypergraphs, due to the density of information stored inherently. Therefore, we can explore the potential of quantum walks on hypergraphs. In this paper, by presenting the one-to-one correspondence between regular uniform hypergraphs and bipartite graphs, we construct a model for quantum walks on bipartite graphs of regular uniform hypergraphs with Szegedy's quantum walks, which gives rise to a quadratic speed-up. Furthermore, we deliver spectral properties of the transition matrix, given that the cardinalities of the two disjoint sets are different in the bipartite graph. Our model provides the foundation for building quantum algorithms on the strength of quantum walks, suah as quantum walks search, quantized Google's PageRank and quantum machine learning, based on hypergraphs."
661,https://arxiv.org/abs/1709.02499,Model updating using sum of squares (SOS) optimization to minimize modal dynamic residuals,"This research studies finite element (FE) model updating through sum of squares (SOS) optimization to minimize modal dynamic residuals. In the past few decades, many FE model updating algorithms have been studied to improve the similitude between a numerical model and the as-built structure. FE model updating usually requires solving nonconvex optimization problems, while most off-the-shelf optimization solvers can only find local optima. To improve the model updating performance, this paper proposes the SOS global optimization method for minimizing modal dynamic residuals of the generalized eigenvalue equations in structural dynamics. The proposed method is validated through both numerical simulation and experimental study of a four-story shear frame structure."
662,https://arxiv.org/abs/1709.01709,Active Sampling for Large-scale Information Retrieval Evaluation,"Evaluation is crucial in Information Retrieval. The development of models, tools and methods has significantly benefited from the availability of reusable test collections formed through a standardized and thoroughly tested methodology, known as the Cranfield paradigm. Constructing these collections requires obtaining relevance judgments for a pool of documents, retrieved by systems participating in an evaluation task; thus involves immense human labor. To alleviate this effort different methods for constructing collections have been proposed in the literature, falling under two broad categories: (a) sampling, and (b) active selection of documents. The former devises a smart sampling strategy by choosing only a subset of documents to be assessed and inferring evaluation measure on the basis of the obtained sample; the sampling distribution is being fixed at the beginning of the process. The latter recognizes that systems contributing documents to be judged vary in quality, and actively selects documents from good systems. The quality of systems is measured every time a new document is being judged. In this paper we seek to solve the problem of large-scale retrieval evaluation combining the two approaches. We devise an active sampling method that avoids the bias of the active selection methods towards good systems, and at the same time reduces the variance of the current sampling approaches by placing a distribution over systems, which varies as judgments become available. We validate the proposed method using TREC data and demonstrate the advantages of this new method compared to past approaches."
663,https://arxiv.org/abs/1708.08026,Mid-Infrared Polarization of Herbig Ae/Be Discs,"We measured mid-infrared polarization of protoplanetary discs to gain new insight into their magnetic fields. Using CanariCam at the 10.4 m Gran Telescopio Canarias, we detected linear polarization at 8.7, 10.3, and 12.5 $μ$m from discs around eight Herbig Ae/Be stars and one T-Tauri star. We analyzed polarimetric properties of each object to find out the most likely interpretation of the data. While the observed mid-infrared polarization from most objects is consistent with polarized emission and/or absorption arising from aligned dust particles, we cannot rule out polarization due to dust scattering for a few objects in our sample. For those objects for which polarization can be explained by polarized emission and/or absorption, we examined how the derived magnetic field structure correlates with the disc position angle and inclination. We found no preference for a certain type of magnetic field. Instead, various configurations (toroidal, poloidal, or complex) are inferred from the observations. The detection rate (64 per cent) of polarized mid-infrared emission and/or absorption supports the expectation that magnetic fields and suitable conditions for grain alignment are common in protoplanetary discs around Herbig Ae/Be stars."
664,https://arxiv.org/abs/1707.07389,Controlled Alternate Quantum Walks based Quantum Hash Function,"Through introducing controlled alternative quantum walks, we present controlled alternate quantum walks (CAQW) based quantum hash function. CAQW based quantum hash function have excellent security, outstanding statistical performance and splendid expansibility. Furthermore, due to the structure of alternative quantum walks, implementing CAQW based quantum hash function significantly reduces the resources necessary for its feasible experimental realization than implementing other quantum hash functions. Besides, CAQW based quantum hash function has expansibility."
665,https://arxiv.org/abs/1707.06380,Threshold dynamics and ergodicity of an SIRS epidemic model with Markovian switching,"This paper studies the spread dynamics of a stochastic SIRS epidemic model with nonlinear incidence and varying population size, which is formulated as a piecewise deterministic Markov process. A threshold dynamic determined by the basic reproduction number $\mathcal{R}_{0}$ is established: the disease can be eradicated almost surely if $\mathcal{R}_{0}<1$, while the disease persists almost surely if $\mathcal{R}_{0}>1$. The existing method for analyzing ergodic behavior of population systems has been generalized. The modified method weakens the required conditions and has no limitations for both the number of environmental regimes and the dimension of the considered system. When $\mathcal{R}_{0}>1$, the existence of a stationary probability measure is obtained. Furthermore, with the modified method, the global attractivity of the $Ω$-limit set of the system and the convergence in total variation to the stationary measure are both demonstrated under a mild extra condition."
666,https://arxiv.org/abs/1706.02015,Detection of Polarized Infrared Emission by Polycyclic Aromatic Hydrocarbons in the MWC 1080 Nebula,"Polycyclic aromatic hydrocarbons (PAHs) are ubiquitous in astrophysical environments, as revealed by their pronounced emission features at 3.3, 6.2, 7.7, 8.6, 11.3, and 12.7 $μ$m commonly ascribed to the C--H and C--C vibrational modes. Although these features have long been predicted to be polarized, previous searches for PAH polarization led to null or, at best, tentative detections. Here we report the definite detection of polarized PAH emission at 11.3 $μ$m in the nebula associated with the Herbig Be star MWC 1080. We measure a polarization degree of 1.9$\pm$0.2\%, which is unexpectedly high compared to models. This poses a challenge in the current understanding of the alignment of PAHs, which is required to polarize the PAH emission but thought to be substantially suppressed. PAH alignment with a magnetic field via a resonance paramagnetic relaxation process may account for such a high level of polarization."
667,https://arxiv.org/abs/2305.13663,iCOIL: Scenario Aware Autonomous Parking Via Integrated Constrained Optimization and Imitation Learning,"Autonomous parking (AP) is an emering technique to navigate an intelligent vehicle to a parking space without any human intervention. Existing AP methods based on mathematical optimization or machine learning may lead to potential failures due to either excessive execution time or lack of generalization. To fill this gap, this paper proposes an integrated constrained optimization and imitation learning (iCOIL) approach to achieve efficient and reliable AP. The iCOIL method has two candidate working modes, i.e., CO and IL, and adopts a hybrid scenario analysis (HSA) model to determine the better mode under various scenarios. We implement and verify iCOIL on the Macao Car Racing Metaverse (MoCAM) platform. Results show that iCOIL properly adapts to different scenarios during the entire AP procedure, and achieves significantly larger success rates than other benchmarks."
668,https://arxiv.org/abs/2304.09692,GeoGauss: Strongly Consistent and Light-Coordinated OLTP for Geo-Replicated SQL Database,"Multinational enterprises conduct global business that has a demand for geo-distributed transactional databases. Existing state-of-the-art databases adopt a sharded master-follower replication architecture. However, the single-master serving mode incurs massive cross-region writes from clients, and the sharded architecture requires multiple round-trip acknowledgments (e.g., 2PC) to ensure atomicity for cross-shard transactions. These limitations drive us to seek yet another design choice. In this paper, we propose a strongly consistent OLTP database GeoGauss with full replica multi-master architecture. To efficiently merge the updates from different master nodes, we propose a multi-master OCC that unifies data replication and concurrent transaction processing. By leveraging an epoch-based delta state merge rule and the optimistic asynchronous execution, GeoGauss ensures strong consistency with light-coordinated protocol and allows more concurrency with weak isolation, which are sufficient to meet our needs. Our geo-distributed experimental results show that GeoGauss achieves 7.06X higher throughput and 17.41X lower latency than the state-of-the-art geo-distributed database CockroachDB on the TPC-C benchmark."
669,https://arxiv.org/abs/2212.10804,Local Group Dwarf Galaxy Detection Limit in the CSST survey,"We predict the dwarf galaxy detection limits for the upcoming Chinese Space Station Telescope (CSST) survey that will cover 17,500 deg$^{2}$ of the sky with a wide field of view of 1.1 deg$^2$. The point-source depth reaches 26.3 mag in the $g$ band and 25.9 mag in the $i$ band. Constructing mock survey data based on the designed photometric bands, we estimate the recovery rate of artificial dwarf galaxies from mock point-source photometric catalogues. The detection of these artificial dwarf galaxies is strongly dependent on their distance, magnitude and size, in agreement with searches in current surveys. We expect CSST to enable the detection of dwarf galaxies with $M_V = -3.0$ and $μ_{250} = 32.0$ mag/arcsec$^2$ (surface-brightness limit for a system of half-light radius $r_{\rm h}$ = 250 pc at 400 kpc, and $M_V = -4.9$ and $μ_{250} = 30.5$ mag/arcsec$^2$ around the Andromeda galaxy. Beyond the Local Group, the CSST survey will achieve $M_V = -5.8$, and $μ_{250}$ = 29.7 mag/arcsec$^2$ in the distance range of 1--2 Mpc, opening up an exciting discovery space for faint field dwarf galaxies. With its optical bands, wide survey footprint, and space resolution, CSST will undoubtedly expand our knowledge of low-mass dwarf galaxies to an unprecedented volume."
670,https://arxiv.org/abs/2211.16825,Testing parity symmetry of gravity with gravitational waves,"The examination of parity symmetry in gravitational interactions has drawn increasing attention. Although Einstein's General Relativity is parity-conserved, numerous theories of parity-violating (PV) gravity in different frameworks have recently been proposed for different motivations. In this review, we briefly summarize the recent progress of these theories, and focus on the observable effects of PV terms in the gravitational waves (GWs), which are mainly reflected in the difference between the left-hand and right-hand polarization modes. We are primarily concerned with the implications of these theories for GWs generated by the compact binary coalescences and the primordial GWs generated in the early Universe. The deviation of GW waveforms and/or primordial power spectrum can always be quantified by the energy scale of parity violation of the theory. Applying the current and future GW observation from laser interferometers and cosmic microwave background radiation, the current and potential constraints on the PV energy scales are presented, which indicates that the parity symmetry of gravity can be tested in high energy scale in this new era of gravitational waves."
671,https://arxiv.org/abs/2210.07021,The spherical Fast Multipole Method (sFMM) for Gravitational Lensing Simulation,"In this paper, we present a spherical Fast Multipole Method (sFMM) for ray tracing simulation of gravitational lensing (GL) on a curved sky. The sFMM is a non-trivial extension of the Fast Multiple Method (FMM) to sphere $\mathbb S^2$, and it can accurately solve the Poisson equation with time complexity of $O(N)\log(N)$, where $N$ is the number of particles. It is found that the time complexity of the sFMM is near $O(N)$ and the computational accuracy can reach $10^{-10}$ in our test. In addition, compared with the Fast Spherical Harmonic Transform (FSHT), the sFMM is not only faster but more accurate, as it has the ability to reserve high-frequency components of the density field. These merits make the sFMM an optimum method to simulate the gravitational lensing on a curved sky, which is the case for upcoming large-area sky surveys, such as the Vera Rubin Observatory and the China Space Station Telescope."
672,https://arxiv.org/abs/2210.06761,Effects of galaxy intrinsic alignment on weak lensing peak statistics,"The galaxy intrinsic alignment (IA) is a dominant source of systematics in weak lensing (WL) studies. In this paper, by employing large simulations with semi-analytical galaxy formation, we investigate the IA effects on WL peak statistics. Different simulated source galaxy samples of different redshift distributions are constructed, where both WL shear and IA signals are included. Convergence reconstruction and peak statistics are then performed for these samples. Our results show that the IA effects on peak abundances mainly consist of two aspects. One is the additional contribution from IA to the shape noise. The other is from the satellite IA that can affect the peak signals from their host clusters significantly. The latter depends on the level of inclusion in a shear sample of the satellite galaxies of the clusters that contribute to WL peaks, and thus is sensitive to the redshift distribution of source galaxies. We pay particular attention to satellite IA and adjust it artificially in the simulations to analyze the dependence of the satellite IA impacts on its strength. This information can potentially be incorporated into the modeling of WL peak abundances, especially for high peaks physically originated from massive clusters of galaxies, and thus to mitigate the IA systematics on the cosmological constraints derived from WL peaks."
673,https://arxiv.org/abs/2209.07673,Towards Super-resolution via Iterative multi-exposure Coaddition,"In this article, we provide an alternative up-sampling and PSF deconvolution method for the iterative multi-exposure coaddition. Different from the previous works, the new method has a ratio-correction term, which allows the iterations to converge more rapidly to an accurate representation of the underlying image than those with difference-correction terms. By employing this method, one can coadd the under-sampled multi-exposures to a super-resolution and obtain a higher peak signal-to-noise ratio. A set of simulations show that we can take many advantages of the new method, e.g. in the signal-to-noise ratio, the average deviation of all source fluxes, super-resolution, and source distortion ratio, etc., which are friendly to astronomical photometry and morphology, and benefits faint source detection and shear measurement of weak gravitational lensing. It provides an improvement in fidelity over the previous works tested in this paper."
674,https://arxiv.org/abs/2208.13566,Wave effect of gravitational waves intersected with a microlens field: a new algorithm and supplementary study,"The increase in gravitational wave (GW) events has allowed receiving strong lensing image pairs of GWs. However, the wave effect (diffraction and interference) due to the microlens field contaminates the parameter estimation of the image pair, which may lead to a misjudgment of strong lensing signals. To quantify the influence of the microlens field, researchers need a large sample of statistical research. Nevertheless, due to the oscillation characteristic, the Fresnel-Kirchhoff diffraction integral's computational time hinders this aspect's study. Although many algorithms are available, most cannot be well applied to the case where the microlens field is embedded in galaxy/galaxy clusters. This work proposes a faster and more accurate algorithm for studying the wave optics effect of microlenses embedded in different types of strong lensing images. Additionally, we provide a quantitative estimation criterion for the lens plane boundary for the Fresnel-Kirchhoff diffraction integral. This algorithm can significantly facilitate the study of wave optics, particularly in the case of microlens fields embedded in galaxy/galaxy clusters."
675,https://arxiv.org/abs/2208.07592,Multi-Point Integrated Sensing and Communication: Fusion Model and Functionality Selection,"Integrated sensing and communication (ISAC) represents a paradigm shift, where previously competing wireless transmissions are jointly designed to operate in harmony via the shared use of the hardware platform for improving the spectral and energy efficiencies. However, due to adversarial factors such as fading and interference, ISAC may suffer from high sensing uncertainties. This paper presents a multi-point ISAC (MPISAC) system that fuses the outputs from multiple ISAC devices for achieving higher sensing performance by exploiting multi-view data redundancy. Furthermore, we propose to effectively explore the performance trade-off between sensing and communication via a functionality selection module that adaptively determines the working state (i.e., sensing or communication) of an ISAC device. The crux of our approach is to derive a fusion model that predicts the fusion accuracy via hypothesis testing and optimal voting analysis. Simulation results demonstrate the superiority of MPISAC over various benchmark schemes and show that the proposed approach can effectively span the trade-off region in ISAC systems."
676,https://arxiv.org/abs/2206.05830,Stochastic Gradient Descent without Full Data Shuffle,"Stochastic gradient descent (SGD) is the cornerstone of modern machine learning (ML) systems. Despite its computational efficiency, SGD requires random data access that is inherently inefficient when implemented in systems that rely on block-addressable secondary storage such as HDD and SSD, e.g., TensorFlow/PyTorch and in-DB ML systems over large files. To address this impedance mismatch, various data shuffling strategies have been proposed to balance the convergence rate of SGD (which favors randomness) and its I/O performance (which favors sequential access).
  In this paper, we first conduct a systematic empirical study on existing data shuffling strategies, which reveals that all existing strategies have room for improvement -- they all suffer in terms of I/O performance or convergence rate. With this in mind, we propose a simple but novel hierarchical data shuffling strategy, CorgiPile. Compared with existing strategies, CorgiPile avoids a full data shuffle while maintaining comparable convergence rate of SGD as if a full shuffle were performed. We provide a non-trivial theoretical analysis of CorgiPile on its convergence behavior. We further integrate CorgiPile into PyTorch by designing new parallel/distributed shuffle operators inside a new CorgiPileDataSet API. We also integrate CorgiPile into PostgreSQL by introducing three new physical operators with optimizations. Our experimental results show that CorgiPile can achieve comparable convergence rate with the full shuffle based SGD for both deep learning and generalized linear models. For deep learning models on ImageNet dataset, CorgiPile is 1.5X faster than PyTorch with full data shuffle. For in-DB ML with linear models, CorgiPile is 1.6X-12.8X faster than two state-of-the-art in-DB ML systems, Apache MADlib and Bismarck, on both HDD and SSD."
677,https://arxiv.org/abs/2206.01377,Tolerance For the Pixelation Effect in Shear Measurement,"Images taken by space telescopes typically have a superb spatial resolution, but a relatively poor sampling rate due to the finite CCD pixel size. Beyond the Nyquist limit, it becomes uncertain how much the pixelation effect may affect the accuracy of galaxy shape measurement. It is timely to study this issue given that a number of space-based large-scale weak lensing surveys are planned. Using the Fourier_Quad method, we quantify the shear recovery error as a function of the sampling factor Q, i.e., the ratio between the FWHM of the point-spread-function (PSF) and the pixel size of the CCD, for different PSFs and galaxies of different sizes and noise levels. We show that sub-percent-level accuracy in shear recovery is achievable with single-exposure images for $Q\lesssim 2$. The conclusion holds for galaxies much smaller than the PSF, and those with a significant level of noise."
678,https://arxiv.org/abs/2204.10871,An Improved GPU-Based Ray-Shooting Code For Gravitational Microlensing,"We present an improved inverse ray-shooting code based on GPUs for generating microlensing magnification maps. In addition to introducing GPUs for acceleration, we put the efforts in two aspects: (i) A standard circular lens plane is replaced by a rectangular one to reduce the number of unnecessary lenses as a result of an extremely prolate rectangular image plane. (ii) Interpolation method is applied in our implementation which has achieved an significant acceleration when dealing with large number of lenses and light rays required by high resolution maps. With these applications, we have greatly reduced the running time while maintaining high accuracy: the speed has been increased by about 100 times compared with ordinary GPU based IRS code and GPU-D code when handling large number of lenses. If encountered the high resolution situation up to $10000^2$ pixels, resulting in almost $10^{11}$ light rays, the running time can also be reduced by two orders of magnitude."
679,https://arxiv.org/abs/2203.15680,Forecast of observing time delay of the strongly lensed quasars with Muztagh-Ata 1.93m telescope,"As a completely independent method, the measurement of time delay of strongly lensed quasars (TDSL) are crucial to resolve the Hubble tension. Extensive monitoring is required but so far limited to a small sample of strongly lensed quasars. Together with several partner institutes, Beijing Normal University is constructing a 1.93m reflector telescope at the Muztagh-Ata site in west China, which has the world class observing conditions. The telescope will be equipped with both a three-channel imager/photometer which covers $3500-11000$ Angstrom wavelength band, and a low-medium resolution ($λ/δλ=500/2000/7500$) spectrograph. In this paper, we investigate the capability of Muztagh-Ata 1.93m telescope in measuring time delays of strongly lensed quasars. We generate mock strongly lensed quasar systems and light curves with microlensing effects based on five known strongly lensed quasars, i.e., RX J1131-1231, HE 0435-1223, PG 1115+080, WFI 2033-4723 and SDSS 1206+4332. In particular, RX J1131-1231 is generated with lens modeling in this work. Due to lack of enough information, we simulate the other 4 systems with the public data without lens modeling. According to simulations, for RX J1131-like systems (wide variation in time delay between images) the TDSL measurement can be achieved with the precision about $Δt=0.5$ day with 4 seasons campaign length and 1 day cadence. This accuracy is comparable to the up-coming TDCOSMO project. And it would be better when the campaign length keeps longer and with high cadence. As a result, the capability of Muztagh-Ata 1.93m telescope allows it to join the network of TDSL observatories. It will enrich the database for strongly lensed quasar observations and make more precise measurements of time delays, especially considering the unique coordinate of the site."
680,https://arxiv.org/abs/2112.12926,nvBench: A Large-Scale Synthesized Dataset for Cross-Domain Natural Language to Visualization Task,"NL2VIS - which translates natural language (NL) queries to corresponding visualizations (VIS) - has attracted more and more attention both in commercial visualization vendors and academic researchers. In the last few years, the advanced deep learning-based models have achieved human-like abilities in many natural language processing (NLP) tasks, which clearly tells us that the deep learning-based technique is a good choice to push the field of NL2VIS. However, a big balk is the lack of benchmarks with lots of (NL, VIS) pairs. We present nvBench, the first large-scale NL2VIS benchmark, containing 25,750 (NL, VIS) pairs from 750 tables over 105 domains, synthesized from (NL, SQL) benchmarks to support cross-domain NL2VIS task. The quality of nvBench has been extensively validated by 23 experts and 300+ crowd workers. Deep learning-based models training using nvBench demonstrate that nvBench can push the field of NL2VIS."
681,https://arxiv.org/abs/2110.09033,Post-Newtonian parameters of ghost-free parity-violating gravities,"We investigate the slow-motion and weak-field approximation of the general ghost-free parity-violating (PV) theory of gravity in the parametrized post-Newtonian (PPN) framework and derive the perturbative field equations, which are modified by the PV terms of this theory. The complete PPN parameters are obtained by solving the perturbative field equations. We find that all the PPN parameters are exactly the same as those in general relativity, except for an extra parameter $κ$, which is caused by the new curl-type term in the gravitomagnetic sector of the metric in this theory. We calculate the precession effects of gyroscopes in this theory and constrain the model parameters by the observations of the Gravity Probe B experiment."
682,https://arxiv.org/abs/2110.07643,FRBs Lensed by Point Masses II. The multi-peaked FRBs from the point view of microlensing,"The microlensing effect has developed into a powerful technique for a diverse range of applications including exoplanet discoveries, structure of the Milky Way, constraints on MAssive Compact Halo Objects, and measurements of the size and profile of quasar accretion discs. In this paper, we consider a special type of microlensing events where the sources are fast radio bursts with $\sim$milliseconds (ms) durations for which the relative motion between the lens and source is negligible. In this scenario, it is possible to temporally resolve the individual microimages. As a result, a method beyond the inverse ray shooting (IRS) method, which only evaluates the total magnification of all microimages, is needed. We therefore implement an algorithm for identifying individual microimages and computing their magnifications and relative time delays. We validate our algorithm by comparing to analytical predictions for a single microlens case and find excellent agreement. We show that the superposition of pulses from individual microimages produces a light curve that appears as multi-peaked FRBs. The relative time delays between pulses can reach 0.1--1 ms for stellar-mass lenses and hence can already be resolved temporally by current facilities. Although not yet discovered, microlensing of FRBs will become regular events and surpass the number of quasar microlensing events in the near future when $10^{4-5}$ FRBs are expected to be discovered on a daily basis. Our algorithm provides a way of generating the microlensing light curve that can be used for constraining stellar mass distribution in distant galaxies."
683,https://arxiv.org/abs/2109.14953,About One-point Statistics of the Ratio of Two Fourier-transformed Cosmic Fields and an Application,"The Fourier transformation is an effective and efficient operation of Gaussianization at the one-point level. Using a set of N-body simulation data, we verified that the one-point distribution functions of the dark matter momentum divergence and density fields closely follow complex Gaussian distributions. The one-point distribution function of the quotient of two complex Gaussian variables is introduced and studied. Statistical theories are then applied to model one-point statistics about the growth of individual Fourier mode of the dark matter density field, which can be obtained by the ratio of two Fourier transformed cosmic fields. Our simulation results proved that the models based on the Gaussian approximation are impressively accurate, and our analysis revealed many interesting aspects about the growth of dark matter's density fluctuation in Fourier space."
684,https://arxiv.org/abs/2108.10417,Recurrent multiple shared layers in Depth for Neural Machine Translation,"Learning deeper models is usually a simple and effective approach to improve model performance, but deeper models have larger model parameters and are more difficult to train. To get a deeper model, simply stacking more layers of the model seems to work well, but previous works have claimed that it cannot benefit the model. We propose to train a deeper model with recurrent mechanism, which loops the encoder and decoder blocks of Transformer in the depth direction. To address the increasing of model parameters, we choose to share parameters in different recursive moments. We conduct our experiments on WMT16 English-to-German and WMT14 English-to-France translation tasks, our model outperforms the shallow Transformer-Base/Big baseline by 0.35, 1.45 BLEU points, which is 27.23% of Transformer-Big model parameters. Compared to the deep Transformer(20-layer encoder, 6-layer decoder), our model has similar model performance and infer speed, but our model parameters are 54.72% of the former."
685,https://arxiv.org/abs/2107.14590,Residual Tree Aggregation of Layers for Neural Machine Translation,"Although attention-based Neural Machine Translation has achieved remarkable progress in recent layers, it still suffers from issue of making insufficient use of the output of each layer. In transformer, it only uses the top layer of encoder and decoder in the subsequent process, which makes it impossible to take advantage of the useful information in other layers. To address this issue, we propose a residual tree aggregation of layers for Transformer(RTAL), which helps to fuse information across layers. Specifically, we try to fuse the information across layers by constructing a post-order binary tree. In additional to the last node, we add the residual connection to the process of generating child nodes. Our model is based on the Neural Machine Translation model Transformer and we conduct our experiments on WMT14 English-to-German and WMT17 English-to-France translation tasks. Experimental results across language pairs show that the proposed approach outperforms the strong baseline model significantly"
686,https://arxiv.org/abs/2107.09621,Integrated Sensing and Communication from Learning Perspective: An SDP3 Approach,"Characterizing the sensing and communication performance tradeoff in integrated sensing and communication (ISAC) systems is challenging in the applications of learning-based human motion recognition. This is because of the large experimental datasets and the black-box nature of deep neural networks. This paper presents SDP3, a Simulation-Driven Performance Predictor and oPtimizer, which consists of SDP3 data simulator, SDP3 performance predictor and SDP3 performance optimizer. Specifically, the SDP3 data simulator generates vivid wireless sensing datasets in a virtual environment, the SDP3 performance predictor predicts the sensing performance based on the function regression method, and the SDP3 performance optimizer investigates the sensing and communication performance tradeoff analytically. It is shown that the simulated sensing dataset matches the experimental dataset very well in the motion recognition accuracy. By leveraging SDP3, it is found that the achievable region of recognition accuracy and communication throughput consists of a communication saturation zone, a sensing saturation zone, and a communication-sensing adversarial zone, of which the desired balanced performance for ISAC systems lies in the third one."
687,https://arxiv.org/abs/2107.09574,Accelerating Edge Intelligence via Integrated Sensing and Communication,"Realizing edge intelligence consists of sensing, communication, training, and inference stages. Conventionally, the sensing and communication stages are executed sequentially, which results in excessive amount of dataset generation and uploading time. This paper proposes to accelerate edge intelligence via integrated sensing and communication (ISAC). As such, the sensing and communication stages are merged so as to make the best use of the wireless signals for the dual purpose of dataset generation and uploading. However, ISAC also introduces additional interference between sensing and communication functionalities. To address this challenge, this paper proposes a classification error minimization formulation to design the ISAC beamforming and time allocation. The globally optimal solution is derived via the rank-1 guaranteed semidefinite relaxation, and performance analysis is performed to quantify the ISAC gain over that of conventional edge intelligence. Simulation results are provided to verify the effectiveness of the proposed ISAC-assisted edge intelligence system. Interestingly, we find that ISAC is always beneficial, when the duration of generating a sample is more than the duration of uploading a sample. Otherwise, the ISAC gain can vanish or even be negative. Nevertheless, we still derive a sufficient condition, under which a positive ISAC gain is feasible."
688,https://arxiv.org/abs/2106.02901,Hierarchical Temperature Imaging Using Pseudo-Inversed Convolutional Neural Network Aided TDLAS Tomography,"As an in situ combustion diagnostic tool, Tunable Diode Laser Absorption Spectroscopy (TDLAS) tomography has been widely used for imaging of two-dimensional temperature distributions in reactive flows. Compared with the computational tomographic algorithms, Convolutional Neural Networks (CNNs) have been proofed to be more robust and accurate for image reconstruction, particularly in case of limited access of laser beams in the Region of Interest (RoI). In practice, flame in the RoI that requires to be reconstructed with good spatial resolution is commonly surrounded by low-temperature background. Although the background is not of high interest, spectroscopic absorption still exists due to heat dissipation and gas convection. Therefore, we propose a Pseudo-Inversed CNN (PI-CNN) for hierarchical temperature imaging that (a) uses efficiently the training and learning resources for temperature imaging in the RoI with good spatial resolution, and (b) reconstructs the less spatially resolved background temperature by adequately addressing the integrity of the spectroscopic absorption model. In comparison with the traditional CNN, the newly introduced pseudo inversion of the RoI sensitivity matrix is more penetrating for revealing the inherent correlation between the projection data and the RoI to be reconstructed, thus prioritising the temperature imaging in the RoI with high accuracy and high computational efficiency. In this paper, the proposed algorithm was validated by both numerical simulation and lab-scale experiment, indicating good agreement between the phantoms and the high-fidelity reconstructions."
689,https://arxiv.org/abs/2105.05868,FRBs Lensed by Point Masses I. Lens Mass Estimation for Doubly Imaged FRBs,"Fast radio bursts (FRBs) are bright radio transient events with durations on the order of milliseconds. The majority of FRB sources discovered so far have a single peak, with the exception of a few showing multiple-peaked profiles, the origin of which is unknown. In this work, we show that the strong lensing effect of a point mass or a point mass $+$ external shear on a single-peak FRB can produce double peaks (i.e. lensed images). In particular, the leading peak will always be more magnified and hence brighter than the trailing peak for a point-mass lens model, while the point-mass $+$ external shear lens model can produce a less magnified leading peak. We find that, for a point-mass lens model, the combination of lens mass $M$ and redshift $z_l$ in the form of $M(1+z_l)$ can be directly computed from two observables -- the delayed time $Δt$ and the flux ratio of the leading peak to the trailing peak $R$. For a point-mass $+$ external shear lens model, upper and lower limits in $M(1+z_l)$ can also be obtained from $Δt$ and $R$ for a given external shear strength. In particular, tighter lens mass constraints can be achieved when the observed $R$ is larger. Lastly, we show the process of constraining lens mass using the observed values of $Δt$ and $R$ of two double-peaked FRB sources, i.e. FRB 121002 and FRB 130729, as references, although the double-peaked profiles are not necessarily caused by strong lensing."
690,https://arxiv.org/abs/2104.13595,Detection of Cosmic Magnification via Galaxy Shear -- Galaxy Number Density Correlation from HSC Survey Data,"We propose a novel method to detect cosmic magnification signals by cross-correlating foreground convergence fields constructed from galaxy shear measurements with background galaxy positional distributions, namely shear-number density correlation. We apply it to the Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP) survey data. With 27 non-independent data points and their full covariance, $χ_0^2\approx 34.1$ and $χ_T^2\approx 24.0$ with respect to the null and the cosmological model with the parameters from HSC shear correlation analyses in Hamana et al. 2020 (arXiv:1906.06041), respectively. The Bayes factor of the two is $\log_{10}B_{T0}\approx 2.2$ assuming equal model probabilities of null and HSC cosmology, showing a clear detection of the magnification signals. Theoretically, the ratio of the shear-number density and shear-shear correlations can provide a constraint on the effective multiplicative shear bias $\bar m$ using internal data themselves. We demonstrate the idea with the signals from our HSC-SSP mock simulations and rescaling the statistical uncertainties to a survey of $15000°^2$. For two-bin analyses with background galaxies brighter than $m_{lim}=23$, the combined analyses lead to a forecasted constraint of $σ(\bar m) \sim 0.032$, $2.3$ times tighter than that of using the shear-shear correlation alone. Correspondingly, $σ(S_8)$ with $S_8=σ_8(Ω_\mathrm{m}/0.3)^{0.5}$ is tightened by $\sim 2.1$ times. Importantly, the joint constraint on $\bar m$ is nearly independent of cosmological parameters. Our studies therefore point to the importance of including the shear-number density correlation in weak lensing analyses, which can provide valuable consistency tests of observational data, and thus to solidify the derived cosmological constraints."
691,https://arxiv.org/abs/2104.10378,Wireless Sensing With Deep Spectrogram Network and Primitive Based Autoregressive Hybrid Channel Model,"Human motion recognition (HMR) based on wireless sensing is a low-cost technique for scene understanding. Current HMR systems adopt support vector machines (SVMs) and convolutional neural networks (CNNs) to classify radar signals. However, whether a deeper learning model could improve the system performance is currently not known. On the other hand, training a machine learning model requires a large dataset, but data gathering from experiment is cost-expensive and time-consuming. Although wireless channel models can be adopted for dataset generation, current channel models are mostly designed for communication rather than sensing. To address the above problems, this paper proposes a deep spectrogram network (DSN) by leveraging the residual mapping technique to enhance the HMR performance. Furthermore, a primitive based autoregressive hybrid (PBAH) channel model is developed, which facilitates efficient training and testing dataset generation for HMR in a virtual environment. Experimental results demonstrate that the proposed PBAH channel model matches the actual experimental data very well and the proposed DSN achieves significantly smaller recognition error than that of CNN."
692,https://arxiv.org/abs/2012.02469,RPT: Relational Pre-trained Transformer Is Almost All You Need towards Democratizing Data Preparation,"Can AI help automate human-easy but computer-hard data preparation tasks that burden data scientists, practitioners, and crowd workers? We answer this question by presenting RPT, a denoising auto-encoder for tuple-to-X models (X could be tuple, token, label, JSON, and so on). RPT is pre-trained for a tuple-to-tuple model by corrupting the input tuple and then learning a model to reconstruct the original tuple. It adopts a Transformer-based neural translation architecture that consists of a bidirectional encoder (similar to BERT) and a left-to-right autoregressive decoder (similar to GPT), leading to a generalization of both BERT and GPT. The pre-trained RPT can already support several common data preparation tasks such as data cleaning, auto-completion and schema matching. Better still, RPT can be fine-tuned on a wide range of data preparation tasks, such as value normalization, data transformation, data annotation, etc. To complement RPT, we also discuss several appealing techniques such as collaborative training and few-shot learning for entity resolution, and few-shot learning and NLP question-answering for information extraction. In addition, we identify a series of research opportunities to advance the field of data preparation."
693,https://arxiv.org/abs/2009.11222,Representation Learning from Limited Educational Data with Crowdsourced Labels,"Representation learning has been proven to play an important role in the unprecedented success of machine learning models in numerous tasks, such as machine translation, face recognition and recommendation. The majority of existing representation learning approaches often require a large number of consistent and noise-free labels. However, due to various reasons such as budget constraints and privacy concerns, labels are very limited in many real-world scenarios. Directly applying standard representation learning approaches on small labeled data sets will easily run into over-fitting problems and lead to sub-optimal solutions. Even worse, in some domains such as education, the limited labels are usually annotated by multiple workers with diverse expertise, which yields noises and inconsistency in such crowdsourcing settings. In this paper, we propose a novel framework which aims to learn effective representations from limited data with crowdsourced labels. Specifically, we design a grouping based deep neural network to learn embeddings from a limited number of training samples and present a Bayesian confidence estimator to capture the inconsistency among crowdsourced labels. Furthermore, to expedite the training process, we develop a hard example selection procedure to adaptively pick up training examples that are misclassified by the model. Extensive experiments conducted on three real-world data sets demonstrate the superiority of our framework on learning representations from limited data with crowdsourced labels, comparing with various state-of-the-art baselines. In addition, we provide a comprehensive analysis on each of the main components of our proposed framework and also introduce the promising results it achieved in our real production to fully understand the proposed framework."
694,https://arxiv.org/abs/2008.12763,Relational Data Synthesis using Generative Adversarial Networks: A Design Space Exploration,"The proliferation of big data has brought an urgent demand for privacy-preserving data publishing. Traditional solutions to this demand have limitations on effectively balancing the tradeoff between privacy and utility of the released data. Thus, the database community and machine learning community have recently studied a new problem of relational data synthesis using generative adversarial networks (GAN) and proposed various algorithms. However, these algorithms are not compared under the same framework and thus it is hard for practitioners to understand GAN's benefits and limitations. To bridge the gaps, we conduct so far the most comprehensive experimental study that investigates applying GAN to relational data synthesis. We introduce a unified GAN-based framework and define a space of design solutions for each component in the framework, including neural network architectures and training strategies. We conduct extensive experiments to explore the design space and compare with traditional data synthesis approaches. Through extensive experiments, we find that GAN is very promising for relational data synthesis, and provide guidance for selecting appropriate design solutions. We also point out limitations of GAN and identify future research directions."
695,https://arxiv.org/abs/2005.07845,Neural Multi-Task Learning for Teacher Question Detection in Online Classrooms,"Asking questions is one of the most crucial pedagogical techniques used by teachers in class. It not only offers open-ended discussions between teachers and students to exchange ideas but also provokes deeper student thought and critical analysis. Providing teachers with such pedagogical feedback will remarkably help teachers improve their overall teaching quality over time in classrooms. Therefore, in this work, we build an end-to-end neural framework that automatically detects questions from teachers' audio recordings. Compared with traditional methods, our approach not only avoids cumbersome feature engineering, but also adapts to the task of multi-class question detection in real education scenarios. By incorporating multi-task learning techniques, we are able to strengthen the understanding of semantic relations among different types of questions. We conducted extensive experiments on the question detection tasks in a real-world online classroom dataset and the results demonstrate the superiority of our model in terms of various evaluation metrics."
696,https://arxiv.org/abs/2003.13212,Temporal Network Representation Learning via Historical Neighborhoods Aggregation,"Network embedding is an effective method to learn low-dimensional representations of nodes, which can be applied to various real-life applications such as visualization, node classification, and link prediction. Although significant progress has been made on this problem in recent years, several important challenges remain, such as how to properly capture temporal information in evolving networks. In practice, most networks are continually evolving. Some networks only add new edges or nodes such as authorship networks, while others support removal of nodes or edges such as internet data routing. If patterns exist in the changes of the network structure, we can better understand the relationships between nodes and the evolution of the network, which can be further leveraged to learn node representations with more meaningful information. In this paper, we propose the Embedding via Historical Neighborhoods Aggregation (EHNA) algorithm. More specifically, we first propose a temporal random walk that can identify relevant nodes in historical neighborhoods which have impact on edge formations. Then we apply a deep learning model which uses a custom attention mechanism to induce node embeddings that directly capture temporal information in the underlying feature representation. We perform extensive experiments on a range of real-world datasets, and the results demonstrate the effectiveness of our new approach in the network reconstruction task and the link prediction task."
697,https://arxiv.org/abs/1912.02947,Towards Interpretable and Learnable Risk Analysis for Entity Resolution,"Machine-learning-based entity resolution has been widely studied. However, some entity pairs may be mislabeled by machine learning models and existing studies do not study the risk analysis problem -- predicting and interpreting which entity pairs are mislabeled. In this paper, we propose an interpretable and learnable framework for risk analysis, which aims to rank the labeled pairs based on their risks of being mislabeled. We first describe how to automatically generate interpretable risk features, and then present a learnable risk model and its training technique. Finally, we empirically evaluate the performance of the proposed approach on real data. Our extensive experiments have shown that the learning risk model can identify the mislabeled pairs with considerably higher accuracy than the existing alternatives."
698,https://arxiv.org/abs/1911.01030,An End-to-End Deep RL Framework for Task Arrangement in Crowdsourcing Platforms,"In this paper, we propose a Deep Reinforcement Learning (RL) framework for task arrangement, which is a critical problem for the success of crowdsourcing platforms. Previous works conduct the personalized recommendation of tasks to workers via supervised learning methods. However, the majority of them only consider the benefit of either workers or requesters independently. In addition, they cannot handle the dynamic environment and may produce sub-optimal results. To address these issues, we utilize Deep Q-Network (DQN), an RL-based method combined with a neural network to estimate the expected long-term return of recommending a task. DQN inherently considers the immediate and future reward simultaneously and can be updated in real-time to deal with evolving data and dynamic changes. Furthermore, we design two DQNs that capture the benefit of both workers and requesters and maximize the profit of the platform. To learn value functions in DQN effectively, we also propose novel state representations, carefully design the computation of Q values, and predict transition probabilities and future states. Experiments on synthetic and real datasets demonstrate the superior performance of our framework."
699,https://arxiv.org/abs/1906.06574,Technical Report: Optimizing Human Involvement for Entity Matching and Consolidation,"An end-to-end data integration system requires human feedback in several phases, including collecting training data for entity matching, debugging the resulting clusters, confirming transformations applied on these clusters for data standardization, and finally, reducing each cluster to a single, canonical representation (or ""golden record""). The traditional wisdom is to sequentially apply the human feedback, obtained by asking specific questions, within some budget in each phase. However, these questions are highly correlated; the answer to one can influence the outcome of any of the phases of the pipeline. Hence, interleaving them has the potential to offer significant benefits.
  In this paper, we propose a human-in-the-loop framework that interleaves different types of questions to optimize human involvement. We propose benefit models to measure the quality improvement from asking a question, and cost models to measure the human time it takes to answer a question. We develop a question scheduling framework that judiciously selects questions to maximize the accuracy of the final golden records. Experimental results on three real-world datasets show that our holistic method significantly improves the quality of golden records from 70% to 90%, compared with the state-of-the-art approaches."
700,https://arxiv.org/abs/1906.02560,An End-to-End Learning-based Cost Estimator,"Cost and cardinality estimation is vital to query optimizer, which can guide the plan selection. However traditional empirical cost and cardinality estimation techniques cannot provide high-quality estimation, because they cannot capture the correlation between multiple columns. Recently the database community shows that the learning-based cardinality estimation is better than the empirical methods. However, existing learning-based methods have several limitations. Firstly, they can only estimate the cardinality, but cannot estimate the cost. Secondly, convolutional neural network (CNN) with average pooling is hard to represent complicated structures, e.g., complex predicates, and the model is hard to be generalized.
  To address these challenges, we propose an effective end-to-end learning-based cost estimation framework based on a tree-structured model, which can estimate both cost and cardinality simultaneously. To the best of our knowledge, this is the first end-to-end cost estimator based on deep learning. We propose effective feature extraction and encoding techniques, which consider both queries and physical operations in feature extraction. We embed these features into our tree-structured model. We propose an effective method to encode string values, which can improve the generalization ability for predicate matching. As it is prohibitively expensive to enumerate all string values, we design a patten-based method, which selects patterns to cover string values and utilizes the patterns to embed string values. We conducted experiments on real-world datasets and experimental results showed that our method outperformed baselines."
701,https://arxiv.org/abs/1905.04616,VizNet: Towards A Large-Scale Visualization Learning and Benchmarking Repository,"Researchers currently rely on ad hoc datasets to train automated visualization tools and evaluate the effectiveness of visualization designs. These exemplars often lack the characteristics of real-world datasets, and their one-off nature makes it difficult to compare different techniques. In this paper, we present VizNet: a large-scale corpus of over 31 million datasets compiled from open data repositories and online visualization galleries. On average, these datasets comprise 17 records over 3 dimensions and across the corpus, we find 51% of the dimensions record categorical data, 44% quantitative, and only 5% temporal. VizNet provides the necessary common baseline for comparing visualization design techniques, and developing benchmark models and algorithms for automating visual analysis. To demonstrate VizNet's utility as a platform for conducting online crowdsourced experiments at scale, we replicate a prior study assessing the influence of user task and data distribution on visual encoding effectiveness, and extend it by considering an additional task: outlier detection. To contend with running such studies at scale, we demonstrate how a metric of perceptual effectiveness can be learned from experimental results, and show its predictive power across test datasets."
702,https://arxiv.org/abs/1808.02593,Testing Shear Recovery with Field Distortion,"The tilt, rotation, or offset of each CCD with respect to the focal plane, as well as the distortion of the focal plane itself, cause shape distortions to the observed objects, an effect typically known as field distortion (FD). We point out that FD provides a unique way of quantifying the accuracy of cosmic shear measurement. The idea is to stack the shear estimators from galaxies that share similar FD-induced shape distortions. Given that the latter can be calculated with parameters from astrometric calibrations, the accuracy of the shear estimator can be directly tested on real images. It provides a way to calibrate the multiplicative and additive shear recovery biases within the scientific data itself, without requiring simulations or any external data sets. We use the CFHTLenS images to demonstrate the accuracy of the Fourier_Quad shear recovery method. We highlight some details in our image processing pipeline, including background removal, source identification and deblending, astrometric calibration, star selection for PSF reconstruction, noise reduction, etc.. We show that in the shear ranges of -0.005 < g_1 < 0.005 and -0.008 < g_2 < 0.008, the multiplicative biases are at the level of < 0.04. Slight additive biases on the order of 5E-4 (6 sigma) are identified for sources provided by the official CFHTLenS catalog (not using its shear catalog), but are minor (4 sigma) for source catalog generated by our Fourier_Quad pipeline."
703,https://arxiv.org/abs/1806.04968,Crowd-Powered Data Mining,"Many data mining tasks cannot be completely addressed by auto- mated processes, such as sentiment analysis and image classification. Crowdsourcing is an effective way to harness the human cognitive ability to process these machine-hard tasks. Thanks to public crowdsourcing platforms, e.g., Amazon Mechanical Turk and Crowd- Flower, we can easily involve hundreds of thousands of ordinary workers (i.e., the crowd) to address these machine-hard tasks. In this tutorial, we will survey and synthesize a wide spectrum of existing studies on crowd-powered data mining. We first give an overview of crowdsourcing, and then summarize the fundamental techniques, including quality control, cost control, and latency control, which must be considered in crowdsourced data mining. Next we review crowd-powered data mining operations, including classification, clustering, pattern mining, machine learning using the crowd (including deep learning, transfer learning and semi-supervised learning) and knowledge discovery. Finally, we provide the emerging challenges in crowdsourced data mining."
704,https://arxiv.org/abs/1805.04114,The Correspondence between Convergence Peaks from Weak Lensing and Massive Dark Matter Haloes,"The convergence peaks, constructed from galaxy shape measurement in weak lensing, is a powerful probe of cosmology as the peaks can be connected with the underlined dark matter haloes. However the capability of convergence peak statistic is affected by the noise in galaxy shape measurement, signal to noise ratio as well as the contribution from the projected mass distribution from the large-scale structures along the line of sight (LOS). In this paper we use the ray-tracing simulation on a curved sky to investigate the correspondence between the convergence peak and the dark matter haloes at the LOS. We find that, in case of no noise and for source galaxies at $z_{\rm s}=1$, more than $65\%$ peaks with $\text{SNR} \geq 3$ (signal to noise ratio) are related to more than one massive haloes with mass larger than $10^{13} {\rm M}_{\odot}$. Those massive haloes contribute $87.2\%$ to high peaks ($\text{SNR} \geq 5$) with the remaining contributions are from the large-scale structures. On the other hand, the peaks distribution is skewed by the noise in galaxy shape measurement, especially for lower SNR peaks. In the noisy field where the shape noise is modelled as a Gaussian distribution, about $60\%$ high peaks ($\text{SNR} \geq 5$) are true peaks and the fraction decreases to $20\%$ for lower peaks ($ 3 \leq \text{SNR} < 5$). Furthermore, we find that high peaks ($\text{SNR} \geq 5$) are dominated by very massive haloes larger than $10^{14} {\rm M}_{\odot}$."
705,https://arxiv.org/abs/1804.09997,PANDA: Facilitating Usable AI Development,"Recent advances in artificial intelligence (AI) and machine learning have created a general perception that AI could be used to solve complex problems, and in some situations over-hyped as a tool that can be so easily used. Unfortunately, the barrier to realization of mass adoption of AI on various business domains is too high because most domain experts have no background in AI. Developing AI applications involves multiple phases, namely data preparation, application modeling, and product deployment. The effort of AI research has been spent mostly on new AI models (in the model training stage) to improve the performance of benchmark tasks such as image recognition. Many other factors such as usability, efficiency and security of AI have not been well addressed, and therefore form a barrier to democratizing AI. Further, for many real world applications such as healthcare and autonomous driving, learning via huge amounts of possibility exploration is not feasible since humans are involved. In many complex applications such as healthcare, subject matter experts (e.g. Clinicians) are the ones who appreciate the importance of features that affect health, and their knowledge together with existing knowledge bases are critical to the end results. In this paper, we take a new perspective on developing AI solutions, and present a solution for making AI usable. We hope that this resolution will enable all subject matter experts (eg. Clinicians) to exploit AI like data scientists."
706,https://arxiv.org/abs/1803.07569,Prediction of Supernova Rates in Known Galaxy-galaxy Strong-lens Systems,"We propose a new strategy of finding strongly-lensed supernovae (SNe) by monitoring known galaxy-scale strong-lens systems. Strongly lensed SNe are potentially powerful tools for the study of cosmology, galaxy evolution, and stellar populations, but they are extremely rare. By targeting known strongly lensed starforming galaxies, our strategy significantly boosts the detection efficiency for lensed SNe compared to a blind search. As a reference sample, we compile the 128 galaxy-galaxy strong-lens systems from the Sloan Lens ACS Survey (SLACS), the SLACS for the Masses Survey, and the Baryon Oscillation Spectroscopic Survey Emission-Line Lens Survey. Within this sample, we estimate the rates of strongly-lensed Type Ia SN (SNIa) and core-collapse SN (CCSN) to be $1.23 \pm 0.12$ and $10.4 \pm 1.1$ events per year, respectively. The lensed SN images are expected to be widely separated with a median separation of 2 arcsec. Assuming a conservative fiducial lensing magnification factor of 5 for the most highly magnified SN image, we forecast that a monitoring program with a single-visit depth of 24.7 mag (5$σ$ point source, $r$ band) and a cadence of 5 days can detect 0.49 strongly-lensed SNIa event and 2.1 strongly-lensed CCSN events per year within this sample. Our proposed targeted-search strategy is particularly useful for prompt and efficient identifications and follow-up observations of strongly-lensed SN candidates. It also allows telescopes with small field of views and limited time to efficiently discover strongly-lensed SNe with a pencil-beam scanning strategy."
707,https://arxiv.org/abs/1802.02254,Trajectory-driven Influential Billboard Placement,"In this paper we propose and study the problem of trajectory-driven influential billboard placement: given a set of billboards $U$ (each with a location and a cost), a database of trajectories $\mathcal{T}$ and a budget $L$, find a set of billboards within the budget to influence the largest number of trajectories. One core challenge is to identify and reduce the overlap of the influence from different billboards to the same trajectories, while keeping the budget constraint into consideration. We show that this problem is NP-hard and present an enumeration based algorithm with $(1-1/e)$ approximation ratio. However, the enumeration should be very costly when $|U|$ is large. By exploiting the locality property of billboards' influence, we propose a partition-based framework PartSel. PartSel partitions $U$ into a set of small clusters, computes the locally influential billboards for each cluster, and merges them to generate the global solution. Since the local solutions can be obtained much more efficient than the global one, PartSel should reduce the computation cost greatly; meanwhile it achieves a non-trivial approximation ratio guarantee. Then we propose a LazyProbe method to further prune billboards with low marginal influence, while achieving the same approximation ratio as PartSel. Experiments on real datasets verify the efficiency and effectiveness of our methods."
708,https://arxiv.org/abs/1801.03941,Full-sky ray-tracing simulation of weak lensing using ELUCID simulations: exploring galaxy intrinsic alignment and cosmic shear correlations,"The intrinsic alignment of galaxies is an important systematic effect in weak-lensing surveys, which can affect the derived cosmological parameters. One direct way to distinguish different alignment models and quantify their effects on the measurement is to produce mocked weak-lensing surveys. In this work, we use full-sky ray-tracing technique to produce mock images of galaxies from the ELUCID $N$-body simulation run with the WMAP9 cosmology. In our model we assume that the shape of central elliptical galaxy follows that of the dark matter halo, and spiral galaxy follows the halo spin. Using the mocked galaxy images, a combination of galaxy intrinsic shape and the gravitational shear, we compare the predicted tomographic shear correlations to the results of KiDS and DLS. It is found that our predictions stay between the KiDS and DLS results. We rule out a model in which the satellite galaxies are radially aligned with the center galaxy, otherwise the shear-correlations on small scales are too high. Most important, we find that although the intrinsic alignment of spiral galaxies is very weak, they induce a positive correlation between the gravitational shear signal and the intrinsic galaxy orientation (GI). This is because the spiral galaxy is tangentially aligned with the nearby large-scale overdensity, contrary to the radial alignment of elliptical galaxy. Our results explain the origin of detected positive GI term from the weak-lensing surveys. We conclude that in future analysis, the GI model must include the dependence on galaxy types in more detail."
709,https://arxiv.org/abs/1801.01015,An accurate centroid algorithm for PSF reconstruction,"In this work, we present a novel centroiding method based on Fourier space Phase Fitting(FPF) for Point Spread Function(PSF) reconstruction. We generate two sets of simulations to test our method. The first set is generated by GalSim with elliptical Moffat profile and strong anisotropy which shifts the center of the PSF. The second set of simulation is drawn from CFHT i band stellar imaging data. We find non-negligible anisotropy from CFHT stellar images, which leads to $\sim$0.08 scatter in unit of pixels using polynomial fitting method Vakili and Hogg (2016). And we apply FPF method to estimate the centroid in real space, this scatter reduces to $\sim$0.04 in SNR=200 CFHT like sample. In low SNR (50 and 100) CFHT like samples, the background noise dominates the shifting of the centroid, therefore the scatter estimated from different methods are similar. We compare polynomial fitting and FPF using GalSim simulation with optical anisotropy. We find that in all SNR$\sim$50, 100 and 200) samples, FPF performs better than polynomial fitting by a factor of $\sim$3. In general, we suggest that in real observations there are anisotropy which shift the centroid, and FPF method is a better way to accurately locate it."
710,https://arxiv.org/abs/1712.09736,Removing the Impact of Correlated PSF Uncertainties in Weak Lensing,"Accurate reconstruction of the spatial distributions of the Point Spread Function (PSF) is crucial for high precision cosmic shear measurements. Nevertheless, current methods are not good at recovering the PSF fluctuations of high spatial frequencies. In general, the residual PSF fluctuations are spatially correlated, therefore can significantly contaminate the correlation functions of the weak lensing signals. We propose a method to correct for this contamination statistically, without any assumptions on the PSF and galaxy morphologies or their spatial distribution. We demonstrate our idea with the data from the W2 field of CFHTLenS."
711,https://arxiv.org/abs/1712.00883,ELUCID V. Lighting dark matter halos with galaxies,"In a recent study, using the distribution of galaxies in the north galactic pole of SDSS DR7 region enclosed in a 500$\mpch$ box, we carried out our ELUCID simulation (Wang et al. 2016, ELUCID III). Here we {\it light} the dark matter halos and subhalos in the reconstructed region in the simulation with galaxies in the SDSS observations using a novel {\it neighborhood} abundance matching method. Before we make use of thus established galaxy-subhalo connections in the ELUCID simulation to evaluate galaxy formation models, we set out to explore the reliability of such a link. For this purpose, we focus on the following a few aspects of galaxies: (1) the central-subhalo luminosity and mass relations; (2) the satellite fraction of galaxies; (3) the conditional luminosity function (CLF) and conditional stellar mass function (CSMF) of galaxies; and (4) the cross correlation functions between galaxies and the dark matter particles, most of which are measured separately for all, red and blue galaxy populations. We find that our neighborhood abundance matching method accurately reproduces the central-subhalo relations, satellite fraction, the CLFs and CSMFs and the biases of galaxies. These features ensure that thus established galaxy-subhalo connections will be very useful in constraining galaxy formation processes. And we provide some suggestions on the three levels of using the galaxy-subhalo pairs for galaxy formation constraints. The galaxy-subhalo links and the subhalo merger trees in the SDSS DR7 region extracted from our ELUCID simulation are available upon request."
712,https://arxiv.org/abs/1709.10436,Unsupervised String Transformation Learning for Entity Consolidation,"Data integration has been a long-standing challenge in data management with many applications. A key step in data integration is entity consolidation. It takes a collection of clusters of duplicate records as input and produces a single ""golden record"" for each cluster, which contains the canonical value for each attribute. Truth discovery and data fusion methods, as well as Master Data Management (MDM) systems, can be used for entity consolidation. However, to achieve better results, the variant values (i.e., values that are logically the same with different formats) in the clusters need to be consolidated before applying these methods.
  For this purpose, we propose a data-driven method to standardize the variant values based on two observations: (1) the variant values usually can be transformed to the same representation (e.g., ""Mary Lee"" and ""Lee, Mary"") and (2) the same transformation often appears repeatedly across different clusters (e.g., transpose the first and last name). Our approach first uses an unsupervised method to generate groups of value pairs that can be transformed in the same way (i.e., they share a transformation). Then the groups are presented to a human for verification and the approved ones are used to standardize the data. In a real-world dataset with 17,497 records, our method achieved 75% recall and 99.5% precision in standardizing variant values by asking a human 100 yes/no questions, which completely outperformed a state of the art data wrangling tool."
713,https://arxiv.org/abs/1708.02125,T-Crowd: Effective Crowdsourcing for Tabular Data,"Crowdsourcing employs human workers to solve computer-hard problems, such as data cleaning, entity resolution, and sentiment analysis. When crowdsourcing tabular data, e.g., the attribute values of an entity set, a worker's answers on the different attributes (e.g., the nationality and age of a celebrity star) are often treated independently. This assumption is not always true and can lead to suboptimal crowdsourcing performance. In this paper, we present the T-Crowd system, which takes into consideration the intricate relationships among tasks, in order to converge faster to their true values. Particularly, T-Crowd integrates each worker's answers on different attributes to effectively learn his/her trustworthiness and the true data values. The attribute relationship information is also used to guide task allocation to workers. Finally, T-Crowd seamlessly supports categorical and continuous attributes, which are the two main datatypes found in typical databases. Our extensive experiments on real and synthetic datasets show that T-Crowd outperforms state-of-the-art methods in terms of truth inference and reducing the cost of crowdsourcing."
714,https://arxiv.org/abs/1707.09002,"ELUCID IV: Galaxy Quenching and its Relation to Halo Mass, Environment, and Assembly Bias","We examine the quenched fraction of central and satellite galaxies as a function of galaxy stellar mass, halo mass, and the matter density of their large scale environment. Matter densities are inferred from our ELUCID simulation, a constrained simulation of local Universe sampled by SDSS, while halo masses and central/satellite classification are taken from the galaxy group catalog of Yang et al. The quenched fraction for the total population increases systematically with the three quantities. We find that the `environmental quenching efficiency', which quantifies the quenched fraction as function of halo mass, is independent of stellar mass. And this independence is the origin of the stellar mass-independence of density-based quenching efficiency, found in previous studies. Considering centrals and satellites separately, we find that the two populations follow similar correlations of quenching efficiency with halo mass and stellar mass, suggesting that they have experienced similar quenching processes in their host halo. We demonstrate that satellite quenching alone cannot account for the environmental quenching efficiency of the total galaxy population and the difference between the two populations found previously mainly arises from the fact that centrals and satellites of the same stellar mass reside, on average, in halos of different mass. After removing these halo-mass and stellar-mass effects, there remains a weak, but significant, residual dependence on environmental density, which is eliminated when halo assembly bias is taken into account. Our results therefore indicate that halo mass is the prime environmental parameter that regulates the quenching of both centrals and satellites."
715,https://arxiv.org/abs/1610.09828,Testing PSF Interpolation In Weak Lensing With Real Data,"Reconstruction of the point spread function (PSF) is a critical process in weak lensing measurement. We develop a real-data based and galaxy-oriented pipeline to compare the performances of various PSF reconstruction schemes. Making use of a large amount of the CFHTLenS data, the performances of three classes of interpolating schemes - polynomial, Kriging, and Shepard - are evaluated. We find that polynomial interpolations with optimal orders and domains perform the best. We quantify the effect of the residual PSF reconstruction error on shear recovery in terms of the multiplicative and additive biases, and their spatial correlations using the shear measurement method of Zhang et al. (2015). We find that the impact of PSF reconstruction uncertainty on the shear-shear correlation can be significantly reduced by cross correlating the shear estimators from different exposures. It takes only 0.2 stars (SNR > 100) per square arcmin on each exposure to reach the best performance of PSF interpolation, a requirement that is generally satisfied in the CFHTlenS data."
716,https://arxiv.org/abs/1604.07126,The Point Spread Function Reconstruction by Using Moffatlets - I,"The shear measurement is a crucial task in the current and the future weak lensing survey projects. And the reconstruction of the point spread function(PSF) is one of the essential steps. In this work, we present three different methods, including Gaussianlets, Moffatlets and EMPCA to quantify their efficiency on PSF reconstruction using four sets of simulated LSST star images. Gaussianlets and Moffatlets are two different sets of basis functions whose profiles are based on Gaussian and Moffat functions respectively. Expectation Maximization(EM) PCA is a statistical method performing iterative procedure to find principal components of an ensemble of star images. Our tests show that: 1) Moffatlets always perform better than Gaussianlets. 2) EMPCA is more compact and flexible, but the noise existing in the Principal Components (PCs) will contaminate the size and ellipticity of PSF while Moffatlets keeps them very well."
717,https://arxiv.org/abs/1510.06916,NXgraph: An Efficient Graph Processing System on a Single Machine,"Recent studies show that graph processing systems on a single machine can achieve competitive performance compared with cluster-based graph processing systems. In this paper, we present NXgraph, an efficient graph processing system on a single machine. With the abstraction of vertex intervals and edge sub-shards, we propose the Destination-Sorted Sub-Shard (DSSS) structure to store a graph. By dividing vertices and edges into intervals and sub-shards, NXgraph ensures graph data access locality and enables fine-grained scheduling. By sorting edges within each sub-shard according to their destination vertices, NXgraph reduces write conflicts among different threads and achieves a high degree of parallelism. Then, three updating strategies, i.e., Single-Phase Update (SPU), Double-Phase Update (DPU), and Mixed-Phase Update (MPU), are proposed in this paper. NXgraph can adaptively choose the fastest strategy for different graph problems according to the graph size and the available memory resources to fully utilize the memory space and reduce the amount of data transfer. All these three strategies exploit streamlined disk access pattern. Extensive experiments on three real-world graphs and five synthetic graphs show that NXgraph can outperform GraphChi, TurboGraph, VENUS, and GridGraph in various situations. Moreover, NXgraph, running on a single commodity PC, can finish an iteration of PageRank on the Twitter graph with 1.5 billion edges in 2.05 seconds; while PowerGraph, a distributed graph processing system, needs 3.6s to finish the same task."
718,https://arxiv.org/abs/1509.08430,Characterizing localized surface plasmons using electron energy-loss spectroscopy,"Electron energy-loss spectroscopy (EELS) offers a window to view nanoscale properties and processes. When performed in a scanning transmission electron microscope, EELS can simultaneously render images of nanoscale objects with sub-nanometer spatial resolution and correlate them with spectroscopic information of $\sim10 - 100$ meV spectral resolution. Consequently, EELS is a near-perfect tool for understanding the optical and electronic properties of individual and few-particle plasmonic metal nanoparticles assemblies, which are significant in a wide range of fields. This review presents an overview of basic plasmonics and EELS theory and highlights several recent noteworthy experiments involving the electron-beam interrogation of plasmonic metal nanoparticle systems."
719,https://arxiv.org/abs/1409.7472,The Expected Optimal Labeling Order Problem for Crowdsourced Joins and Entity Resolution,"In the SIGMOD 2013 conference, we published a paper extending our earlier work on crowdsourced entity resolution to improve crowdsourced join processing by exploiting transitive relationships [Wang et al. 2013]. The VLDB 2014 conference has a paper that follows up on our previous work [Vesdapunt et al., 2014], which points out and corrects a mistake we made in our SIGMOD paper. Specifically, in Section 4.2 of our SIGMOD paper, we defined the ""Expected Optimal Labeling Order"" (EOLO) problem, and proposed an algorithm for solving it. We incorrectly claimed that our algorithm is optimal. In their paper, Vesdapunt et al. show that the problem is actually NP-Hard, and based on that observation, propose a new algorithm to solve it. In this note, we would like to put the Vesdapunt et al. results in context, something we believe that their paper does not adequately do."
720,https://arxiv.org/abs/1408.6916,Leveraging Transitive Relations for Crowdsourced Joins,"The development of crowdsourced query processing systems has recently attracted a significant attention in the database community. A variety of crowdsourced queries have been investigated. In this paper, we focus on the crowdsourced join query which aims to utilize humans to find all pairs of matching objects from two collections. As a human-only solution is expensive, we adopt a hybrid human-machine approach which first uses machines to generate a candidate set of matching pairs, and then asks humans to label the pairs in the candidate set as either matching or non-matching. Given the candidate pairs, existing approaches will publish all pairs for verification to a crowdsourcing platform. However, they neglect the fact that the pairs satisfy transitive relations. As an example, if $o_1$ matches with $o_2$, and $o_2$ matches with $o_3$, then we can deduce that $o_1$ matches with $o_3$ without needing to crowdsource $(o_1, o_3)$. To this end, we study how to leverage transitive relations for crowdsourced joins. We propose a hybrid transitive-relations and crowdsourcing labeling framework which aims to crowdsource the minimum number of pairs to label all the candidate pairs. We prove the optimal labeling order in an ideal setting and propose a heuristic labeling order in practice. We devise a parallel labeling algorithm to efficiently crowdsource the pairs following the order. We evaluate our approaches in both simulated environment and a real crowdsourcing platform. Experimental results show that our approaches with transitive relations can save much more money and time than existing methods, with a little loss in the result quality."
721,https://arxiv.org/abs/1301.0360,Gravitational lensing effects on sub-millimetre galaxy counts,"We study the effects on the number counts of sub-millimetre galaxies due to gravitational lensing. We explore the effects on the magnification cross section due to halo density profiles, ellipticity and cosmological parameter (the power-spectrum normalisation $σ_8$). We show that the ellipticity does not strongly affect the magnification cross section in gravitational lensing while the halo radial profiles do. Since the baryonic cooling effect is stronger in galaxies than clusters, galactic haloes are more concentrated. In light of this, a new scenario of two halo population model is explored where galaxies are modeled as a singular isothermal sphere profile and clusters as a Navarro, Frenk and White (NFW) profile. We find the transition mass between the two has modest effects on the lensing probability. The cosmological parameter $σ_8$ alters the abundance of haloes and therefore affects our results. Compared with other methods, our model is simpler and more realistic. The conclusions of previous works is confirm that gravitational lensing is a natural explanation for the number count excess at the bright end."
722,https://arxiv.org/abs/1208.2448,Breaking Out The XML MisMatch Trap,"In keyword search, when user cannot get what she wants, query refinement is needed and reason can be various. We first give a thorough categorization of the reason, then focus on solving one category of query refinement problem in the context of XML keyword search, where what user searches for does not exist in the data. We refer to it as the MisMatch problem in this paper. Then we propose a practical way to detect the MisMatch problem and generate helpful suggestions to users. Our approach can be viewed as a post-processing job of query evaluation, and has three main features: (1) it adopts both the suggested queries and their sample results as the output to user, helping user judge whether the MisMatch problem is solved without consuming all query results; (2) it is portable in the sense that it can work with any LCA-based matching semantics and orthogonal to the choice of result retrieval method adopted; (3) it is lightweight in the way that it occupies a very small proportion of the whole query evaluation time. Extensive experiments on three real datasets verify the effectiveness, efficiency and scalability of our approach. An online XML keyword search engine called XClear that embeds the MisMatch problem detector and suggester has been built."
723,https://arxiv.org/abs/1205.6694,SEAL: Spatio-Textual Similarity Search,"Location-based services (LBS) have become more and more ubiquitous recently. Existing methods focus on finding relevant points-of-interest (POIs) based on users' locations and query keywords. Nowadays, modern LBS applications generate a new kind of spatio-textual data, regions-of-interest (ROIs), containing region-based spatial information and textual description, e.g., mobile user profiles with active regions and interest tags. To satisfy search requirements on ROIs, we study a new research problem, called spatio-textual similarity search: Given a set of ROIs and a query ROI, we find the similar ROIs by considering spatial overlap and textual similarity. Spatio-textual similarity search has many important applications, e.g., social marketing in location-aware social networks. It calls for an efficient search method to support large scales of spatio-textual data in LBS systems. To this end, we introduce a filter-and-verification framework to compute the answers. In the filter step, we generate signatures for the ROIs and the query, and utilize the signatures to generate candidates whose signatures are similar to that of the query. In the verification step, we verify the candidates and identify the final answers. To achieve high performance, we generate effective high-quality signatures, and devise efficient filtering algorithms as well as pruning techniques. Experimental results on real and synthetic datasets show that our method achieves high performance."
724,https://arxiv.org/abs/1203.0571,Fast Shape Estimation for Galaxies and Stars,"Model fitting is frequently used to determine the shape of galaxies and the point spread function, for examples, in weak lensing analyses or morphology studies aiming at probing the evolution of galaxies. However, the number of parameters in the model, as well as the number of objects, are often so large as to limit the use of model fitting for future large surveys. In this article, we propose a set of algorithms to speed up the fitting process. Our approach is divided into three distinctive steps: centroiding, ellipticity measurement, and profile fitting. We demonstrate that we can derive the position and ellipticity of an object analytically in the first two steps and thus leave only a small number of parameters to be derived through model fitting. The position, ellipticity, and shape parameters can then used in constructing orthonomal basis functions such as sérsiclets for better galaxy image reconstruction. We assess the efficiency and accuracy of the algorithms with simulated images. We have not taken into account the deconvolution of the point spread function, which most weak lensing analyses do."
725,https://arxiv.org/abs/1111.7171,PASS-JOIN: A Partition-based Method for Similarity Joins,"As an essential operation in data cleaning, the similarity join has attracted considerable attention from the database community. In this paper, we study string similarity joins with edit-distance constraints, which find similar string pairs from two large sets of strings whose edit distance is within a given threshold. Existing algorithms are efficient either for short strings or for long strings, and there is no algorithm that can efficiently and adaptively support both short strings and long strings. To address this problem, we propose a partition-based method called Pass-Join. Pass-Join partitions a string into a set of segments and creates inverted indices for the segments. Then for each string, Pass-Join selects some of its substrings and uses the selected substrings to find candidate pairs using the inverted indices. We devise efficient techniques to select the substrings and prove that our method can minimize the number of selected substrings. We develop novel pruning techniques to efficiently verify the candidate pairs. Experimental results show that our algorithms are efficient for both short strings and long strings, and outperform state-of-the-art methods on real datasets."
726,https://arxiv.org/abs/1008.3088,Mass reconstruction by gravitational shear and flexion,"Galaxy clusters are considered as excellent probes for cosmology. For that purpose, their mass needs to be measured and their structural properties needs to be understood. We propose a method for galaxy cluster mass reconstruction which combines information from strong lensing, weak lensing shear and flexion. We extend the weak lensing analysis to the inner parts of the cluster and, in particular, improve the resolution of substructure. We use simulations to show that the method recovers the mass density profiles of the cluster. We find that the weak lensing flexion is sensitive to substructure. After combining the flexion data into the joint weak and strong lensing analysis, we can resolve the cluster properties with substructures."
727,https://arxiv.org/abs/0906.3106,Analyzing the Accuracy of the Fitch Method for Reconstructing Ancestral States on Ultrametric Phylogenies,"Recurrence formulas are presented for studying the accuracy of the Fitch method for reconstructing the ancestral states in a given phylogenetic tree. As their applications, we analyze the convergence of the accuracy of reconstructing the root state in a complete binary tree of $2^n$ as $n$ goes to infinity and also give a lower bound on the accuracy of reconstructing the root state in an ultrametric tree."
728,https://arxiv.org/abs/astro-ph/0701801,Properties of Wide-separation Lensed Quasars by Clusters of Galaxies in the SDSS,"We use high-resolution N-body numerical simulations to study the number of predicted large-separation multiply-imaged systems produced by clusters of galaxies in the SDSS photometric and spectroscopic quasar samples. We incorporate the condensation of baryons at the centre of clusters by (artificially) adding a brightest central galaxy (BCG) as a truncated isothermal sphere. We make predictions in two flat cosmological models: a LCDM model with a matter density $Ω_0=0.3$, and $σ_8=0.9$ (LCDM0), and a model favoured by the WMAP three-year data with $Ω_0=0.238$, and $σ_8=0.74$ (WMAP3). We found that the predicted multiply-imaged quasars with separation >10"" is about 6.2 and 2.6 for the SDSS photometric (with an effective area 8000 deg$^2$) and spectroscopic (with an effective area 5000 deg$^2$) quasar samples respectively in the LCDM0 model; the predicted numbers of large-separation lensed quasars agree well with the observations. These numbers are reduced by a factor of 7 or more in the WMAP3 model, and are consistent with data at < 8% level. The predicted cluster lens redshift peaks around redshift 0.5, and 90% are between 0.3 and 1. We find that the BCG creates a central circular region, comparable to the Einstein ring of the BCG, where the central image disappears in the usual three-image and five-image configurations. If we include four image systems as an extreme case of five-image systems (with an infinitely demagnified central image), we find that 68% of the central images are fainter by a factor of 100 than the brightest image, and about 80% are within 1.5""of the BCG."
729,https://arxiv.org/abs/astro-ph/0701492,Detecting First Star Lyman-$α$ Spheres through Gravitational Telescopes,"Lyman-$α$ spheres, i.e. regions around the first stars which are illuminated by Lyman-$α$ photons and show 21cm absorption feature against the CMB, are smoking guns at the dawn of the reionization epoch. Though overwhelming radio foreground makes their detections extremely difficult, we pointed out that, strong gravitational lensing can significantly improve their observational feasibility. Since Lyman-$α$ spheres have ~10"" sizes, comparable to the caustic size of galaxy clusters, individual images of each strongly lensed Lyman-$α$ sphere often merge together and form single structures in the 21cm sky with irregular shapes. Using high-resolution N-body LCDM simulations, we found that the lensing probability to have magnification bigger than 10 is ~10^{-5}. This results in $\ga 10^6$ strongly lensed Lyman-$α$ spheres across the sky, which should be the primary targets for first detections of Lyman-$α$ spheres. Although the required total radio array collecting area for their detection is large (~100 km^2), the design of long fixed cylindrical reflectors can significantly reduce the total cost of such array to the level of the square kilometer array (SKA) and makes the detection of these very first objects feasible."
730,https://arxiv.org/abs/astro-ph/0608192,The giant arc statistics in the three year WMAP cosmological model,"We use high-resolution $N$-body simulations to investigate the optical depth of giant arcs with length-to-width ratio larger than 7.5 and 10 in the `standard' $\LCDM$ model with $σ_8=0.9$ and $Ω_{\rm m,0}=0.3$ and a model based on three-year Wilkinson Microwave Anisotropy Probe (WMAP) data. We find that, in dark-matter only simulations, the lensing probability in the three-year WMAP model (with $σ_8=0.74$ and $Ω_{\rm m,0}=0.238)$ decreases by a factor of $\sim 6$ compared with that in the `standard' $\LCDM$ model. The effects of baryonic cooling, star formation and feedbacks are uncertain, but we argue that baryons will only increase the the lensing cross-section by a moderate factor, $\sim 2$. We conclude that the low central value of $σ_8$ and $Ω_{\rm m,0}$ preferred by the WMAP three-year data may be too low to be compatible with observations if conventional assumptions of the background source population are correct."
731,https://arxiv.org/abs/astro-ph/0606006,"Arc sensitivity to cluster ellipticity, asymmetries and substructures","We investigate how ellipticity, asymmetries and substructures separately affect the ability of galaxy clusters to produce strong lensing events, i.e. gravitational arcs, and how they influence the arc morphologies and fluxes. This is important for those studies aiming, for example, at constraining cosmological parameters from statistical lensing, or at determining the inner structure of galaxy clusters through gravitational arcs. We do so by creating two-dimensional gradually smoothed, differently elliptical and asymmetric versions of some numerical models. On average, we find that the contributions of ellipticity, asymmetries and substructures amount to ~40%, ~10% and ~30% of the total strong lensing cross section, respectively. However, our analysis shows that substructures play a more important role in less elliptical and asymmetric clusters, even if located at large distances from the cluster centers (~1Mpc/h). Conversely, their effect is less important in highly asymmetric lenses. The morphology, position and flux of individual arcs are strongly affected by the presence of substructures in the clusters. Removing substructures on spatial scales <~50kpc/h, roughly corresponding to mass scales <~5 10^{10}M_\odot/h, alters the image multiplicity of ~35% of the sources used in the simulations and causes position shifts larger than 5'' for ~40% of the arcs longer than 5''. We conclude that any model for cluster lens cannot neglect the effects of ellipticity, asymmetries and substructures. On the other hand, the high sensitivity of gravitational arcs to deviations from regular, smooth and symmetric mass distributions suggests that strong gravitational lensing is potentially a powerfull tool to measure the level of substructures and asymmetries in clusters."
732,https://arxiv.org/abs/astro-ph/0603557,Smoothing Algorithms and High-order Singularities in Gravitational Lensing,"We propose a new smoothing method for obtaining surface densities from discrete particle positions from numerical simulations. This is an essential step for many applications in gravitational lensing. This method is based on the ``scatter'' interpretation of the discrete density field in the Smoothed Particle Hydrodynamics. We use Monte Carlo simulations of uniform density fields and one isothermal ellipsoid to empirically derive the noise properties, and best smoothing parameters (such as the number of nearest neighbors used). A cluster from high-resolution simulations is then used to assess the reality of high-order singularities such as swallowtails and butterflies in caustics, which are important for the interpretation of substructures in gravitational lenses. We also compare our method with the Delaunay tesselation field estimator using the galaxy studied by Bradac et al. (2004), and find good agreements. We show that higher order singularities are not only connected with bound subhaloes but also with the satellite streams. However, the presence of high-order singularities are sensitive to not only the fluctuation amplitude of the surface density, but also the detailed form of the underlying smooth lensing potential (such as ellipticity and external shear)."
733,https://arxiv.org/abs/astro-ph/0503172,Is the Number of Giant Arcs in LCDM Consistent With Observations?,"We use high-resolution N-body simulations to study the galaxy-cluster cross-sections and the abundance of giant arcs in the $Λ$CDM model. Clusters are selected from the simulations using the friends-of-friends method, and their cross-sections for forming giant arcs are analyzed. The background sources are assumed to follow a uniform ellipticity distribution from 0 to 0.5 and to have an area identical to a circular source with diameter $1\arcsec$. We find that the optical depth scales as the source redshift approximately as $τ_{1''} = 2.25 \times 10^{-6}/[1+(\zs/3.14)^{-3.42}]$ ($0.6<\zs<7$). The amplitude is about 50% higher for an effective source diameter of $0.5\arcsec$. The optimal lens redshift for giant arcs with the length-to-width ratio ($L/W$) larger than 10 increases from 0.3 for $\zs=1$, to 0.5 for $\zs=2$, and to 0.7-0.8 for $\zs>3$. The optical depth is sensitive to the source redshift, in qualitative agreement with Wambsganss et al. (2004). However, our overall optical depth appears to be only $\sim$ 10% to 70% of those from previous studies. The differences can be mostly explained by different power spectrum normalizations ($σ_8$) used and different ways of determining the $L/W$ ratio. Finite source size and ellipticity have modest effects on the optical depth. We also found that the number of highly magnified (with magnification $|μ|>10$) and ``undistorted'' images (with $L/W<3$) is comparable to the number of giant arcs with $|μ|>10$ and $L/W>10$. We conclude that our predicted rate of giant arcs may be lower than the observed rate, although the precise `discrepancy' is still unclear due to uncertainties both in theory and observations."
734,https://arxiv.org/abs/2305.15725,Learn to Not Link: Exploring NIL Prediction in Entity Linking,"Entity linking models have achieved significant success via utilizing pretrained language models to capture semantic features. However, the NIL prediction problem, which aims to identify mentions without a corresponding entity in the knowledge base, has received insufficient attention. We categorize mentions linking to NIL into Missing Entity and Non-Entity Phrase, and propose an entity linking dataset NEL that focuses on the NIL prediction problem. NEL takes ambiguous entities as seeds, collects relevant mention context in the Wikipedia corpus, and ensures the presence of mentions linking to NIL by human annotation and entity masking. We conduct a series of experiments with the widely used bi-encoder and cross-encoder entity linking models, results show that both types of NIL mentions in training data have a significant influence on the accuracy of NIL prediction. Our code and dataset can be accessed at https://github.com/solitaryzero/NIL_EL"
735,https://arxiv.org/abs/2305.15056,Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering,"Explainable question answering (XQA) aims to answer a given question and provide an explanation why the answer is selected. Existing XQA methods focus on reasoning on a single knowledge source, e.g., structured knowledge bases, unstructured corpora, etc. However, integrating information from heterogeneous knowledge sources is essential to answer complex questions. In this paper, we propose to leverage question decomposing for heterogeneous knowledge integration, by breaking down a complex question into simpler ones, and selecting the appropriate knowledge source for each sub-question. To facilitate reasoning, we propose a novel two-stage XQA framework, Reasoning over Hierarchical Question Decomposition Tree (RoHT). First, we build the Hierarchical Question Decomposition Tree (HQDT) to understand the semantics of a complex question; then, we conduct probabilistic reasoning over HQDT from root to leaves recursively, to aggregate heterogeneous knowledge at different tree levels and search for a best solution considering the decomposing and answering probabilities. The experiments on complex QA datasets KQA Pro and Musique show that our framework outperforms SOTA methods significantly, demonstrating the effectiveness of leveraging question decomposing for knowledge integration and our RoHT framework."
736,https://arxiv.org/abs/2305.13981,Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction,"The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial measurement of robustness. In this paper, we present the first benchmark that simulates the evaluation of open information extraction models in the real world, where the syntactic and expressive distributions under the same knowledge meaning may drift variously. We design and annotate a large-scale testbed in which each example is a knowledge-invariant clique that consists of sentences with structured knowledge of the same meaning but with different syntactic and expressive forms. By further elaborating the robustness metric, a model is judged to be robust if its performance is consistently accurate on the overall cliques. We perform experiments on typical models published in the last decade as well as a popular large language model, the results show that the existing successful models exhibit a frustrating degradation, with a maximum drop of 23.43 F1 score. Our resources and code will be publicly available."
737,https://arxiv.org/abs/2304.14106,ChatLog: Recording and Analyzing ChatGPT Across Time,"While there are abundant researches about evaluating ChatGPT on natural language understanding and generation tasks, few studies have investigated how ChatGPT's behavior changes over time. In this paper, we collect a coarse-to-fine temporal dataset called ChatLog, consisting of two parts that update monthly and daily: ChatLog-Monthly is a dataset of 38,730 question-answer pairs collected every month including questions from both the reasoning and classification tasks. ChatLog-Daily, on the other hand, consists of ChatGPT's responses to 1000 identical questions for long-form generation every day. We conduct comprehensive automatic and human evaluation to provide the evidence for the existence of ChatGPT evolving patterns. We further analyze the unchanged characteristics of ChatGPT over time by extracting its knowledge and linguistic features. We find some stable features to improve the robustness of a RoBERTa-based detector on new versions of ChatGPT. We will continuously maintain our project at https://github.com/THU-KEG/ChatLog."
738,https://arxiv.org/abs/2304.02205,MoocRadar: A Fine-grained and Multi-aspect Knowledge Repository for Improving Cognitive Student Modeling in MOOCs,"Student modeling, the task of inferring a student's learning characteristics through their interactions with coursework, is a fundamental issue in intelligent education. Although the recent attempts from knowledge tracing and cognitive diagnosis propose several promising directions for improving the usability and effectiveness of current models, the existing public datasets are still insufficient to meet the need for these potential solutions due to their ignorance of complete exercising contexts, fine-grained concepts, and cognitive labels. In this paper, we present MoocRadar, a fine-grained, multi-aspect knowledge repository consisting of 2,513 exercise questions, 5,600 knowledge concepts, and over 12 million behavioral records. Specifically, we propose a framework to guarantee a high-quality and comprehensive annotation of fine-grained concepts and cognitive labels. The statistical and experimental results indicate that our dataset provides the basis for the future improvements of existing methods. Moreover, to support the convenient usage for researchers, we release a set of tools for data querying, model adaption, and even the extension of our repository, which are now available at https://github.com/THU-KEG/MOOC-Radar."
739,https://arxiv.org/abs/2302.14401,GLM-Dialog: Noise-tolerant Pre-training for Knowledge-grounded Dialogue Generation,"We present GLM-Dialog, a large-scale language model (LLM) with 10B parameters capable of knowledge-grounded conversation in Chinese using a search engine to access the Internet knowledge. GLM-Dialog offers a series of applicable techniques for exploiting various external knowledge including both helpful and noisy knowledge, enabling the creation of robust knowledge-grounded dialogue LLMs with limited proper datasets. To evaluate the GLM-Dialog more fairly, we also propose a novel evaluation method to allow humans to converse with multiple deployed bots simultaneously and compare their performance implicitly instead of explicitly rating using multidimensional metrics.Comprehensive evaluations from automatic to human perspective demonstrate the advantages of GLM-Dialog comparing with existing open source Chinese dialogue models. We release both the model checkpoint and source code, and also deploy it as a WeChat application to interact with users. We offer our evaluation platform online in an effort to prompt the development of open source models and reliable dialogue evaluation systems. The additional easy-to-use toolkit that consists of short text entity linking, query generation, and helpful knowledge classification is also released to enable diverse applications. All the source code is available on Github."
740,https://arxiv.org/abs/2301.06841,Syntactically Robust Training on Partially-Observed Data for Open Information Extraction,"Open Information Extraction models have shown promising results with sufficient supervision. However, these models face a fundamental challenge that the syntactic distribution of training data is partially observable in comparison to the real world. In this paper, we propose a syntactically robust training framework that enables models to be trained on a syntactic-abundant distribution based on diverse paraphrase generation. To tackle the intrinsic problem of knowledge deformation of paraphrasing, two algorithms based on semantic similarity matching and syntactic tree walking are used to restore the expressionally transformed knowledge. The training framework can be generally applied to other syntactic partial observable domains. Based on the proposed framework, we build a new evaluation set called CaRB-AutoPara, a syntactically diverse dataset consistent with the real-world setting for validating the robustness of the models. Experiments including a thorough analysis show that the performance of the model degrades with the increase of the difference in syntactic distribution, while our framework gives a robust boundary. The source code is publicly available at https://github.com/qijimrc/RobustOIE."
741,https://arxiv.org/abs/2212.09567,Answering Complex Logical Queries on Knowledge Graphs via Query Computation Tree Optimization,"Answering complex logical queries on incomplete knowledge graphs is a challenging task, and has been widely studied. Embedding-based methods require training on complex queries, and cannot generalize well to out-of-distribution query structures. Recent work frames this task as an end-to-end optimization problem, and it only requires a pretrained link predictor. However, due to the exponentially large combinatorial search space, the optimal solution can only be approximated, limiting the final accuracy. In this work, we propose QTO (Query Computation Tree Optimization) that can efficiently find the exact optimal solution. QTO finds the optimal solution by a forward-backward propagation on the tree-like computation graph, i.e., query computation tree. In particular, QTO utilizes the independence encoded in the query computation tree to reduce the search space, where only local computations are involved during the optimization procedure. Experiments on 3 datasets show that QTO obtains state-of-the-art performance on complex query answering, outperforming previous best results by an average of 22%. Moreover, QTO can interpret the intermediate solutions for each of the one-hop atoms in the query with over 90% accuracy."
742,https://arxiv.org/abs/2211.07349,Finding Skill Neurons in Pre-trained Transformer-based Language Models,"Transformer-based pre-trained language models have demonstrated superior performance on various natural language processing tasks. However, it remains unclear how the skills required to handle these tasks distribute among model parameters. In this paper, we find that after prompt tuning for specific tasks, the activations of some neurons within pre-trained Transformers are highly predictive of the task labels. We dub these neurons skill neurons and confirm they encode task-specific skills by finding that: (1) Skill neurons are crucial for handling tasks. Performances of pre-trained Transformers on a task significantly drop when corresponding skill neurons are perturbed. (2) Skill neurons are task-specific. Similar tasks tend to have similar distributions of skill neurons. Furthermore, we demonstrate the skill neurons are most likely generated in pre-training rather than fine-tuning by showing that the skill neurons found with prompt tuning are also crucial for other fine-tuning methods freezing neuron weights, such as the adapter-based tuning and BitFit. We also explore the applications of skill neurons, including accelerating Transformers with network pruning and building better transferability indicators. These findings may promote further research on understanding Transformers. The source code can be obtained from https://github.com/THU-KEG/Skill-Neuron."
743,https://arxiv.org/abs/2211.07342,"MAVEN-ERE: A Unified Large-scale Dataset for Event Coreference, Temporal, Causal, and Subevent Relation Extraction","The diverse relationships among real-world events, including coreference, temporal, causal, and subevent relations, are fundamental to understanding natural languages. However, two drawbacks of existing datasets limit event relation extraction (ERE) tasks: (1) Small scale. Due to the annotation complexity, the data scale of existing datasets is limited, which cannot well train and evaluate data-hungry models. (2) Absence of unified annotation. Different types of event relations naturally interact with each other, but existing datasets only cover limited relation types at once, which prevents models from taking full advantage of relation interactions. To address these issues, we construct a unified large-scale human-annotated ERE dataset MAVEN-ERE with improved annotation schemes. It contains 103,193 event coreference chains, 1,216,217 temporal relations, 57,992 causal relations, and 15,841 subevent relations, which is larger than existing datasets of all the ERE tasks by at least an order of magnitude. Experiments show that ERE on MAVEN-ERE is quite challenging, and considering relation interactions with joint learning can improve performances. The dataset and source codes can be obtained from https://github.com/THU-KEG/MAVEN-ERE."
744,https://arxiv.org/abs/2211.05994,A Survey of Knowledge-Enhanced Pre-trained Language Models,"Pre-trained Language Models (PLMs) which are trained on large text corpus via self-supervised learning method, have yielded promising performance on various tasks in Natural Language Processing (NLP). However, though PLMs with huge parameters can effectively possess rich knowledge learned from massive training text and benefit downstream tasks at the fine-tuning stage, they still have some limitations such as poor reasoning ability due to the lack of external knowledge. Research has been dedicated to incorporating knowledge into PLMs to tackle these issues. In this paper, we present a comprehensive review of Knowledge-Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear insight into this thriving field. We introduce appropriate taxonomies respectively for Natural Language Understanding (NLU) and Natural Language Generation (NLG) to highlight these two main tasks of NLP. For NLU, we divide the types of knowledge into four categories: linguistic knowledge, text knowledge, knowledge graph (KG), and rule knowledge. The KE-PLMs for NLG are categorized into KG-based and retrieval-based methods. Finally, we point out some promising future directions of KE-PLMs."
745,https://arxiv.org/abs/2211.04079,COPEN: Probing Conceptual Knowledge in Pre-trained Language Models,"Conceptual knowledge is fundamental to human cognition and knowledge bases. However, existing knowledge probing works only focus on evaluating factual knowledge of pre-trained language models (PLMs) and ignore conceptual knowledge. Since conceptual knowledge often appears as implicit commonsense behind texts, designing probes for conceptual knowledge is hard. Inspired by knowledge representation schemata, we comprehensively evaluate conceptual knowledge of PLMs by designing three tasks to probe whether PLMs organize entities by conceptual similarities, learn conceptual properties, and conceptualize entities in contexts, respectively. For the tasks, we collect and annotate 24k data instances covering 393 concepts, which is COPEN, a COnceptual knowledge Probing bENchmark. Extensive experiments on different sizes and types of PLMs show that existing PLMs systematically lack conceptual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing human-like cognition in PLMs. COPEN and our codes are publicly released at https://github.com/THU-KEG/COPEN."
746,https://arxiv.org/abs/2210.12228,EDUKG: a Heterogeneous Sustainable K-12 Educational Knowledge Graph,"Web and artificial intelligence technologies, especially semantic web and knowledge graph (KG), have recently raised significant attention in educational scenarios. Nevertheless, subject-specific KGs for K-12 education still lack sufficiency and sustainability from knowledge and data perspectives. To tackle these issues, we propose EDUKG, a heterogeneous sustainable K-12 Educational Knowledge Graph. We first design an interdisciplinary and fine-grained ontology for uniformly modeling knowledge and resource in K-12 education, where we define 635 classes, 445 object properties, and 1314 datatype properties in total. Guided by this ontology, we propose a flexible methodology for interactively extracting factual knowledge from textbooks. Furthermore, we establish a general mechanism based on our proposed generalized entity linking system for EDUKG's sustainable maintenance, which can dynamically index numerous heterogeneous resources and data with knowledge topics in EDUKG. We further evaluate EDUKG to illustrate its sufficiency, richness, and variability. We publish EDUKG with more than 252 million entities and 3.86 billion triplets. Our code and data repository is now available at https://github.com/THU-KEG/EDUKG."
747,https://arxiv.org/abs/2210.05921,Step out of KG: Knowledge Graph Completion via Knowledgeable Retrieval and Reading Comprehension,"Knowledge graphs, as the cornerstone of many AI applications, usually face serious incompleteness problems. In recent years, there have been many efforts to study automatic knowledge graph completion (KGC), most of which use existing knowledge to infer new knowledge. However, in our experiments, we find that not all relations can be obtained by inference, which constrains the performance of existing models. To alleviate this problem, we propose a new model based on information retrieval and reading comprehension, namely IR4KGC. Specifically, we pre-train a knowledge-based information retrieval module that can retrieve documents related to the triples to be completed. Then, the retrieved documents are handed over to the reading comprehension module to generate the predicted answers. In experiments, we find that our model can well solve relations that cannot be inferred from existing knowledge, and achieve good results on KGC datasets."
748,https://arxiv.org/abs/2210.03949,ConstGCN: Constrained Transmission-based Graph Convolutional Networks for Document-level Relation Extraction,"Document-level relation extraction with graph neural networks faces a fundamental graph construction gap between training and inference - the golden graph structure only available during training, which causes that most methods adopt heuristic or syntactic rules to construct a prior graph as a pseudo proxy. In this paper, we propose $\textbf{ConstGCN}$, a novel graph convolutional network which performs knowledge-based information propagation between entities along with all specific relation spaces without any prior graph construction. Specifically, it updates the entity representation by aggregating information from all other entities along with each relation space, thus modeling the relation-aware spatial information. To control the information flow passing through the indeterminate relation spaces, we propose to constrain the propagation using transmitting scores learned from the Noise Contrastive Estimation between fact triples. Experimental results show that our method outperforms the previous state-of-the-art (SOTA) approaches on the DocRE dataset."
749,https://arxiv.org/abs/2210.01425,Unveiling the Black Box of PLMs with Semantic Anchors: Towards Interpretable Neural Semantic Parsing,"The recent prevalence of pretrained language models (PLMs) has dramatically shifted the paradigm of semantic parsing, where the mapping from natural language utterances to structured logical forms is now formulated as a Seq2Seq task. Despite the promising performance, previous PLM-based approaches often suffer from hallucination problems due to their negligence of the structural information contained in the sentence, which essentially constitutes the key semantics of the logical forms. Furthermore, most works treat PLM as a black box in which the generation process of the target logical form is hidden beneath the decoder modules, which greatly hinders the model's intrinsic interpretability. To address these two issues, we propose to incorporate the current PLMs with a hierarchical decoder network. By taking the first-principle structures as the semantic anchors, we propose two novel intermediate supervision tasks, namely Semantic Anchor Extraction and Semantic Anchor Alignment, for training the hierarchical decoders and probing the model intermediate representations in a self-adaptive manner alongside the fine-tuning process. We conduct intensive experiments on several semantic parsing benchmarks and demonstrate that our approach can consistently outperform the baselines. More importantly, by analyzing the intermediate representations of the hierarchical decoders, our approach also makes a huge step toward the intrinsic interpretability of PLMs in the domain of semantic parsing."
750,https://arxiv.org/abs/2209.13464,Information Extraction and Human-Robot Dialogue towards Real-life Tasks: A Baseline Study with the MobileCS Dataset,"Recently, there have merged a class of task-oriented dialogue (TOD) datasets collected through Wizard-of-Oz simulated games. However, the Wizard-of-Oz data are in fact simulated data and thus are fundamentally different from real-life conversations, which are more noisy and casual. Recently, the SereTOD challenge is organized and releases the MobileCS dataset, which consists of real-world dialog transcripts between real users and customer-service staffs from China Mobile. Based on the MobileCS dataset, the SereTOD challenge has two tasks, not only evaluating the construction of the dialogue system itself, but also examining information extraction from dialog transcripts, which is crucial for building the knowledge base for TOD. This paper mainly presents a baseline study of the two tasks with the MobileCS dataset. We introduce how the two baselines are constructed, the problems encountered, and the results. We anticipate that the baselines can facilitate exciting future research to build human-robot dialogue systems for real-life tasks."
751,https://arxiv.org/abs/2208.04708,Towards a General Pre-training Framework for Adaptive Learning in MOOCs,"Adaptive learning aims to stimulate and meet the needs of individual learners, which requires sophisticated system-level coordination of diverse tasks, including modeling learning resources, estimating student states, and making personalized recommendations. Existing deep learning methods have achieved great success over statistical models; however, they still lack generalization for diverse tasks and suffer from insufficient capacity since they are composed of highly-coupled task-specific architectures and rely on small-scale, coarse-grained recommendation scenarios. To realize the idea of general adaptive systems proposed in pedagogical theory, with the emerging pre-training techniques in NLP, we try to conduct a practical exploration on applying pre-training to adaptive learning, to propose a unified framework based on data observation and learning style analysis, properly leveraging heterogeneous learning elements. Through a series of downstream tasks of Learning Recommendation, Learning Resource Evaluation, Knowledge Tracing, and Dropout Prediction, we find that course structures, text, and knowledge are helpful for modeling and inherently coherent to student non-sequential learning behaviors and that indirectly relevant information included in the pre-training foundation can be shared across downstream tasks to facilitate effectiveness. We finally build a simplified systematic application of adaptive learning and reflect on the insights brought back to pedagogy. The source code and dataset will be released."
752,https://arxiv.org/abs/2207.02657,A Challenge on Semi-Supervised and Reinforced Task-Oriented Dialog Systems,"A challenge on Semi-Supervised and Reinforced Task-Oriented Dialog Systems, Co-located with EMNLP2022 SereTOD Workshop."
753,https://arxiv.org/abs/2205.12078,GraphQ IR: Unifying the Semantic Parsing of Graph Query Languages with One Intermediate Representation,"Subject to the huge semantic gap between natural and formal languages, neural semantic parsing is typically bottlenecked by its complexity of dealing with both input semantics and output syntax. Recent works have proposed several forms of supplementary supervision but none is generalized across multiple formal languages. This paper proposes a unified intermediate representation (IR) for graph query languages, named GraphQ IR. It has a natural-language-like expression that bridges the semantic gap and formally defined syntax that maintains the graph structure. Therefore, a neural semantic parser can more precisely convert user queries into GraphQ IR, which can be later losslessly compiled into various downstream graph query languages. Extensive experiments on several benchmarks including KQA Pro, Overnight, GrailQA, and MetaQA-Cypher under standard i.i.d., out-of-distribution, and low-resource settings validate GraphQ IR's superiority over the previous state-of-the-arts with a maximum 11% accuracy improvement."
754,https://arxiv.org/abs/2203.08556,LEVEN: A Large-Scale Chinese Legal Event Detection Dataset,"Recognizing facts is the most fundamental step in making judgments, hence detecting events in the legal documents is important to legal case analysis tasks. However, existing Legal Event Detection (LED) datasets only concern incomprehensive event types and have limited annotated data, which restricts the development of LED methods and their downstream applications. To alleviate these issues, we present LEVEN a large-scale Chinese LEgal eVENt detection dataset, with 8,116 legal documents and 150,977 human-annotated event mentions in 108 event types. Not only charge-related events, LEVEN also covers general events, which are critical for legal case understanding but neglected in existing LED datasets. To our knowledge, LEVEN is the largest LED dataset and has dozens of times the data scale of others, which shall significantly promote the training and evaluation of LED methods. The results of extensive experiments indicate that LED is challenging and needs further effort. Moreover, we simply utilize legal events as side information to promote downstream applications. The method achieves improvements of average 2.2 points precision in low-resource judgment prediction, and 1.5 points mean average precision in unsupervised case retrieval, which suggests the fundamentality of LED. The source code and dataset can be obtained from https://github.com/thunlp/LEVEN."
755,https://arxiv.org/abs/2201.12407,Schema-Free Dependency Parsing via Sequence Generation,"Dependency parsing aims to extract syntactic dependency structure or semantic dependency structure for sentences. Existing methods suffer the drawbacks of lacking universality or highly relying on the auxiliary decoder. To remedy these drawbacks, we propose to achieve universal and schema-free Dependency Parsing (DP) via Sequence Generation (SG) DPSG by utilizing only the pre-trained language model (PLM) without any auxiliary structures or parsing algorithms. We first explore different serialization designing strategies for converting parsing structures into sequences. Then we design dependency units and concatenate these units into the sequence for DPSG. Thanks to the high flexibility of the sequence generation, our DPSG can achieve both syntactic DP and semantic DP using a single model. By concatenating the prefix to indicate the specific schema with the sequence, our DPSG can even accomplish multi-schemata parsing. The effectiveness of our DPSG is demonstrated by the experiments on widely used DP benchmarks, i.e., PTB, CODT, SDP15, and SemEval16. DPSG achieves comparable results with the first-tier methods on all the benchmarks and even the state-of-the-art (SOTA) performance in CODT and SemEval16. This paper demonstrates our DPSG has the potential to be a new parsing paradigm. We will release our codes upon acceptance."
756,https://arxiv.org/abs/2201.06206,SQUIRE: A Sequence-to-sequence Framework for Multi-hop Knowledge Graph Reasoning,"Multi-hop knowledge graph (KG) reasoning has been widely studied in recent years to provide interpretable predictions on missing links with evidential paths. Most previous works use reinforcement learning (RL) based methods that learn to navigate the path towards the target entity. However, these methods suffer from slow and poor convergence, and they may fail to infer a certain path when there is a missing edge along the path. Here we present SQUIRE, the first Sequence-to-sequence based multi-hop reasoning framework, which utilizes an encoder-decoder Transformer structure to translate the query to a path. Our framework brings about two benefits: (1) It can learn and predict in an end-to-end fashion, which gives better and faster convergence; (2) Our Transformer model does not rely on existing edges to generate the path, and has the flexibility to complete missing edges along the path, especially in sparse KGs. Experiments on standard and sparse KGs show that our approach yields significant improvement over prior methods, while converging 4x-7x faster."
757,https://arxiv.org/abs/2201.00693,Multimodal Entity Tagging with Multimodal Knowledge Base,"To enhance research on multimodal knowledge base and multimodal information processing, we propose a new task called multimodal entity tagging (MET) with a multimodal knowledge base (MKB). We also develop a dataset for the problem using an existing MKB. In an MKB, there are entities and their associated texts and images. In MET, given a text-image pair, one uses the information in the MKB to automatically identify the related entity in the text-image pair. We solve the task by using the information retrieval paradigm and implement several baselines using state-of-the-art methods in NLP and CV. We conduct extensive experiments and make analyses on the experimental results. The results show that the task is challenging, but current technologies can achieve relatively high performance. We will release the dataset, code, and models for future research."
758,https://arxiv.org/abs/2111.06719,On Transferability of Prompt Tuning for Natural Language Processing,"Prompt tuning (PT) is a promising parameter-efficient method to utilize extremely large pre-trained language models (PLMs), which can achieve comparable performance to full-parameter fine-tuning by only tuning a few soft prompts. However, PT requires much more training time than fine-tuning. Intuitively, knowledge transfer can help to improve the efficiency. To explore whether we can improve PT via prompt transfer, we empirically investigate the transferability of soft prompts across different downstream tasks and PLMs in this work. We find that (1) in zero-shot setting, trained soft prompts can effectively transfer to similar tasks on the same PLM and also to other PLMs with a cross-model projector trained on similar tasks; (2) when used as initialization, trained soft prompts of similar tasks and projected prompts of other PLMs can significantly accelerate training and also improve the performance of PT. Moreover, to explore what decides prompt transferability, we investigate various transferability indicators and find that the overlapping rate of activated neurons strongly reflects the transferability, which suggests how the prompts stimulate PLMs is essential. Our findings show that prompt transfer is promising for improving PT, and further research shall focus more on prompts' stimulation to PLMs. The source code can be obtained from https://github.com/thunlp/Prompt-Transferability."
759,https://arxiv.org/abs/2110.07867,Exploring Universal Intrinsic Task Subspace via Prompt Tuning,"Why can pre-trained language models (PLMs) learn universal representations and effectively adapt to broad NLP tasks differing a lot superficially? In this work, we empirically find evidence indicating that the adaptations of PLMs to various few-shot tasks can be reparameterized as optimizing only a few free parameters in a unified low-dimensional intrinsic task subspace, which may help us understand why PLMs could easily adapt to various NLP tasks with small-scale data. To find such a subspace and examine its universality, we propose an analysis pipeline called intrinsic prompt tuning (IPT). Specifically, we resort to the recent success of prompt tuning and decompose the soft prompts of multiple NLP tasks into the same low-dimensional nonlinear subspace, then we learn to adapt the PLM to unseen data or tasks by only tuning parameters in this subspace. In the experiments, we study diverse few-shot NLP tasks and surprisingly find that in a 250-dimensional subspace found with 100 tasks, by only tuning 250 free parameters, we can recover 97% and 83% of the full prompt tuning performance for 100 seen tasks (using different training data) and 20 unseen tasks, respectively, showing great generalization ability of the found intrinsic task subspace. Besides being an analysis tool, IPT could further help us improve the prompt tuning stability."
760,https://arxiv.org/abs/2110.05743,Program Transfer for Answering Complex Questions over Knowledge Bases,"Program induction for answering complex questions over knowledge bases (KBs) aims to decompose a question into a multi-step program, whose execution against the KB produces the final answer. Learning to induce programs relies on a large number of parallel question-program pairs for the given KB. However, for most KBs, the gold program annotations are usually lacking, making learning difficult. In this paper, we propose the approach of program transfer, which aims to leverage the valuable program annotations on the rich-resourced KBs as external supervision signals to aid program induction for the low-resourced KBs that lack program annotations. For program transfer, we design a novel two-stage parsing framework with an efficient ontology-guided pruning strategy. First, a sketch parser translates the question into a high-level program sketch, which is the composition of functions. Second, given the question and sketch, an argument parser searches the detailed arguments from the KB for functions. During the searching, we incorporate the KB ontology to prune the search space. The experiments on ComplexWebQuestions and WebQuestionSP show that our method outperforms SOTA methods significantly, demonstrating the effectiveness of program transfer and our framework. Our codes and datasets can be obtained from https://github.com/THU-KEG/ProgramTransfer."
761,https://arxiv.org/abs/2109.01048,TravelBERT: Pre-training Language Model Incorporating Domain-specific Heterogeneous Knowledge into A Unified Representation,"Existing technologies expand BERT from different perspectives, e.g. designing different pre-training tasks, different semantic granularities and different model architectures. Few models consider expanding BERT from different text formats. In this paper, we propose a heterogeneous knowledge language model (HKLM), a unified pre-trained language model (PLM) for all forms of text, including unstructured text, semi-structured text and well-structured text. To capture the corresponding relations among these multi-format knowledge, our approach uses masked language model objective to learn word knowledge, uses triple classification objective and title matching objective to learn entity knowledge and topic knowledge respectively. To obtain the aforementioned multi-format text, we construct a corpus in the tourism domain and conduct experiments on 5 tourism NLP datasets. The results show that our approach outperforms the pre-training of plain text using only 1/4 of the data. The code, datasets, corpus and knowledge graph will be released."
762,https://arxiv.org/abs/2108.10604,Prompt-Learning for Fine-Grained Entity Typing,"As an effective approach to tune pre-trained language models (PLMs) for specific tasks, prompt-learning has recently attracted much attention from researchers. By using \textit{cloze}-style language prompts to stimulate the versatile knowledge of PLMs, prompt-learning can achieve promising results on a series of NLP tasks, such as natural language inference, sentiment classification, and knowledge probing. In this work, we investigate the application of prompt-learning on fine-grained entity typing in fully supervised, few-shot and zero-shot scenarios. We first develop a simple and effective prompt-learning pipeline by constructing entity-oriented verbalizers and templates and conducting masked language modeling. Further, to tackle the zero-shot regime, we propose a self-supervised strategy that carries out distribution-level optimization in prompt-learning to automatically summarize the information of entity types. Extensive experiments on three fine-grained entity typing benchmarks (with up to 86 classes) under fully supervised, few-shot and zero-shot settings show that prompt-learning methods significantly outperform fine-tuning baselines, especially when the training data is insufficient."
763,https://arxiv.org/abs/2108.02035,Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification,"Tuning pre-trained language models (PLMs) with task-specific prompts has been a promising approach for text classification. Particularly, previous studies suggest that prompt-tuning has remarkable superiority in the low-data scenario over the generic fine-tuning methods with extra classifiers. The core idea of prompt-tuning is to insert text pieces, i.e., template, to the input and transform a classification problem into a masked language modeling problem, where a crucial step is to construct a projection, i.e., verbalizer, between a label space and a label word space. A verbalizer is usually handcrafted or searched by gradient descent, which may lack coverage and bring considerable bias and high variances to the results. In this work, we focus on incorporating external knowledge into the verbalizer, forming a knowledgeable prompt-tuning (KPT), to improve and stabilize prompt-tuning. Specifically, we expand the label word space of the verbalizer using external knowledge bases (KBs) and refine the expanded label word space with the PLM itself before predicting with the expanded label word space. Extensive experiments on zero and few-shot text classification tasks demonstrate the effectiveness of knowledgeable prompt-tuning."
764,https://arxiv.org/abs/2108.01387,Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion,"We present InferWiki, a Knowledge Graph Completion (KGC) dataset that improves upon existing benchmarks in inferential ability, assumptions, and patterns. First, each testing sample is predictable with supportive data in the training set. To ensure it, we propose to utilize rule-guided train/test generation, instead of conventional random split. Second, InferWiki initiates the evaluation following the open-world assumption and improves the inferential difficulty of the closed-world assumption, by providing manually annotated negative and unknown triples. Third, we include various inference patterns (e.g., reasoning path length and types) for comprehensive evaluation. In experiments, we curate two settings of InferWiki varying in sizes and structures, and apply the construction process on CoDEx as comparative datasets. The results and empirical analyses demonstrate the necessity and high-quality of InferWiki. Nevertheless, the performance gap among various inferential assumptions and patterns presents the difficulty and inspires future research direction. Our datasets can be found in https://github.com/TaoMiner/inferwiki"
765,https://arxiv.org/abs/2106.15167,Learning from Miscellaneous Other-Class Words for Few-shot Named Entity Recognition,"Few-shot Named Entity Recognition (NER) exploits only a handful of annotations to identify and classify named entity mentions. Prototypical network shows superior performance on few-shot NER. However, existing prototypical methods fail to differentiate rich semantics in other-class words, which will aggravate overfitting under few shot scenario. To address the issue, we propose a novel model, Mining Undefined Classes from Other-class (MUCO), that can automatically induce different undefined classes from the other class to improve few-shot NER. With these extra-labeled undefined classes, our method will improve the discriminative ability of NER classifier and enhance the understanding of predefined classes with stand-by semantic knowledge. Experimental results demonstrate that our model outperforms five state-of-the-art models in both 1-shot and 5-shots settings on four NER benchmarks. We will release the code upon acceptance. The source code is released on https: //github.com/shuaiwa16/OtherClassNER.git."
766,https://arxiv.org/abs/2106.15135,TWAG: A Topic-Guided Wikipedia Abstract Generator,"Wikipedia abstract generation aims to distill a Wikipedia abstract from web sources and has met significant success by adopting multi-document summarization techniques. However, previous works generally view the abstract as plain text, ignoring the fact that it is a description of a certain entity and can be decomposed into different topics. In this paper, we propose a two-stage model TWAG that guides the abstract generation with topical information. First, we detect the topic of each input paragraph with a classifier trained on existing Wikipedia articles to divide input documents into different topics. Then, we predict the topic distribution of each abstract sentence, and decode the sentence from topic-aware representations with a Pointer-Generator network. We evaluate our model on the WikiCatSum dataset, and the results show that \modelnames outperforms various existing baselines and is capable of generating comprehensive abstracts. Our code and dataset can be accessed at \url{https://github.com/THU-KEG/TWAG}"
767,https://arxiv.org/abs/2106.04174,Interpretable and Low-Resource Entity Matching via Decoupling Feature Learning from Decision Making,"Entity Matching (EM) aims at recognizing entity records that denote the same real-world object. Neural EM models learn vector representation of entity descriptions and match entities end-to-end. Though robust, these methods require many resources for training, and lack of interpretability. In this paper, we propose a novel EM framework that consists of Heterogeneous Information Fusion (HIF) and Key Attribute Tree (KAT) Induction to decouple feature representation from matching decision. Using self-supervised learning and mask mechanism in pre-trained language modeling, HIF learns the embeddings of noisy attribute values by inter-attribute attention with unlabeled data. Using a set of comparison features and a limited amount of annotated data, KAT Induction learns an efficient decision tree that can be interpreted by generating entity matching rules whose structure is advocated by domain experts. Experiments on 6 public datasets and 3 industrial datasets show that our method is highly efficient and outperforms SOTA EM models in most cases. Our codes and datasets can be obtained from https://github.com/THU-KEG/HIF-KAT."
768,https://arxiv.org/abs/2105.14485,CLEVE: Contrastive Pre-training for Event Extraction,"Event extraction (EE) has considerably benefited from pre-trained language models (PLMs) by fine-tuning. However, existing pre-training methods have not involved modeling event characteristics, resulting in the developed EE models cannot take full advantage of large-scale unsupervised data. To this end, we propose CLEVE, a contrastive pre-training framework for EE to better learn event knowledge from large unsupervised data and their semantic structures (e.g. AMR) obtained with automatic parsers. CLEVE contains a text encoder to learn event semantics and a graph encoder to learn event structures respectively. Specifically, the text encoder learns event semantic representations by self-supervised contrastive learning to represent the words of the same events closer than those unrelated words; the graph encoder learns event structure representations by graph contrastive pre-training on parsed event-related semantic structures. The two complementary representations then work together to improve both the conventional supervised EE and the unsupervised ""liberal"" EE, which requires jointly extracting events and discovering event schemata without any annotated data. Experiments on ACE 2005 and MAVEN datasets show that CLEVE achieves significant improvements, especially in the challenging unsupervised setting. The source code and pre-trained checkpoints can be obtained from https://github.com/THU-KEG/CLEVE."
769,https://arxiv.org/abs/2104.07302,TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph,"Multi-hop Question Answering (QA) is a challenging task because it requires precise reasoning with entity relations at every step towards the answer. The relations can be represented in terms of labels in knowledge graph (e.g., \textit{spouse}) or text in text corpus (e.g., \textit{they have been married for 26 years}). Existing models usually infer the answer by predicting the sequential relation path or aggregating the hidden graph features. The former is hard to optimize, and the latter lacks interpretability. In this paper, we propose TransferNet, an effective and transparent model for multi-hop QA, which supports both label and text relations in a unified framework. TransferNet jumps across entities at multiple steps. At each step, it attends to different parts of the question, computes activated scores for relations, and then transfer the previous entity scores along activated relations in a differentiable way. We carry out extensive experiments on three datasets and demonstrate that TransferNet surpasses the state-of-the-art models by a large margin. In particular, on MetaQA, it achieves 100\% accuracy in 2-hop and 3-hop questions. By qualitative analysis, we show that TransferNet has transparent and interpretable intermediate results."
770,https://arxiv.org/abs/2104.06751,Is Multi-Hop Reasoning Really Explainable? Towards Benchmarking Reasoning Interpretability,"Multi-hop reasoning has been widely studied in recent years to obtain more interpretable link prediction. However, we find in experiments that many paths given by these models are actually unreasonable, while little works have been done on interpretability evaluation for them. In this paper, we propose a unified framework to quantitatively evaluate the interpretability of multi-hop reasoning models so as to advance their development. In specific, we define three metrics including path recall, local interpretability, and global interpretability for evaluation, and design an approximate strategy to calculate them using the interpretability scores of rules. Furthermore, we manually annotate all possible rules and establish a Benchmark to detect the Interpretability of Multi-hop Reasoning (BIMR). In experiments, we run nine baselines on our benchmark. The experimental results show that the interpretability of current multi-hop reasoning models is less satisfactory and is still far from the upper bound given by our benchmark. Moreover, the rule-based models outperform the multi-hop reasoning models in terms of performance and interpretability, which points to a direction for future research, i.e., we should investigate how to better incorporate rule information into the multi-hop reasoning model. Our codes and datasets can be obtained from https://github.com/THU-KEG/BIMR."
771,https://arxiv.org/abs/2010.03249,"Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment","Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs. GNN-based EA methods present promising performances by modeling the KG structure defined by relation triples. However, attribute triples can also provide crucial alignment signal but have not been well explored yet. In this paper, we propose to utilize an attributed value encoder and partition the KG into subgraphs to model the various types of attribute triples efficiently. Besides, the performances of current EA methods are overestimated because of the name-bias of existing EA datasets. To make an objective evaluation, we propose a hard experimental setting where we select equivalent entity pairs with very different names as the test set. Under both the regular and hard settings, our method achieves significant improvements ($5.10\%$ on average Hits@$1$ in DBP$15$k) over $12$ baselines in cross-lingual and monolingual datasets. Ablation studies on different subgraphs and a case study about attribute types further demonstrate the effectiveness of our method. Source code and data can be found at https://github.com/thunlp/explore-and-evaluate."
772,https://arxiv.org/abs/2010.01899,Dynamic Anticipation and Completion for Multi-Hop Reasoning over Sparse Knowledge Graph,"Multi-hop reasoning has been widely studied in recent years to seek an effective and interpretable method for knowledge graph (KG) completion. Most previous reasoning methods are designed for dense KGs with enough paths between entities, but cannot work well on those sparse KGs that only contain sparse paths for reasoning. On the one hand, sparse KGs contain less information, which makes it difficult for the model to choose correct paths. On the other hand, the lack of evidential paths to target entities also makes the reasoning process difficult. To solve these problems, we propose a multi-hop reasoning model named DacKGR over sparse KGs, by applying novel dynamic anticipation and completion strategies: (1) The anticipation strategy utilizes the latent prediction of embedding-based models to make our model perform more potential path search over sparse KGs. (2) Based on the anticipation information, the completion strategy dynamically adds edges as additional actions during the path search, which further alleviates the sparseness problem of KGs. The experimental results on five datasets sampled from Freebase, NELL and Wikidata show that our method outperforms state-of-the-art baselines. Our codes and datasets can be obtained from https://github.com/THU-KEG/DacKGR"
773,https://arxiv.org/abs/2007.07320,Learning Syllogism with Euler Neural-Networks,"Traditional neural networks represent everything as a vector, and are able to approximate a subset of logical reasoning to a certain degree. As basic logic relations are better represented by topological relations between regions, we propose a novel neural network that represents everything as a ball and is able to learn topological configuration as an Euler diagram. So comes the name Euler Neural-Network (ENN). The central vector of a ball is a vector that can inherit representation power of traditional neural network. ENN distinguishes four spatial statuses between balls, namely, being disconnected, being partially overlapped, being part of, being inverse part of. Within each status, ideal values are defined for efficient reasoning. A novel back-propagation algorithm with six Rectified Spatial Units (ReSU) can optimize an Euler diagram representing logical premises, from which logical conclusion can be deduced. In contrast to traditional neural network, ENN can precisely represent all 24 different structures of Syllogism. Two large datasets are created: one extracted from WordNet-3.0 covers all types of Syllogism reasoning, the other extracted all family relations from DBpedia. Experiment results approve the superior power of ENN in logical representation and reasoning. Datasets and source code are available upon request."
774,https://arxiv.org/abs/2007.03875,KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base,"Complex question answering over knowledge base (Complex KBQA) is challenging because it requires various compositional reasoning capabilities, such as multi-hop inference, attribute comparison, set operation. Existing benchmarks have some shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are poor in diversity or scale. To this end, we introduce KQA Pro, a dataset for Complex KBQA including ~120K diverse natural language questions. We introduce a compositional and interpretable programming language KoPL to represent the reasoning process of complex questions. For each question, we provide the corresponding KoPL program and SPARQL query, so that KQA Pro serves for both KBQA and semantic parsing tasks. Experimental results show that SOTA KBQA methods cannot achieve promising results on KQA Pro as on current datasets, which suggests that KQA Pro is challenging and Complex KBQA requires further research efforts. We also treat KQA Pro as a diagnostic dataset for testing multiple reasoning skills, conduct a thorough evaluation of existing models and discuss further directions for Complex KBQA. Our codes and datasets can be obtained from https://github.com/shijx12/KQAPro_Baselines."
775,https://arxiv.org/abs/2004.13631,"KACC: A Multi-task Benchmark for Knowledge Abstraction, Concretization and Completion","A comprehensive knowledge graph (KG) contains an instance-level entity graph and an ontology-level concept graph. The two-view KG provides a testbed for models to ""simulate"" human's abilities on knowledge abstraction, concretization, and completion (KACC), which are crucial for human to recognize the world and manage learned knowledge. Existing studies mainly focus on partial aspects of KACC. In order to promote thorough analyses for KACC abilities of models, we propose a unified KG benchmark by improving existing benchmarks in terms of dataset scale, task coverage, and difficulty. Specifically, we collect new datasets that contain larger concept graphs, abundant cross-view links as well as dense entity graphs. Based on the datasets, we propose novel tasks such as multi-hop knowledge abstraction (MKA), multi-hop knowledge concretization (MKC) and then design a comprehensive benchmark. For MKA and MKC tasks, we further annotate multi-hop hierarchical triples as harder samples. The experimental results of existing methods demonstrate the challenges of our benchmark. The resource is available at https://github.com/thunlp/KACC."
776,https://arxiv.org/abs/2004.13590,MAVEN: A Massive General Domain Event Detection Dataset,"Event detection (ED), which means identifying event trigger words and classifying event types, is the first and most fundamental step for extracting event knowledge from plain text. Most existing datasets exhibit the following issues that limit further development of ED: (1) Data scarcity. Existing small-scale datasets are not sufficient for training and stably benchmarking increasingly sophisticated modern neural methods. (2) Low coverage. Limited event types of existing datasets cannot well cover general-domain events, which restricts the applications of ED models. To alleviate these problems, we present a MAssive eVENt detection dataset (MAVEN), which contains 4,480 Wikipedia documents, 118,732 event mention instances, and 168 event types. MAVEN alleviates the data scarcity problem and covers much more general event types. We reproduce the recent state-of-the-art ED models and conduct a thorough evaluation on MAVEN. The experimental results show that existing ED methods cannot achieve promising results on MAVEN as on the small datasets, which suggests that ED in the real world remains a challenging task and requires further research efforts. We also discuss further directions for general domain ED with empirical analyses. The source code and dataset can be obtained from https://github.com/THU-KEG/MAVEN-dataset."
777,https://arxiv.org/abs/1911.06136,KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation,"Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagE Representation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M, a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from https://github.com/THU-KEG/KEPLER."
778,https://arxiv.org/abs/1909.07739,Course Concept Expansion in MOOCs with External Knowledge and Interactive Game,"As Massive Open Online Courses (MOOCs) become increasingly popular, it is promising to automatically provide extracurricular knowledge for MOOC users. Suffering from semantic drifts and lack of knowledge guidance, existing methods can not effectively expand course concepts in complex MOOC environments. In this paper, we first build a novel boundary during searching for new concepts via external knowledge base and then utilize heterogeneous features to verify the high-quality results. In addition, to involve human efforts in our model, we design an interactive optimization mechanism based on a game. Our experiments on the four datasets from Coursera and XuetangX show that the proposed method achieves significant improvements(+0.19 by MAP) over existing methods. The source code and datasets have been published."
779,https://arxiv.org/abs/1908.11513,Adapting Meta Knowledge Graph Information for Multi-Hop Reasoning over Few-Shot Relations,"Multi-hop knowledge graph (KG) reasoning is an effective and explainable method for predicting the target entity via reasoning paths in query answering (QA) task. Most previous methods assume that every relation in KGs has enough training triples, regardless of those few-shot relations which cannot provide sufficient triples for training robust reasoning models. In fact, the performance of existing multi-hop reasoning methods drops significantly on few-shot relations. In this paper, we propose a meta-based multi-hop reasoning method (Meta-KGR), which adopts meta-learning to learn effective meta parameters from high-frequency relations that could quickly adapt to few-shot relations. We evaluate Meta-KGR on two public datasets sampled from Freebase and NELL, and the experimental results show that Meta-KGR outperforms the current state-of-the-art methods in few-shot scenarios. Our code and datasets can be obtained from https://github.com/ THU-KEG/MetaKGR."
780,https://arxiv.org/abs/1908.09898,Multi-Channel Graph Neural Network for Entity Alignment,"Entity alignment typically suffers from the issues of structural heterogeneity and limited seed alignments. In this paper, we propose a novel Multi-channel Graph Neural Network model (MuGNN) to learn alignment-oriented knowledge graph (KG) embeddings by robustly encoding two KGs via multiple channels. Each channel encodes KGs via different relation weighting schemes with respect to self-attention towards KG completion and cross-KG attention for pruning exclusive entities respectively, which are further combined via pooling techniques. Moreover, we also infer and transfer rule knowledge for completing two KGs consistently. MuGNN is expected to reconcile the structural differences of two KGs, and thus make better use of seed alignments. Extensive experiments on five publicly available datasets demonstrate our superior performance (5% Hits@1 up on average)."
781,https://arxiv.org/abs/1812.01855,Explainable and Explicit Visual Reasoning over Scene Graphs,"We aim to dismantle the prevalent black-box neural architectures used in complex visual reasoning tasks, into the proposed eXplainable and eXplicit Neural Modules (XNMs), which advance beyond existing neural module networks towards using scene graphs --- objects as nodes and the pairwise relationships as edges --- for explainable and explicit reasoning with structured knowledge. XNMs allow us to pay more attention to teach machines how to ""think"", regardless of what they ""look"". As we will show in the paper, by using scene graphs as an inductive bias, 1) we can design XNMs in a concise and flexible fashion, i.e., XNMs merely consist of 4 meta-types, which significantly reduce the number of parameters by 10 to 100 times, and 2) we can explicitly trace the reasoning-flow in terms of graph attentions. XNMs are so generic that they support a wide range of scene graph implementations with various qualities. For example, when the graphs are detected perfectly, XNMs achieve 100% accuracy on both CLEVR and CLEVR CoGenT, establishing an empirical performance upper-bound for visual reasoning; when the graphs are noisily detected from real-world images, XNMs are still robust to achieve a competitive 67.5% accuracy on VQAv2.0, surpassing the popular bag-of-objects attention models without graph structures."
782,https://arxiv.org/abs/1811.10776,Joint Representation Learning of Cross-lingual Words and Entities via Attentive Distant Supervision,"Joint representation learning of words and entities benefits many NLP tasks, but has not been well explored in cross-lingual settings. In this paper, we propose a novel method for joint representation learning of cross-lingual words and entities. It captures mutually complementary knowledge, and enables cross-lingual inferences among knowledge bases and texts. Our method does not require parallel corpora, and automatically generates comparable data via distant supervision using multi-lingual knowledge bases. We utilize two types of regularizers to align cross-lingual words and entities, and design knowledge attention and cross-lingual attention to further reduce noises. We conducted a series of experiments on three tasks: word translation, entity relatedness, and cross-lingual entity linking. The results, both qualitatively and quantitatively, demonstrate the significance of our method."
783,https://arxiv.org/abs/1811.08603,Neural Collective Entity Linking,"Entity Linking aims to link entity mentions in texts to knowledge bases, and neural models have achieved recent success in this task. However, most existing methods rely on local contexts to resolve entities independently, which may usually fail due to the data sparsity of local information. To address this issue, we propose a novel neural model for collective entity linking, named as NCEL. NCEL applies Graph Convolutional Network to integrate both local contextual features and global coherence information for entity linking. To improve the computation efficiency, we approximately perform graph convolution on a subgraph of adjacent entity mentions instead of those in the entire text. We further introduce an attention scheme to improve the robustness of NCEL to data noise and train the model on Wikipedia hyperlinks to avoid overfitting and domain bias. In experiments, we evaluate NCEL on five publicly available datasets to verify the linking performance as well as generalization ability. We also conduct an extensive analysis of time complexity, the impact of key modules, and qualitative results, which demonstrate the effectiveness and efficiency of our proposed method."
784,https://arxiv.org/abs/1811.04588,Differentiating Concepts and Instances for Knowledge Graph Embedding,"Concepts, which represent a group of different instances sharing common properties, are essential information in knowledge representation. Most conventional knowledge embedding methods encode both entities (concepts and instances) and relations as vectors in a low dimensional semantic space equally, ignoring the difference between concepts and instances. In this paper, we propose a novel knowledge graph embedding model named TransC by differentiating concepts and instances. Specifically, TransC encodes each concept in knowledge graph as a sphere and each instance as a vector in the same semantic space. We use the relative positions to model the relations between concepts and instances (i.e., instanceOf), and the relations between concepts and sub-concepts (i.e., subClassOf). We evaluate our model on both link prediction and triple classification tasks on the dataset based on YAGO. Experimental results show that TransC outperforms state-of-the-art methods, and captures the semantic transitivity for instanceOf and subClassOf relation. Our codes and datasets can be obtained from https:// github.com/davidlvxin/TransC."
785,https://arxiv.org/abs/1811.02394,DeepChannel: Salience Estimation by Contrastive Learning for Extractive Document Summarization,"We propose DeepChannel, a robust, data-efficient, and interpretable neural model for extractive document summarization. Given any document-summary pair, we estimate a salience score, which is modeled using an attention-based deep neural network, to represent the salience degree of the summary for yielding the document. We devise a contrastive training strategy to learn the salience estimation network, and then use the learned salience score as a guide and iteratively extract the most salient sentences from the document as our generated summary. In experiments, our model not only achieves state-of-the-art ROUGE scores on CNN/Daily Mail dataset, but also shows strong robustness in the out-of-domain test on DUC2007 test set. Moreover, our model reaches a ROUGE-1 F-1 score of 39.41 on CNN/Daily Mail test set with merely $1 / 100$ training set, demonstrating a tremendous data efficiency."
786,https://arxiv.org/abs/1811.02338,Learning to Embed Sentences Using Attentive Recursive Trees,"Sentence embedding is an effective feature representation for most deep learning-based NLP tasks. One prevailing line of methods is using recursive latent tree-structured networks to embed sentences with task-specific structures. However, existing models have no explicit mechanism to emphasize task-informative words in the tree structure. To this end, we propose an Attentive Recursive Tree model (AR-Tree), where the words are dynamically located according to their importance in the task. Specifically, we construct the latent tree for a sentence in a proposed important-first strategy, and place more attentive words nearer to the root; thus, AR-Tree can inherently emphasize important words during the bottom-up composition of the sentence embedding. We propose an end-to-end reinforced training strategy for AR-Tree, which is demonstrated to consistently outperform, or be at least comparable to, the state-of-the-art sentence embedding methods on three sentence understanding tasks."
787,https://arxiv.org/abs/1611.04369,Feature Engineering and Ensemble Modeling for Paper Acceptance Rank Prediction,"Measuring research impact and ranking academic achievement are important and challenging problems. Having an objective picture of research institution is particularly valuable for students, parents and funding agencies, and also attracts attention from government and industry. KDD Cup 2016 proposes the paper acceptance rank prediction task, in which the participants are asked to rank the importance of institutions based on predicting how many of their papers will be accepted at the 8 top conferences in computer science. In our work, we adopt a three-step feature engineering method, including basic features definition, finding similar conferences to enhance the feature set, and dimension reduction using PCA. We propose three ranking models and the ensemble methods for combining such models. Our experiment verifies the effectiveness of our approach. In KDD Cup 2016, we achieved the overall rank of the 2nd place."
788,https://arxiv.org/abs/1512.07734,RDF2Rules: Learning Rules from RDF Knowledge Bases by Mining Frequent Predicate Cycles,"Recently, several large-scale RDF knowledge bases have been built and applied in many knowledge-based applications. To further increase the number of facts in RDF knowledge bases, logic rules can be used to predict new facts based on the existing ones. Therefore, how to automatically learn reliable rules from large-scale knowledge bases becomes increasingly important. In this paper, we propose a novel rule learning approach named RDF2Rules for RDF knowledge bases. RDF2Rules first mines frequent predicate cycles (FPCs), a kind of interesting frequent patterns in knowledge bases, and then generates rules from the mined FPCs. Because each FPC can produce multiple rules, and effective pruning strategy is used in the process of mining FPCs, RDF2Rules works very efficiently. Another advantage of RDF2Rules is that it uses the entity type information when generates and evaluates rules, which makes the learned rules more accurate. Experiments show that our approach outperforms the compared approach in terms of both efficiency and accuracy."
789,https://arxiv.org/abs/1504.02577,Panther: Fast Top-k Similarity Search in Large Networks,"Estimating similarity between vertices is a fundamental issue in network analysis across various domains, such as social networks and biological networks. Methods based on common neighbors and structural contexts have received much attention. However, both categories of methods are difficult to scale up to handle large networks (with billions of nodes). In this paper, we propose a sampling method that provably and accurately estimates the similarity between vertices. The algorithm is based on a novel idea of random path, and an extended method is also presented, to enhance the structural similarity when two vertices are completely disconnected. We provide theoretical proofs for the error-bound and confidence of the proposed algorithm. We perform extensive empirical study and show that our algorithm can obtain top-k similar vertices for any vertex in a network approximately 300x faster than state-of-the-art methods. We also use identity resolution and structural hole spanner finding, two important applications in social networks, to evaluate the accuracy of the estimated similarities. Our experimental results demonstrate that the proposed algorithm achieves clearly better performance than several alternative methods."
790,https://arxiv.org/abs/2305.16812,A Comprehensive Study on Quality Assurance Tools for Java,"Quality assurance (QA) tools are receiving more and more attention and are widely used by developers. Given the wide range of solutions for QA technology, it is still a question of evaluating QA tools. Most existing research is limited in the following ways: (i) They compare tools without considering scanning rules analysis. (ii) They disagree on the effectiveness of tools due to the study methodology and benchmark dataset. (iii) They do not separately analyze the role of the warnings. (iv) There is no large-scale study on the analysis of time performance. To address these problems, in the paper, we systematically select 6 free or open-source tools for a comprehensive study from a list of 148 existing Java QA tools. To carry out a comprehensive study and evaluate tools in multi-level dimensions, we first mapped the scanning rules to the CWE and analyze the coverage and granularity of the scanning rules. Then we conducted an experiment on 5 benchmarks, including 1,425 bugs, to investigate the effectiveness of these tools. Furthermore, we took substantial effort to investigate the effectiveness of warnings by comparing the real labeled bugs with the warnings and investigating their role in bug detection. Finally, we assessed these tools' time performance on 1,049 projects. The useful findings based on our comprehensive study can help developers improve their tools and provide users with suggestions for selecting QA tools."
791,https://arxiv.org/abs/2305.16601,The mechanism of the Silicon irradiation synergistic effect explained by multiscale simulations of Monte Carlo and excited-state first-principle calculations,"Neutron and $γ$-ray irradiation damages to transistors are found to be non-additive, and this is denoted as the irradiation synergistic effect (ISE). Its mechanism is not well-understood. The recent defect-based model [ACS Appl. Electron. Mater. 2, 3783 (2020)] for Silicon bipolar junction transistors (BJT) achieve quantitative agreement with experiments, but it remains phenomenological and its assumptions on the defect reactions are unverified. Going beyond the phenomenological model requires directly representing the effect of $γ$-ray irradiation in first-principles calculations, which is not feasible previously. In this work, we examine the defect-based model of the ISE by developing a multiscale method for the simulation of the $γ$-ray irradiation, where the $γ$-ray-induced electronic excitations are treated explicitly in excited-state first-principles calculations. We find the calculations agree with experiments, and the effect of the $γ$-ray-induced excitation is significantly different from the effects of defect charge state and temperature. We propose a diffusion-based qualitative explanation of the mechanism of positive/negative ISE in NPN/PNP BJTs in the end."
792,https://arxiv.org/abs/2305.16220,On the Robustness of Segment Anything,"Segment anything model (SAM) has presented impressive objectness identification capability with the idea of prompt learning and a new collected large-scale dataset. Given a prompt (e.g., points, bounding boxes, or masks) and an input image, SAM is able to generate valid segment masks for all objects indicated by the prompts, presenting high generalization across diverse scenarios and being a general method for zero-shot transfer to downstream vision tasks. Nevertheless, it remains unclear whether SAM may introduce errors in certain threatening scenarios. Clarifying this is of significant importance for applications that require robustness, such as autonomous vehicles. In this paper, we aim to study the testing-time robustness of SAM under adversarial scenarios and common corruptions. To this end, we first build a testing-time robustness evaluation benchmark for SAM by integrating existing public datasets. Second, we extend representative adversarial attacks against SAM and study the influence of different prompts on robustness. Third, we study the robustness of SAM under diverse corruption types by evaluating SAM on corrupted datasets with different prompts. With experiments conducted on SA-1B and KITTI datasets, we find that SAM exhibits remarkable robustness against various corruptions, except for blur-related corruption. Furthermore, SAM remains susceptible to adversarial attacks, particularly when subjected to PGD and BIM attacks. We think such a comprehensive study could highlight the importance of the robustness issues of SAM and trigger a series of new tasks for SAM as well as downstream vision tasks."
793,https://arxiv.org/abs/2305.15770,TLNets: Transformation Learning Networks for long-range time-series prediction,"Time series prediction is a prevalent issue across various disciplines, such as meteorology, traffic surveillance, investment, and energy production and consumption. Many statistical and machine-learning strategies have been developed to tackle this problem. However, these approaches either lack explainability or exhibit less satisfactory performance when the prediction horizon increases. To this end, we propose a novel plan for the designing of networks' architecture based on transformations, possessing the potential to achieve an enhanced receptive field in learning which brings benefits to fuse features across scales. In this context, we introduce four different transformation mechanisms as bases to construct the learning model including Fourier Transform (FT), Singular Value Decomposition (SVD), matrix multiplication and Conv block. Hence, we develop four learning models based on the above building blocks, namely, FT-Matrix, FT-SVD, FT-Conv, and Conv-SVD. Note that the FT and SVD blocks are capable of learning global information, while the Conv blocks focus on learning local information. The matrix block is sparsely designed to learn both global and local information simultaneously. The above Transformation Learning Networks (TLNets) have been extensively tested and compared with multiple baseline models based on several real-world datasets and showed clear potential in long-range time-series forecasting."
794,https://arxiv.org/abs/2305.15483,Weakly Supervised Vision-and-Language Pre-training with Relative Representations,"Weakly supervised vision-and-language pre-training (WVLP), which learns cross-modal representations with limited cross-modal supervision, has been shown to effectively reduce the data cost of pre-training while maintaining decent performance on downstream tasks. However, current WVLP methods use only local descriptions of images, i.e., object tags, as cross-modal anchors to construct weakly-aligned image-text pairs for pre-training. This affects the data quality and thus the effectiveness of pre-training. In this paper, we propose to directly take a small number of aligned image-text pairs as anchors, and represent each unaligned image and text by its similarities to these anchors, i.e., relative representations. We build a WVLP framework based on the relative representations, namely RELIT, which collects high-quality weakly-aligned image-text pairs from large-scale image-only and text-only data for pre-training through relative representation-based retrieval and generation. Experiments on four downstream tasks show that RELIT achieves new state-of-the-art results under the weakly supervised setting."
795,https://arxiv.org/abs/2305.14726,In-Context Demonstration Selection with Cross Entropy Difference,"Large language models (LLMs) can use in-context demonstrations to improve performance on zero-shot tasks. However, selecting the best in-context examples is challenging because model performance can vary widely depending on the selected examples. We present a cross-entropy difference (CED) method for selecting in-context demonstrations. Our method is based on the observation that the effectiveness of in-context demonstrations negatively correlates with the perplexity of the test example by a language model that was finetuned on that demonstration. We utilize parameter efficient finetuning to train small models on training data that are used for computing the cross-entropy difference between a test example and every candidate in-context demonstration. This metric is used to rank and select in-context demonstrations independently for each test input. We evaluate our method on a mix-domain dataset that combines 8 benchmarks, representing 4 text generation tasks, showing that CED for in-context demonstration selection can improve performance for a variety of LLMs."
796,https://arxiv.org/abs/2305.14656,RSRM: Reinforcement Symbolic Regression Machine,"In nature, the behaviors of many complex systems can be described by parsimonious math equations. Automatically distilling these equations from limited data is cast as a symbolic regression process which hitherto remains a grand challenge. Keen efforts in recent years have been placed on tackling this issue and demonstrated success in symbolic regression. However, there still exist bottlenecks that current methods struggle to break when the discrete search space tends toward infinity and especially when the underlying math formula is intricate. To this end, we propose a novel Reinforcement Symbolic Regression Machine (RSRM) that masters the capability of uncovering complex math equations from only scarce data. The RSRM model is composed of three key modules: (1) a Monte Carlo tree search (MCTS) agent that explores optimal math expression trees consisting of pre-defined math operators and variables, (2) a Double Q-learning block that helps reduce the feasible search space of MCTS via properly understanding the distribution of reward, and (3) a modulated sub-tree discovery block that heuristically learns and defines new math operators to improve representation ability of math expression trees. Biding of these modules yields the state-of-the-art performance of RSRM in symbolic regression as demonstrated by multiple sets of benchmark examples. The RSRM model shows clear superiority over several representative baseline models."
797,https://arxiv.org/abs/2305.14564,PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents,"Strategies such as chain-of-thought prompting improve the performance of large language models (LLMs) on complex reasoning tasks by decomposing input examples into intermediate steps. However, it remains unclear how to apply such methods to reason over long input documents, in which both the decomposition and the output of each intermediate step are non-trivial to obtain. In this work, we propose PEARL, a prompting framework to improve reasoning over long documents, which consists of three stages: action mining, plan formulation, and plan execution. More specifically, given a question about a long document, PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE, FIND_EVENT, FIND_RELATION) and then executes them over the document to obtain the answer. Each stage of PEARL is implemented via zero-shot or few-shot prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate PEARL on a challenging subset of the QuALITY dataset, which contains questions that require complex reasoning over long narrative texts. PEARL outperforms zero-shot and chain-of-thought prompting on this dataset, and ablation experiments show that each stage of PEARL is critical to its performance. Overall, PEARL is a first step towards leveraging LLMs to reason over long documents."
798,https://arxiv.org/abs/2305.13860,Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study,"Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse. Our study investigates three key research questions: (1) the number of different prompt types that can jailbreak LLMs, (2) the effectiveness of jailbreak prompts in circumventing LLM constraints, and (3) the resilience of ChatGPT against these jailbreak prompts. Initially, we develop a classification model to analyze the distribution of existing prompts, identifying ten distinct patterns and three categories of jailbreak prompts. Subsequently, we assess the jailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a dataset of 3,120 jailbreak questions across eight prohibited scenarios. Finally, we evaluate the resistance of ChatGPT against jailbreak prompts, finding that the prompts can consistently evade the restrictions in 40 use-case scenarios. The study underscores the importance of prompt structures in jailbreaking LLMs and discusses the challenges of robust jailbreak prompt generation and prevention."
799,https://arxiv.org/abs/2305.13310,Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching,"Powered by large-scale pre-training, vision foundation models exhibit significant potential in open-world image understanding. Even though individual models have limited capabilities, combining multiple such models properly can lead to positive synergies and unleash their full potential. In this work, we present Matcher, which segments anything with one shot by integrating an all-purpose feature extraction model and a class-agnostic segmentation model. Naively connecting the models results in unsatisfying performance, e.g., the models tend to generate matching outliers and false-positive mask fragments. To address these issues, we design a bidirectional matching strategy for accurate cross-image semantic dense matching and a robust prompt sampler for mask proposal generation. In addition, we propose a novel instance-level matching strategy for controllable mask merging. The proposed Matcher method delivers impressive generalization performance across various segmentation tasks, all without training. For example, it achieves 52.7% mIoU on COCO-20$^i$ for one-shot semantic segmentation, surpassing the state-of-the-art specialist model by 1.6%. In addition, our visualization results show open-world generality and flexibility on images in the wild. The code shall be released at https://github.com/aim-uofa/Matcher."
800,https://arxiv.org/abs/2305.13086,LMGQS: A Large-scale Dataset for Query-focused Summarization,"Query-focused summarization (QFS) aims to extract or generate a summary of an input document that directly answers or is relevant to a given query. The lack of large-scale datasets in the form of documents, queries, and summaries has hindered model development in this area. In contrast, multiple large-scale high-quality datasets for generic summarization exist. We hypothesize that there is a hidden query for each summary sentence in a generic summarization annotation, and we utilize a large-scale pretrained language model to recover it. In this way, we convert four generic summarization benchmarks into a new QFS benchmark dataset, LMGQS, which consists of over 1 million document-query-summary samples. We thoroughly investigate the properties of our proposed dataset and establish baselines with state-of-the-art summarization models. By fine-tuning a language model on LMGQS, we achieve state-of-the-art zero-shot and supervised performance on multiple existing QFS benchmarks, demonstrating the high quality and diversity of LMGQS."
801,https://arxiv.org/abs/2305.13083,"InheritSumm: A General, Versatile and Compact Summarizer by Distilling from GPT","While large models such as GPT-3 demonstrate exceptional performance in zeroshot and fewshot summarization tasks, their extensive serving and fine-tuning costs hinder their utilization in various applications. Conversely, previous studies have found that although automatic metrics tend to favor smaller fine-tuned models, the quality of the summaries they generate is inferior to that of larger models like GPT-3 when assessed by human evaluators. To address this issue, we propose InheritSumm, a versatile and compact summarization model derived from GPT-3.5 through distillation. InheritSumm not only exhibits comparable zeroshot and fewshot summarization capabilities to GPT-3.5 but is also sufficiently compact for fine-tuning purposes. Experimental results demonstrate that InheritSumm achieves similar or superior performance to GPT-3.5 in zeroshot and fewshot settings. Furthermore, it outperforms the previously established best small models in both prefix-tuning and full-data fine-tuning scenarios."
802,https://arxiv.org/abs/2305.12865,Automatic Code Summarization via ChatGPT: How Far Are We?,"To support software developers in understanding and maintaining programs, various automatic code summarization techniques have been proposed to generate a concise natural language comment for a given code snippet. Recently, the emergence of large language models (LLMs) has led to a great boost in the performance of natural language processing tasks. Among them, ChatGPT is the most popular one which has attracted wide attention from the software engineering community. However, it still remains unclear how ChatGPT performs in (automatic) code summarization. Therefore, in this paper, we focus on evaluating ChatGPT on a widely-used Python dataset called CSN-Python and comparing it with several state-of-the-art (SOTA) code summarization models. Specifically, we first explore an appropriate prompt to guide ChatGPT to generate in-distribution comments. Then, we use such a prompt to ask ChatGPT to generate comments for all code snippets in the CSN-Python test set. We adopt three widely-used metrics (including BLEU, METEOR, and ROUGE-L) to measure the quality of the comments generated by ChatGPT and SOTA models (including NCS, CodeBERT, and CodeT5). The experimental results show that in terms of BLEU and ROUGE-L, ChatGPT's code summarization performance is significantly worse than all three SOTA models. We also present some cases and discuss the advantages and disadvantages of ChatGPT in code summarization. Based on the findings, we outline several open challenges and opportunities in ChatGPT-based code summarization."
803,https://arxiv.org/abs/2305.12138,The Scope of ChatGPT in Software Engineering: A Thorough Investigation,"ChatGPT demonstrates immense potential to transform software engineering (SE) by exhibiting outstanding performance in tasks such as code and document generation. However, the high reliability and risk control requirements of SE make the lack of interpretability for ChatGPT a concern. To address this issue, we carried out a study evaluating ChatGPT's capabilities and limitations in SE. We broke down the abilities needed for AI models to tackle SE tasks into three categories: 1) syntax understanding, 2) static behavior understanding, and 3) dynamic behavior understanding. Our investigation focused on ChatGPT's ability to comprehend code syntax and semantic structures, including abstract syntax trees (AST), control flow graphs (CFG), and call graphs (CG). We assessed ChatGPT's performance on cross-language tasks involving C, Java, Python, and Solidity. Our findings revealed that while ChatGPT excels at understanding code syntax (AST), it struggles with comprehending code semantics, particularly dynamic semantics. We conclude that ChatGPT possesses capabilities akin to an Abstract Syntax Tree (AST) parser, demonstrating initial competencies in static code analysis. Additionally, our study highlights that ChatGPT is susceptible to hallucination when interpreting code semantic structures and fabricating non-existent facts. These results underscore the need to explore methods for verifying the correctness of ChatGPT's outputs to ensure its dependability in SE. More importantly, our study provide an iniital answer why the generated codes from LLMs are usually synatx correct but vulnerabale."
804,https://arxiv.org/abs/2305.12091,"""What do others think?"": Task-Oriented Conversational Modeling with Subjective Knowledge","Task-oriented Dialogue (TOD) Systems aim to build dialogue systems that assist users in accomplishing specific goals, such as booking a hotel or a restaurant. Traditional TODs rely on domain-specific APIs/DBs or external factual knowledge to generate responses, which cannot accommodate subjective user requests (e.g., ""Is the WIFI reliable?"" or ""Does the restaurant have a good atmosphere?""). To address this issue, we propose a novel task of subjective-knowledge-based TOD (SK-TOD). We also propose the first corresponding dataset, which contains subjective knowledge-seeking dialogue contexts and manually annotated responses grounded in subjective knowledge sources. When evaluated with existing TOD approaches, we find that this task poses new challenges such as aggregating diverse opinions from multiple knowledge snippets. We hope this task and dataset can promote further research on TOD and subjective content understanding. The code and the dataset are available at https://github.com/alexa/dstc11-track5."
805,https://arxiv.org/abs/2305.10929,Architecture-agnostic Iterative Black-box Certified Defense against Adversarial Patches,"The adversarial patch attack aims to fool image classifiers within a bounded, contiguous region of arbitrary changes, posing a real threat to computer vision systems (e.g., autonomous driving, content moderation, biometric authentication, medical imaging) in the physical world. To address this problem in a trustworthy way, proposals have been made for certified patch defenses that ensure the robustness of classification models and prevent future patch attacks from breaching the defense. State-of-the-art certified defenses can be compatible with any model architecture, as well as achieve high clean and certified accuracy. Although the methods are adaptive to arbitrary patch positions, they inevitably need to access the size of the adversarial patch, which is unreasonable and impractical in real-world attack scenarios. To improve the feasibility of the architecture-agnostic certified defense in a black-box setting (i.e., position and size of the patch are both unknown), we propose a novel two-stage Iterative Black-box Certified Defense method, termed IBCD.In the first stage, it estimates the patch size in a search-based manner by evaluating the size relationship between the patch and mask with pixel masking. In the second stage, the accuracy results are calculated by the existing white-box certified defense methods with the estimated patch size. The experiments conducted on two popular model architectures and two datasets verify the effectiveness and efficiency of IBCD."
806,https://arxiv.org/abs/2305.10730,FedMR: Federated Learning via Model Recombination,"Although Federated Learning (FL) enables global model training across clients without compromising their raw data, existing Federated Averaging (FedAvg)-based methods suffer from the problem of low inference performance, especially for unevenly distributed data among clients. This is mainly because i) FedAvg initializes client models with the same global models, which makes the local training hard to escape from the local search for optimal solutions; and ii) by averaging model parameters in a coarse manner, FedAvg eclipses the individual characteristics of local models. To address such issues that strongly limit the inference capability of FL, we propose a novel and effective FL paradigm named FedMR (Federated Model Recombination). Unlike conventional FedAvg-based methods, the cloud server of FedMR shuffles each layer of collected local models and recombines them to achieve new models for local training on clients. Due to the diversified initialization models for clients coupled with fine-grained model recombination, FedMR can converge to a well-generalized global model for all the clients, leading to a superior inference performance. Experimental results show that, compared with state-of-the-art FL methods, FedMR can significantly improve inference accuracy in a quicker manner without exposing client privacy."
807,https://arxiv.org/abs/2305.10502,EENED: End-to-End Neural Epilepsy Detection based on Convolutional Transformer,"Recently Transformer and Convolution neural network (CNN) based models have shown promising results in EEG signal processing. Transformer models can capture the global dependencies in EEG signals through a self-attention mechanism, while CNN models can capture local features such as sawtooth waves. In this work, we propose an end-to-end neural epilepsy detection model, EENED, that combines CNN and Transformer. Specifically, by introducing the convolution module into the Transformer encoder, EENED can learn the time-dependent relationship of the patient's EEG signal features and notice local EEG abnormal mutations closely related to epilepsy, such as the appearance of spikes and the sprinkling of sharp and slow waves. Our proposed framework combines the ability of Transformer and CNN to capture different scale features of EEG signals and holds promise for improving the accuracy and reliability of epilepsy detection. Our source code will be released soon on GitHub."
808,https://arxiv.org/abs/2305.10424,ZeroFlow: Fast Zero Label Scene Flow via Distillation,"Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds. State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds for large-scale point clouds, making them unusable as computer vision primitives for real-time applications such as open world object detection. Feed forward methods are considerably faster, running on the order of tens to hundreds of milliseconds for large-scale point clouds, but require expensive human supervision. To address both limitations, we propose Scene Flow via Distillation, a simple distillation framework that uses a label-free optimization method to produce pseudo-labels to supervise a feed forward model. Our instantiation of this framework, ZeroFlow, produces scene flow estimates in real-time on large-scale point clouds at quality competitive with state-of-the-art methods while using zero human labels. Notably, at test-time ZeroFlow is over 1000$\times$ faster than label-free state-of-the-art optimization-based methods on large-scale point clouds and over 1000$\times$ cheaper to train on unlabeled data compared to the cost of human annotation of that data. To facilitate research reuse, we release our code, trained model weights, and high quality pseudo-labels for the Argoverse 2 and Waymo Open datasets."
809,https://arxiv.org/abs/2305.10320,CostFormer:Cost Transformer for Cost Aggregation in Multi-view Stereo,"The core of Multi-view Stereo(MVS) is the matching process among reference and source pixels. Cost aggregation plays a significant role in this process, while previous methods focus on handling it via CNNs. This may inherit the natural limitation of CNNs that fail to discriminate repetitive or incorrect matches due to limited local receptive fields. To handle the issue, we aim to involve Transformer into cost aggregation. However, another problem may occur due to the quadratically growing computational complexity caused by Transformer, resulting in memory overflow and inference latency. In this paper, we overcome these limits with an efficient Transformer-based cost aggregation network, namely CostFormer. The Residual Depth-Aware Cost Transformer(RDACT) is proposed to aggregate long-range features on cost volume via self-attention mechanisms along the depth and spatial dimensions. Furthermore, Residual Regression Transformer(RRT) is proposed to enhance spatial attention. The proposed method is a universal plug-in to improve learning-based MVS methods."
810,https://arxiv.org/abs/2305.09543,EEG-based Sleep Staging with Hybrid Attention,"Sleep staging is critical for assessing sleep quality and diagnosing sleep disorders. However, capturing both the spatial and temporal relationships within electroencephalogram (EEG) signals during different sleep stages remains challenging. In this paper, we propose a novel framework called the Hybrid Attention EEG Sleep Staging (HASS) Framework. Specifically, we propose a well-designed spatio-temporal attention mechanism to adaptively assign weights to inter-channels and intra-channel EEG segments based on the spatio-temporal relationship of the brain during different sleep stages. Experiment results on the MASS and ISRUC datasets demonstrate that HASS can significantly improve typical sleep staging networks. Our proposed framework alleviates the difficulties of capturing the spatial-temporal relationship of EEG signals during sleep staging and holds promise for improving the accuracy and reliability of sleep assessment in both clinical and research settings."
811,https://arxiv.org/abs/2305.09198,Capacitor Voltage Synchronizing Control of 100% Full-Scale Wind Power Generator-Supplied Power Systems,"This paper proposes a capacitor voltage synchronizing control (CVSC) system for the regulation of full-scale wind power generator-supplied power systems (FWPS). The capacitor combined with the inverter of a full-scale wind power generator (WPG) is controlled with a CVSC system to mimic the rotor dynamics of a synchronous generator (SG). WPGs are enabled to offer inertial response and primary regulation. The generation and load unbalance of a FWPS is reflected by the deviation of capacitor voltages of WPGs. Small-signal analysis was carried out to investigate the oscillatory modes of a FWPS with the CVSC system. Time-domain simulation studies were undertaken on the FWPS, and the performance of the CVSC system was studied in the cases where a load increase and a three-phase-to-ground fault occurred on the FWPS, respectively."
812,https://arxiv.org/abs/2305.09154,Progressive Translation: Improving Domain Robustness of Neural Machine Translation with Intermediate Sequences,"Previous studies show that intermediate supervision signals benefit various Natural Language Processing tasks. However, it is not clear whether there exist intermediate signals that benefit Neural Machine Translation (NMT). Borrowing techniques from Statistical Machine Translation, we propose intermediate signals which are intermediate sequences from the ""source-like"" structure to the ""target-like"" structure. Such intermediate sequences introduce an inductive bias that reflects a domain-agnostic principle of translation, which reduces spurious correlations that are harmful to out-of-domain generalisation. Furthermore, we introduce a full-permutation multi-task learning to alleviate the spurious causal relations from intermediate sequences to the target, which results from exposure bias. The Minimum Bayes Risk decoding algorithm is used to pick the best candidate translation from all permutations to further improve the performance. Experiments show that the introduced intermediate signals can effectively improve the domain robustness of NMT and reduces the amount of hallucinations on out-of-domain translation. Further analysis shows that our methods are especially promising in low-resource scenarios."
813,https://arxiv.org/abs/2305.08989,LoViT: Long Video Transformer for Surgical Phase Recognition,"Online surgical phase recognition plays a significant role towards building contextual tools that could quantify performance and oversee the execution of surgical workflows. Current approaches are limited since they train spatial feature extractors using frame-level supervision that could lead to incorrect predictions due to similar frames appearing at different phases, and poorly fuse local and global features due to computational constraints which can affect the analysis of long videos commonly encountered in surgical interventions. In this paper, we present a two-stage method, called Long Video Transformer (LoViT) for fusing short- and long-term temporal information that combines a temporally-rich spatial feature extractor and a multi-scale temporal aggregator consisting of two cascaded L-Trans modules based on self-attention, followed by a G-Informer module based on ProbSparse self-attention for processing global temporal information. The multi-scale temporal head then combines local and global features and classifies surgical phases using phase transition-aware supervision. Our approach outperforms state-of-the-art methods on the Cholec80 and AutoLaparo datasets consistently. Compared to Trans-SVNet, LoViT achieves a 2.39 pp (percentage point) improvement in video-level accuracy on Cholec80 and a 3.14 pp improvement on AutoLaparo. Moreover, it achieves a 5.25 pp improvement in phase-level Jaccard on AutoLaparo and a 1.55 pp improvement on Cholec80. Our results demonstrate the effectiveness of our approach in achieving state-of-the-art performance of surgical phase recognition on two datasets of different surgical procedures and temporal sequencing characteristics whilst introducing mechanisms that cope with long videos."
814,https://arxiv.org/abs/2305.08848,Small Models are Valuable Plug-ins for Large Language Models,"Large language models (LLMs) such as GPT-3 and GPT-4 are powerful but their weights are often publicly unavailable and their immense sizes make the models difficult to be tuned with common hardware. As a result, effectively tuning these models with large-scale supervised data can be challenging. As an alternative, In-Context Learning (ICL) can only use a small number of supervised examples due to context length limits. In this paper, we propose Super In-Context Learning (SuperICL) which allows black-box LLMs to work with locally fine-tuned smaller models, resulting in superior performance on supervised tasks. Our experiments demonstrate that SuperICL can improve performance beyond state-of-the-art fine-tuned models while addressing the instability problem of in-context learning. Furthermore, SuperICL can enhance the capabilities of smaller models, such as multilinguality and interpretability."
815,https://arxiv.org/abs/2305.08323,Approximation and Progressive Display of Multiverse Analyses,"A multiverse analysis evaluates all combinations of ""reasonable"" analytic decisions to promote robustness and transparency, but can lead to a combinatorial explosion of analyses to compute. Long delays before assessing results prevent users from diagnosing errors and iterating early. We contribute (1) approximation algorithms for estimating multiverse sensitivity and (2) monitoring visualizations for assessing progress and controlling execution on the fly. We evaluate how quickly three sampling-based algorithms converge to accurately rank sensitive decisions in both synthetic and real multiverse analyses. Compared to uniform random sampling, round robin and sketching approaches are 2 times faster in the best case, while on average estimating sensitivity accurately using 20% of the full multiverse. To enable analysts to stop early to fix errors or decide when results are ""good enough"" to move forward, we visualize both effect size and decision sensitivity estimates with confidence intervals, and surface potential issues including runtime warnings and model quality metrics."
816,https://arxiv.org/abs/2305.07795,Constructing Holistic Measures for Social Biases in Masked Language Models,"Masked Language Models (MLMs) have been successful in many natural language processing tasks. However, real-world stereotype biases are likely to be reflected in MLMs due to their learning from large text corpora. Most of the evaluation metrics proposed in the past adopt different masking strategies, designed with the log-likelihood of MLMs. They lack holistic considerations such as variance for stereotype bias and anti-stereotype bias samples. In this paper, the log-likelihoods of stereotype bias and anti-stereotype bias samples output by MLMs are considered Gaussian distributions. Two evaluation metrics, Kullback Leibler Divergence Score (KLDivS) and Jensen Shannon Divergence Score (JSDivS) are proposed to evaluate social biases in MLMs The experimental results on the public datasets StereoSet and CrowS-Pairs demonstrate that KLDivS and JSDivS are more stable and interpretable compared to the metrics proposed in the past."
817,https://arxiv.org/abs/2305.07328,Configurable Spatial-Temporal Hierarchical Analysis for Flexible Video Anomaly Detection,"Video anomaly detection (VAD) is a vital task with great practical applications in industrial surveillance, security system, and traffic control. Unlike previous unsupervised VAD methods that adopt a fixed structure to learn normality without considering different detection demands, we design a spatial-temporal hierarchical architecture (STHA) as a configurable architecture to flexibly detect different degrees of anomaly. The comprehensive structure of the STHA is delineated into a tripartite hierarchy, encompassing the following tiers: the stream level, the stack level, and the block level. Specifically, we design several auto-encoder-based blocks that possess varying capacities for extracting normal patterns. Then, we stack blocks according to the complexity degrees with both intra-stack and inter-stack residual links to learn hierarchical normality gradually. Considering the multisource knowledge of videos, we also model the spatial normality of video frames and temporal normality of RGB difference by designing two parallel streams consisting of stacks. Thus, STHA can provide various representation learning abilities by expanding or contracting hierarchically to detect anomalies of different degrees. Since the anomaly set is complicated and unbounded, our STHA can adjust its detection ability to adapt to the human detection demands and the complexity degree of anomaly that happened in the history of a scene. We conduct experiments on three benchmarks and perform extensive analysis, and the results demonstrate that our method performs comparablely to the state-of-the-art methods. In addition, we design a toy dataset to prove that our model can better balance the learning ability to adapt to different detection demands."
818,https://arxiv.org/abs/2305.07290,The 3rd Anti-UAV Workshop & Challenge: Methods and Results,"The 3rd Anti-UAV Workshop & Challenge aims to encourage research in developing novel and accurate methods for multi-scale object tracking. The Anti-UAV dataset used for the Anti-UAV Challenge has been publicly released. There are two main differences between this year's competition and the previous two. First, we have expanded the existing dataset, and for the first time, released a training set so that participants can focus on improving their models. Second, we set up two tracks for the first time, i.e., Anti-UAV Tracking and Anti-UAV Detection & Tracking. Around 76 participating teams from the globe competed in the 3rd Anti-UAV Challenge. In this paper, we provide a brief summary of the 3rd Anti-UAV Workshop & Challenge including brief introductions to the top three methods in each track. The submission leaderboard will be reopened for researchers that are interested in the Anti-UAV challenge. The benchmark dataset and other information can be found at: https://anti-uav.github.io/."
819,https://arxiv.org/abs/2305.06683,Cost-efficient Crowdsourcing for Span-based Sequence Labeling: Worker Selection and Data Augmentation,"This paper introduces a novel worker selection algorithm, enhancing annotation quality and reducing costs in challenging span-based sequence labeling tasks in Natural Language Processing (NLP). Unlike previous studies targeting simpler tasks, this study contends with the complexities of label interdependencies in sequence labeling tasks. The proposed algorithm utilizes a Combinatorial Multi-Armed Bandit (CMAB) approach for worker selection. The challenge of dealing with imbalanced and small-scale datasets, which hinders offline simulation of worker selection, is tackled using an innovative data augmentation method termed shifting, expanding, and shrinking (SES). The SES method is designed specifically for sequence labeling tasks. Rigorous testing on CoNLL 2003 NER and Chinese OEI datasets showcased the algorithm's efficiency, with an increase in F1 score up to 100.04% of the expert-only baseline, alongside cost savings up to 65.97%. The paper also encompasses a dataset-independent test emulating annotation evaluation through a Bernoulli distribution, which still led to an impressive 97.56% F1 score of the expert baseline and 59.88% cost savings. This research addresses and overcomes numerous obstacles in worker selection for complex NLP tasks."
820,https://arxiv.org/abs/2305.06622,PerFedRec++: Enhancing Personalized Federated Recommendation with Self-Supervised Pre-Training,"Federated recommendation systems employ federated learning techniques to safeguard user privacy by transmitting model parameters instead of raw user data between user devices and the central server. Nevertheless, the current federated recommender system faces challenges such as heterogeneity and personalization, model performance degradation, and communication bottleneck. Previous studies have attempted to address these issues, but none have been able to solve them simultaneously.
  In this paper, we propose a novel framework, named PerFedRec++, to enhance the personalized federated recommendation with self-supervised pre-training. Specifically, we utilize the privacy-preserving mechanism of federated recommender systems to generate two augmented graph views, which are used as contrastive tasks in self-supervised graph learning to pre-train the model. Pre-training enhances the performance of federated models by improving the uniformity of representation learning. Also, by providing a better initial state for federated training, pre-training makes the overall training converge faster, thus alleviating the heavy communication burden. We then construct a collaborative graph to learn the client representation through a federated graph neural network. Based on these learned representations, we cluster users into different user groups and learn personalized models for each cluster. Each user learns a personalized model by combining the global federated model, the cluster-level federated model, and its own fine-tuned local model. Experiments on three real-world datasets show that our proposed method achieves superior performance over existing methods."
821,https://arxiv.org/abs/2305.06212,Privacy-Preserving Prompt Tuning for Large Language Model Services,"Prompt tuning provides an efficient way for users to customize Large Language Models (LLMs) with their private data in the emerging LLM service scenario. However, the sensitive nature of private data brings the need for privacy preservation in LLM service customization. Based on prompt tuning, we propose Privacy-Preserving Prompt Tuning (RAPT), a framework that provides privacy guarantees for LLM services. \textsc{rapt} adopts a local privacy setting, allowing users to privatize their data locally with local differential privacy. As prompt tuning performs poorly when directly trained on privatized data, we introduce a novel privatized token reconstruction task that is trained jointly with the downstream task, allowing LLMs to learn better task-dependent representations. Despite the simplicity of our framework, experiments show that RAPT achieves competitive performance across tasks while providing privacy guarantees against adversaries."
822,https://arxiv.org/abs/2305.05918,Detecting resonance of radio-frequency cavities using fast direct integral equation solvers and augmented Bayesian optimization,"This paper presents a computationally efficient framework for identifying resonance modes of 3D radio-frequency (RF) cavities with damping waveguide ports. The proposed framework relies on surface integral equation (IE) formulations to convert the task of resonance detection to the task of finding resonance frequencies at which the lowest few eigenvalues of the system matrix is close to zero. For the linear eigenvalue problem \rev{with a fixed frequency}, we propose leveraging fast direct solvers to efficiently invert the system matrix; for the frequency search problem, we develop a hybrid optimization algorithm that combines Bayesian optimization with down-hill simplex optimization. The proposed IE-based resonance detection framework (IERD) has been applied to detection of high-order resonance modes (HOMs) of realistic accelerator RF cavities to demonstrate its efficiency and accuracy."
823,https://arxiv.org/abs/2305.05896,RNNS: Representation Nearest Neighbor Search Black-Box Attack on Code Models,"Pre-trained code models are mainly evaluated using the in-distribution test data. The robustness of models, i.e., the ability to handle hard unseen data, still lacks evaluation. In this paper, we propose a novel search-based black-box adversarial attack guided by model behaviours for pre-trained programming language models, named Representation Nearest Neighbor Search(RNNS), to evaluate the robustness of Pre-trained PL models. Unlike other black-box adversarial attacks, RNNS uses the model-change signal to guide the search in the space of the variable names collected from real-world projects. Specifically, RNNS contains two main steps, 1) indicate which variable (attack position location) we should attack based on model uncertainty, and 2) search which adversarial tokens we should use for variable renaming according to the model behaviour observations. We evaluate RNNS on 6 code tasks (e.g., clone detection), 3 programming languages (Java, Python, and C), and 3 pre-trained code models: CodeBERT, GraphCodeBERT, and CodeT5. The results demonstrate that RNNS outperforms the state-of-the-art black-box attacking methods (MHM and ALERT) in terms of attack success rate (ASR) and query times (QT). The perturbation of generated adversarial examples from RNNS is smaller than the baselines with respect to the number of replaced variables and the variable length change. Our experiments also show that RNNS is efficient in attacking the defended models and is useful for adversarial training."
824,https://arxiv.org/abs/2305.05150,Physics-informed neural network for seismic wave inversion in layered semi-infinite domain,"Estimating the material distribution of Earth's subsurface is a challenging task in seismology and earthquake engineering. The recent development of physics-informed neural network (PINN) has shed new light on seismic inversion. In this paper, we present a PINN framework for seismic wave inversion in layered (1D) semi-infinite domain. The absorbing boundary condition is incorporated into the network as a soft regularizer for avoiding excessive computation. In specific, we design a lightweight network to learn the unknown material distribution and a deep neural network to approximate solution variables. The entire network is end-to-end and constrained by both sparse measurement data and the underlying physical laws (i.e., governing equations and initial/boundary conditions). Various experiments have been conducted to validate the effectiveness of our proposed approach for inverse modeling of seismic wave propagation in 1D semi-infinite domain."
825,https://arxiv.org/abs/2305.05090,Performative Federated Learning: A Solution to Model-Dependent and Heterogeneous Distribution Shifts,"We consider a federated learning (FL) system consisting of multiple clients and a server, where the clients aim to collaboratively learn a common decision model from their distributed data. Unlike the conventional FL framework that assumes the client's data is static, we consider scenarios where the clients' data distributions may be reshaped by the deployed decision model. In this work, we leverage the idea of distribution shift mappings in performative prediction to formalize this model-dependent data distribution shift and propose a performative federated learning framework. We first introduce necessary and sufficient conditions for the existence of a unique performative stable solution and characterize its distance to the performative optimal solution. Then we propose the performative FedAvg algorithm and show that it converges to the performative stable solution at a rate of O(1/T) under both full and partial participation schemes. In particular, we use novel proof techniques and show how the clients' heterogeneity influences the convergence. Numerical results validate our analysis and provide valuable insights into real-world applications."
826,https://arxiv.org/abs/2305.04461,Locally Attentional SDF Diffusion for Controllable 3D Shape Generation,"Although the recent rapid evolution of 3D generative neural networks greatly improves 3D shape generation, it is still not convenient for ordinary users to create 3D shapes and control the local geometry of generated shapes. To address these challenges, we propose a diffusion-based 3D generation framework -- locally attentional SDF diffusion, to model plausible 3D shapes, via 2D sketch image input. Our method is built on a two-stage diffusion model. The first stage, named occupancy-diffusion, aims to generate a low-resolution occupancy field to approximate the shape shell. The second stage, named SDF-diffusion, synthesizes a high-resolution signed distance field within the occupied voxels determined by the first stage to extract fine geometry. Our model is empowered by a novel view-aware local attention mechanism for image-conditioned shape generation, which takes advantage of 2D image patch features to guide 3D voxel feature learning, greatly improving local controllability and model generalizability. Through extensive experiments in sketch-conditioned and category-conditioned 3D shape generation tasks, we validate and demonstrate the ability of our method to provide plausible and diverse 3D shapes, as well as its superior controllability and generalizability over existing work. Our code and trained models are available at https://zhengxinyang.github.io/projects/LAS-Diffusion.html"
827,https://arxiv.org/abs/2305.04224,Visual Causal Scene Refinement for Video Question Answering,"Existing methods for video question answering (VideoQA) often suffer from spurious correlations between different modalities, leading to a failure in identifying the dominant visual evidence and the intended question. Moreover, these methods function as black boxes, making it difficult to interpret the visual scene during the QA process. In this paper, to discover critical video segments and frames that serve as the visual causal scene for generating reliable answers, we present a causal analysis of VideoQA and propose a framework for cross-modal causal relational reasoning, named Visual Causal Scene Refinement (VCSR). Particularly, a set of causal front-door intervention operations is introduced to explicitly find the visual causal scenes at both segment and frame levels. Our VCSR involves two essential modules: i) the Question-Guided Refiner (QGR) module, which refines consecutive video frames guided by the question semantics to obtain more representative segment features for causal front-door intervention; ii) the Causal Scene Separator (CSS) module, which discovers a collection of visual causal and non-causal scenes based on the visual-linguistic causal relevance and estimates the causal effect of the scene-separating intervention in a contrastive learning manner. Extensive experiments on the NExT-QA, Causal-VidQA, and MSRVTT-QA datasets demonstrate the superiority of our VCSR in discovering visual causal scene and achieving robust video question answering."
828,https://arxiv.org/abs/2305.03859,Open problems in causal structure learning: A case study of COVID-19 in the UK,"Causal machine learning (ML) algorithms recover graphical structures that tell us something about cause-and-effect relationships. The causal representation provided by these algorithms enables transparency and explainability, which is necessary in critical real-world problems. Yet, causal ML has had limited impact in practice compared to associational ML. This paper investigates the challenges of causal ML with application to COVID-19 UK pandemic data. We collate data from various public sources and investigate what the various structure learning algorithms learn from these data. We explore the impact of different data formats on algorithms spanning different classes of learning, and assess the results produced by each algorithm, and groups of algorithms, in terms of graphical structure, model dimensionality, sensitivity analysis, confounding variables, predictive and interventional inference. We use these results to highlight open problems in causal structure learning and directions for future research. To facilitate future work, we make all graphs, models and data sets publicly available online."
829,https://arxiv.org/abs/2305.03652,A fully hybrid integrated Erbium-based laser,"Erbium-doped fiber lasers exhibit high coherence and low noise as required for applications in fiber optic sensing, gyroscopes, LiDAR, and optical frequency metrology. Endowing Erbium-based gain in photonic integrated circuits can provide a basis for miniaturizing low-noise fiber lasers to chip-scale form factor, and enable large-volume applications. Yet, while major progress has been made in the last decade on integrated lasers based on silicon photonics with III-V gain media, the integration of Erbium lasers on chip has been compounded by large laser linewidth. Recent advances in photonic integrated circuit-based high-power Erbium-doped amplifiers, make a new class of rare-earth-ion-based lasers possible. Here, we demonstrate a fully integrated chip-scale Erbium laser that achieves high power, narrow linewidth, frequency agility, and the integration of a III-V pump laser. The laser circuit is based on an Erbium-implanted ultralow-loss silicon nitride Si$_3$N$4$ photonic integrated circuit. This device achieves single-mode lasing with a free-running intrinsic linewidth of 50 Hz, a relative intensity noise of $<$-150 dBc/Hz at $>$10 MHz offset, and output power up to 17 mW, approaching the performance of fiber lasers and state-of-the-art semiconductor extended cavity lasers. An intra-cavity microring-based Vernier filter enables wavelength tunability of $>$ 40 nm within the C- and L-bands while attaining side mode suppression ratio (SMSR) of $>$ 70 dB, surpassing legacy fiber lasers in tuning and SMRS performance. This new class of low-noise, tuneable Erbium waveguide laser could find applications in LiDAR, microwave photonics, optical frequency synthesis, and free-space communications. Our approach is extendable to other wavelengths, and more broadly, constitutes a novel way to photonic integrated circuit-based rare-earth-ion-doped lasers."
830,https://arxiv.org/abs/2305.03518,Black-box Prompt Tuning with Subspace Learning,"Black-box prompt tuning uses derivative-free optimization algorithms to learn prompts in low-dimensional subspaces instead of back-propagating through the network of Large Language Models (LLMs). Recent studies have found that black-box prompt tuning lacks versatility across tasks and LLMs, which we believe is related to the inappropriate choice of subspaces. In this paper, we propose Black-box prompt tuning with Subspace Learning (BSL) to improve the versatility of black-box prompt tuning. Based on the assumption that nearly optimal prompts for similar tasks exist in a common subspace, we propose identifying such subspaces by meta-learning on a set of similar source tasks. Therefore, for a target task that shares similarities with source tasks, we guarantee that optimizing in the subspace can find a prompt that performs well on the target task. Experiments confirm that our BSL framework consistently achieves competitive performance regardless of downstream tasks and LLMs."
831,https://arxiv.org/abs/2305.01094,Performative Prediction with Bandit Feedback: Learning through Reparameterization,"Performative prediction, as introduced by Perdomo et al. (2020), is a framework for studying social prediction in which the data distribution itself changes in response to the deployment of a model. Existing work on optimizing accuracy in this setting hinges on two assumptions that are easily violated in practice: that the performative risk is convex over the deployed model, and that the mapping from the model to the data distribution is known to the model designer in advance. In this paper, we initiate the study of tractable performative prediction problems that do not require these assumptions. To tackle this more challenging setting, we develop a two-level zeroth-order optimization algorithm, where one level aims to compute the distribution map, and the other level reparameterizes the performative prediction objective as a function of the induced data distribution. Under mild conditions, this reparameterization allows us to transform the non-convex objective into a convex one and achieve provable regret guarantees. In particular, we provide a regret bound that is sublinear in the total number of performative samples taken and only polynomial in the dimension of the model parameter."
832,https://arxiv.org/abs/2304.14920,An EEG Channel Selection Framework for Driver Drowsiness Detection via Interpretability Guidance,"Drowsy driving has a crucial influence on driving safety, creating an urgent demand for driver drowsiness detection. Electroencephalogram (EEG) signal can accurately reflect the mental fatigue state and thus has been widely studied in drowsiness monitoring. However, the raw EEG data is inherently noisy and redundant, which is neglected by existing works that just use single-channel EEG data or full-head channel EEG data for model training, resulting in limited performance of driver drowsiness detection. In this paper, we are the first to propose an Interpretability-guided Channel Selection (ICS) framework for the driver drowsiness detection task. Specifically, we design a two-stage training strategy to progressively select the key contributing channels with the guidance of interpretability. We first train a teacher network in the first stage using full-head channel EEG data. Then we apply the class activation mapping (CAM) to the trained teacher model to highlight the high-contributing EEG channels and further propose a channel voting scheme to select the top N contributing EEG channels. Finally, we train a student network with the selected channels of EEG data in the second stage for driver drowsiness detection. Experiments are designed on a public dataset, and the results demonstrate that our method is highly applicable and can significantly improve the performance of cross-subject driver drowsiness detection."
833,https://arxiv.org/abs/2304.14846,Ultrafast and Electrically Tunable Rabi Frequency in a Germanium Hut Wire Hole Spin Qubit,"Hole spin qubits based on germanium (Ge) have strong tunable spin orbit interaction (SOI) and ultrafast qubit operation speed. Here we report that the Rabi frequency (f_Rabi) of a hole spin qubit in a Ge hut wire (HW) double quantum dot (DQD) is electrically tuned through the detuning energy and middle gate voltage (V_M). f_Rabi gradually decreases with increasing detuning energy; on the contrary, f_Rabi is positively correlated with V_M. We attribute our results to the change of electric field on SOI and the contribution of the excited state in quantum dots to f_Rabi. We further demonstrate an ultrafast f_Rabi exceeding 1.2 GHz, which evidences the strong SOI in our device. The discovery of an ultrafast and electrically tunable f_Rabi in a hole spin qubit has potential applications in semiconductor quantum computing."
834,https://arxiv.org/abs/2304.13399,Mechanical cooling at the bistable regime of a dissipative optomechanical cavity with a Kerr medium,"In this paper, we study static bistability and mechanical cooling of a dissipative optomechanical cavity filled with a Kerr medium. The system exhibits optical bistability for a wide input-power range with the power threshold being greatly reduced, in contrast to the case of purely dissipative coupling. At the bistable regime, the membrane can be effectively cooled down to a few millikelvin from the room temperature under the unresolved sideband condition, where the effective mechanical temperature is a nonmonotonic function of intracavity intensity and reaches its minimum near the turning point of the upper stable branch. When the system is in the cryogenics environment, the effective mechanical temperature at the bistable regime shows a similar feature as in the room temperature case, but the optimal cooling appears at the monostable regime and approaches the mechanical ground state. Our results are of interest for further understanding bistable optomechanical systems, which have many applications in nonclassical state preparations and quantum information processing."
835,https://arxiv.org/abs/2304.12310,Fully Sparse Fusion for 3D Object Detection,"Currently prevalent multimodal 3D detection methods are built upon LiDAR-based detectors that usually use dense Bird's-Eye-View (BEV) feature maps. However, the cost of such BEV feature maps is quadratic to the detection range, making it not suitable for long-range detection. Fully sparse architecture is gaining attention as they are highly efficient in long-range perception. In this paper, we study how to effectively leverage image modality in the emerging fully sparse architecture. Particularly, utilizing instance queries, our framework integrates the well-studied 2D instance segmentation into the LiDAR side, which is parallel to the 3D instance segmentation part in the fully sparse detector. This design achieves a uniform query-based fusion framework in both the 2D and 3D sides while maintaining the fully sparse characteristic. Extensive experiments showcase state-of-the-art results on the widely used nuScenes dataset and the long-range Argoverse 2 dataset. Notably, the inference speed of the proposed method under the long-range LiDAR perception setting is 2.7 $\times$ faster than that of other state-of-the-art multimodal 3D detection methods. Code will be released at \url{https://github.com/BraveGroup/FullySparseFusion}."
836,https://arxiv.org/abs/2304.11436,Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack,"Federated Learning with Model Distillation (FedMD) is a nascent collaborative learning paradigm, where only output logits of public datasets are transmitted as distilled knowledge, instead of passing on private model parameters that are susceptible to gradient inversion attacks, a known privacy risk in federated learning. In this paper, we found that even though sharing output logits of public datasets is safer than directly sharing gradients, there still exists a substantial risk of data exposure caused by carefully designed malicious attacks. Our study shows that a malicious server can inject a PLI (Paired-Logits Inversion) attack against FedMD and its variants by training an inversion neural network that exploits the confidence gap between the server and client models. Experiments on multiple facial recognition datasets validate that under FedMD-like schemes, by using paired server-client logits of public datasets only, the malicious server is able to reconstruct private images on all tested benchmarks with a high success rate."
837,https://arxiv.org/abs/2304.11183,Electric conductivity in non-Hermitian holography,"We study the phase structure and charge transport at finite temperature and chemical potential in the non-Hermitian PT-symmetric holographic model of arXiv:1912.06647. The non-Hermitian PT-symmetric deformation is realized by promoting the parameter of a global U(1) symmetry to a complex number. Depending on the strength of the deformation, we find three phases: stable PT-symmetric phase, unstable PT-symmetric phase, and an unstable PT-symmetry broken phase. In the three phases, the square of the condensate and also the spectral weight of the AC conductivity at zero frequency are, respectively, positive, negative, and complex. We check that the Ferrell-Glover-Tinkham sum rule for the AC conductivity holds in all the three phases. We also investigate a complexified U(1) rotor model with PT-symmetric deformation, derive its phase structure and condensation pattern, and find a zero frequency spectral weight analogous to the holographic model."
838,https://arxiv.org/abs/2304.10755,Interpretable and Robust AI in EEG Systems: A Survey,"The close coupling of artificial intelligence (AI) and electroencephalography (EEG) has substantially advanced human-computer interaction (HCI) technologies in the AI era. Different from traditional EEG systems, the interpretability and robustness of AI-based EEG systems are becoming particularly crucial. The interpretability clarifies the inner working mechanisms of AI models and thus can gain the trust of users. The robustness reflects the AI's reliability against attacks and perturbations, which is essential for sensitive and fragile EEG signals. Thus the interpretability and robustness of AI in EEG systems have attracted increasing attention, and their research has achieved great progress recently. However, there is still no survey covering recent advances in this field. In this paper, we present the first comprehensive survey and summarize the interpretable and robust AI techniques for EEG systems. Specifically, we first propose a taxonomy of interpretability by characterizing it into three types: backpropagation, perturbation, and inherently interpretable methods. Then we classify the robustness mechanisms into four classes: noise and artifacts, human variability, data acquisition instability, and adversarial attacks. Finally, we identify several critical and unresolved challenges for interpretable and robust AI in EEG systems and further discuss their future directions."
839,https://arxiv.org/abs/2304.09536,Decadal Temperature Prediction via Chaotic Behavior Tracking,"Decadal temperature prediction provides crucial information for quantifying the expected effects of future climate changes and thus informs strategic planning and decision-making in various domains. However, such long-term predictions are extremely challenging, due to the chaotic nature of temperature variations. Moreover, the usefulness of existing simulation-based and machine learning-based methods for this task is limited because initial simulation or prediction errors increase exponentially over time. To address this challenging task, we devise a novel prediction method involving an information tracking mechanism that aims to track and adapt to changes in temperature dynamics during the prediction phase by providing probabilistic feedback on the prediction error of the next step based on the current prediction. We integrate this information tracking mechanism, which can be considered as a model calibrator, into the objective function of our method to obtain the corrections needed to avoid error accumulation. Our results show the ability of our method to accurately predict global land-surface temperatures over a decadal range. Furthermore, we demonstrate that our results are meaningful in a real-world context: the temperatures predicted using our method are consistent with and can be used to explain the well-known teleconnections within and between different continents."
840,https://arxiv.org/abs/2304.09362,Long-Term Fairness with Unknown Dynamics,"While machine learning can myopically reinforce social inequalities, it may also be used to dynamically seek equitable outcomes. In this paper, we formalize long-term fairness in the context of online reinforcement learning. This formulation can accommodate dynamical control objectives, such as driving equity inherent in the state of a population, that cannot be incorporated into static formulations of fairness. We demonstrate that this framing allows an algorithm to adapt to unknown dynamics by sacrificing short-term incentives to drive a classifier-population system towards more desirable equilibria. For the proposed setting, we develop an algorithm that adapts recent work in online learning. We prove that this algorithm achieves simultaneous probabilistic bounds on cumulative loss and cumulative violations of fairness (as statistical regularities between demographic groups). We compare our proposed algorithm to the repeated retraining of myopic classifiers, as a baseline, and to a deep reinforcement learning algorithm that lacks safety guarantees. Our experiments model human populations according to evolutionary game theory and integrate real-world datasets."
841,https://arxiv.org/abs/2304.09190,Holographic Non-Abelian Flavour Symmetry Breaking,"We investigate a holographic model for both spontaneous and explicit symmetry breaking of non-abelian flavour symmetries. This consists of a bottom-up model inspired by the top-down D3/D7 probe brane model that incorporates the running anomalous dimensions of the fields. We ensure that in the holographic bulk, the full non-abelian flavour symmetries for massless quarks are present. The quark masses are spontaneously generated field values in the bulk and there is a resultant bulk Higgs mechanism. We provide a numerical technique to find the mass eigenvalues for a system of coupled holographic fields. We test this approach using an analytic model of ${\cal N}=2$ supersymmetric matter. We apply this approach to two-flavour QCD with both $u-d$ quark mass splitting and multi-trace bulk action terms that are expected to break $U(N_f)_V$ to $SU(N_f)_V \times U(1)_V$ away from large $N_c$. We also discuss three-flavour QCD with strange quark mass splitting and applications to more exotic symmetry breaking patterns of potential relevance for composite Higgs models."
842,https://arxiv.org/abs/2304.08756,AutoTaskFormer: Searching Vision Transformers for Multi-task Learning,"Vision Transformers have shown great performance in single tasks such as classification and segmentation. However, real-world problems are not isolated, which calls for vision transformers that can perform multiple tasks concurrently. Existing multi-task vision transformers are handcrafted and heavily rely on human expertise. In this work, we propose a novel one-shot neural architecture search framework, dubbed AutoTaskFormer (Automated Multi-Task Vision TransFormer), to automate this process. AutoTaskFormer not only identifies the weights to share across multiple tasks automatically, but also provides thousands of well-trained vision transformers with a wide range of parameters (e.g., number of heads and network depth) for deployment under various resource constraints. Experiments on both small-scale (2-task Cityscapes and 3-task NYUv2) and large-scale (16-task Taskonomy) datasets show that AutoTaskFormer outperforms state-of-the-art handcrafted vision transformers in multi-task learning. The entire code and models will be open-sourced."
843,https://arxiv.org/abs/2304.08733,Do humans and machines have the same eyes? Human-machine perceptual differences on image classification,"Trained computer vision models are assumed to solve vision tasks by imitating human behavior learned from training labels. Most efforts in recent vision research focus on measuring the model task performance using standardized benchmarks. Limited work has been done to understand the perceptual difference between humans and machines. To fill this gap, our study first quantifies and analyzes the statistical distributions of mistakes from the two sources. We then explore human vs. machine expertise after ranking tasks by difficulty levels. Even when humans and machines have similar overall accuracies, the distribution of answers may vary. Leveraging the perceptual difference between humans and machines, we empirically demonstrate a post-hoc human-machine collaboration that outperforms humans or machines alone."
844,https://arxiv.org/abs/2304.08197,Competing charge-density wave instabilities in the kagome metal ScV$_6$Sn$_6$,"Owing to its unique geometry, the kagome lattice hosts various many-body quantum states including frustrated magnetism, superconductivity, and charge-density waves (CDWs), with intense efforts focused on kagome metals exhibiting $2\times2$ CDWs associated with the nesting of van Hove saddle points. Recently, a $\sqrt{3}\times\sqrt{3}$ CDW was discovered in the kagome metal ScV$_6$Sn$_6$ below $T_{\rm CDW}\approx91$~K, whose underlying mechanism and formation process remain unclear. Using inelastic X-ray scattering, we discover a short-range $\sqrt{3}\times\sqrt{3}\times2$ CDW that is dominant in ScV$_6$Sn$_6$ well above $T_{\rm CDW}$, distinct from the $\sqrt{3}\times\sqrt{3}\times3$ CDW below $T_{\rm CDW}$. The short-range CDW grows upon cooling, and is accompanied by the softening of phonons, indicative of its dynamic nature. As the $\sqrt{3}\times\sqrt{3}\times3$ CDW appears, the short-range CDW becomes suppressed, revealing a competition between these CDW instabilities. Our first-principles calculations indicate that the $\sqrt{3}\times\sqrt{3}\times2$ CDW is energetically favored, consistent with experimental observations at high temperatures. However, the $\sqrt{3}\times\sqrt{3}\times3$ CDW is selected as the ground state likely due to a large wavevector-dependent electron-phonon coupling, which also accounts for the enhanced electron scattering above $T_{\rm CDW}$. The competing CDW instabilities in ScV$_6$Sn$_6$ lead to an unusual CDW formation process, with the most pronounced phonon softening and the static CDW occurring at different wavevectors."
845,https://arxiv.org/abs/2304.08083,Causality-aware Visual Scene Discovery for Cross-Modal Question Reasoning,"Existing visual question reasoning methods usually fail to explicitly discover the inherent causal mechanism and ignore the complex event-level understanding that requires jointly modeling cross-modal event temporality and causality. In this paper, we propose an event-level visual question reasoning framework named Cross-Modal Question Reasoning (CMQR), to explicitly discover temporal causal structure and mitigate visual spurious correlation by causal intervention. To explicitly discover visual causal structure, the Visual Causality Discovery (VCD) architecture is proposed to find question-critical scene temporally and disentangle the visual spurious correlations by attention-based front-door causal intervention module named Local-Global Causal Attention Module (LGCAM). To align the fine-grained interactions between linguistic semantics and spatial-temporal representations, we build an Interactive Visual-Linguistic Transformer (IVLT) that builds the multi-modal co-occurrence interactions between visual and linguistic content. Extensive experiments on four datasets demonstrate the superiority of CMQR for discovering visual causal structures and achieving robust question reasoning."
846,https://arxiv.org/abs/2304.07793,Dark matter search with CMB: a study of foregrounds,"The energy injected from dark matter annihilation and decay processes potentially raises the ionisation of the intergalactic medium and leaves visible footprints on the anisotropy maps of the cosmic microwave background (CMB). Galactic foregrounds emission in the microwave bands contaminate the CMB measurement and may affect the search for dark matter's signature. In this paper, we construct a full CMB data and foreground simulation based on the design of the next-generation ground-based CMB experiments. The foreground residual after the components separation on maps is fully considered in our data analysis, accounting for various contamination from the emission of synchrotron, thermal dust, free-free and spinning dust. We analyse the corresponding sensitivity on dark matter parameters from the temperature and polarization maps, and we find that the CMB foregrounds leave a non-zero yet controllable impact on the sensitivity. Comparing with statistics-only analysis, the CMB foreground residual leads to a factor of 7%-23% weakening on energy-injection constraints, depending on the specific dark matter process and experimental configuration. Strong limits on dark matter annihilation rate and decay lifetime can be expected after foreground subtraction."
847,https://arxiv.org/abs/2304.06996,Implementation of electromagnetic analogy to gravity mediated entanglement,"Recently, experiments aimed at measuring gravity mediated entanglement (GME) using quantum information techniques have been proposed, based on the assumption that if two systems get entangled through local interactions with gravitational field, then this field must be quantum. While there is a debate about what could be drawn from GME, quantum simulation might provide some clarification. Here, we present electromagnetic analogy of GME using magnetic-field mediated interaction between the electron and nucleus in a single atom. Our work successfully implements the general procedures of GME experiments and confirms that the mediating field does not support the mean-field description. It also clarifies that, without considering the light-crossing time, the GME experiment would not distinguish a quantum-field-theory description from a quantum-controlled classical field one. Furthermore, this work provides a novel method to construct two-qubit systems in a single atom, and providing the first quantum simulation of GME using material qubits. It helps to conceive the future GME experiments on the scale of light-crossing time."
848,https://arxiv.org/abs/2304.06911,3D Feature Prediction for Masked-AutoEncoder-Based Point Cloud Pretraining,"Masked autoencoders (MAE) have recently been introduced to 3D self-supervised pretraining for point clouds due to their great success in NLP and computer vision. Unlike MAEs used in the image domain, where the pretext task is to restore features at the masked pixels, such as colors, the existing 3D MAE works reconstruct the missing geometry only, i.e, the location of the masked points. In contrast to previous studies, we advocate that point location recovery is inessential and restoring intrinsic point features is much superior. To this end, we propose to ignore point position reconstruction and recover high-order features at masked points including surface normals and surface variations, through a novel attention-based decoder which is independent of the encoder design. We validate the effectiveness of our pretext task and decoder design using different encoder structures for 3D training and demonstrate the advantages of our pretrained networks on various point cloud analysis tasks."
849,https://arxiv.org/abs/2304.06906,Swin3D: A Pretrained Transformer Backbone for 3D Indoor Scene Understanding,"Pretrained backbones with fine-tuning have been widely adopted in 2D vision and natural language processing tasks and demonstrated significant advantages to task-specific networks. In this paper, we present a pretrained 3D backbone, named Swin3D, which first outperforms all state-of-the-art methods in downstream 3D indoor scene understanding tasks. Our backbone network is based on a 3D Swin transformer and carefully designed to efficiently conduct self-attention on sparse voxels with linear memory complexity and capture the irregularity of point signals via generalized contextual relative positional embedding. Based on this backbone design, we pretrained a large Swin3D model on a synthetic Structured3D dataset that is 10 times larger than the ScanNet dataset and fine-tuned the pretrained model in various downstream real-world indoor scene understanding tasks. The results demonstrate that our model pretrained on the synthetic dataset not only exhibits good generality in both downstream segmentation and detection on real 3D point datasets, but also surpasses the state-of-the-art methods on downstream tasks after fine-tuning with +2.3 mIoU and +2.2 mIoU on S3DIS Area5 and 6-fold semantic segmentation, +2.1 mIoU on ScanNet segmentation (val), +1.9 mAP@0.5 on ScanNet detection, +8.1 mAP@0.5 on S3DIS detection. Our method demonstrates the great potential of pretrained 3D backbones with fine-tuning for 3D understanding tasks. The code and models are available at https://github.com/microsoft/Swin3D ."
850,https://arxiv.org/abs/2304.06431,Phonon promoted charge density wave in topological kagome metal ScV$_{6}$Sn$_{6}$,"Charge density wave (CDW) orders in vanadium-based kagome metals have recently received tremendous attention due to their unique properties and intricate interplay with exotic correlated phenomena, topological and symmetry-breaking states. However, the origin of the CDW order remains a topic of debate. The discovery of ScV$_{6}$Sn$_{6}$, a vanadium-based bilayer kagome metal exhibiting an in-plane $\sqrt{3}$ x $\sqrt{3} $ $\textit{R}$30$°$ CDW order with time-reversal symmetry breaking, provides a novel platform to explore the underlying mechanism behind the unconventional CDW. Here, we combine high-resolution angle-resolved photoemission spectroscopy, Raman scattering measurements and density functional theory to investigate the electronic structures and phonon modes of ScV$_{6}$Sn$_{6}$ and their evolution with temperature. We identify topologically nontrivial Dirac surface states and multiple van Hove singularities (VHSs) in the vicinity of the Fermi level, with one VHS near the K point exhibiting nesting wave vectors in proximity to the $\sqrt{3}$ x $\sqrt{3}$ $\textit{R}$30$°$ CDW wave vector. Additionally, Raman measurements indicate a strong intrinsic electron-phonon coupling in ScV$_{6}$Sn$_{6}$, as evidenced by the presence of a two-phonon mode and a large frequency amplitude mode. Our findings highlight the fundamental role of lattice degrees of freedom in promoting the CDW in ScV$_{6}$Sn$_{6}$ and provide important insights into the fascinating correlation phenomena observed in kagome metals."
851,https://arxiv.org/abs/2304.06267,Piggyback on Idle Ride-Sourcing Drivers for Intracity Parcel Delivery,"This paper investigates the operational strategies for an integrated platform that provides both ride-sourcing services and intracity parcel delivery services over a transportation network utilizing the idle time of ride-sourcing drivers. Specifically, the integrated platform simultaneously offers on-demand ride-sourcing services for passengers and multiple modes of parcel delivery services for customers, including: (1) on-demand delivery, where drivers immediately pick up and deliver parcels upon receiving a delivery request; and (2) flexible delivery, where drivers can pick up (or drop off) parcels only when they are idle and waiting for the next ride-sourcing request. A continuous-time Markov Chain (CTMC) model is proposed to characterize the status change of drivers under joint movement of passengers and parcels over the transportation network with limited vehicle capacity, where the service quality of ride-sourcing services, on-demand delivery services, and flexible delivery services are rigorously quantified. Building on the CTMC model, incentives for ride-sourcing passengers, delivery customers, drivers, and the platform are captured through an economic equilibrium model, and the optimal operational decisions of the platform are derived by solving a non-convex profit-maximizing problem. We prove the well-posedness of the model and develop a tailored algorithm to compute the optimal decisions of the platform at an accelerated speed. Furthermore, we validate the proposed model in a comprehensive case study for San Francisco, demonstrating that joint management of ride-sourcing services and intracity package delivery services can lead to a Pareto improvement that benefits all stakeholders in the integrated ride-sourcing and parcel delivery market."
852,https://arxiv.org/abs/2304.04934,Model Sparsification Can Simplify Machine Unlearning,"Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forgetting. One highlight is the 77% unlearning efficacy gain of fine-tuning (one of the simplest approximate unlearning methods) in the proposed sparsity-aware unlearning paradigm. Codes are available at https://github.com/OPTML-Group/Unlearn-Sparse."
853,https://arxiv.org/abs/2304.01340,A Scale-Invariant Trajectory Simplification Method for Efficient Data Collection in Videos,"Training data is a critical requirement for machine learning tasks, and labeled training data can be expensive to acquire, often requiring manual or semi-automated data collection pipelines. For tracking applications, the data collection involves drawing bounding boxes around the classes of interest on each frame, and associate detections of the same ""instance"" over frames. In a semi-automated data collection pipeline, this can be achieved by running a baseline detection and tracking algorithm, and relying on manual correction to add/remove/change bounding boxes on each frame, as well as resolving errors in the associations over frames (track switches). In this paper, we propose a data correction pipeline to generate ground-truth data more efficiently in this semi-automated scenario. Our method simplifies the trajectories from the tracking systems and let the annotator verify and correct the objects in the sampled keyframes. Once the objects in the keyframes are corrected, the bounding boxes in the other frames are obtained by interpolation. Our method achieves substantial reduction in the number of frames requiring manual correction. In the MOT dataset, it reduces the number of frames by 30x while maintaining a HOTA score of 89.61% . Moreover, it reduces the number of frames by a factor of 10x while achieving a HOTA score of 79.24% in the SoccerNet dataset, and 85.79% in the DanceTrack dataset. The project code and data are publicly released at https://github.com/foreverYoungGitHub/trajectory-simplify-benchmark."
854,https://arxiv.org/abs/2303.17723,The cosmological constant is probably still zero,"We consider a wide class of four-dimensional effective field theories in which gravity is coupled to multiple four-forms and their dual scalar fields, with membrane sources charged under the corresponding three-form potentials. Four-form flux, quantised in units of the membrane charges, generically generates a landscape of vacua with a range of values for the cosmological constant that is scanned through membrane nucleation. We list various ways in which the landscape can be made sufficiently dense to be compatible with observations of the current vacuum without running into the empty universe problem. Further, we establish the general criteria required to ensure the absolute stability of the Minkowski vacuum under membrane nucleation and the longevity of those vacua that are parametrically close by. This selects the current vacuum on probabilistic grounds and can even be applied in the classic model of Bousso and Polchinski, albeit with some mild violation of the membrane weak gravity conjecture. We present other models where the membrane weak gravity conjecture is not violated but where the same probabilistic methods can be used to tackle the cosmological constant problem."
855,https://arxiv.org/abs/2303.16634,G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment,"The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval"
856,https://arxiv.org/abs/2303.16290,Two-step flux synthesis of ultrapure transition metal dichalcogenides,"Here, we describe synthesis of TMD crystals using a two-step flux growth method that eliminates a major potential source of contamination. Detailed characterization of TMDs grown by this two-step method reveals charged and isovalent defects with densities an order of magnitude lower than in TMDs grown by a single-step flux technique. Initial temperature-dependent electrical transport measurements of monolayer WSe2 yield room-temperature hole mobility above 840 cm2/Vs and low-temperature disorder-limited mobility above 44,000 cm2/Vs. Electrical transport measurements of graphene-WSe2 heterostructures fabricated from the two-step flux grown WSe2 also show superior performance: higher graphene mobility, lower charged impurity density, and well-resolved integer quantum Hall states."
857,https://arxiv.org/abs/2303.15795,Experimental Twin-Field Quantum Key Distribution Over 1000 km Fiber Distance,"Quantum key distribution (QKD) aims to generate secure private keys shared by two remote parties. With its security being protected by principles of quantum mechanics, some technology challenges remain towards practical application of QKD. The major one is the distance limit, which is caused by the fact that a quantum signal cannot be amplified while the channel loss is exponential with the distance for photon transmission in optical fiber. Here using the 3-intensity sending-or-not-sending protocol with the actively-odd-parity-pairing method, we demonstrate a fiber-based twin-field QKD over 1002 km. In our experiment, we developed a dual-band phase estimation and ultra-low noise superconducting nanowire single-photon detectors to suppress the system noise to around 0.02 Hz. The secure key rate is $9.53\times10^{-12}$ per pulse through 1002 km fiber in the asymptotic regime, and $8.75\times10^{-12}$ per pulse at 952 km considering the finite size effect. Our work constitutes a critical step towards the future large-scale quantum network."
858,https://arxiv.org/abs/2303.14453,Federated Learning without Full Labels: A Survey,"Data privacy has become an increasingly important concern in real-world big data applications such as machine learning. To address the problem, federated learning (FL) has been a promising solution to building effective machine learning models from decentralized and private data. Existing federated learning algorithms mainly tackle the supervised learning problem, where data are assumed to be fully labeled. However, in practice, fully labeled data is often hard to obtain, as the participants may not have sufficient domain expertise, or they lack the motivation and tools to label data. Therefore, the problem of federated learning without full labels is important in real-world FL applications. In this paper, we discuss how the problem can be solved with machine learning techniques that leverage unlabeled data. We present a survey of methods that combine FL with semi-supervised learning, self-supervised learning, and transfer learning methods. We also summarize the datasets used to evaluate FL methods without full labels. Finally, we highlight future directions in the context of FL without full labels."
859,https://arxiv.org/abs/2303.13976,Analysis of the D-wave $Σ$-type charmed baryon states with the QCD sum rules,"We construct the $Σ$-type currents to investigate the D-wave charmed baryon states with the QCD sum rules systematically. The predicted masses $M=3.35^{+0.13}_{-0.18}\,\rm{GeV}$ ($3.33^{+0.13}_{-0.16}\,\rm{GeV}$), $3.34^{+0.14}_{-0.18}\,\rm{GeV}$ ($3.35^{+0.13}_{-0.16}\,\rm{GeV}$) and $3.35^{+0.12}_{-0.13}\,\rm{GeV}$ ($3.35^{+0.12}_{-0.14}\,\rm{GeV}$) for the $Ω_c(0,2,{\frac{1}{2}}^+)$, $Ω_c(0,2,{\frac{3}{2}}^+)$ and $Ω_c(0,2,{\frac{5}{2}}^+)$ states are in excellent agreement with the experimental data $ 3327.1\pm1.2 \mbox{ MeV}$ from the LHCb collaboration, and support assigning the $Ω_c(3327)$ to be the $Σ$-type D-wave $Ω_c$ state with the spin-parity $J^P={\frac{1}{2}}^+$, ${\frac{3}{2}}^+$ or ${\frac{5}{2}}^+$."
860,https://arxiv.org/abs/2303.12801,A Data Augmentation Method and the Embedding Mechanism for Detection and Classification of Pulmonary Nodules on Small Samples,"Detection of pulmonary nodules by CT is used for screening lung cancer in early stages.omputer aided diagnosis (CAD) based on deep-learning method can identify the suspected areas of pulmonary nodules in CT images, thus improving the accuracy and efficiency of CT diagnosis. The accuracy and robustness of deep learning models. Method:In this paper, we explore (1) the data augmentation method based on the generation model and (2) the model structure improvement method based on the embedding mechanism. Two strategies have been introduced in this study: a new data augmentation method and a embedding mechanism. In the augmentation method, a 3D pixel-level statistics algorithm is proposed to generate pulmonary nodule and by combing the faked pulmonary nodule and healthy lung, we generate new pulmonary nodule samples. The embedding mechanism are designed to better understand the meaning of pixels of the pulmonary nodule samples by introducing hidden variables. Result: The result of the 3DVNET model with the augmentation method for pulmonary nodule detection shows that the proposed data augmentation method outperforms the method based on generative adversarial network (GAN) framework, training accuracy improved by 1.5%, and with embedding mechanism for pulmonary nodules classification shows that the embedding mechanism improves the accuracy and robustness for the classification of pulmonary nodules obviously, the model training accuracy is close to 1 and the model testing F1-score is 0.90.Conclusion:he proposed data augmentation method and embedding mechanism are beneficial to improve the accuracy and robustness of the model, and can be further applied in other common diagnostic imaging tasks."
861,https://arxiv.org/abs/2303.12291,Fairness Improves Learning from Noisily Labeled Long-Tailed Data,"Both long-tailed and noisily labeled data frequently appear in real-world applications and impose significant challenges for learning. Most prior works treat either problem in an isolated way and do not explicitly consider the coupling effects of the two. Our empirical observation reveals that such solutions fail to consistently improve the learning when the dataset is long-tailed with label noise. Moreover, with the presence of label noise, existing methods do not observe universal improvements across different sub-populations; in other words, some sub-populations enjoyed the benefits of improved accuracy at the cost of hurting others. Based on these observations, we introduce the Fairness Regularizer (FR), inspired by regularizing the performance gap between any two sub-populations. We show that the introduced fairness regularizer improves the performances of sub-populations on the tail and the overall learning performance. Extensive experiments demonstrate the effectiveness of the proposed solution when complemented with certain existing popular robust or class-balanced methods."
862,https://arxiv.org/abs/2303.11531,A reproducible approach to merging behavior analysis based on High Definition Map,"Existing research on merging behavior generally prioritize the application of various algorithms, but often overlooks the fine-grained process and analysis of trajectories. This leads to the neglect of surrounding vehicle matching, the opaqueness of indicators definition, and reproducible crisis. To address these gaps, this paper presents a reproducible approach to merging behavior analysis. Specifically, we outline the causes of subjectivity and irreproducibility in existing studies. Thereafter, we employ lanelet2 High Definition (HD) map to construct a reproducible framework, that minimizes subjectivities, defines standardized indicators, identifies alongside vehicles, and divides scenarios. A comparative macroscopic and microscopic analysis is subsequently conducted. More importantly, this paper adheres to the Reproducible Research concept, providing all the source codes and reproduction instructions. Our results demonstrate that although scenarios with alongside vehicles occur in less than 6% of cases, their characteristics are significantly different from others, and these scenarios are often accompanied by high risk. This paper refines the understanding of merging behavior, raises awareness of reproducible studies, and serves as a watershed moment."
863,https://arxiv.org/abs/2303.10662,Generalization of the Orthodiagonal Involutive Type of Kokotsakis Flexible Polyhedra,"In this paper we introduce and study a remarkable class of mechanisms formed by a $3 \times 3$ arrangement of rigid and skew quadrilateral faces with revolute joints at the common edges. These Kokotsakis-type mechanisms with a quadrangular base and non-planar faces are a generalization of Izmestiev's orthodiagonal involutive type of Kokotsakis polyhedra formed by planar quadrilateral faces. Our algebraic approach yields a complete characterization of all complexes of the orthodiagonal involutive type. It is shown that one has 8 degrees of freedom to construct such mechanisms. This is illustrated by several examples, including cases that are not possible with planar faces."
864,https://arxiv.org/abs/2303.10079,What Can We Learn from a Semiparametric Factor Analysis of Item Responses and Response Time? An Illustration with the PISA 2015 Data,"It is widely believed that a joint factor analysis of item responses and response time (RT) may yield more precise ability scores that are conventionally predicted from responses only. For this purpose, a simple-structure factor model is often preferred as it only requires specifying an additional measurement model for item-level RT while leaving the original item response theory (IRT) model for responses intact. The added speed factor indicated by item-level RT correlates with the ability factor in the IRT model, allowing RT data to carry additional information about respondents' ability. However, parametric simple-structure factor models are often restrictive and fit poorly to empirical data, which prompts under-confidence in the suitablity of a simple factor structure. In the present paper, we analyze the 2015 Programme for International Student Assessment (PISA) mathematics data using a semiparametric simple-structure model. We conclude that a simple factor structure attains a decent fit after further parametric assumptions in the measurement model are sufficiently relaxed. Furthermore, our semiparametric model implies that the association between latent ability and speed/slowness is strong in the population, but the form of association is nonlinear. It follows that scoring based on the fitted model can substantially improve the precision of ability scores."
865,https://arxiv.org/abs/2303.09789,Urban Regional Function Guided Traffic Flow Prediction,"The prediction of traffic flow is a challenging yet crucial problem in spatial-temporal analysis, which has recently gained increasing interest. In addition to spatial-temporal correlations, the functionality of urban areas also plays a crucial role in traffic flow prediction. However, the exploration of regional functional attributes mainly focuses on adding additional topological structures, ignoring the influence of functional attributes on regional traffic patterns. Different from the existing works, we propose a novel module named POI-MetaBlock, which utilizes the functionality of each region (represented by Point of Interest distribution) as metadata to further mine different traffic characteristics in areas with different functions. Specifically, the proposed POI-MetaBlock employs a self-attention architecture and incorporates POI and time information to generate dynamic attention parameters for each region, which enables the model to fit different traffic patterns of various areas at different times. Furthermore, our lightweight POI-MetaBlock can be easily integrated into conventional traffic flow prediction models. Extensive experiments demonstrate that our module significantly improves the performance of traffic flow prediction and outperforms state-of-the-art methods that use metadata."
866,https://arxiv.org/abs/2303.09600,"Characterisation of the strain rate sensitivity of basal, prismatic and pyramidal slip in Zircaloy-4 using micropillar compression","For polycrystalline deformation in metals such as Zircaloy-4, the slip strength at different strain rates will control mechanical response and strongly influence the anisotropy of plastic deformation. In this work, the slip activity and strain rate sensitivity of the <a> basal, <a> prismatic, and <c+a> pyramidal slip systems were explored by testing at variable strain rates (1E-4 s^-1 and 125 s^-1) using single crystal micropillar compression tests. These systematic experiments enabled the direct fitting of the strain rate sensitivities of the different slip systems using a simple analytical model to reveal that deformation will be accommodated using different slip systems depending on the strain rate of deformation in addition to the stress state (i.e. Schmid's law). This insight helps inform metal forming and understanding of the mechanical performance of these engineering alloys in the extremes of service conditions."
867,https://arxiv.org/abs/2303.09117,Visual-Linguistic Causal Intervention for Radiology Report Generation,"Automatic radiology report generation is essential for computer-aided diagnosis and medication guidance. Importantly, automatic radiology report generation (RRG) can relieve the heavy burden of radiologists by generating medical reports automatically from visual-linguistic data relations. However, due to the spurious correlations within image-text data induced by visual and linguistic biases, it is challenging to generate accurate reports that reliably describe abnormalities. Besides, the cross-modal confounder is usually unobservable and difficult to be eliminated explicitly. In this paper, we mitigate the cross-modal data bias for RRG from a new perspective, i.e., visual-linguistic causal intervention, and propose a novel Visual-Linguistic Causal Intervention (VLCI) framework for RRG, which consists of a visual deconfounding module (VDM) and a linguistic deconfounding module (LDM), to implicitly deconfound the visual-linguistic confounder by causal front-door intervention. Specifically, the VDM explores and disentangles the visual confounder from the patch-based local and global features without object detection due to the absence of universal clinic semantic extraction. Simultaneously, the LDM eliminates the linguistic confounder caused by salient visual features and high-frequency context without constructing specific dictionaries. Extensive experiments on IU-Xray and MIMIC-CXR datasets show that our VLCI outperforms the state-of-the-art RRG methods significantly. Source code and models are available at https://github.com/WissingChen/VLCI."
868,https://arxiv.org/abs/2303.07709,3D Face Arbitrary Style Transfer,"Style transfer of 3D faces has gained more and more attention. However, previous methods mainly use images of artistic faces for style transfer while ignoring arbitrary style images such as abstract paintings. To solve this problem, we propose a novel method, namely Face-guided Dual Style Transfer (FDST). To begin with, FDST employs a 3D decoupling module to separate facial geometry and texture. Then we propose a style fusion strategy for facial geometry. Subsequently, we design an optimization-based DDSG mechanism for textures that can guide the style transfer by two style images. Besides the normal style image input, DDSG can utilize the original face input as another style input as the face prior. By this means, high-quality face arbitrary style transfer results can be obtained. Furthermore, FDST can be applied in many downstream tasks, including region-controllable style transfer, high-fidelity face texture reconstruction, large-pose face reconstruction, and artistic face reconstruction. Comprehensive quantitative and qualitative results show that our method can achieve comparable performance. All source codes and pre-trained weights will be released to the public."
869,https://arxiv.org/abs/2303.07259,A novel approach of empirical likelihood with massive data,"Statistical analysis of large datasets is a challenge because of the limitation of computing devices' memory and excessive computation time. Divide and Conquer (DC) algorithm is an effective solution path, but the DC algorithm still has limitations for statistical inference. Empirical likelihood is an important semiparametric and nonparametric statistical method for parameter estimation and statistical inference, and the estimating equation builds a bridge between empirical likelihood and traditional statistical methods, which makes empirical likelihood widely used in various traditional statistical models. In this paper, we propose a novel approach to address the challenges posed by empirical likelihood with massive data, which is called split sample mean empirical likelihood(SSMEL), our approach provides a unique perspective for sovling big data problem. We show that the SSMEL estimator has the same estimation efficiency as the empirical likelihood estimator with the full dataset, and maintains the important statistical property of Wilks' theorem, allowing our proposed approach to be used for statistical inference. The effectiveness of the proposed approach is illustrated using simulation studies and real data analysis."
870,https://arxiv.org/abs/2303.05705,Forecasts of CMB lensing reconstruction of AliCPT-1 from the foreground cleaned polarization data,"Cosmic microwave background radiation (CMB) observations are unavoidably contaminated by emission from various extra-galactic foregrounds, which must be removed to obtain reliable measurements of the cosmological signal. In this paper, we demonstrate CMB lensing reconstruction in AliCPT-1 after foreground removal, combine the two bands of AliCPT-1 (90 and 150~GHz) with Planck HFI bands (100, 143, 217 and 353~GHz) and with the WMAP-K band (23~GHz). In order to balance contamination by instrumental noise and foreground residual bias, we adopt the Needlet Internal Linear Combination (NILC) method to clean the E-map and the constrained Internal Linear Combination (cILC) method to clean the B-map. The latter utilizes additional constraints on average frequency scaling of the dust and synchrotron to remove foregrounds at the expense of somewhat noisier maps. Assuming 4 modules observing 1 season from simulation data, the resulting effective residual noise in E- and B-map are roughly $15~μ{\rm K}\cdot{\rm arcmin}$ and $25~μ{\rm K}\cdot{\rm arcmin}$, respectively. As a result, the CMB lensing reconstruction signal-to-noise ratio (SNR) from polarization data is about SNR$\,\approx\,$4.5. This lensing reconstruction capability is comparable to that of other stage-III small aperture millimeter CMB telescopes."
871,https://arxiv.org/abs/2303.04582,Interaction-induced topological pumping in a solid-state quantum system,"As the basis for generating multi-particle quantum correlations, inter-particle interaction plays a crucial role in collective quantum phenomena, quantum phase transitions, and quantum information processing. It can profoundly alter the band structure of quantum many-body systems and give rise to exotic topological phenomena. Conventional topological pumping, which has been well demonstrated in driven linear or noninteracting systems, may break down in the presence of strong interaction. However, the interplay between band topology and interaction could also induce emergent topological pumping of interacting particles, but its experimental realization has proven challenging. Here we demonstrate interaction-induced topological pumping in a solid-state quantum system comprising an array of 36 superconducting qubits. With strong interaction inherent in the qubits and site-resolved controllability of the lattice potential and hopping strength, we realize the topological Thouless pumping of single and two bounded particles. Beyond these topological phenomena with linear or noninteracting counterparts, we also observe topologically resonant tunneling and asymmetric edge-state transport of interacting particles. Our work creates a paradigm for multi-particle topological effects, and provides a new pathway to the study of exotic topological phenomena, many-body quantum transport, and quantum information transfer."
872,https://arxiv.org/abs/2303.03486,Sampling-based Exploration for Reinforcement Learning of Dexterous Manipulation,"In this paper, we present a novel method for achieving dexterous manipulation of complex objects, while simultaneously securing the object without the use of passive support surfaces. We posit that a key difficulty for training such policies in a Reinforcement Learning framework is the difficulty of exploring the problem state space, as the accessible regions of this space form a complex structure along manifolds of a high-dimensional space. To address this challenge, we use two versions of the non-holonomic Rapidly-Exploring Random Trees algorithm; one version is more general, but requires explicit use of the environment's transition function, while the second version uses manipulation-specific kinematic constraints to attain better sample efficiency. In both cases, we use states found via sampling-based exploration to generate reset distributions that enable training control policies under full dynamic constraints via model-free Reinforcement Learning. We show that these policies are effective at manipulation problems of higher difficulty than previously shown, and also transfer effectively to real robots. Videos of the real-hand demonstrations can be found on the project website: https://sbrl.cs.columbia.edu/"
873,https://arxiv.org/abs/2303.02096,A Practical Study on Developing Mathematical Computation Ability of Ninth-Grade Students,"Practical research was conducted to cultivate students' mathematical computation ability using literature analysis, theoretical practice, and statistical analysis methods. The research involved 171 ninth-grade students from the author's school and was divided into two experimental groups (A and B) and a control group (C). The study aimed to improve students' mathematical computation ability through algorithm analysis and comparison. The results showed that after a long period of practice, there was an improvement in students' computational ability, and most students could reach the level of high school mathematical computation ability. However, further research is needed to determine how to achieve level three of mathematical computation ability.
  The study found that students' computational ability levels were similar in the experimental and control groups at the beginning of the study. Through targeted discussion courses, students' computational ability levels were improved. The degree of improvement was not significantly related to students' gender but had a moderate positive correlation with the number of times students participated in related discussions.
  Based on the practical results, the author proposes several suggestions to help teachers improve students' mathematical computation ability. Firstly, students' interest in learning computation can be stimulated by arousing their curiosity, using typical problems to motivate their interest, and encouraging their participation. Secondly, teachers should pay more attention to selecting typical problems to help students better understand computational logic. Lastly, in addition to understanding computational logic, teachers should also focus on developing students' mathematical thinking quality, enabling students to select optimal computational strategies and improve their algorithm selection abilities."
874,https://arxiv.org/abs/2303.01901,Unconventional fully-gapped superconductivity in the heavy-fermion metal CeCu$_2$Si$_2$,"The heavy-fermion metal CeCu$_2$Si$_2$ was the first discovered unconventional, non-phonon-mediated superconductor, and for a long time was believed to exhibit single-band $d$-wave superconductivity, as inferred from various measurements hinting at a nodal gap structure. More recently however, measurements using a range of techniques at very low temperatures ($T \lessapprox 0.1$ K) provided evidence for a fully-gapped superconducting order parameter. In this Colloquium, after a brief historical overview we survey the apparently conflicting results of numerous experimental studies on this compound. We then address the different theoretical scenarios which have been applied to understand the particular gap structure, including both isotropic (sign-preserving) and anisotropic two-band $s$-wave superconductivity, as well as an effective two-band $d$-wave model, where the latter can explain the currently available experimental data on CeCu$_2$Si$_2$. The lessons from CeCu$_2$Si$_2$ are expected to help uncover the Cooper-pair states in other unconventional, fully-gapped superconductors with strongly correlated carriers, and in particular highlight the rich variety of such states enabled by orbital degrees of freedom."
875,https://arxiv.org/abs/2303.01426,Topological phononic metamaterials,"The concept of topological energy bands and their manifestations have been demonstrated in condensed matter systems as a fantastic paradigm toward unprecedented physical phenomena and properties that are robust against disorders. Recent years, this paradigm was extended to phononic metamaterials (including mechanical and acoustic metamaterials), giving rise to the discovery of remarkable phenomena that were not observed elsewhere thanks to the extraordinary controllability and tunability of phononic metamaterials as well as versatile measuring techniques. These phenomena include, but not limited to, topological negative refraction, topological 'sasers' (i.e., the phonon analog of lasers), higher-order topological insulating states, non-Abelian topological phases, higher-order Weyl semimetal phases, Majorana-like modes in Dirac vortex structures and fragile topological phases with spectral flows. Here we review the developments in the field of topological phononic metamaterials from both theoretical and experimental perspectives with emphasis on the underlying physics principles. To give a broad view of topological phononics, we also discuss the synergy with non-Hermitian effects and cover topics including synthetic dimensions, artificial gauge fields, Floquet topological acoustics, bulk topological transport, topological pumping, and topological active matters as well as potential applications, materials fabrications and measurements of topological phononic metamaterials. Finally, we discuss the challenges, opportunities and future developments in this intriguing field and its potential impact on physics and materials science."
876,https://arxiv.org/abs/2303.00040,Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training,"The correlation between the vision and text is essential for video moment retrieval (VMR), however, existing methods heavily rely on separate pre-training feature extractors for visual and textual understanding. Without sufficient temporal boundary annotations, it is non-trivial to learn universal video-text alignments. In this work, we explore multi-modal correlations derived from large-scale image-text data to facilitate generalisable VMR. To address the limitations of image-text pre-training models on capturing the video changes, we propose a generic method, referred to as Visual-Dynamic Injection (VDI), to empower the model's understanding of video moments. Whilst existing VMR methods are focusing on building temporal-aware video features, being aware of the text descriptions about the temporal changes is also critical but originally overlooked in pre-training by matching static images with sentences. Therefore, we extract visual context and spatial dynamic information from video frames and explicitly enforce their alignments with the phrases describing video changes (e.g. verb). By doing so, the potentially relevant visual and motion patterns in videos are encoded in the corresponding text embeddings (injected) so to enable more accurate video-text alignments. We conduct extensive experiments on two VMR benchmark datasets (Charades-STA and ActivityNet-Captions) and achieve state-of-the-art performances. Especially, VDI yields notable advantages when being tested on the out-of-distribution splits where the testing samples involve novel scenes and vocabulary."
877,https://arxiv.org/abs/2302.13313,ARPES signature of the competition between magnetic order and Kondo effect in CeCoGe3,"The competition between magnetic order and Kondo effect is essential for the rich physics of heavy fermion systems. Nevertheless, how such competition is manifested in the quasiparticle bands in a real periodic lattice remains elusive in spectroscopic experiments. Here we report a high-resolution photoemission study of the antiferromagnetic Kondo lattice system CeCoGe3 with a high TN1 of 21K. Our measurements reveal a weakly dispersive 4f band at the Fermi level near the Z point, arisingfrom moderate Kondo effect. The intensity of this heavy 4f band exhibits a logarithmic increase with lowering temperature and begins to deviate from this Kondo-like behavior below 25 K, just above TN1, and eventually ceases to grow below 12 K. Our work provides direct spectroscopic evidence for the competition between magnetic order and the Kondo effect in a Kondo lattice system with local-moment antiferromagnetism, indicating a distinct scenario for the microscopic coexistence and competition of these phenomena, which might be related to the real-space modulation."
878,https://arxiv.org/abs/2302.12369,Factual Consistency Oriented Speech Recognition,"This paper presents a novel optimization framework for automatic speech recognition (ASR) with the aim of reducing hallucinations produced by an ASR model. The proposed framework optimizes the ASR model to maximize an expected factual consistency score between ASR hypotheses and ground-truth transcriptions, where the factual consistency score is computed by a separately trained estimator. Experimental results using the AMI meeting corpus and the VoxPopuli corpus show that the ASR model trained with the proposed framework generates ASR hypotheses that have significantly higher consistency scores with ground-truth transcriptions while maintaining the word error rates close to those of cross entropy-trained ASR models. Furthermore, it is shown that training the ASR models with the proposed framework improves the speech summarization quality as measured by the factual consistency of meeting conversation summaries generated by a large language model."
879,https://arxiv.org/abs/2302.11521,How Does In-Context Learning Help Prompt Tuning?,"Fine-tuning large language models is becoming ever more impractical due to their rapidly-growing scale. This motivates the use of parameter-efficient adaptation methods such as prompt tuning (PT), which adds a small number of tunable embeddings to an otherwise frozen model, and in-context learning (ICL), in which demonstrations of the task are provided to the model in natural language without any additional training. Recently, Singhal et al. (2022) propose ``instruction prompt tuning'' (IPT), which combines PT with ICL by concatenating a natural language demonstration with learned prompt embeddings. While all of these methods have proven effective on different tasks, how they interact with each other remains unexplored. In this paper, we empirically study when and how in-context examples improve prompt tuning by measuring the effectiveness of ICL, PT, and IPT on five text generation tasks with multiple base language models. We observe that (1) IPT does \emph{not} always outperform PT, and in fact requires the in-context demonstration to be semantically similar to the test input to yield improvements; (2) PT is unstable and exhibits high variance, but combining PT and ICL (into IPT) consistently reduces variance across all five tasks; and (3) prompts learned for a specific source task via PT exhibit positive transfer when paired with in-context examples of a different target task. Our results offer actionable insights on choosing a suitable parameter-efficient adaptation method for a given task."
880,https://arxiv.org/abs/2302.10454,KG-ECO: Knowledge Graph Enhanced Entity Correction for Query Rewriting,"Query Rewriting (QR) plays a critical role in large-scale dialogue systems for reducing frictions. When there is an entity error, it imposes extra challenges for a dialogue system to produce satisfactory responses. In this work, we propose KG-ECO: Knowledge Graph enhanced Entity COrrection for query rewriting, an entity correction system with corrupt entity span detection and entity retrieval/re-ranking functionalities. To boost the model performance, we incorporate Knowledge Graph (KG) to provide entity structural information (neighboring entities encoded by graph neural networks) and textual information (KG entity descriptions encoded by RoBERTa). Experimental results show that our approach yields a clear performance gain over two baselines: utterance level QR and entity correction without utilizing KG information. The proposed system is particularly effective for few-shot learning cases where target entities are rarely seen in training or there is a KG relation between the target entity and other contextual entities in the query."
881,https://arxiv.org/abs/2302.10417,FedSDG-FS: Efficient and Secure Feature Selection for Vertical Federated Learning,"Vertical Federated Learning (VFL) enables multiple data owners, each holding a different subset of features about largely overlapping sets of data sample(s), to jointly train a useful global model. Feature selection (FS) is important to VFL. It is still an open research problem as existing FS works designed for VFL either assumes prior knowledge on the number of noisy features or prior knowledge on the post-training threshold of useful features to be selected, making them unsuitable for practical applications. To bridge this gap, we propose the Federated Stochastic Dual-Gate based Feature Selection (FedSDG-FS) approach. It consists of a Gaussian stochastic dual-gate to efficiently approximate the probability of a feature being selected, with privacy protection through Partially Homomorphic Encryption without a trusted third-party. To reduce overhead, we propose a feature importance initialization method based on Gini impurity, which can accomplish its goals with only two parameter transmissions between the server and the clients. Extensive experiments on both synthetic and real-world datasets show that FedSDG-FS significantly outperforms existing approaches in terms of achieving accurate selection of high-quality features as well as building global models with improved performance."
882,https://arxiv.org/abs/2302.09342,Semi-Analytical Electromagnetic Transient Simulation Using Differential Transformation,"For electromagnetic transient (EMT) simulation of a power system, a state-space-based approach needs to solve state-space EMT equations by using numerical integration methods, e.g., the Euler method, Runge-Kutta methods, and trapezoidal-rule method, at small time steps. The simulation can be slow on a power system having multiple generators. To speed up state-space-based EMT simulations, this paper proposes a Differential Transformation based semi-analytical method that repeatedly utilizes a high-order semi-analytical solution of the EMT equations at longer time steps. The proposed semi-analytical method is tested on the detailed EMT model of a four-generator two-area system. Simulation results show the significant potential of the proposed method to accelerate EMT simulations of power systems compared with traditional numerical methods."
883,https://arxiv.org/abs/2302.09341,A Heterogeneous Multiscale Method for Power System Simulation Considering Electromagnetic Transients,"Traditional dynamic security assessment faces challenges as power systems are experiencing a transformation to inverter-based-resource (IBR) dominated systems, for which electromagnetic transient (EMT) dynamics have to be considered. However, EMT simulation is time-consuming especially for a large power grid because the mathematical model based on detailed component modeling is highly stiff and needs to be integrated at tiny time steps due to numerical stability. This paper proposes a heterogeneous multiscale method (HMM) to address the simulation of a power system considering EMT dynamics as a multiscale problem. The method aims to accurately simulate the macroscopic dynamics of the system even when EMT dynamics are dominating. By force estimation using a kernel function, the proposed method automatically generates a macro model on the fly of simulation based on the micro model of EMT dynamics. It can flexibly switch between the micro- and macro-models to capture important EMT dynamics during some time intervals while skipping over other time intervals of less interest to achieve a superior simulation speed. The method is illustrated by a case study on a two-machine EMT model to demonstrate its potential for power system simulation."
884,https://arxiv.org/abs/2302.09170,KILM: Knowledge Injection into Encoder-Decoder Language Models,"Large pre-trained language models (PLMs) have been shown to retain implicit knowledge within their parameters. To enhance this implicit knowledge, we propose Knowledge Injection into Language Models (KILM), a novel approach that injects entity-related knowledge into encoder-decoder PLMs, via a generative knowledge infilling objective through continued pre-training. This is done without architectural modifications to the PLMs or adding additional parameters. Experimental results over a suite of knowledge-intensive tasks spanning numerous datasets show that KILM enables models to retain more knowledge and hallucinate less, while preserving their original performance on general NLU and NLG tasks. KILM also demonstrates improved zero-shot performances on tasks such as entity disambiguation, outperforming state-of-the-art models having 30x more parameters."
885,https://arxiv.org/abs/2302.08888,Multimodal Federated Learning via Contrastive Representation Ensemble,"With the increasing amount of multimedia data on modern mobile systems and IoT infrastructures, harnessing these rich multimodal data without breaching user privacy becomes a critical issue. Federated learning (FL) serves as a privacy-conscious alternative to centralized machine learning. However, existing FL methods extended to multimodal data all rely on model aggregation on single modality level, which restrains the server and clients to have identical model architecture for each modality. This limits the global model in terms of both model complexity and data capacity, not to mention task diversity. In this work, we propose Contrastive Representation Ensemble and Aggregation for Multimodal FL (CreamFL), a multimodal federated learning framework that enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset. To achieve better multimodal representation fusion, we design a global-local cross-modal ensemble strategy to aggregate client representations. To mitigate local model drift caused by two unprecedented heterogeneous factors stemming from multimodal discrepancy (modality gap and task gap), we further propose two inter-modal and intra-modal contrasts to regularize local training, which complements information of the absent modality for uni-modal clients and regularizes local clients to head towards global consensus. Thorough evaluations and ablation studies on image-text retrieval and visual question answering tasks showcase the superiority of CreamFL over state-of-the-art FL methods and its practical value."
886,https://arxiv.org/abs/2302.08764,Adversarial Contrastive Distillation with Adaptive Denoising,"Adversarial Robustness Distillation (ARD) is a novel method to boost the robustness of small models. Unlike general adversarial training, its robust knowledge transfer can be less easily restricted by the model capacity. However, the teacher model that provides the robustness of knowledge does not always make correct predictions, interfering with the student's robust performances. Besides, in the previous ARD methods, the robustness comes entirely from one-to-one imitation, ignoring the relationship between examples. To this end, we propose a novel structured ARD method called Contrastive Relationship DeNoise Distillation (CRDND). We design an adaptive compensation module to model the instability of the teacher. Moreover, we utilize the contrastive relationship to explore implicit robustness knowledge among multiple examples. Experimental results on multiple attack benchmarks show CRDND can transfer robust knowledge efficiently and achieves state-of-the-art performances."
887,https://arxiv.org/abs/2302.08756,Deterministic quantum teleportation between distant superconducting chips,"Quantum teleportation~\cite{Bennett1993} is of both fundamental interest and great practical importance in quantum information science. To date, quantum teleportation has been implemented in various physical systems~\cite{Pirandola2015}, among which superconducting qubits are of particular practical significance as they emerge as a leading system to realize large-scale quantum computation~\cite{Arute2019,Wu2021}. Nevertheless, the number of superconducting qubits on the same chip is severely limited by the available chip size, the cooling power, and the wiring complexity. Realization of quantum teleportation and remote computation over qubits on distant superconducting chips is a key quantum communication technology to scaling up the system through a distributed quantum computational network~\cite{Gottesman1999,Eisert2000,Jiang2007,Kimble2008,Monroe2014}. However, this goal has not been realized yet in experiments due to the technical challenge of making a quantum interconnect between distant superconducting chips and the inefficient transfer of flying microwave photons over the lossy interconnects~\cite{Kurpiers2018,Axline2018,Campagne2018,Magnard2020}. Here we demonstrate deterministic teleportation of quantum states and entangling gates between distant superconducting chips connected by a 64-meter-long cable bus featuring an ultralow loss of 0.32~dB/km at cryogenic temperatures, where high fidelity remote entanglement is generated via flying microwave photons utilizing time-reversal-symmetry~\cite{Cirac1997,Korotkov2011}. Apart from the fundamental interest of teleporting macroscopic superconducting qubits over a long distance, our work lays a foundation to realization of large-scale superconducting quantum computation through a distributed computational network~\cite{Gottesman1999,Eisert2000,Jiang2007,Kimble2008,Monroe2014}."
888,https://arxiv.org/abs/2302.08543,Numerical analysis of a multistable capsule system under the delayed feedback control with a constant delay,"The vibro-impact capsule system is a self-propelled mechanism that has abundant coexisting attractors and moves rectilinearly under periodic excitation when overcoming environmental resistance. In this paper, we study the control of coexisting attractors in this system by using a delayed feedback controller (DFC) with a constant delay. The aim of our control is to steer this complex system toward an attractor with preferable performance characteristics among multiple coexisting attractors, e.g., a periodically fast forward progression. For this purpose, we give an example of a feedback-controlled transition from a period-3 motion with low progression speed to a period-1 motion with high progression speed at the system parameters where both responses coexist. The effectiveness of this controller is investigated numerically by considering its convergence time and the required control energy input to achieve transition. We combine pseudo-spectral approximation of the delay, event detection for the discontinuities and path-following (continuation) techniques for non-smooth delay dynamical systems to carry out bifurcation analysis. We systematically study the dynamical performance of the controlled system when varying its control gain and delay time. Our numerical simulations show the effectiveness of DFC under a wide range of system parameters. We find that the desired period-1 motion is achievable in a range of control delays between a period-doubling and a grazing bifurcation. Therefore, two-parameter continuation of these two bifurcations with respect to the control delay and control gain is conducted to identify the delay-gain parameter region where the period-1 motion is stable. The findings of this work can be used for tuning control parameters in experiments, and similar analysis can be carried out for other non-smooth dynamical systems with a constant delay term."
889,https://arxiv.org/abs/2302.08116,Entropy-bounded solutions to the Cauchy problem of compressible planar non-resistive magnetohydrodynamics equations with far field vacuum,"We investigate the Cauchy problem to the compressible planar non-resistive magnetohydrodynamic equations with zero heat conduction. The global existence of strong solutions to such a model has been established by Li and Li (J. Differential Equations 316: 136--157, 2022). However, to our best knowledge, so far there is no result on the behavior of the entropy near the vacuum region for this model. The main novelty of this paper is to give a positive response to this problem. More precisely, by a series of a priori estimates, especially the singular type estimates, we show that the boundedness of the entropy can be propagated up to any finite time provided that the initial vacuum presents only at far fields with sufficiently slow decay of the initial density."
890,https://arxiv.org/abs/2305.14685,Fusion-in-T5: Unifying Document Ranking Signals for Improved Information Retrieval,"Common IR pipelines are typically cascade systems that may involve multiple rankers and/or fusion models to integrate different information step-by-step. In this paper, we propose a novel re-ranker named Fusion-in-T5 (FiT5), which integrates document text information, retrieval features, and global document information into a single unified model using templated-based input and global attention. Experiments on passage ranking benchmarks MS MARCO and TREC DL show that FiT5 significantly improves ranking performance over prior pipelines. Analyses find that through global attention, FiT5 is able to jointly utilize the ranking features via gradually attending to related documents, and thus improve the detection of subtle nuances between them. Our code will be open-sourced."
891,https://arxiv.org/abs/2305.14318,CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation,"Large Language Models (LLMs) have demonstrated significant progress in utilizing external APIs as tools for various tasks. However, their tool-using ability is limited by the availability of suitable APIs and the instability of implicit reasoning, particularly when simultaneously engaging in reasoning about plans and actual calculations. To address these limitations, we propose CREATOR, a novel framework that empowers LLMs to create their own tools through documentation and code realization. CREATOR disentangles the LLM's ability into two distinct phases: abstract tool creation and concrete decision execution, which results in improved LLM performance. We evaluate CREATOR on two established benchmarks: MATH, which consists of challenging math competition problems, and TabMWP, which includes diverse tabular contents for problem-solving. Remarkably, CREATOR significantly outperforms existing chain-of-thought (CoT), program-of-thought (PoT), and tool-using baselines on these two benchmarks. Additionally, we present a new dataset, Creation Challenge, comprising 2K diverse questions, to highlight the necessity and benefits of LLMs' tool creation ability in effectively addressing these problems. Furthermore, our research reveals that leveraging LLMs as tool creators facilitates knowledge transfer, and LLMs exhibit varying levels of tool creation abilities, enabling them to flexibly tackle diverse situations. Our study represents a promising avenue for maximizing the potential of LLMs and advancing toward truly intelligent and adaptable AI systems."
892,https://arxiv.org/abs/2305.14233,Enhancing Chat Language Models by Scaling High-quality Instructional Conversations,"Fine-tuning on instruction data has been widely validated as an effective practice for implementing chat language models like ChatGPT. Scaling the diversity and quality of such data, although straightforward, stands a great chance of leading to improved performance. This paper aims to improve the upper bound of open-source models further. We first provide a systematically designed, diverse, informative, large-scale dataset of instructional conversations, UltraChat, which does not involve human queries. Our objective is to capture the breadth of interactions that a human might have with an AI assistant and employs a comprehensive framework to generate multi-turn conversation iteratively. UltraChat contains 1.5 million high-quality multi-turn dialogues and covers a wide range of topics and instructions. Our statistical analysis of UltraChat reveals its superiority in various key metrics, including scale, average length, diversity, coherence, etc., solidifying its position as a leading open-source dataset. Building upon UltraChat, we fine-tune a LLaMA model to create a powerful conversational model, UltraLLaMA. Our evaluations indicate that UltraLLaMA consistently outperforms other open-source models, including Vicuna, the previously recognized state-of-the-art open-source model. The dataset and the model will be publicly released\footnote{\url{https://github.com/thunlp/UltraChat}}."
893,https://arxiv.org/abs/2305.11540,Efficient Cross-Lingual Transfer for Chinese Stable Diffusion with Images as Pivots,"Diffusion models have made impressive progress in text-to-image synthesis. However, training such large-scale models (e.g. Stable Diffusion), from scratch requires high computational costs and massive high-quality text-image pairs, which becomes unaffordable in other languages. To handle this challenge, we propose IAP, a simple but effective method to transfer English Stable Diffusion into Chinese. IAP optimizes only a separate Chinese text encoder with all other parameters fixed to align Chinese semantics space to the English one in CLIP. To achieve this, we innovatively treat images as pivots and minimize the distance of attentive features produced from cross-attention between images and each language respectively. In this way, IAP establishes connections of Chinese, English and visual semantics in CLIP's embedding space efficiently, advancing the quality of the generated image with direct Chinese prompts. Experimental results show that our method outperforms several strong Chinese diffusion models with only 5%~10% training data."
894,https://arxiv.org/abs/2305.08702,Recyclable Tuning for Continual Pre-training,"Continual pre-training is the paradigm where pre-trained language models (PLMs) continually acquire fresh knowledge from growing data and gradually get upgraded. Before an upgraded PLM is released, we may have tuned the original PLM for various tasks and stored the adapted weights. However, when tuning the upgraded PLM, these outdated adapted weights will typically be ignored and discarded, causing a potential waste of resources. We bring this issue to the forefront and contend that proper algorithms for recycling outdated adapted weights should be developed. To this end, we formulate the task of recyclable tuning for continual pre-training. In pilot studies, we find that after continual pre-training, the upgraded PLM remains compatible with the outdated adapted weights to some extent. Motivated by this finding, we analyze the connection between continually pre-trained PLMs from two novel aspects, i.e., mode connectivity, and functional similarity. Based on the corresponding findings, we propose both an initialization-based method and a distillation-based method for our task. We demonstrate their feasibility in improving the convergence and performance for tuning the upgraded PLM. We also show that both methods can be combined to achieve better performance. The source codes are publicly available at https://github.com/thunlp/RecyclableTuning."
895,https://arxiv.org/abs/2305.06849,WebCPM: Interactive Web Search for Chinese Long-form Question Answering,"Long-form question answering (LFQA) aims at answering complex, open-ended questions with detailed, paragraph-length responses. The de facto paradigm of LFQA necessitates two procedures: information retrieval, which searches for relevant supporting facts, and information synthesis, which integrates these facts into a coherent answer. In this paper, we introduce WebCPM, the first Chinese LFQA dataset. One unique feature of WebCPM is that its information retrieval is based on interactive web search, which engages with a search engine in real time. Following WebGPT, we develop a web search interface. We recruit annotators to search for relevant information using our interface and then answer questions. Meanwhile, the web search behaviors of our annotators would be recorded. In total, we collect 5,500 high-quality question-answer pairs, together with 14,315 supporting facts and 121,330 web search actions. We fine-tune pre-trained language models to imitate human behaviors for web search and to generate answers based on the collected facts. Our LFQA pipeline, built on these fine-tuned models, generates answers that are no worse than human-written ones in 32.5% and 47.5% of the cases on our dataset and DuReader, respectively."
896,https://arxiv.org/abs/2305.02496,Revisiting Graph Contrastive Learning for Anomaly Detection,"Combining Graph neural networks (GNNs) with contrastive learning for anomaly detection has drawn rising attention recently. Existing graph contrastive anomaly detection (GCAD) methods have primarily focused on improving detection capability through graph augmentation and multi-scale contrast modules. However, the underlying mechanisms of how these modules work have not been fully explored. We dive into the multi-scale and graph augmentation mechanism and observed that multi-scale contrast modules do not enhance the expression, while the multi-GNN modules are the hidden contributors. Previous studies have tended to attribute the benefits brought by multi-GNN to the multi-scale modules. In the paper, we delve into the misconception and propose Multi-GNN and Augmented Graph contrastive framework MAG, which unified the existing GCAD methods in the contrastive self-supervised perspective. We extracted two variants from the MAG framework, L-MAG and M-MAG. The L-MAG is the lightweight instance of the MAG, which outperform the state-of-the-art on Cora and Pubmed with the low computational cost. The variant M-MAG equipped with multi-GNN modules further improve the detection performance. Our study sheds light on the drawback of the existing GCAD methods and demonstrates the potential of multi-GNN and graph augmentation modules. Our code is available at https://github.com/liuyishoua/MAG-Framework."
897,https://arxiv.org/abs/2305.01278,Transfer Visual Prompt Generator across LLMs,"While developing a new vision-language LLM (VL-LLM) by pre-training on tremendous image-text pairs from scratch can be exceedingly resource-consuming, connecting an existing LLM with a comparatively lightweight visual prompt generator (VPG) becomes a feasible paradigm. However, further tuning the VPG part of the VL-LLM still suffers from indispensable computational costs, i.e., requiring thousands of GPU hours and millions of training data. One alternative solution is to transfer an existing VPG from any existing VL-LLMs for the target VL-LLM.
  In this work, we for the first time investigate the VPG transferability across LLMs, and explore a solution to reduce the cost of VPG transfer. We first study the VPG transfer across different LLM sizes (e.g., small-to-large), and across different LLM types, through which we diagnose the key factors to maximize the transfer efficiency. Based on our observation, we design a two-stage transfer framework named VPGTrans, which is simple yet highly effective. Through extensive experiments, we demonstrate that VPGTrans helps significantly speed up the transfer learning process without compromising performance. Remarkably, it helps achieve the VPG transfer from BLIP-2 OPT$_\text{2.7B}$ to BLIP-2 OPT$_\text{6.7B}$ with over 10 times speed-up and 10.7% training data compared with connecting a VPG to OPT$_\text{6.7B}$ from scratch. Further, a series of intriguing findings and potential rationales behind them are provided and discussed. Finally, we showcase the practical value of our VPGTrans approach, by customizing two novel VL-LLMs, including VL-LLaMA and VL-Vicuna, with recently released LLaMA and Vicuna LLMs."
898,https://arxiv.org/abs/2304.08354,Tool Learning with Foundation Models,"Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of foundation models, AI systems have the potential to be equally adept in tool use as humans. This paradigm, i.e., tool learning with foundation models, combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research into tool-augmented and tool-oriented learning. We formulate a general tool learning framework: starting from understanding the user instruction, models should learn to decompose a complex task into several subtasks, dynamically adjust their plan through reasoning, and effectively conquer each sub-task by selecting appropriate tools. We also discuss how to train models for improved tool-use capabilities and facilitate the generalization in tool learning. Considering the lack of a systematic tool learning evaluation in prior works, we experiment with 17 representative tools and show the potential of current foundation models in skillfully utilizing tools. Finally, we discuss several open problems that require further investigation for tool learning. Overall, we hope this paper could inspire future research in integrating tools with foundation models."
899,https://arxiv.org/abs/2304.05845,Rethinking Dense Retrieval's Few-Shot Ability,"Few-shot dense retrieval (DR) aims to effectively generalize to novel search scenarios by learning a few samples. Despite its importance, there is little study on specialized datasets and standardized evaluation protocols. As a result, current methods often resort to random sampling from supervised datasets to create ""few-data"" setups and employ inconsistent training strategies during evaluations, which poses a challenge in accurately comparing recent progress. In this paper, we propose a customized FewDR dataset and a unified evaluation benchmark. Specifically, FewDR employs class-wise sampling to establish a standardized ""few-shot"" setting with finely-defined classes, reducing variability in multiple sampling rounds. Moreover, the dataset is disjointed into base and novel classes, allowing DR models to be continuously trained on ample data from base classes and a few samples in novel classes. This benchmark eliminates the risk of novel class leakage, providing a reliable estimation of the DR model's few-shot ability. Our extensive empirical results reveal that current state-of-the-art DR models still face challenges in the standard few-shot scene. Our code and data will be open-sourced at https://github.com/OpenMatch/ANCE-Tele."
900,https://arxiv.org/abs/2304.04052,Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder,"The sequence-to-sequence (seq2seq) task aims at generating the target sequence based on the given input source sequence. Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture. This paper aims to address this gap by conducting a detailed comparison between the encoder-decoder architecture and the decoder-only language model framework through the analysis of a regularized encoder-decoder structure. This structure is designed to replicate all behaviors in the classical decoder-only language model but has an encoder and a decoder making it easier to be compared with the classical encoder-decoder structure. Based on the analysis, we unveil the attention degeneration problem in the language model, namely, as the generation step number grows, less and less attention is focused on the source sequence. To give a quantitative understanding of this problem, we conduct a theoretical sensitivity analysis of the attention output with respect to the source input. Grounded on our analysis, we propose a novel partial attention language model to solve the attention degeneration problem. Experimental results on machine translation, summarization, and data-to-text generation tasks support our analysis and demonstrate the effectiveness of our proposed model."
901,https://arxiv.org/abs/2303.04925,Exploring Spin AGP Ansatze for Strongly Correlated Spin Systems,"The antisymmetrized geminal power (AGP), a wave function equivalent to number-projected Hartree-Fock-Bogoliubov (HFB), has proven to be an excellent reference for strong pairing interactions. Several correlation methods have also been applied on top of AGP. In this work, we show how AGP can also be applied to spin systems by simply basing its formulation on a spin $su(2)$ algebra. We here implement spin AGP and spin AGP-based correlation techniques on the XXZ and $\mathrm{J_1-J_2}$ Heisenberg models, both in 1 and 2 dimensions. Our results indicate that AGP is a promising starting pointing for modeling spin systems."
902,https://arxiv.org/abs/2302.09582,Human Emotion Knowledge Representation Emerges in Large Language Model and Supports Discrete Emotion Inference,"How humans infer discrete emotions is a fundamental research question in the field of psychology. While conceptual knowledge about emotions (emotion knowledge) has been suggested to be essential for emotion inference, evidence to date is mostly indirect and inconclusive. As the large language models (LLMs) have been shown to support effective representations of various human conceptual knowledge, the present study further employed artificial neurons in LLMs to investigate the mechanism of human emotion inference. With artificial neurons activated by prompts, the LLM (RoBERTa) demonstrated a similar conceptual structure of 27 discrete emotions as that of human behaviors. Furthermore, the LLM-based conceptual structure revealed a human-like reliance on 14 underlying conceptual attributes of emotions for emotion inference. Most importantly, by manipulating attribute-specific neurons, we found that the corresponding LLM's emotion inference performance deteriorated, and the performance deterioration was correlated to the effectiveness of representations of the conceptual attributes on the human side. Our findings provide direct evidence for the emergence of emotion knowledge representation in large language models and suggest its casual support for discrete emotion inference."
903,https://arxiv.org/abs/2302.07324,READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises,"For many real-world applications, the user-generated inputs usually contain various noises due to speech recognition errors caused by linguistic variations1 or typographical errors (typos). Thus, it is crucial to test model performance on data with realistic input noises to ensure robustness and fairness. However, little study has been done to construct such benchmarks for Chinese, where various language-specific input noises happen in the real world. In order to fill this important gap, we construct READIN: a Chinese multi-task benchmark with REalistic And Diverse Input Noises. READIN contains four diverse tasks and requests annotators to re-enter the original test data with two commonly used Chinese input methods: Pinyin input and speech input. We designed our annotation pipeline to maximize diversity, for example by instructing the annotators to use diverse input method editors (IMEs) for keyboard noises and recruiting speakers from diverse dialectical groups for speech noises. We experiment with a series of strong pretrained language models as well as robust training methods, we find that these models often suffer significant performance drops on READIN even with robustness methods like data augmentation. As the first large-scale attempt in creating a benchmark with noises geared towards user-generated inputs, we believe that READIN serves as an important complement to existing Chinese NLP benchmarks. The source code and dataset can be obtained from https://github.com/thunlp/READIN."
904,https://arxiv.org/abs/2302.05541,Semi-supervised Large-scale Fiber Detection in Material Images with Synthetic Data,"Accurate detection of large-scale, elliptical-shape fibers, including their parameters of center, orientation and major/minor axes, on the 2D cross-sectioned image slices is very important for characterizing the underlying cylinder 3D structures in microscopic material images. Detecting fibers in a degraded image poses a challenge to both current fiber detection and ellipse detection methods. This paper proposes a new semi-supervised deep learning method for large-scale elliptical fiber detection with synthetic data, which frees people from heavy data annotations and is robust to various kinds of image degradations. A domain adaptation strategy is utilized to reduce the domain distribution discrepancy between the synthetic data and the real data, and a new Region of Interest (RoI)-ellipse learning and a novel RoI ranking with the symmetry constraint are embedded in the proposed method. Experiments on real microscopic material images demonstrate the effectiveness of the proposed approach in large-scale fiber detection."
905,https://arxiv.org/abs/2212.08408,Decoder Tuning: Efficient Language Understanding as Decoding,"With the evergrowing sizes of pre-trained models (PTMs), it has been an emerging practice to only provide the inference APIs for users, namely model-as-a-service (MaaS) setting. To adapt PTMs with model parameters frozen, most current approaches focus on the input side, seeking for powerful prompts to stimulate models for correct answers. However, we argue that input-side adaptation could be arduous due to the lack of gradient signals and they usually require thousands of API queries, resulting in high computation and time costs. In light of this, we present Decoder Tuning (DecT), which in contrast optimizes task-specific decoder networks on the output side. Specifically, DecT first extracts prompt-stimulated output scores for initial predictions. On top of that, we train an additional decoder network on the output representations to incorporate posterior data knowledge. By gradient-based optimization, DecT can be trained within several seconds and requires only one PTM query per sample. Empirically, we conduct extensive natural language understanding experiments and show that DecT significantly outperforms state-of-the-art algorithms with a $200\times$ speed-up."
906,https://arxiv.org/abs/2212.05478,Mul-GAD: a semi-supervised graph anomaly detection framework via aggregating multi-view information,"Anomaly detection is defined as discovering patterns that do not conform to the expected behavior. Previously, anomaly detection was mostly conducted using traditional shallow learning techniques, but with little improvement. As the emergence of graph neural networks (GNN), graph anomaly detection has been greatly developed. However, recent studies have shown that GNN-based methods encounter challenge, in that no graph anomaly detection algorithm can perform generalization on most datasets. To bridge the tap, we propose a multi-view fusion approach for graph anomaly detection (Mul-GAD). The view-level fusion captures the extent of significance between different views, while the feature-level fusion makes full use of complementary information. We theoretically and experimentally elaborate the effectiveness of the fusion strategies. For a more comprehensive conclusion, we further investigate the effect of the objective function and the number of fused views on detection performance. Exploiting these findings, our Mul-GAD is proposed equipped with fusion strategies and the well-performed objective function. Compared with other state-of-the-art detection methods, we achieve a better detection performance and generalization in most scenarios via a series of experiments conducted on Pubmed, Amazon Computer, Amazon Photo, Weibo and Books. Our code is available at https://github.com/liuyishoua/Mul-Graph-Fusion."
907,https://arxiv.org/abs/2211.12054,Visually Grounded Commonsense Knowledge Acquisition,"Large-scale commonsense knowledge bases empower a broad range of AI applications, where the automatic extraction of commonsense knowledge (CKE) is a fundamental and challenging problem. CKE from text is known for suffering from the inherent sparsity and reporting bias of commonsense in text. Visual perception, on the other hand, contains rich commonsense knowledge about real-world entities, e.g., (person, can_hold, bottle), which can serve as promising sources for acquiring grounded commonsense knowledge. In this work, we present CLEVER, which formulates CKE as a distantly supervised multi-instance learning problem, where models learn to summarize commonsense relations from a bag of images about an entity pair without any human annotation on image instances. To address the problem, CLEVER leverages vision-language pre-training models for deep understanding of each image in the bag, and selects informative instances from the bag to summarize commonsense entity relations via a novel contrastive attention mechanism. Comprehensive experimental results in held-out and human evaluation show that CLEVER can extract commonsense knowledge in promising quality, outperforming pre-trained language model-based methods by 3.9 AUC and 6.4 mAUC points. The predicted commonsense scores show strong correlation with human judgment with a 0.78 Spearman coefficient. Moreover, the extracted commonsense can also be grounded into images with reasonable interpretability. The data and codes can be obtained at https://github.com/thunlp/CLEVER."
908,https://arxiv.org/abs/2211.06840,FPT: Improving Prompt Tuning Efficiency via Progressive Training,"Recently, prompt tuning (PT) has gained increasing attention as a parameter-efficient way of tuning pre-trained language models (PLMs). Despite extensively reducing the number of tunable parameters and achieving satisfying performance, PT is training-inefficient due to its slow convergence. To improve PT's training efficiency, we first make some novel observations about the prompt transferability of ""partial PLMs"", which are defined by compressing a PLM in depth or width. We observe that the soft prompts learned by different partial PLMs of various sizes are similar in the parameter space, implying that these soft prompts could potentially be transferred among partial PLMs. Inspired by these observations, we propose Fast Prompt Tuning (FPT), which starts by conducting PT using a small-scale partial PLM, and then progressively expands its depth and width until the full-model size. After each expansion, we recycle the previously learned soft prompts as initialization for the enlarged partial PLM and then proceed PT. We demonstrate the feasibility of FPT on 5 tasks and show that FPT could save over 30% training computations while achieving comparable performance."
909,https://arxiv.org/abs/2211.05319,Few-shot Classification with Hypersphere Modeling of Prototypes,"Metric-based meta-learning is one of the de facto standards in few-shot learning. It composes of representation learning and metrics calculation designs. Previous works construct class representations in different ways, varying from mean output embedding to covariance and distributions. However, using embeddings in space lacks expressivity and cannot capture class information robustly, while statistical complex modeling poses difficulty to metric designs. In this work, we use tensor fields (``areas'') to model classes from the geometrical perspective for few-shot learning. We present a simple and effective method, dubbed hypersphere prototypes (HyperProto), where class information is represented by hyperspheres with dynamic sizes with two sets of learnable parameters: the hypersphere's center and the radius. Extending from points to areas, hyperspheres are much more expressive than embeddings. Moreover, it is more convenient to perform metric-based classification with hypersphere prototypes than statistical modeling, as we only need to calculate the distance from a data point to the surface of the hypersphere. Following this idea, we also develop two variants of prototypes under other measurements. Extensive experiments and analysis on few-shot learning tasks across NLP and CV and comparison with 20+ competitive baselines demonstrate the effectiveness of our approach."
910,https://arxiv.org/abs/2211.00151,A Close Look into the Calibration of Pre-trained Language Models,"Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs' calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don't learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs' confidence in wrong predictions. The code is available at \url{https://github.com/lifan-yuan/PLMCalibration}."
911,https://arxiv.org/abs/2210.17167,Reduce Catastrophic Forgetting of Dense Retrieval Training with Teleportation Negatives,"In this paper, we investigate the instability in the standard dense retrieval training, which iterates between model training and hard negative selection using the being-trained model. We show the catastrophic forgetting phenomena behind the training instability, where models learn and forget different negative groups during training iterations. We then propose ANCE-Tele, which accumulates momentum negatives from past iterations and approximates future iterations using lookahead negatives, as ""teleportations"" along the time axis to smooth the learning process. On web search and OpenQA, ANCE-Tele outperforms previous state-of-the-art systems of similar size, eliminates the dependency on sparse retrieval negatives, and is competitive among systems using significantly more (50x) parameters. Our analysis demonstrates that teleportation negatives reduce catastrophic forgetting and improve convergence speed for dense retrieval training. Our code is available at https://github.com/OpenMatch/ANCE-Tele."
912,https://arxiv.org/abs/2210.14102,Exploring Mode Connectivity for Pre-trained Language Models,"Recent years have witnessed the prevalent application of pre-trained language models (PLMs) in NLP. From the perspective of parameter space, PLMs provide generic initialization, starting from which high-performance minima could be found. Although plenty of works have studied how to effectively and efficiently adapt PLMs to high-performance minima, little is known about the connection of various minima reached under different adaptation configurations. In this paper, we investigate the geometric connections of different minima through the lens of mode connectivity, which measures whether two minima can be connected with a low-loss path. We conduct empirical analyses to investigate three questions: (1) how could hyperparameters, specific tuning methods, and training data affect PLM's mode connectivity? (2) How does mode connectivity change during pre-training? (3) How does the PLM's task knowledge change along the path connecting two minima? In general, exploring the mode connectivity of PLMs conduces to understanding the geometric connection of different minima, which may help us fathom the inner workings of PLM downstream adaptation."
913,https://arxiv.org/abs/2210.13311,Different Tunes Played with Equal Skill: Exploring a Unified Optimization Subspace for Delta Tuning,"Delta tuning (DET, also known as parameter-efficient tuning) is deemed as the new paradigm for using pre-trained language models (PLMs). Up to now, various DETs with distinct design elements have been proposed, achieving performance on par with fine-tuning. However, the mechanisms behind the above success are still under-explored, especially the connections among various DETs. To fathom the mystery, we hypothesize that the adaptations of different DETs could all be reparameterized as low-dimensional optimizations in a unified optimization subspace, which could be found by jointly decomposing independent solutions of different DETs. Then we explore the connections among different DETs by conducting optimization within the subspace. In experiments, we find that, for a certain DET, conducting optimization simply in the subspace could achieve comparable performance to its original space, and the found solution in the subspace could be transferred to another DET and achieve non-trivial performance. We also visualize the performance landscape of the subspace and find that there exists a substantial region where different DETs all perform well. Finally, we extend our analysis and show the strong connections between fine-tuning and DETs."
914,https://arxiv.org/abs/2210.10683,Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP,"Textual adversarial samples play important roles in multiple subfields of NLP research, including security, evaluation, explainability, and data augmentation. However, most work mixes all these roles, obscuring the problem definitions and research goals of the security role that aims to reveal the practical concerns of NLP models. In this paper, we rethink the research paradigm of textual adversarial samples in security scenarios. We discuss the deficiencies in previous work and propose our suggestions that the research on the Security-oriented adversarial NLP (SoadNLP) should: (1) evaluate their methods on security tasks to demonstrate the real-world concerns; (2) consider real-world attackers' goals, instead of developing impractical methods. To this end, we first collect, process, and release a security datasets collection Advbench. Then, we reformalize the task and adjust the emphasis on different goals in SoadNLP. Next, we propose a simple method based on heuristic rules that can easily fulfill the actual adversarial goals to simulate real-world attack methods. We conduct experiments on both the attack and the defense sides on Advbench. Experimental results show that our method has higher practical value, indicating that the research paradigm in SoadNLP may start from our new benchmark. All the code and data of Advbench can be obtained at \url{https://github.com/thunlp/Advbench}."
915,https://arxiv.org/abs/2209.09401,Automatic Label Sequence Generation for Prompting Sequence-to-sequence Models,"Prompting, which casts downstream applications as language modeling tasks, has shown to be sample efficient compared to standard fine-tuning with pre-trained models. However, one pitfall of prompting is the need of manually-designed patterns, whose outcome can be unintuitive and requires large validation sets to tune. To tackle the challenge, we propose AutoSeq, a fully automatic prompting method: (1) We adopt natural language prompts on sequence-to-sequence models, enabling free-form generation and larger label search space; (2) We propose label sequences -- phrases with indefinite lengths to verbalize the labels -- which eliminate the need of manual templates and are more expressive than single label words; (3) We use beam search to automatically generate a large amount of label sequence candidates and propose contrastive re-ranking to get the best combinations. AutoSeq significantly outperforms other no-manual-design methods, such as soft prompt tuning, adapter tuning, and automatic search on single label words; the generated label sequences are even better than curated manual ones on a variety of tasks. Our method reveals the potential of sequence-to-sequence models in few-shot learning and sheds light on a path to generic and automatic prompting. The source code of this paper can be obtained from https://github.com/thunlp/Seq2Seq-Prompt."
916,https://arxiv.org/abs/2209.00179,Universal Vision-Language Dense Retrieval: Learning A Unified Representation Space for Multi-Modal Retrieval,"This paper presents Universal Vision-Language Dense Retrieval (UniVL-DR), which builds a unified model for multi-modal retrieval. UniVL-DR encodes queries and multi-modality resources in an embedding space for searching candidates from different modalities. To learn a unified embedding space for multi-modal retrieval, UniVL-DR proposes two techniques: 1) Universal embedding optimization strategy, which contrastively optimizes the embedding space using the modality-balanced hard negatives; 2) Image verbalization method, which bridges the modality gap between images and texts in the raw data space. UniVL-DR achieves the state-of-the-art on the multi-modal open-domain question answering benchmark, WebQA, and outperforms all retrieval models on the two subtasks, text-text retrieval and text-image retrieval. It demonstrates that universal multi-modal search is feasible to replace the divide-and-conquer pipeline with a united model and also benefits single/cross modality tasks. All source codes of this work are available at https://github.com/OpenMatch/UniVL-DR."
917,https://arxiv.org/abs/2208.03229,Improving Task Generalization via Unified Schema Prompt,"Task generalization has been a long standing challenge in Natural Language Processing (NLP). Recent research attempts to improve the task generalization ability of pre-trained language models by mapping NLP tasks into human-readable prompted forms. However, these approaches require laborious and inflexible manual collection of prompts, and different prompts on the same downstream task may receive unstable performance. We propose Unified Schema Prompt, a flexible and extensible prompting method, which automatically customizes the learnable prompts for each task according to the task input schema. It models the shared knowledge between tasks, while keeping the characteristics of different task schema, and thus enhances task generalization ability. The schema prompt takes the explicit data structure of each task to formulate prompts so that little human effort is involved. To test the task generalization ability of schema prompt at scale, we conduct schema prompt-based multitask pre-training on a wide variety of general NLP tasks. The framework achieves strong zero-shot and few-shot generalization performance on 16 unseen downstream tasks from 8 task types (e.g., QA, NLI, etc). Furthermore, comprehensive analyses demonstrate the effectiveness of each component in the schema prompt, its flexibility in task compositionality, and its ability to improve performance under a full-data fine-tuning setting."
918,https://arxiv.org/abs/2207.05280,Effective Few-Shot Named Entity Linking by Meta-Learning,"Entity linking aims to link ambiguous mentions to their corresponding entities in a knowledge base, which is significant and fundamental for various downstream applications, e.g., knowledge base completion, question answering, and information extraction. While great efforts have been devoted to this task, most of these studies follow the assumption that large-scale labeled data is available. However, when the labeled data is insufficient for specific domains due to labor-intensive annotation work, the performance of existing algorithms will suffer an intolerable decline. In this paper, we endeavor to solve the problem of few-shot entity linking, which only requires a minimal amount of in-domain labeled data and is more practical in real situations. Specifically, we firstly propose a novel weak supervision strategy to generate non-trivial synthetic entity-mention pairs based on mention rewriting. Since the quality of the synthetic data has a critical impact on effective model training, we further design a meta-learning mechanism to assign different weights to each synthetic entity-mention pair automatically. Through this way, we can profoundly exploit rich and precious semantic information to derive a well-trained entity linking model under the few-shot setting. The experiments on real-world datasets show that the proposed method can extensively improve the state-of-the-art few-shot entity linking model and achieve impressive performance when only a small amount of labeled data is available. Moreover, we also demonstrate the outstanding ability of the model's transferability."
919,https://arxiv.org/abs/2206.09355,A Unified Understanding of Deep NLP Models for Text Classification,"The rapid development of deep natural language processing (NLP) models for text classification has led to an urgent need for a unified understanding of these models proposed individually. Existing methods cannot meet the need for understanding different models in one framework due to the lack of a unified measure for explaining both low-level (e.g., words) and high-level (e.g., phrases) features. We have developed a visual analysis tool, DeepNLPVis, to enable a unified understanding of NLP models for text classification. The key idea is a mutual information-based measure, which provides quantitative explanations on how each layer of a model maintains the information of input words in a sample. We model the intra- and inter-word information at each layer measuring the importance of a word to the final prediction as well as the relationships between words, such as the formation of phrases. A multi-level visualization, which consists of a corpus-level, a sample-level, and a word-level visualization, supports the analysis from the overall training set to individual samples. Two case studies on classification tasks and comparison between models demonstrate that DeepNLPVis can help users effectively identify potential problems caused by samples and model architectures and then make informed improvements."
920,https://arxiv.org/abs/2206.08514,A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks,"Textual backdoor attacks are a kind of practical threat to NLP systems. By injecting a backdoor in the training phase, the adversary could control model predictions via predefined triggers. As various attack and defense models have been proposed, it is of great significance to perform rigorous evaluations. However, we highlight two issues in previous backdoor learning evaluations: (1) The differences between real-world scenarios (e.g. releasing poisoned datasets or models) are neglected, and we argue that each scenario has its own constraints and concerns, thus requires specific evaluation protocols; (2) The evaluation metrics only consider whether the attacks could flip the models' predictions on poisoned samples and retain performances on benign samples, but ignore that poisoned samples should also be stealthy and semantic-preserving. To address these issues, we categorize existing works into three practical scenarios in which attackers release datasets, pre-trained models, and fine-tuned models respectively, then discuss their unique evaluation methodologies. On metrics, to completely evaluate poisoned samples, we use grammar error increase and perplexity difference for stealthiness, along with text similarity for validity. After formalizing the frameworks, we develop an open-source toolkit OpenBackdoor to foster the implementations and evaluations of textual backdoor learning. With this toolkit, we perform extensive experiments to benchmark attack and defense models under the suggested paradigm. To facilitate the underexplored defenses against poisoned datasets, we further propose CUBE, a simple yet strong clustering-based defense baseline. We hope that our frameworks and benchmarks could serve as the cornerstones for future model development and evaluations."
921,https://arxiv.org/abs/2206.07382,Sparse Structure Search for Parameter-Efficient Tuning,"Adapting large pre-trained models (PTMs) through fine-tuning imposes prohibitive computational and storage burdens. Recent studies of parameter-efficient tuning (PET) find that only optimizing a small portion of parameters conditioned on PTMs could yield on-par performance compared to conventional fine-tuning. Generally, PET methods exquisitely design parameter-efficient modules (PET modules) which could be applied to arbitrary fine-grained positions inside PTMs. However, the effectiveness of these fine-grained positions largely relies on sophisticated manual designation, thereby usually producing sub-optimal results. In contrast to the manual designation, we explore constructing PET modules in an automatic manner. We automatically \textbf{S}earch for the \textbf{S}parse \textbf{S}tructure of \textbf{P}arameter-\textbf{E}fficient \textbf{T}uning (S$^3$PET). Based on a unified framework of various PET methods, S$^3$PET conducts the differentiable PET structure search through bi-level optimization and proposes shifted global sigmoid method to explicitly control the number of trainable parameters. Extensive experiments show that S$^3$PET surpasses manual and random structures with less trainable parameters. The searched structures preserve more than 99\% fine-tuning performance with 0.01\% trainable parameters. Moreover, the advantage of S$^3$PET is amplified with extremely low trainable parameters budgets (0.0009\%$\sim$0.01\%). The searched structures are transferable and explainable, providing suggestions and guidance for the future design of PET methods."
922,https://arxiv.org/abs/2205.11169,PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models,"Vision-language pre-training (VLP) has shown impressive performance on a wide range of cross-modal tasks, where VLP models without reliance on object detectors are becoming the mainstream due to their superior computation efficiency and competitive performance. However, the removal of object detectors also deprives the capability of VLP models in explicit object modeling, which is essential to various position-sensitive vision-language (VL) tasks, such as referring expression comprehension and visual commonsense reasoning. To address the challenge, we introduce PEVL that enhances the pre-training and prompt tuning of VLP models with explicit object position modeling. Specifically, PEVL reformulates discretized object positions and language in a unified language modeling framework, which facilitates explicit VL alignment during pre-training, and also enables flexible prompt tuning for various downstream tasks. We show that PEVL enables state-of-the-art performance of detector-free VLP models on position-sensitive tasks such as referring expression comprehension and phrase grounding, and also improves the performance on position-insensitive tasks with grounded inputs. We make the data and code for this paper publicly available at https://github.com/thunlp/PEVL."
923,https://arxiv.org/abs/2205.11166,Prompt Tuning for Discriminative Pre-trained Language Models,"Recent works have shown promising results of prompt tuning in stimulating pre-trained language models (PLMs) for natural language processing (NLP) tasks. However, to the best of our knowledge, existing works focus on prompt-tuning generative PLMs that are pre-trained to generate target tokens, such as BERT. It is still unknown whether and how discriminative PLMs, e.g., ELECTRA, can be effectively prompt-tuned. In this work, we present DPT, the first prompt tuning framework for discriminative PLMs, which reformulates NLP tasks into a discriminative language modeling problem. Comprehensive experiments on text classification and question answering show that, compared with vanilla fine-tuning, DPT achieves significantly higher performance, and also prevents the unstable problem in tuning large PLMs in both full-set and low-resource settings. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/DPT."
924,https://arxiv.org/abs/2205.04040,ProQA: Structural Prompt-based Pre-training for Unified Question Answering,"Question Answering (QA) is a longstanding challenge in natural language processing. Existing QA works mostly focus on specific question types, knowledge domains, or reasoning skills. The specialty in QA research hinders systems from modeling commonalities between tasks and generalization for wider applications. To address this issue, we present ProQA, a unified QA paradigm that solves various tasks through a single model. ProQA takes a unified structural prompt as the bridge and improves the QA-centric ability by structural prompt-based pre-training. Through a structurally designed prompt-based input schema, ProQA concurrently models the knowledge generalization for all QA tasks while keeping the knowledge customization for every specific QA task. Furthermore, ProQA is pre-trained with structural prompt-formatted large-scale synthesized corpus, which empowers the model with the commonly-required QA ability. Experimental results on 11 QA benchmarks demonstrate that ProQA consistently boosts performance on both full data fine-tuning, few-shot learning, and zero-shot testing scenarios. Furthermore, ProQA exhibits strong ability in both continual learning and transfer learning by taking the advantages of the structural prompt."
925,https://arxiv.org/abs/2205.03284,Dimension Reduction for Efficient Dense Retrieval via Conditional Autoencoder,"Dense retrievers encode queries and documents and map them in an embedding space using pre-trained language models. These embeddings need to be high-dimensional to fit training signals and guarantee the retrieval effectiveness of dense retrievers. However, these high-dimensional embeddings lead to larger index storage and higher retrieval latency. To reduce the embedding dimensions of dense retrieval, this paper proposes a Conditional Autoencoder (ConAE) to compress the high-dimensional embeddings to maintain the same embedding distribution and better recover the ranking features. Our experiments show that ConAE is effective in compressing embeddings by achieving comparable ranking performance with its teacher model and making the retrieval system more efficient. Our further analyses show that ConAE can alleviate the redundancy of the embeddings of dense retrieval with only one linear layer. All codes of this work are available at https://github.com/NEUIR/ConAE."
926,https://arxiv.org/abs/2205.01886,P^3 Ranker: Mitigating the Gaps between Pre-training and Ranking Fine-tuning with Prompt-based Learning and Pre-finetuning,"Compared to other language tasks, applying pre-trained language models (PLMs) for search ranking often requires more nuances and training signals. In this paper, we identify and study the two mismatches between pre-training and ranking fine-tuning: the training schema gap regarding the differences in training objectives and model architectures, and the task knowledge gap considering the discrepancy between the knowledge needed in ranking and that learned during pre-training. To mitigate these gaps, we propose Pre-trained, Prompt-learned and Pre-finetuned Neural Ranker (P^3 Ranker). P^3 Ranker leverages prompt-based learning to convert the ranking task into a pre-training like schema and uses pre-finetuning to initialize the model on intermediate supervised tasks. Experiments on MS MARCO and Robust04 show the superior performances of P^3 Ranker in few-shot ranking. Analyses reveal that P^3 Ranker is able to better accustom to the ranking task through prompt-based learning and retrieve necessary ranking-oriented knowledge gleaned in pre-finetuning, resulting in data-efficient PLM adaptation. Our code is available at https://github.com/NEUIR/P3Ranker."
927,https://arxiv.org/abs/2204.05239,Exploring the Universal Vulnerability of Prompt-based Learning Paradigm,"Prompt-based learning paradigm bridges the gap between pre-training and fine-tuning, and works effectively under the few-shot setting. However, we find that this learning paradigm inherits the vulnerability from the pre-training stage, where model predictions can be misled by inserting certain triggers into the text. In this paper, we explore this universal vulnerability by either injecting backdoor triggers or searching for adversarial triggers on pre-trained language models using only plain text. In both scenarios, we demonstrate that our triggers can totally control or severely decrease the performance of prompt-based models fine-tuned on arbitrary downstream tasks, reflecting the universal vulnerability of the prompt-based learning paradigm. Further experiments show that adversarial triggers have good transferability among language models. We also find conventional fine-tuning models are not vulnerable to adversarial triggers constructed from pre-trained language models. We conclude by proposing a potential solution to mitigate our attack methods. Code and data are publicly available at https://github.com/leix28/prompt-universal-vulnerability"
928,https://arxiv.org/abs/2203.11654,Fine-Grained Scene Graph Generation with Data Transfer,"Scene graph generation (SGG) is designed to extract (subject, predicate, object) triplets in images. Recent works have made a steady progress on SGG, and provide useful tools for high-level vision and language understanding. However, due to the data distribution problems including long-tail distribution and semantic ambiguity, the predictions of current SGG models tend to collapse to several frequent but uninformative predicates (e.g., on, at), which limits practical application of these models in downstream tasks. To deal with the problems above, we propose a novel Internal and External Data Transfer (IETrans) method, which can be applied in a plug-and-play fashion and expanded to large SGG with 1,807 predicate classes. Our IETrans tries to relieve the data distribution problem by automatically creating an enhanced dataset that provides more sufficient and coherent annotations for all predicates. By training on the enhanced dataset, a Neural Motif model doubles the macro performance while maintaining competitive micro performance. The code and data are publicly available at https://github.com/waxnkw/IETrans-SGG.pytorch."
929,https://arxiv.org/abs/2203.09770,Prototypical Verbalizer for Prompt-based Few-shot Tuning,"Prompt-based tuning for pre-trained language models (PLMs) has shown its effectiveness in few-shot learning. Typically, prompt-based tuning wraps the input text into a cloze question. To make predictions, the model maps the output words to labels via a verbalizer, which is either manually designed or automatically built. However, manual verbalizers heavily depend on domain-specific prior knowledge and human efforts, while finding appropriate label words automatically still remains challenging.In this work, we propose the prototypical verbalizer (ProtoVerb) which is built directly from training data. Specifically, ProtoVerb learns prototype vectors as verbalizers by contrastive learning. In this way, the prototypes summarize training instances and are able to enclose rich class-level semantics. We conduct experiments on both topic classification and entity typing tasks, and the results demonstrate that ProtoVerb significantly outperforms current automatic verbalizers, especially when training data is extremely scarce. More surprisingly, ProtoVerb consistently boosts prompt-based tuning even on untuned PLMs, indicating an elegant non-tuning way to utilize PLMs. Our codes are avaliable at https://github.com/thunlp/OpenPrompt."
930,https://arxiv.org/abs/2203.07426,Sememe Prediction for BabelNet Synsets using Multilingual and Multimodal Information,"In linguistics, a sememe is defined as the minimum semantic unit of languages. Sememe knowledge bases (KBs), which are built by manually annotating words with sememes, have been successfully applied to various NLP tasks. However, existing sememe KBs only cover a few languages, which hinders the wide utilization of sememes. To address this issue, the task of sememe prediction for BabelNet synsets (SPBS) is presented, aiming to build a multilingual sememe KB based on BabelNet, a multilingual encyclopedia dictionary. By automatically predicting sememes for a BabelNet synset, the words in many languages in the synset would obtain sememe annotations simultaneously. However, previous SPBS methods have not taken full advantage of the abundant information in BabelNet. In this paper, we utilize the multilingual synonyms, multilingual glosses and images in BabelNet for SPBS. We design a multimodal information fusion model to encode and combine this information for sememe prediction. Experimental results show the substantial outperformance of our model over previous methods (about 10 MAP and F1 scores). All the code and data of this paper can be obtained at https://github.com/thunlp/MSGI."
931,https://arxiv.org/abs/2203.06311,ELLE: Efficient Lifelong Pre-training for Emerging Data,"Current pre-trained language models (PLM) are typically trained with static data, ignoring that in real-world scenarios, streaming data of various sources may continuously grow. This requires PLMs to integrate the information from all the sources in a lifelong manner. Although this goal could be achieved by exhaustive pre-training on all the existing data, such a process is known to be computationally expensive. To this end, we propose ELLE, aiming at efficient lifelong pre-training for emerging data. Specifically, ELLE consists of (1) function preserved model expansion, which flexibly expands an existing PLM's width and depth to improve the efficiency of knowledge acquisition; and (2) pre-trained domain prompts, which disentangle the versatile knowledge learned during pre-training and stimulate the proper knowledge for downstream tasks. We experiment ELLE with streaming data from 5 domains on BERT and GPT. The results show the superiority of ELLE over various lifelong learning baselines in both pre-training efficiency and downstream performances. The codes are publicly available at https://github.com/thunlp/ELLE."
932,https://arxiv.org/abs/2202.13392,A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models,"Pre-trained language models (PLMs) cannot well recall rich factual knowledge of entities exhibited in large-scale corpora, especially those rare entities. In this paper, we propose to build a simple but effective Pluggable Entity Lookup Table (PELT) on demand by aggregating the entity's output representations of multiple occurrences in the corpora. PELT can be compatibly plugged as inputs to infuse supplemental entity knowledge into PLMs. Compared to previous knowledge-enhanced PLMs, PELT only requires 0.2%-5% pre-computation with capability of acquiring knowledge from out-of-domain corpora for domain adaptation scenario. The experiments on knowledge-related tasks demonstrate that our method, PELT, can flexibly and effectively transfer entity knowledge from related corpora into PLMs with different architectures."
933,https://arxiv.org/abs/2202.13145,QuoteR: A Benchmark of Quote Recommendation for Writing,"It is very common to use quotations (quotes) to make our writings more elegant or convincing. To help people find appropriate quotes efficiently, the task of quote recommendation is presented, aiming to recommend quotes that fit the current context of writing. There have been various quote recommendation approaches, but they are evaluated on different unpublished datasets. To facilitate the research on this task, we build a large and fully open quote recommendation dataset called QuoteR, which comprises three parts including English, standard Chinese and classical Chinese. Any part of it is larger than previous unpublished counterparts. We conduct an extensive evaluation of existing quote recommendation methods on QuoteR. Furthermore, we propose a new quote recommendation model that significantly outperforms previous methods on all three parts of QuoteR. All the code and data of this paper are available at https://github.com/thunlp/QuoteR."
934,https://arxiv.org/abs/2201.05349,Training Free Graph Neural Networks for Graph Matching,"We present a framework of Training Free Graph Matching (TFGM) to boost the performance of Graph Neural Networks (GNNs) based graph matching, providing a fast promising solution without training (training-free). TFGM provides four widely applicable principles for designing training-free GNNs and is generalizable to supervised, semi-supervised, and unsupervised graph matching. The keys are to handcraft the matching priors, which used to be learned by training, into GNN's architecture and discard the components inessential under the training-free setting. Further analysis shows that TFGM is a linear relaxation to the quadratic assignment formulation of graph matching and generalizes TFGM to a broad set of GNNs. Extensive experiments show that GNNs with TFGM achieve comparable (if not better) performances to their fully trained counterparts, and demonstrate TFGM's superiority in the unsupervised setting. Our code is available at https://github.com/acharkq/Training-Free-Graph-Matching."
935,https://arxiv.org/abs/2112.06197,Video as Conditional Graph Hierarchy for Multi-Granular Question Answering,"Video question answering requires the models to understand and reason about both the complex video and language data to correctly derive the answers. Existing efforts have been focused on designing sophisticated cross-modal interactions to fuse the information from two modalities, while encoding the video and question holistically as frame and word sequences. Despite their success, these methods are essentially revolving around the sequential nature of video- and question-contents, providing little insight to the problem of question-answering and lacking interpretability as well. In this work, we argue that while video is presented in frame sequence, the visual elements (e.g., objects, actions, activities and events) are not sequential but rather hierarchical in semantic space. To align with the multi-granular essence of linguistic concepts in language queries, we propose to model video as a conditional graph hierarchy which weaves together visual facts of different granularity in a level-wise manner, with the guidance of corresponding textual cues. Despite the simplicity, our extensive experiments demonstrate the superiority of such conditional hierarchical graph architecture, with clear performance improvements over prior methods and also better generalization across different type of questions. Further analyses also demonstrate the model's reliability as it shows meaningful visual-textual evidences for the predicted answers."
936,https://arxiv.org/abs/2112.01651,Multi-modal application: Image Memes Generation,"Meme is an interesting word. Internet memes offer unique insights into the changes in our perception of the world, the media and our own lives. If you surf the Internet for long enough, you will see it somewhere on the Internet. With the rise of social media platforms and convenient image dissemination, Image Meme has gained fame. Image memes have become a kind of pop culture and they play an important role in communication over social media, blogs, and open messages. With the development of artificial intelligence and the widespread use of deep learning, Natural Language Processing (NLP) and Computer Vision (CV) can also be used to solve more problems in life, including meme generation. An Internet meme commonly takes the form of an image and is created by combining a meme template (image) and a caption (natural language sentence). In our project, we propose an end-to-end encoder-decoder architecture meme generator. For a given input sentence, we use the Meme template selection model to determine the emotion it expresses and select the image template. Then generate captions and memes through to the meme caption generator. Code and models are available at github"
937,https://arxiv.org/abs/2111.12431,A Large-Scale Multi-Rider Matching Problem with Reneging Passengers,"This paper studies a large-scale ride-matching problem in a transport center with a large number of passengers who are different in terms of destinations and travel preferences. Passengers with similar itineraries can match each other and share the same vehicle for their potentially different destinations; and reneging passengers, who become impatient and leave the service system after waiting long time for shared rides, are considered in our model. We aim to maximize the long-run average revenue of the ride service vendor, which is defined as the difference between the long-run average reward earned by providing ride services and the long-run average penalty incurred by reneging passengers. The problem is complicated by its scale, the heterogeneity of passengers, and the reneging behavior. To this end, we formate the ride-matching problem as a specific Markov decision process and propose a scalable ride-matching policy, referred to as Bivariate Index (BI) policy. We demonstrate that the ride-matching problem reduces to a conventional restless multi-armed bandit problem in a special case, for which the BI policy reduces to the well-known Whittle index policy that prioritizes passengers with the highest Whittle indices. For the general case, following the Whittle relaxation technique, BI prioritizes passengers according to their bivariate indices that are curves in a real-valued coordinate and are assigned to each passengers. We prove that the ranking of the bivariate indices can be derived by the ranking of corresponding Whittle indices in a surrogate system. Through extensive numerical simulations for systems with real-world travel demands, it is demonstrated that BI significantly outperforms baseline policies."
938,https://arxiv.org/abs/2111.01998,OpenPrompt: An Open-source Framework for Prompt-learning,"Prompt-learning has become a new paradigm in modern natural language processing, which directly adapts pre-trained language models (PLMs) to $cloze$-style prediction, autoregressive modeling, or sequence to sequence generation, resulting in promising performances on various tasks. However, no standard implementation framework of prompt-learning is proposed yet, and most existing prompt-learning codebases, often unregulated, only provide limited implementations for specific scenarios. Since there are many details such as templating strategy, initializing strategy, and verbalizing strategy, etc. need to be considered in prompt-learning, practitioners face impediments to quickly adapting the desired prompt learning methods to their applications. In this paper, we present {OpenPrompt}, a unified easy-to-use toolkit to conduct prompt-learning over PLMs. OpenPrompt is a research-friendly framework that is equipped with efficiency, modularity, and extendibility, and its combinability allows the freedom to combine different PLMs, task formats, and prompting modules in a unified paradigm. Users could expediently deploy prompt-learning frameworks and evaluate the generalization of them on different NLP tasks without constraints. OpenPrompt is publicly released at {\url{ https://github.com/thunlp/OpenPrompt}}."
939,https://arxiv.org/abs/2110.08247,Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks,"Backdoor attacks are a kind of emergent security threat in deep learning. After being injected with a backdoor, a deep neural model will behave normally on standard inputs but give adversary-specified predictions once the input contains specific backdoor triggers. In this paper, we find two simple tricks that can make existing textual backdoor attacks much more harmful. The first trick is to add an extra training task to distinguish poisoned and clean data during the training of the victim model, and the second one is to use all the clean training data rather than remove the original clean data corresponding to the poisoned data. These two tricks are universally applicable to different attack models. We conduct experiments in three tough situations including clean data fine-tuning, low-poisoning-rate, and label-consistent attacks. Experimental results show that the two tricks can significantly improve attack performance. This paper exhibits the great potential harmfulness of backdoor attacks. All the code and data can be obtained at \url{https://github.com/thunlp/StyleAttack}."
940,https://arxiv.org/abs/2110.07143,bert2BERT: Towards Reusable Pretrained Language Models,"In recent years, researchers tend to pre-train ever-larger language models to explore the upper limit of deep models. However, large language model pre-training costs intensive computational resources and most of the models are trained from scratch without reusing the existing pre-trained models, which is wasteful. In this paper, we propose bert2BERT, which can effectively transfer the knowledge of an existing smaller pre-trained model (e.g., BERT_BASE) to a large model (e.g., BERT_LARGE) through parameter initialization and significantly improve the pre-training efficiency of the large model. Specifically, we extend the previous function-preserving on Transformer-based language model, and further improve it by proposing advanced knowledge for large model's initialization. In addition, a two-stage pre-training method is proposed to further accelerate the training process. We did extensive experiments on representative PLMs (e.g., BERT and GPT) and demonstrate that (1) our method can save a significant amount of training cost compared with baselines including learning from scratch, StackBERT and MSLT; (2) our method is generic and applicable to different types of pre-trained models. In particular, bert2BERT saves about 45% and 47% computational cost of pre-training BERT_BASE and GPT_BASE by reusing the models of almost their half sizes. The source code will be publicly available upon publication."
941,https://arxiv.org/abs/2110.07139,Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer,"Adversarial attacks and backdoor attacks are two common security threats that hang over deep learning. Both of them harness task-irrelevant features of data in their implementation. Text style is a feature that is naturally irrelevant to most NLP tasks, and thus suitable for adversarial and backdoor attacks. In this paper, we make the first attempt to conduct adversarial and backdoor attacks based on text style transfer, which is aimed at altering the style of a sentence while preserving its meaning. We design an adversarial attack method and a backdoor attack method, and conduct extensive experiments to evaluate them. Experimental results show that popular NLP models are vulnerable to both adversarial and backdoor attacks based on text style transfer -- the attack success rates can exceed 90% without much effort. It reflects the limited ability of NLP models to handle the feature of text style that has not been widely realized. In addition, the style transfer-based adversarial and backdoor attack methods show superiority to baselines in many aspects. All the code and data of this paper can be obtained at https://github.com/thunlp/StyleAttack."
942,https://arxiv.org/abs/2110.01786,MoEfication: Transformer Feed-forward Layers are Mixtures of Experts,"Recent work has shown that feed-forward networks (FFNs) in pre-trained Transformers are a key component, storing various linguistic and factual knowledge. However, the computational patterns of FFNs are still unclear. In this work, we study the computational patterns of FFNs and observe that most inputs only activate a tiny ratio of neurons of FFNs. This phenomenon is similar to the sparsity of the human brain, which drives research on functional partitions of the human brain. To verify whether functional partitions also emerge in FFNs, we propose to convert a model into its MoE version with the same parameters, namely MoEfication. Specifically, MoEfication consists of two phases: (1) splitting the parameters of FFNs into multiple functional partitions as experts, and (2) building expert routers to decide which experts will be used for each input. Experimental results show that MoEfication can conditionally use 10% to 30% of FFN parameters while maintaining over 95% original performance for different models on various downstream tasks. Besides, MoEfication brings two advantages: (1) it significantly reduces the FLOPS of inference, i.e., 2x speedup with 25% of FFN parameters, and (2) it provides a fine-grained perspective to study the inner mechanism of FFNs. The source code of this paper can be obtained from https://github.com/thunlp/MoEfication."
943,https://arxiv.org/abs/2109.11797,CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models,"Pre-Trained Vision-Language Models (VL-PTMs) have shown promising capabilities in grounding natural language in image data, facilitating a broad variety of cross-modal tasks. However, we note that there exists a significant gap between the objective forms of model pre-training and fine-tuning, resulting in a need for large amounts of labeled data to stimulate the visual grounding capability of VL-PTMs for downstream tasks. To address the challenge, we present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt Tuning), a novel paradigm for tuning VL-PTMs, which reformulates visual grounding into a fill-in-the-blank problem with color-based co-referential markers in image and text, maximally mitigating the gap. In this way, CPT enables strong few-shot and even zero-shot visual grounding capabilities of VL-PTMs. Comprehensive experimental results show that the prompt-tuned VL-PTMs outperform their fine-tuned counterparts by a large margin (e.g., 17.3% absolute accuracy improvement, and 73.8% relative standard deviation reduction on average with one shot in RefCOCO evaluation). We make the data and code for this paper publicly available at https://github.com/thunlp/CPT."
944,https://arxiv.org/abs/2109.04562,TIAGE: A Benchmark for Topic-Shift Aware Dialog Modeling,"Human conversations naturally evolve around different topics and fluently move between them. In research on dialog systems, the ability to actively and smoothly transition to new topics is often ignored. In this paper we introduce TIAGE, a new topic-shift aware dialog benchmark constructed utilizing human annotations on topic shifts. Based on TIAGE, we introduce three tasks to investigate different scenarios of topic-shift modeling in dialog settings: topic-shift detection, topic-shift triggered response generation and topic-aware dialog generation. Experiments on these tasks show that the topic-shift signals in TIAGE are useful for topic-shift response generation. On the other hand, dialog systems still struggle to decide when to change topic. This indicates further research is needed in topic-shift aware dialog modeling."
945,https://arxiv.org/abs/2109.02230,Non-Euclidean Analysis of Joint Variations in Multi-Object Shapes,"This paper considers joint analysis of multiple functionally related structures in classification tasks. In particular, our method developed is driven by how functionally correlated brain structures vary together between autism and control groups. To do so, we devised a method based on a novel combination of (1) non-Euclidean statistics that can faithfully represent non-Euclidean data in Euclidean spaces and (2) a non-parametric integrative analysis method that can decompose multi-block Euclidean data into joint, individual, and residual structures. We find that the resulting joint structure is effective, robust, and interpretable in recognizing the underlying patterns of the joint variation of multi-block non-Euclidean data. We verified the method in classifying the structural shape data collected from cases that developed and did not develop into Autistic Spectrum Disorder (ASD)."
946,https://arxiv.org/abs/2108.09810,ICLR 2021 Challenge for Computational Geometry & Topology: Design and Results,"This paper presents the computational challenge on differential geometry and topology that happened within the ICLR 2021 workshop ""Geometric and Topological Representation Learning"". The competition asked participants to provide creative contributions to the fields of computational geometry and topology through the open-source repositories Geomstats and Giotto-TDA. The challenge attracted 16 teams in its two month duration. This paper describes the design of the challenge and summarizes its main findings."
947,https://arxiv.org/abs/2107.12095,Robotic Occlusion Reasoning for Efficient Object Existence Prediction,"Reasoning about potential occlusions is essential for robots to efficiently predict whether an object exists in an environment. Though existing work shows that a robot with active perception can achieve various tasks, it is still unclear if occlusion reasoning can be achieved. To answer this question, we introduce the task of robotic object existence prediction: when being asked about an object, a robot needs to move as few steps as possible around a table with randomly placed objects to predict whether the queried object exists. To address this problem, we propose a novel recurrent neural network model that can be jointly trained with supervised and reinforcement learning methods using a curriculum training strategy. Experimental results show that 1) both active perception and occlusion reasoning are necessary to successfully achieve the task; 2) the proposed model demonstrates a good occlusion reasoning ability by achieving a similar prediction accuracy to an exhaustive exploration baseline while requiring only about $10\%$ of the baseline's number of movement steps on average; and 3) the model generalizes to novel object combinations with a moderate loss of accuracy."
948,https://arxiv.org/abs/2107.07773,More Robust Dense Retrieval with Contrastive Dual Learning,"Dense retrieval conducts text retrieval in the embedding space and has shown many advantages compared to sparse retrieval. Existing dense retrievers optimize representations of queries and documents with contrastive training and map them to the embedding space. The embedding space is optimized by aligning the matched query-document pairs and pushing the negative documents away from the query. However, in such training paradigm, the queries are only optimized to align to the documents and are coarsely positioned, leading to an anisotropic query embedding space. In this paper, we analyze the embedding space distributions and propose an effective training paradigm, Contrastive Dual Learning for Approximate Nearest Neighbor (DANCE) to learn fine-grained query representations for dense retrieval. DANCE incorporates an additional dual training object of query retrieval, inspired by the classic information retrieval training axiom, query likelihood. With contrastive learning, the dual training object of DANCE learns more tailored representations for queries and documents to keep the embedding space smooth and uniform, thriving on the ranking performance of DANCE on the MS MARCO document retrieval task. Different from ANCE that only optimized with the document retrieval task, DANCE concentrates the query embeddings closer to document representations while making the document distribution more discriminative. Such concentrated query embedding distribution assigns more uniform negative sampling probabilities to queries and helps to sufficiently optimize query representations in the query retrieval task. Our codes are released at https://github.com/thunlp/DANCE."
949,https://arxiv.org/abs/2106.14198,From Symbols to Embeddings: A Tale of Two Representations in Computational Social Science,"Computational Social Science (CSS), aiming at utilizing computational methods to address social science problems, is a recent emerging and fast-developing field. The study of CSS is data-driven and significantly benefits from the availability of online user-generated contents and social networks, which contain rich text and network data for investigation. However, these large-scale and multi-modal data also present researchers with a great challenge: how to represent data effectively to mine the meanings we want in CSS? To explore the answer, we give a thorough review of data representations in CSS for both text and network. Specifically, we summarize existing representations into two schemes, namely symbol-based and embedding-based representations, and introduce a series of typical methods for each scheme. Afterwards, we present the applications of the above representations based on the investigation of more than 400 research articles from 6 top venues involved with CSS. From the statistics of these applications, we unearth the strength of each kind of representations and discover the tendency that embedding-based representations are emerging and obtaining increasing attention over the last decade. Finally, we discuss several key challenges and open issues for future directions. This survey aims to provide a deeper understanding and more advisable applications of data representations for CSS researchers."
950,https://arxiv.org/abs/2106.08171,Evaluating Modules in Graph Contrastive Learning,"The recent emergence of contrastive learning approaches facilitates the application on graph representation learning (GRL), introducing graph contrastive learning (GCL) into the literature. These methods contrast semantically similar and dissimilar sample pairs to encode the semantics into node or graph embeddings. However, most existing works only performed \textbf{model-level} evaluation, and did not explore the combination space of modules for more comprehensive and systematic studies. For effective \textbf{module-level} evaluation, we propose a framework that decomposes GCL models into four modules: (1) a \textbf{sampler} to generate anchor, positive and negative data samples (nodes or graphs); (2) an \textbf{encoder} and a \textbf{readout} function to get sample embeddings; (3) a \textbf{discriminator} to score each sample pair (anchor-positive and anchor-negative); and (4) an \textbf{estimator} to define the loss function. Based on this framework, we conduct controlled experiments over a wide range of architectural designs and hyperparameter settings on node and graph classification tasks. Specifically, we manage to quantify the impact of a single module, investigate the interaction between modules, and compare the overall performance with current model architectures. Our key findings include a set of module-level guidelines for GCL, e.g., simple samplers from LINE and DeepWalk are strong and robust; an MLP encoder associated with Sum readout could achieve competitive performance on graph classification. Finally, we release our implementations and results as OpenGCL, a modularized toolkit that allows convenient reproduction, standard model and module evaluation, and easy extension. OpenGCL is available at \url{https://github.com/thunlp/OpenGCL}."
951,https://arxiv.org/abs/2106.06361,Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution,"Recent studies show that neural natural language processing (NLP) models are vulnerable to backdoor attacks. Injected with backdoors, models perform normally on benign examples but produce attacker-specified predictions when the backdoor is activated, presenting serious security threats to real-world applications. Since existing textual backdoor attacks pay little attention to the invisibility of backdoors, they can be easily detected and blocked. In this work, we present invisible backdoors that are activated by a learnable combination of word substitution. We show that NLP models can be injected with backdoors that lead to a nearly 100% attack success rate, whereas being highly invisible to existing defense strategies and even human inspections. The results raise a serious alarm to the security of NLP models, which requires further research to be resolved. All the data and code of this paper are released at https://github.com/thunlp/BkdAtk-LWS."
952,https://arxiv.org/abs/2106.00400,Sub-Character Tokenization for Chinese Pretrained Language Models,"Tokenization is fundamental to pretrained language models (PLMs). Existing tokenization methods for Chinese PLMs typically treat each character as an indivisible token. However, they ignore the unique feature of the Chinese writing system where additional linguistic information exists below the character level, i.e., at the sub-character level. To utilize such information, we propose sub-character (SubChar for short) tokenization. Specifically, we first encode the input text by converting each Chinese character into a short sequence based on its glyph or pronunciation, and then construct the vocabulary based on the encoded text with sub-word segmentation. Experimental results show that SubChar tokenizers have two main advantages over existing tokenizers: 1) They can tokenize inputs into much shorter sequences, thus improving the computational efficiency. 2) Pronunciation-based SubChar tokenizers can encode Chinese homophones into the same transliteration sequences and produce the same tokenization output, hence being robust to homophone typos. At the same time, models trained with SubChar tokenizers perform competitively on downstream tasks. We release our code and models at https://github.com/thunlp/SubCharTokenization to facilitate future work."
953,https://arxiv.org/abs/2105.14686,Fully Hyperbolic Neural Networks,"Hyperbolic neural networks have shown great potential for modeling complex data. However, existing hyperbolic networks are not completely hyperbolic, as they encode features in a hyperbolic space yet formalize most of their operations in the tangent space (a Euclidean subspace) at the origin of the hyperbolic space. This hybrid method greatly limits the modeling ability of networks. In this paper, we propose a fully hyperbolic framework to build hyperbolic networks based on the Lorentz model by adapting the Lorentz transformations (including boost and rotation) to formalize essential operations of neural networks. Moreover, we also prove that linear transformation in tangent spaces used by existing hyperbolic networks is a relaxation of the Lorentz rotation and does not include the boost, implicitly limiting the capabilities of existing hyperbolic networks. The experimental results on four NLP tasks show that our method has better performance for building both shallow and deep networks. Our code will be released to facilitate follow-up research."
954,https://arxiv.org/abs/2105.13880,Knowledge Inheritance for Pre-trained Language Models,"Recent explorations of large-scale pre-trained language models (PLMs) have revealed the power of PLMs with huge amounts of parameters, setting off a wave of training ever-larger PLMs. However, it requires tremendous computational resources to train a large-scale PLM, which may be practically unaffordable. In addition, existing large-scale PLMs are mainly trained from scratch individually, ignoring that many well-trained PLMs are available. To this end, we explore the question how could existing PLMs benefit training large-scale PLMs in future. Specifically, we introduce a pre-training framework named ""knowledge inheritance"" (KI) and explore how could knowledge distillation serve as auxiliary supervision during pre-training to efficiently learn larger PLMs. Experimental results demonstrate the superiority of KI in training efficiency. We also conduct empirical analyses to explore the effects of teacher PLMs' pre-training settings, including model architecture, pre-training data, etc. Finally, we show that KI could be applied to domain adaptation and knowledge transfer."
955,https://arxiv.org/abs/2105.12585,Automatic Construction of Sememe Knowledge Bases via Dictionaries,"A sememe is defined as the minimum semantic unit in linguistics. Sememe knowledge bases (SKBs), which comprise words annotated with sememes, enable sememes to be applied to natural language processing. So far a large body of research has showcased the unique advantages and effectiveness of SKBs in various tasks. However, most languages have no SKBs, and manual construction of SKBs is time-consuming and labor-intensive. To tackle this challenge, we propose a simple and fully automatic method of building an SKB via an existing dictionary. We use this method to build an English SKB and a French SKB, and conduct comprehensive evaluations from both intrinsic and extrinsic perspectives. Experimental results demonstrate that the automatically built English SKB is even superior to HowNet, the most widely used SKB that takes decades to build manually. And both the English and French SKBs can bring obvious performance enhancement in multiple downstream tasks. All the code and data of this paper (except the copyrighted dictionaries) can be obtained at https://github.com/thunlp/DictSKB."
956,https://arxiv.org/abs/2105.12400,Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger,"Backdoor attacks are a kind of insidious security threat against machine learning models. After being injected with a backdoor in training, the victim model will produce adversary-specified outputs on the inputs embedded with predesigned triggers but behave properly on normal inputs during inference. As a sort of emergent attack, backdoor attacks in natural language processing (NLP) are investigated insufficiently. As far as we know, almost all existing textual backdoor attack methods insert additional contents into normal samples as triggers, which causes the trigger-embedded samples to be detected and the backdoor attacks to be blocked without much effort. In this paper, we propose to use the syntactic structure as the trigger in textual backdoor attacks. We conduct extensive experiments to demonstrate that the syntactic trigger-based attack method can achieve comparable attack performance (almost 100% success rate) to the insertion-based methods but possesses much higher invisibility and stronger resistance to defenses. These results also reveal the significant insidiousness and harmfulness of textual backdoor attacks. All the code and data of this paper can be obtained at https://github.com/thunlp/HiddenKiller."
957,https://arxiv.org/abs/2105.11259,PTR: Prompt Tuning with Rules for Text Classification,"Fine-tuned pre-trained language models (PLMs) have achieved awesome performance on almost all NLP tasks. By using additional prompts to fine-tune PLMs, we can further stimulate the rich knowledge distributed in PLMs to better serve downstream tasks. Prompt tuning has achieved promising results on some few-class classification tasks such as sentiment classification and natural language inference. However, manually designing lots of language prompts is cumbersome and fallible. For those auto-generated prompts, it is also expensive and time-consuming to verify their effectiveness in non-few-shot scenarios. Hence, it is still challenging for prompt tuning to address many-class classification tasks. To this end, we propose prompt tuning with rules (PTR) for many-class text classification and apply logic rules to construct prompts with several sub-prompts. In this way, PTR is able to encode prior knowledge of each class into prompt tuning. We conduct experiments on relation classification, a typical and complicated many-class classification task, and the results show that PTR can significantly and consistently outperform existing state-of-the-art baselines. This indicates that PTR is a promising approach to take advantage of both human prior knowledge and PLMs for those complicated classification tasks."
958,https://arxiv.org/abs/2105.09543,Manual Evaluation Matters: Reviewing Test Protocols of Distantly Supervised Relation Extraction,"Distantly supervised (DS) relation extraction (RE) has attracted much attention in the past few years as it can utilize large-scale auto-labeled data. However, its evaluation has long been a problem: previous works either took costly and inconsistent methods to manually examine a small sample of model predictions, or directly test models on auto-labeled data -- which, by our check, produce as much as 53% wrong labels at the entity pair level in the popular NYT10 dataset. This problem has not only led to inaccurate evaluation, but also made it hard to understand where we are and what's left to improve in the research of DS-RE. To evaluate DS-RE models in a more credible way, we build manually-annotated test sets for two DS-RE datasets, NYT10 and Wiki20, and thoroughly evaluate several competitive models, especially the latest pre-trained ones. The experimental results show that the manual evaluation can indicate very different conclusions from automatic ones, especially some unexpected observations, e.g., pre-trained models can achieve dominating performance while being more susceptible to false-positives compared to previous methods. We hope that both our manual test sets and novel observations can help advance future DS-RE research."
959,https://arxiv.org/abs/2105.07464,Few-NERD: A Few-Shot Named Entity Recognition Dataset,"Recently, considerable literature has grown up around the theme of few-shot named entity recognition (NER), but little published benchmark data specifically focused on the practical and challenging task. Current approaches collect existing supervised NER datasets and re-organize them to the few-shot setting for empirical study. These strategies conventionally aim to recognize coarse-grained entity types with few examples, while in practice, most unseen entity types are fine-grained. In this paper, we present Few-NERD, a large-scale human-annotated few-shot NER dataset with a hierarchy of 8 coarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238 sentences from Wikipedia, 4,601,160 words are included and each is annotated as context or a part of a two-level entity type. To the best of our knowledge, this is the first few-shot NER dataset and the largest human-crafted NER dataset. We construct benchmark tasks with different emphases to comprehensively assess the generalization capability of models. Extensive empirical results and analysis show that Few-NERD is challenging and the problem requires further research. We make Few-NERD public at https://ningding97.github.io/fewnerd/."
960,https://arxiv.org/abs/2105.04166,Few-Shot Conversational Dense Retrieval,"Dense retrieval (DR) has the potential to resolve the query understanding challenge in conversational search by matching in the learned embedding space. However, this adaptation is challenging due to DR models' extra needs for supervision signals and the long-tail nature of conversational search. In this paper, we present a Conversational Dense Retrieval system, ConvDR, that learns contextualized embeddings for multi-turn conversational queries and retrieves documents solely using embedding dot products. In addition, we grant ConvDR few-shot ability using a teacher-student framework, where we employ an ad hoc dense retriever as the teacher, inherit its document encodings, and learn a student query encoder to mimic the teacher embeddings on oracle reformulated queries. Our experiments on TREC CAsT and OR-QuAC demonstrate ConvDR's effectiveness in both few-shot and fully-supervised settings. It outperforms previous systems that operate in the sparse word space, matches the retrieval accuracy of oracle query reformulations, and is also more efficient thanks to its simplicity. Our analyses reveal that the advantages of ConvDR come from its ability to capture informative context while ignoring the unrelated context in previous conversation rounds. This makes ConvDR more effective as conversations evolve while previous systems may get confused by the increased noise from previous turns. Our code is publicly available at https://github.com/thunlp/ConvDR."
961,https://arxiv.org/abs/2105.03887,Lawformer: A Pre-trained Language Model for Chinese Legal Long Documents,"Legal artificial intelligence (LegalAI) aims to benefit legal systems with the technology of artificial intelligence, especially natural language processing (NLP). Recently, inspired by the success of pre-trained language models (PLMs) in the generic domain, many LegalAI researchers devote their effort to apply PLMs to legal tasks. However, utilizing PLMs to address legal tasks is still challenging, as the legal documents usually consist of thousands of tokens, which is far longer than the length that mainstream PLMs can process. In this paper, we release the Longformer-based pre-trained language model, named as Lawformer, for Chinese legal long documents understanding. We evaluate Lawformer on a variety of LegalAI tasks, including judgment prediction, similar case retrieval, legal reading comprehension, and legal question answering. The experimental results demonstrate that our model can achieve promising improvement on tasks with long documents as inputs."
962,https://arxiv.org/abs/2104.03860,Incentivizing Exploration in Linear Bandits under Information Gap,"We study the problem of incentivizing exploration for myopic users in linear bandits, where the users tend to exploit arm with the highest predicted reward instead of exploring. In order to maximize the long-term reward, the system offers compensation to incentivize the users to pull the exploratory arms, with the goal of balancing the trade-off among exploitation, exploration and compensation. We consider a new and practically motivated setting where the context features observed by the user are more informative than those used by the system, e.g., features based on users' private information are not accessible by the system. We propose a new method to incentivize exploration under such information gap, and prove that the method achieves both sublinear regret and sublinear compensation. We theoretical and empirically analyze the added compensation due to the information gap, compared with the case that the system has access to the same context features as the user, i.e., without information gap. We also provide a compensation lower bound of our problem."
963,https://arxiv.org/abs/2103.15365,Visual Distant Supervision for Scene Graph Generation,"Scene graph generation aims to identify objects and their relations in images, providing structured image representations that can facilitate numerous applications in computer vision. However, scene graph models usually require supervised learning on large quantities of labeled data with intensive human annotation. In this work, we propose visual distant supervision, a novel paradigm of visual relation learning, which can train scene graph models without any human-labeled data. The intuition is that by aligning commonsense knowledge bases and images, we can automatically create large-scale labeled data to provide distant supervision for visual relation learning. To alleviate the noise in distantly labeled data, we further propose a framework that iteratively estimates the probabilistic relation labels and eliminates the noisy ones. Comprehensive experimental results show that our distantly supervised model outperforms strong weakly supervised and semi-supervised baselines. By further incorporating human-labeled data in a semi-supervised fashion, our model outperforms state-of-the-art fully supervised models by a large margin (e.g., 8.3 micro- and 7.8 macro-recall@50 improvements for predicate classification in Visual Genome evaluation). We make the data and code for this paper publicly available at https://github.com/thunlp/VisualDS."
964,https://arxiv.org/abs/2103.13868,Equality before the Law: Legal Judgment Consistency Analysis for Fairness,"In a legal system, judgment consistency is regarded as one of the most important manifestations of fairness. However, due to the complexity of factual elements that impact sentencing in real-world scenarios, few works have been done on quantitatively measuring judgment consistency towards real-world data. In this paper, we propose an evaluation metric for judgment inconsistency, Legal Inconsistency Coefficient (LInCo), which aims to evaluate inconsistency between data groups divided by specific features (e.g., gender, region, race). We propose to simulate judges from different groups with legal judgment prediction (LJP) models and measure the judicial inconsistency with the disagreement of the judgment results given by LJP models trained on different groups. Experimental results on the synthetic data verify the effectiveness of LInCo. We further employ LInCo to explore the inconsistency in real cases and come to the following observations: (1) Both regional and gender inconsistency exist in the legal system, but gender inconsistency is much less than regional inconsistency; (2) The level of regional inconsistency varies little across different time periods; (3) In general, judicial inconsistency is negatively correlated with the severity of the criminal charges. Besides, we use LInCo to evaluate the performance of several de-bias methods, such as adversarial learning, and find that these mechanisms can effectively help LJP models to avoid suffering from data bias."
965,https://arxiv.org/abs/2103.00699,Infrastructure Assisted Constrained Connected Automated Vehicle Trajectory Optimization on Curved Roads: A Spatial Formulation on a Curvilinear Coordinate,"Vehicle trajectory optimization is essential to ensure vehicles travel efficiently and safely. This paper presents an infrastructure assisted constrained connected automated vehicles (CAVs) trajectory optimization method on curved roads. This paper systematically formulates the problem based on a curvilinear coordinate which is flexible to model complex road geometries. Further, to deal with the spatial varying road obstacles, traffic regulations, and geometric characteristics, two-dimensional vehicle kinematics is given in a spatial formulation with exact road information provided by the infrastructure. Consequently, we applied a multi-objective model predictive control (MPC) approach to optimize the trajectories in a rolling horizon while satisfying the collision avoidances and vehicle kinematics constraints. To verify the efficiency of our method, a numerical simulation is conducted. As the results suggest, the proposed method can provide smooth vehicular trajectories, avoid road obstacles, and simultaneously follow traffic regulations, which is robust to road geometries and disturbances."
966,https://arxiv.org/abs/2102.10989,UPRec: User-Aware Pre-training for Recommender Systems,"Existing sequential recommendation methods rely on large amounts of training data and usually suffer from the data sparsity problem. To tackle this, the pre-training mechanism has been widely adopted, which attempts to leverage large-scale data to perform self-supervised learning and transfer the pre-trained parameters to downstream tasks. However, previous pre-trained models for recommendation focus on leverage universal sequence patterns from user behaviour sequences and item information, whereas ignore capturing personalized interests with the heterogeneous user information, which has been shown effective in contributing to personalized recommendation. In this paper, we propose a method to enhance pre-trained models with heterogeneous user information, called User-aware Pre-training for Recommendation (UPRec). Specifically, UPRec leverages the user attributes andstructured social graphs to construct self-supervised objectives in the pre-training stage and proposes two user-aware pre-training tasks. Comprehensive experimental results on several real-world large-scale recommendation datasets demonstrate that UPRec can effectively integrate user information into pre-trained models and thus provide more appropriate recommendations for users."
967,https://arxiv.org/abs/2102.03752,CSS-LM: A Contrastive Framework for Semi-supervised Fine-tuning of Pre-trained Language Models,"Fine-tuning pre-trained language models (PLMs) has demonstrated its effectiveness on various downstream NLP tasks recently. However, in many low-resource scenarios, the conventional fine-tuning strategies cannot sufficiently capture the important semantic features for downstream tasks. To address this issue, we introduce a novel framework (named ""CSS-LM"") to improve the fine-tuning phase of PLMs via contrastive semi-supervised learning. Specifically, given a specific task, we retrieve positive and negative instances from large-scale unlabeled corpora according to their domain-level and class-level semantic relatedness to the task. We then perform contrastive semi-supervised learning on both the retrieved unlabeled and original labeled instances to help PLMs capture crucial task-related semantic features. The experimental results show that CSS-LM achieves better results than the conventional fine-tuning strategy on a series of downstream tasks with few-shot settings, and outperforms the latest supervised contrastive fine-tuning strategies. Our datasets and source code will be available to provide more details."
968,https://arxiv.org/abs/2102.03732,Representation Learning for Natural Language Processing,"This book aims to review and present the recent advances of distributed representation learning for NLP, including why representation learning can improve NLP, how representation learning takes part in various important topics of NLP, and what challenges are still not well addressed by distributed representation."
969,https://arxiv.org/abs/2102.00166,OpenMatch: An Open Source Library for Neu-IR Research,"OpenMatch is a Python-based library that serves for Neural Information Retrieval (Neu-IR) research. It provides self-contained neural and traditional IR modules, making it easy to build customized and higher-capacity IR systems. In order to develop the advantages of Neu-IR models for users, OpenMatch provides implementations of recent neural IR models, complicated experiment instructions, and advanced few-shot training methods. OpenMatch reproduces corresponding ranking results of previous work on widely-used IR benchmarks, liberating users from surplus labor in baseline reimplementation. Our OpenMatch-based solutions conduct top-ranked empirical results on various ranking tasks, such as ad hoc retrieval and conversational retrieval, illustrating the convenience of OpenMatch to facilitate building an effective IR system. The library, experimental methodologies and results of OpenMatch are all publicly available at https://github.com/thunlp/OpenMatch."
970,https://arxiv.org/abs/2101.06969,Red Alarm for Pre-trained Models: Universal Vulnerability to Neuron-Level Backdoor Attacks,"Pre-trained models (PTMs) have been widely used in various downstream tasks. The parameters of PTMs are distributed on the Internet and may suffer backdoor attacks. In this work, we demonstrate the universal vulnerability of PTMs, where fine-tuned PTMs can be easily controlled by backdoor attacks in arbitrary downstream tasks. Specifically, attackers can add a simple pre-training task, which restricts the output representations of trigger instances to pre-defined vectors, namely neuron-level backdoor attack (NeuBA). If the backdoor functionality is not eliminated during fine-tuning, the triggers can make the fine-tuned model predict fixed labels by pre-defined vectors. In the experiments of both natural language processing (NLP) and computer vision (CV), we show that NeuBA absolutely controls the predictions for trigger instances without any knowledge of downstream tasks. Finally, we apply several defense methods to NeuBA and find that model pruning is a promising direction to resist NeuBA by excluding backdoored neurons. Our findings sound a red alarm for the wide use of PTMs. Our source code and models are available at \url{https://github.com/thunlp/NeuBA}."
971,https://arxiv.org/abs/2012.15699,Better Robustness by More Coverage: Adversarial Training with Mixup Augmentation for Robust Fine-tuning,"Pretrained language models (PLMs) perform poorly under adversarial attacks. To improve the adversarial robustness, adversarial data augmentation (ADA) has been widely adopted to cover more search space of adversarial attacks by adding textual adversarial examples during training. However, the number of adversarial examples for text augmentation is still extremely insufficient due to the exponentially large attack search space. In this work, we propose a simple and effective method to cover a much larger proportion of the attack search space, called Adversarial and Mixup Data Augmentation (AMDA). Specifically, AMDA linearly interpolates the representations of pairs of training samples to form new virtual samples, which are more abundant and diverse than the discrete text adversarial examples in conventional ADA. Moreover, to fairly evaluate the robustness of different models, we adopt a challenging evaluation setup, which generates a new set of adversarial examples targeting each model. In text classification experiments of BERT and RoBERTa, AMDA achieves significant robustness gains under two strong adversarial attacks and alleviates the performance degradation of ADA on the clean data. Our code is available at: https://github.com/thunlp/MixADA ."
972,https://arxiv.org/abs/2209.09459,Replicating Persistent Memory Key-Value Stores with Efficient RDMA Abstraction,"Combining persistent memory (PM) with RDMA is a promising approach to performant replicated distributed key-value stores (KVSs). However, existing replication approaches do not work well when applied to PM KVSs: 1) Using RPC induces software queueing and execution at backups, increasing request latency; 2) Using one-sided RDMA WRITE causes many streams of small PM writes, leading to severe device-level write amplification (DLWA) on PM. In this paper, we propose Rowan, an efficient RDMA abstraction to handle replication writes in PM KVSs; it aggregates concurrent remote writes from different servers, and lands these writes to PM in a sequential (thus low DLWA) and one-sided (thus low latency) manner. We realize Rowan with off-the-shelf RDMA NICs. Further, we build Rowan-KV, a log-structured PM KVS using Rowan for replication. Evaluation shows that under write-intensive workloads, compared with PM KVSs using RPC and RDMA WRITE for replication, Rowan-KV boosts throughput by 1.22X and 1.39X as well as lowers median PUT latency by 1.77X and 2.11X, respectively, while largely eliminating DLWA."
973,https://arxiv.org/abs/2112.07320,Sherman: A Write-Optimized Distributed B+Tree Index on Disaggregated Memory,"Memory disaggregation architecture physically separates CPU and memory into independent components, which are connected via high-speed RDMA networks, greatly improving resource utilization of databases. However, such an architecture poses unique challenges to data indexing in databases due to limited RDMA semantics and near-zero computation power at memory-side. Existing indexes supporting disaggregated memory either suffer from low write performance, or require hardware modification.
  This paper presents Sherman, a write-optimized distributed B+Tree index on disaggregated memory that delivers high performance with commodity RDMA NICs. Sherman combines RDMA hardware features and RDMA-friendly software techniques to boost index write performance from three angles. First, to reduce round trips, Sherman coalesces dependent RDMA commands by leveraging in-order delivery property of RDMA. Second, to accelerate concurrent accesses, Sherman introduces a hierarchical lock that exploits on-chip memory of RDMA NICs. Finally, to mitigate write amplification, Sherman tailors the data structure layout of B+Tree with a two-level version mechanism. Our evaluation shows that, Sherman is one order of magnitude faster in terms of both throughput and 99th percentile latency on typical write-intensive workloads, compared with state-of-the-art designs."
974,https://arxiv.org/abs/2107.13848,Revisiting Swapping in User-space with Lightweight Threading,"Memory-intensive applications, such as in-memory databases, caching systems and key-value stores, are increasingly demanding larger main memory to fit their working sets. Conventional swapping can enlarge the memory capacity by paging out inactive pages to disks. However, the heavy I/O stack makes the traditional kernel-based swapping suffers from several critical performance issues.
  In this paper, we redesign the swapping system and propose LightSwap, an high-performance user-space swapping scheme that supports paging with both local SSDs and remote memories. First, to avoids kernel-involving, a novel page fault handling mechanism is proposed to handle page faults in user-space and further eliminates the heavy I/O stack with the help of user-space I/O drivers. Second, we co-design Lightswap with light weight thread (LWT) to improve system throughput and make it be transparent to user applications. Finally, we propose a try-catch framework in Lightswap to deal with paging errors which are exacerbated by the scaling in process technology.
  We implement Lightswap in our production-level system and evaluate it with YCSB workloads running on memcached. Results show that Ligthswap reduces the page faults handling latency by 3--5 times, and improves the throughput of memcached by more than 40% compared with the stat-of-art swapping systems."
975,https://arxiv.org/abs/2007.03220,Sapphire: Automatic Configuration Recommendation for Distributed Storage Systems,"Modern distributed storage systems come with aplethora of configurable parameters that controlmodule behavior and affect system performance. Default settings provided by developers are often suboptimal for specific user cases. Tuning parameters can provide significant performance gains but is a difficult task requiring profound experience and expertise, due to the immense number of configurable parameters, complex inner dependencies and non-linearsystem behaviors. To overcome these difficulties, we propose an automatic simulation-based approach, Sapphire, to recommend optimal configurations by leveraging machine learning and black-box optimization techniques. We evaluate Sapphire on Ceph. Results show that Sapphire significantly boosts Ceph performance to 2.2x compared to the default configuration."
976,https://arxiv.org/abs/1908.10740,Kernel/User-level Collaborative Persistent Memory File System with Efficiency and Protection,"Emerging high performance non-volatile memories recall the importance of efficient file system design. To avoid the virtual file system (VFS) and syscall overhead as in these kernel-based file systems, recent works deploy file systems directly in user level. Unfortunately, a userlevel file system can easily be corrupted by a buggy program with misused pointers, and is hard to scale on multi-core platforms which incorporates a centralized coordination service. In this paper, we propose KucoFS, a Kernel and user-level collaborative file system. It consists of two parts: a user-level library with direct-access interfaces, and a kernel thread, which performs metadata updates and enforces write protection by toggling the permission bits in the page table. Hence, KucoFS achieves both direct-access of user-level designs and fine-grained write protection of kernel-level ones. We further explore its scalability to multicores: For metadata scalability, KucoFS rebalances the pathname resolution overhead between the kernel and userspace, by adopting the index offloading technique. For data access efficiency, it coordinates the data allocation between kernel and userspace, and uses range-lock write and lock-free read to improve concurrency. Experiments on Optane DC persistent memory show that KucoFS significantly outperforms existing file systems and shows better scalability."
977,https://arxiv.org/abs/1705.03623,Improving the Performance and Endurance of Persistent Memory with Loose-Ordering Consistency,"Persistent memory provides high-performance data persistence at main memory. Memory writes need to be performed in strict order to satisfy storage consistency requirements and enable correct recovery from system crashes. Unfortunately, adhering to such a strict order significantly degrades system performance and persistent memory endurance. This paper introduces a new mechanism, Loose-Ordering Consistency (LOC), that satisfies the ordering requirements at significantly lower performance and endurance loss. LOC consists of two key techniques. First, Eager Commit eliminates the need to perform a persistent commit record write within a transaction. We do so by ensuring that we can determine the status of all committed transactions during recovery by storing necessary metadata information statically with blocks of data written to memory. Second, Speculative Persistence relaxes the write ordering between transactions by allowing writes to be speculatively written to persistent memory. A speculative write is made visible to software only after its associated transaction commits. To enable this, our mechanism supports the tracking of committed transaction ID and multi-versioning in the CPU cache. Our evaluations show that LOC reduces the average performance overhead of memory persistence from 66.9% to 34.9% and the memory write traffic overhead from 17.1% to 3.4% on a variety of workloads."
978,https://arxiv.org/abs/2305.13266,Coarse-to-Fine: a Hierarchical Diffusion Model for Molecule Generation in 3D,"Generating desirable molecular structures in 3D is a fundamental problem for drug discovery. Despite the considerable progress we have achieved, existing methods usually generate molecules in atom resolution and ignore intrinsic local structures such as rings, which leads to poor quality in generated structures, especially when generating large molecules. Fragment-based molecule generation is a promising strategy, however, it is nontrivial to be adapted for 3D non-autoregressive generations because of the combinational optimization problems. In this paper, we utilize a coarse-to-fine strategy to tackle this problem, in which a Hierarchical Diffusion-based model (i.e.~HierDiff) is proposed to preserve the validity of local segments without relying on autoregressive modeling. Specifically, HierDiff first generates coarse-grained molecule geometries via an equivariant diffusion process, where each coarse-grained node reflects a fragment in a molecule. Then the coarse-grained nodes are decoded into fine-grained fragments by a message-passing process and a newly designed iterative refined sampling module. Lastly, the fine-grained fragments are then assembled to derive a complete atomic molecular structure. Extensive experiments demonstrate that HierDiff consistently improves the quality of molecule generation over existing methods"
979,https://arxiv.org/abs/2305.05938,V2X-Seq: A Large-Scale Sequential Dataset for Vehicle-Infrastructure Cooperative Perception and Forecasting,"Utilizing infrastructure and vehicle-side information to track and forecast the behaviors of surrounding traffic participants can significantly improve decision-making and safety in autonomous driving. However, the lack of real-world sequential datasets limits research in this area. To address this issue, we introduce V2X-Seq, the first large-scale sequential V2X dataset, which includes data frames, trajectories, vector maps, and traffic lights captured from natural scenery. V2X-Seq comprises two parts: the sequential perception dataset, which includes more than 15,000 frames captured from 95 scenarios, and the trajectory forecasting dataset, which contains about 80,000 infrastructure-view scenarios, 80,000 vehicle-view scenarios, and 50,000 cooperative-view scenarios captured from 28 intersections' areas, covering 672 hours of data. Based on V2X-Seq, we introduce three new tasks for vehicle-infrastructure cooperative (VIC) autonomous driving: VIC3D Tracking, Online-VIC Forecasting, and Offline-VIC Forecasting. We also provide benchmarks for the introduced tasks. Find data, code, and more up-to-date information at \href{https://github.com/AIR-THU/DAIR-V2X-Seq}{https://github.com/AIR-THU/DAIR-V2X-Seq}."
980,https://arxiv.org/abs/2303.10552,Vehicle-Infrastructure Cooperative 3D Object Detection via Feature Flow Prediction,"Cooperatively utilizing both ego-vehicle and infrastructure sensor data can significantly enhance autonomous driving perception abilities. However, temporal asynchrony and limited wireless communication in traffic environments can lead to fusion misalignment and impact detection performance. This paper proposes Feature Flow Net (FFNet), a novel cooperative detection framework that uses a feature flow prediction module to address these issues in vehicle-infrastructure cooperative 3D object detection. Rather than transmitting feature maps extracted from still-images, FFNet transmits feature flow, which leverages the temporal coherence of sequential infrastructure frames to predict future features and compensate for asynchrony. Additionally, we introduce a self-supervised approach to enable FFNet to generate feature flow with feature prediction ability. Experimental results demonstrate that our proposed method outperforms existing cooperative detection methods while requiring no more than 1/10 transmission cost of raw data on the DAIR-V2X dataset when temporal asynchrony exceeds 200$ms$. The code is available at \href{https://github.com/haibao-yu/FFNet-VIC3D}{https://github.com/haibao-yu/FFNet-VIC3D}."
981,https://arxiv.org/abs/2204.05575,DAIR-V2X: A Large-Scale Dataset for Vehicle-Infrastructure Cooperative 3D Object Detection,"Autonomous driving faces great safety challenges for a lack of global perspective and the limitation of long-range perception capabilities. It has been widely agreed that vehicle-infrastructure cooperation is required to achieve Level 5 autonomy. However, there is still NO dataset from real scenarios available for computer vision researchers to work on vehicle-infrastructure cooperation-related problems. To accelerate computer vision research and innovation for Vehicle-Infrastructure Cooperative Autonomous Driving (VICAD), we release DAIR-V2X Dataset, which is the first large-scale, multi-modality, multi-view dataset from real scenarios for VICAD. DAIR-V2X comprises 71254 LiDAR frames and 71254 Camera frames, and all frames are captured from real scenes with 3D annotations. The Vehicle-Infrastructure Cooperative 3D Object Detection problem (VIC3D) is introduced, formulating the problem of collaboratively locating and identifying 3D objects using sensory inputs from both vehicle and infrastructure. In addition to solving traditional 3D object detection problems, the solution of VIC3D needs to consider the temporal asynchrony problem between vehicle and infrastructure sensors and the data transmission cost between them. Furthermore, we propose Time Compensation Late Fusion (TCLF), a late fusion framework for the VIC3D task as a benchmark based on DAIR-V2X. Find data, code, and more up-to-date information at https://thudair.baai.ac.cn/index and https://github.com/AIR-THU/DAIR-V2X."
982,https://arxiv.org/abs/2012.01675,Federated Learning for Personalized Humor Recognition,"Computational understanding of humor is an important topic under creative language understanding and modeling. It can play a key role in complex human-AI interactions. The challenge here is that human perception of humorous content is highly subjective. The same joke may receive different funniness ratings from different readers. This makes it highly challenging for humor recognition models to achieve personalization in practical scenarios. Existing approaches are generally designed based on the assumption that users have a consensus on whether a given text is humorous or not. Thus, they cannot handle diverse humor preferences well. In this paper, we propose the FedHumor approach for the recognition of humorous content in a personalized manner through Federated Learning (FL). Extending a pre-trained language model, FedHumor guides the fine-tuning process by considering diverse distributions of humor preferences from individuals. It incorporates a diversity adaptation strategy into the FL paradigm to train a personalized humor recognition model. To the best of our knowledge, FedHumor is the first text-based personalized humor recognition model through federated learning. Extensive experiments demonstrate the advantage of FedHumor in recognizing humorous texts compared to nine state-of-the-art humor recognition approaches with superior capability for handling the diversity in humor labels produced by users with diverse preferences."
983,https://arxiv.org/abs/2010.05522,Pre-trained Language Model Based Active Learning for Sentence Matching,"Active learning is able to significantly reduce the annotation cost for data-driven techniques. However, previous active learning approaches for natural language processing mainly depend on the entropy-based uncertainty criterion, and ignore the characteristics of natural language. In this paper, we propose a pre-trained language model based active learning approach for sentence matching. Differing from previous active learning, it can provide linguistic criteria to measure instances and help select more efficient instances for annotation. Experiments demonstrate our approach can achieve greater accuracy with fewer labeled training instances."
984,https://arxiv.org/abs/2004.11588,Learning Hierarchical Review Graph Representations for Recommendation,"The user review data have been demonstrated to be effective in solving different recommendation problems. Previous review-based recommendation methods usually employ sophisticated compositional models, such as Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN), to learn semantic representations from the review data for recommendation. However, these methods mainly capture the local dependency between neighbouring words in a word window, and they treat each review equally. Therefore, they may not be effective in capturing the global dependency between words, and tend to be easily biased by noise review information. In this paper, we propose a novel review-based recommendation model, named Review Graph Neural Network (RGNN). Specifically, RGNN builds a specific review graph for each individual user/item, which provides a global view about the user/item properties to help weaken the biases caused by noise review information. A type-aware graph attention mechanism is developed to learn semantic embeddings of words. Moreover, a personalized graph pooling operator is proposed to learn hierarchical representations of the review graph to form the semantic representation for each user/item. We compared RGNN with state-of-the-art review-based recommendation approaches on two real-world datasets. The experimental results indicate that RGNN consistently outperforms baseline methods, in terms of Mean Square Error (MSE)."
985,https://arxiv.org/abs/1910.13108,Generating Questions for Knowledge Bases via Incorporating Diversified Contexts and Answer-Aware Loss,"We tackle the task of question generation over knowledge bases. Conventional methods for this task neglect two crucial research issues: 1) the given predicate needs to be expressed; 2) the answer to the generated question needs to be definitive. In this paper, we strive toward the above two issues via incorporating diversified contexts and answer-aware loss. Specifically, we propose a neural encoder-decoder model with multi-level copy mechanisms to generate such questions. Furthermore, the answer aware loss is introduced to make generated questions corresponding to more definitive answers. Experiments demonstrate that our model achieves state-of-the-art performance. Meanwhile, such generated question can express the given predicate and correspond to a definitive answer."
986,https://arxiv.org/abs/1910.13106,Incorporating Interlocutor-Aware Context into Response Generation on Multi-Party Chatbots,"Conventional chatbots focus on two-party response generation, which simplifies the real dialogue scene. In this paper, we strive toward a novel task of Response Generation on Multi-Party Chatbot (RGMPC), where the generated responses heavily rely on the interlocutors' roles (e.g., speaker and addressee) and their utterances. Unfortunately, complex interactions among the interlocutors' roles make it challenging to precisely capture conversational contexts and interlocutors' information. Facing this challenge, we present a response generation model which incorporates Interlocutor-aware Contexts into Recurrent Encoder-Decoder frameworks (ICRED) for RGMPC. Specifically, we employ interactive representations to capture dialogue contexts for different interlocutors. Moreover, we leverage an addressee memory to enhance contextual interlocutor information for the target addressee. Finally, we construct a corpus for RGMPC based on an existing open-access dataset. Automatic and manual evaluations demonstrate that the ICRED remarkably outperforms strong baselines."
987,https://arxiv.org/abs/1909.10924,Understanding Semantics from Speech Through Pre-training,"End-to-end Spoken Language Understanding (SLU) is proposed to infer the semantic meaning directly from audio features without intermediate text representation. Although the acoustic model component of an end-to-end SLU system can be pre-trained with Automatic Speech Recognition (ASR) targets, the SLU component can only learn semantic features from limited task-specific training data. In this paper, for the first time we propose to do large-scale unsupervised pre-training for the SLU component of an end-to-end SLU system, so that the SLU component may preserve semantic features from massive unlabeled audio data. As the output of the acoustic model component, i.e. phoneme posterior sequences, has much different characteristic from text sequences, we propose a novel pre-training model called BERT-PLM, which stands for Bidirectional Encoder Representations from Transformers through Permutation Language Modeling. BERT-PLM trains the SLU component on unlabeled data through a regression objective equivalent to the partial permutation language modeling objective, while leverages full bi-directional context information with BERT networks. The experiment results show that our approach out-perform the state-of-the-art end-to-end systems with over 12.5% error reduction."
988,https://arxiv.org/abs/1811.04604,Learning Personalized End-to-End Goal-Oriented Dialog,"Most existing works on dialog systems only consider conversation content while neglecting the personality of the user the bot is interacting with, which begets several unsolved issues. In this paper, we present a personalized end-to-end model in an attempt to leverage personalization in goal-oriented dialogs. We first introduce a Profile Model which encodes user profiles into distributed embeddings and refers to conversation history from other similar users. Then a Preference Model captures user preferences over knowledge base entities to handle the ambiguity in user requests. The two models are combined into the Personalized MemN2N. Experiments show that the proposed model achieves qualitative performance improvements over state-of-the-art methods. As for human evaluation, it also outperforms other approaches in terms of task completion rate and user satisfaction."
989,https://arxiv.org/abs/2305.03331,Generic and Robust Root Cause Localization for Multi-Dimensional Data in Online Service Systems,"Localizing root causes for multi-dimensional data is critical to ensure online service systems' reliability. When a fault occurs, only the measure values within specific attribute combinations are abnormal. Such attribute combinations are substantial clues to the underlying root causes and thus are called root causes of multidimensional data. This paper proposes a generic and robust root cause localization approach for multi-dimensional data, PSqueeze. We propose a generic property of root cause for multi-dimensional data, generalized ripple effect (GRE). Based on it, we propose a novel probabilistic cluster method and a robust heuristic search method. Moreover, we identify the importance of determining external root causes and propose an effective method for the first time in literature. Our experiments on two real-world datasets with 5400 faults show that the F1-score of PSqueeze outperforms baselines by 32.89%, while the localization time is around 10 seconds across all cases. The F1-score in determining external root causes of PSqueeze achieves 0.90. Furthermore, case studies in several production systems demonstrate that PSqueeze is helpful to fault diagnosis in the real world."
990,https://arxiv.org/abs/2302.10512,Robust Failure Diagnosis of Microservice System through Multimodal Data,"Automatic failure diagnosis is crucial for large microservice systems. Currently, most failure diagnosis methods rely solely on single-modal data (i.e., using either metrics, logs, or traces). In this study, we conduct an empirical study using real-world failure cases to show that combining these sources of data (multimodal data) leads to a more accurate diagnosis. However, effectively representing this data and addressing imbalanced failures remain a challenge. To tackle these issues, we introduce DiagFusion, a robust failure diagnosis approach that uses multimodal data. It leverages embedding techniques and data augmentation to represent the multimodal data of service instances, combines deployment data and traces to build a dependency graph, and uses a graph neural network to localize the root cause instance and determine the failure type. Our evaluations using real-world datasets show that DiagFusion outperforms existing methods in terms of root cause instance localization and failure type determination."
991,https://arxiv.org/abs/2301.08851,LWS: A Framework for Log-based Workload Simulation in Session-based SUT,"Artificial intelligence for IT Operations (AIOps) plays a critical role in operating and managing cloud-native systems and microservice-based applications but is limited by the lack of high-quality datasets with diverse scenarios. Realistic workloads are the premise and basis of generating such AIOps datasets, with the session-based workload being one of the most typical examples. Due to privacy concerns, complexity, variety, and requirements for reasonable intervention, it is difficult to copy or generate such workloads directly, showing the importance of effective and intervenable workload simulation. In this paper, we formulate the task of workload simulation and propose a framework for Log-based Workload Simulation (LWS) in session-based systems. LWS extracts the workload specification including the user behavior abstraction based on agglomerative clustering as well as relational models and the intervenable workload intensity from session logs. Then LWS combines the user behavior abstraction with the workload intensity to generate simulated workloads. The experimental evaluation is performed on an open-source cloud-native application with both well-designed and public real-world workloads, showing that the simulated workload generated by LWS is effective and intervenable, which provides the foundation of generating high-quality AIOps datasets."
992,https://arxiv.org/abs/2208.03938,Constructing Large-Scale Real-World Benchmark Datasets for AIOps,"Recently, AIOps (Artificial Intelligence for IT Operations) has been well studied in academia and industry to enable automated and effective software service management. Plenty of efforts have been dedicated to AIOps, including anomaly detection, root cause localization, incident management, etc. However, most existing works are evaluated on private datasets, so their generality and real performance cannot be guaranteed. The lack of public large-scale real-world datasets has prevented researchers and engineers from enhancing the development of AIOps. To tackle this dilemma, in this work, we introduce three public real-world, large-scale datasets about AIOps, mainly aiming at KPI anomaly detection, root cause localization on multi-dimensional data, and failure discovery and diagnosis. More importantly, we held three competitions in 2018/2019/2020 based on these datasets, attracting thousands of teams to participate. In the future, we will continue to publish more datasets and hold competitions to promote the development of AIOps further."
993,https://arxiv.org/abs/2207.09021,Actionable and Interpretable Fault Localization for Recurring Failures in Online Service Systems,"Fault localization is challenging in an online service system due to its monitoring data's large volume and variety and complex dependencies across or within its components (e.g., services or databases). Furthermore, engineers require fault localization solutions to be actionable and interpretable, which existing research approaches cannot satisfy. Therefore, the common industry practice is that, for a specific online service system, its experienced engineers focus on localization for recurring failures based on the knowledge accumulated about the system and historical failures. Although the above common practice is actionable and interpretable, it is largely manual, thus slow and sometimes inaccurate. In this paper, we aim to automate this practice through machine learning. That is, we propose an actionable and interpretable fault localization approach, DejaVu, for recurring failures in online service systems. For a specific online service system, DejaVu takes historical failures and dependencies in the system as input and trains a localization model offline; for an incoming failure, the trained model online recommends where the failure occurs (i.e., the faulty components) and which kind of failure occurs (i.e., the indicative group of metrics) (thus actionable), which are further interpreted by both global and local interpretation methods (thus interpretable). Based on the evaluation on 601 failures from three production systems and one open-source benchmark, in less than one second, DejaVu can on average rank the ground truths at 1.66-th to 5.03-th among a long candidate list, outperforming baselines by at least 51.51%."
994,https://arxiv.org/abs/2206.05871,Causal Inference-Based Root Cause Analysis for Online Service Systems with Intervention Recognition,"Fault diagnosis is critical in many domains, as faults may lead to safety threats or economic losses. In the field of online service systems, operators rely on enormous monitoring data to detect and mitigate failures. Quickly recognizing a small set of root cause indicators for the underlying fault can save much time for failure mitigation. In this paper, we formulate the root cause analysis problem as a new causal inference task named intervention recognition. We proposed a novel unsupervised causal inference-based method named Causal Inference-based Root Cause Analysis (CIRCA). The core idea is a sufficient condition for a monitoring variable to be a root cause indicator, i.e., the change of probability distribution conditioned on the parents in the Causal Bayesian Network (CBN). Towards the application in online service systems, CIRCA constructs a graph among monitoring metrics based on the knowledge of system architecture and a set of causal assumptions. The simulation study illustrates the theoretical reliability of CIRCA. The performance on a real-world dataset further shows that CIRCA can improve the recall of the top-1 recommendation by 25% over the best baseline method."
995,https://arxiv.org/abs/2112.03159,UniLog: Deploy One Model and Specialize it for All Log Analysis Tasks,UniLog: Deploy One Model and Specialize it for All Log Analysis Tasks
996,https://arxiv.org/abs/2108.05509,DOI: Divergence-based Out-of-Distribution Indicators via Deep Generative Models,"To ensure robust and reliable classification results, OoD (out-of-distribution) indicators based on deep generative models are proposed recently and are shown to work well on small datasets. In this paper, we conduct the first large collection of benchmarks (containing 92 dataset pairs, which is 1 order of magnitude larger than previous ones) for existing OoD indicators and observe that none perform well. We thus advocate that a large collection of benchmarks is mandatory for evaluating OoD indicators. We propose a novel theoretical framework, DOI, for divergence-based Out-of-Distribution indicators (instead of traditional likelihood-based) in deep generative models. Following this framework, we further propose a simple and effective OoD detection algorithm: Single-shot Fine-tune. It significantly outperforms past works by 5~8 in AUROC, and its performance is close to optimal. In recent, the likelihood criterion is shown to be ineffective in detecting OoD. Single-shot Fine-tune proposes a novel fine-tune criterion to detect OoD, by whether the likelihood of the testing sample is improved after fine-tuning a well-trained model on it. Fine-tune criterion is a clear and easy-following criterion, which will lead the OoD domain into a new stage."
997,https://arxiv.org/abs/2104.05490,DockerMock: Pre-Build Detection of Dockerfile Faults through Mocking Instruction Execution,"Continuous Integration (CI) and Continuous Deployment (CD) are widely adopted in software engineering practice. In reality, the CI/CD pipeline execution is not yet reliably continuous because it is often interrupted by Docker build failures. However, the existing trial-and-error practice to detect faults is time-consuming. To timely detect Dockerfile faults, we propose a context-based pre-build analysis approach, named DockerMock, through mocking the execution of common Dockerfile instructions. A Dockerfile fault is declared when an instruction conflicts with the approximated and accumulated running context. By explicitly keeping track of whether the context is fuzzy, DockerMock strikes a good balance of detection precision and recall. We evaluated DockerMock with 53 faults in 41 Dockerfiles from open source projects on GitHub and 130 faults in 105 Dockerfiles from student course projects. On average, DockerMock detected 68.0% Dockerfile faults in these two datasets. While baseline hadolint detected 6.5%, and baseline BuildKit detected 60.5% without instruction execution. In the GitHub dataset, DockerMock reduces the number of builds to 47, outperforming that of hadolint (73) and BuildKit (74)."
998,https://arxiv.org/abs/2012.08938,Summarizing Unstructured Logs in Online Services,"Logs are one of the most valuable data sources for managing large-scale online services. After a failure is detected/diagnosed/predicted, operators still have to inspect the raw logs to gain a summarized view before take actions. However, manual or rule-based log summarization has become inefficient and ineffective. In this work, we propose LogSummary, an automatic, unsupervised end-to-end log summarization framework for online services. LogSummary obtains the summarized triples of important logs for a given log sequence. It integrates a novel information extraction method taking both semantic information and domain knowledge into consideration, with a new triple ranking approach using the global knowledge learned from all logs. Given the lack of a publicly-available gold standard for log summarization, we have manually labelled the summaries of four open-source log datasets and made them publicly available. The evaluation on these datasets as well as the case studies on real-world logs demonstrate that LogSummary produces a highly representative (average ROUGE F1 score of 0.741) summaries. We have packaged LogSummary into an open-source toolkit and hope that it can benefit for future NLP-powered summarization works."
999,https://arxiv.org/abs/1912.02386,"The Search for Sparse, Robust Neural Networks","Recent work on deep neural network pruning has shown there exist sparse subnetworks that achieve equal or improved accuracy, training time, and loss using fewer network parameters when compared to their dense counterparts. Orthogonal to pruning literature, deep neural networks are known to be susceptible to adversarial examples, which may pose risks in security- or safety-critical applications. Intuition suggests that there is an inherent trade-off between sparsity and robustness such that these characteristics could not co-exist. We perform an extensive empirical evaluation and analysis testing the Lottery Ticket Hypothesis with adversarial training and show this approach enables us to find sparse, robust neural networks. Code for reproducing experiments is available here: https://github.com/justincosentino/robust-sparse-networks."
1000,https://arxiv.org/abs/1905.13452,On the Necessity and Effectiveness of Learning the Prior of Variational Auto-Encoder,"Using powerful posterior distributions is a popular approach to achieving better variational inference. However, recent works showed that the aggregated posterior may fail to match unit Gaussian prior, thus learning the prior becomes an alternative way to improve the lower-bound. In this paper, for the first time in the literature, we prove the necessity and effectiveness of learning the prior when aggregated posterior does not match unit Gaussian prior, analyze why this situation may happen, and propose a hypothesis that learning the prior may improve reconstruction loss, all of which are supported by our extensive experiment results. We show that using learned Real NVP prior and just one latent variable in VAE, we can achieve test NLL comparable to very deep state-of-the-art hierarchical VAE, outperforming many previous works with complex hierarchical VAE architectures."
1001,https://arxiv.org/abs/1802.03903,Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications,"To ensure undisrupted business, large Internet companies need to closely monitor various KPIs (e.g., Page Views, number of online users, and number of orders) of its Web applications, to accurately detect anomalies and trigger timely troubleshooting/mitigation. However, anomaly detection for these seasonal KPIs with various patterns and data quality has been a great challenge, especially without labels. In this paper, we proposed Donut, an unsupervised anomaly detection algorithm based on VAE. Thanks to a few of our key techniques, Donut greatly outperforms a state-of-arts supervised ensemble approach and a baseline VAE approach, and its best F-scores range from 0.75 to 0.9 for the studied KPIs from a top global Internet company. We come up with a novel KDE interpretation of reconstruction for Donut, making it the first VAE-based anomaly detection algorithm with solid theoretical explanation."
1002,https://arxiv.org/abs/1701.02528,Why It Takes So Long to Connect to a WiFi Access Point,"Today's WiFi networks deliver a large fraction of traffic. However, the performance and quality of WiFi networks are still far from satisfactory. Among many popular quality metrics (throughput, latency), the probability of successfully connecting to WiFi APs and the time cost of the WiFi connection set-up process are the two of the most critical metrics that affect WiFi users' experience. To understand the WiFi connection set-up process in real-world settings, we carry out measurement studies on $5$ million mobile users from $4$ representative cities associating with $7$ million APs in $0.4$ billion WiFi sessions, collected from a mobile ""WiFi Manager"" App that tops the Android/iOS App market. To the best of our knowledge, we are the first to do such large scale study on: how large the WiFi connection set-up time cost is, what factors affect the WiFi connection set-up process, and what can be done to reduce the WiFi connection set-up time cost. Based on the measurement analysis, we develop a machine learning based AP selection strategy that can significantly improve WiFi connection set-up performance, against the conventional strategy purely based on signal strength, by reducing the connection set-up failures from $33\%$ to $3.6\%$ and reducing $80\%$ time costs of the connection set-up processes by more than $10$ times."
1003,https://arxiv.org/abs/1112.5396,AdCell: Ad Allocation in Cellular Networks,"With more than four billion usage of cellular phones worldwide, mobile advertising has become an attractive alternative to online advertisements. In this paper, we propose a new targeted advertising policy for Wireless Service Providers (WSPs) via SMS or MMS- namely {\em AdCell}. In our model, a WSP charges the advertisers for showing their ads. Each advertiser has a valuation for specific types of customers in various times and locations and has a limit on the maximum available budget. Each query is in the form of time and location and is associated with one individual customer. In order to achieve a non-intrusive delivery, only a limited number of ads can be sent to each customer. Recently, new services have been introduced that offer location-based advertising over cellular network that fit in our model (e.g., ShopAlerts by AT&T) .
  We consider both online and offline version of the AdCell problem and develop approximation algorithms with constant competitive ratio. For the online version, we assume that the appearances of the queries follow a stochastic distribution and thus consider a Bayesian setting. Furthermore, queries may come from different distributions on different times. This model generalizes several previous advertising models such as online secretary problem \cite{HKP04}, online bipartite matching \cite{KVV90,FMMM09} and AdWords \cite{saberi05}. ..."
1004,https://arxiv.org/abs/2303.10445,EarCough: Enabling Continuous Subject Cough Event Detection on Hearables,"Cough monitoring can enable new individual pulmonary health applications. Subject cough event detection is the foundation for continuous cough monitoring. Recently, the rapid growth in smart hearables has opened new opportunities for such needs. This paper proposes EarCough, which enables continuous subject cough event detection on edge computing hearables by leveraging the always-on active noise cancellation (ANC) microphones. Specifically, we proposed a lightweight end-to-end neural network model -- EarCoughNet. To evaluate the effectiveness of our method, we constructed a synchronous motion and audio dataset through a user study. Results show that EarCough achieved an accuracy of 95.4% and an F1-score of 92.9% with a space requirement of only 385 kB. We envision EarCough as a low-cost add-on for future hearables to enable continuous subject cough event detection."
1005,https://arxiv.org/abs/2303.10443,GazeReader: Detecting Unknown Word Using Webcam for English as a Second Language (ESL) Learners,"Automatic unknown word detection techniques can enable new applications for assisting English as a Second Language (ESL) learners, thus improving their reading experiences. However, most modern unknown word detection methods require dedicated eye-tracking devices with high precision that are not easily accessible to end-users. In this work, we propose GazeReader, an unknown word detection method only using a webcam. GazeReader tracks the learner's gaze and then applies a transformer-based machine learning model that encodes the text information to locate the unknown word. We applied knowledge enhancement including term frequency, part of speech, and named entity recognition to improve the performance. The user study indicates that the accuracy and F1-score of our method were 98.09% and 75.73%, respectively. Lastly, we explored the design scope for ESL reading and discussed the findings."
1006,https://arxiv.org/abs/2303.10441,Enabling Voice-Accompanying Hand-to-Face Gesture Recognition with Cross-Device Sensing,"Gestures performed accompanying the voice are essential for voice interaction to convey complementary semantics for interaction purposes such as wake-up state and input modality. In this paper, we investigated voice-accompanying hand-to-face (VAHF) gestures for voice interaction. We targeted hand-to-face gestures because such gestures relate closely to speech and yield significant acoustic features (e.g., impeding voice propagation). We conducted a user study to explore the design space of VAHF gestures, where we first gathered candidate gestures and then applied a structural analysis to them in different dimensions (e.g., contact position and type), outputting a total of 8 VAHF gestures with good usability and least confusion. To facilitate VAHF gesture recognition, we proposed a novel cross-device sensing method that leverages heterogeneous channels (vocal, ultrasound, and IMU) of data from commodity devices (earbuds, watches, and rings). Our recognition model achieved an accuracy of 97.3% for recognizing 3 gestures and 91.5% for recognizing 8 gestures, excluding the ""empty"" gesture, proving the high applicability. Quantitative analysis also sheds light on the recognition capability of each sensor channel and their different combinations. In the end, we illustrated the feasible use cases and their design principles to demonstrate the applicability of our system in various scenarios."
1007,https://arxiv.org/abs/2303.10435,Modeling the Trade-off of Privacy Preservation and Activity Recognition on Low-Resolution Images,"A computer vision system using low-resolution image sensors can provide intelligent services (e.g., activity recognition) but preserve unnecessary visual privacy information from the hardware level. However, preserving visual privacy and enabling accurate machine recognition have adversarial needs on image resolution. Modeling the trade-off of privacy preservation and machine recognition performance can guide future privacy-preserving computer vision systems using low-resolution image sensors. In this paper, using the at-home activity of daily livings (ADLs) as the scenario, we first obtained the most important visual privacy features through a user survey. Then we quantified and analyzed the effects of image resolution on human and machine recognition performance in activity recognition and privacy awareness tasks. We also investigated how modern image super-resolution techniques influence these effects. Based on the results, we proposed a method for modeling the trade-off of privacy preservation and activity recognition on low-resolution images."
1008,https://arxiv.org/abs/2302.03840,MMPD: Multi-Domain Mobile Video Physiology Dataset,"Remote photoplethysmography (rPPG) is an attractive method for noninvasive, convenient and concomitant measurement of physiological vital signals. Public benchmark datasets have served a valuable role in the development of this technology and improvements in accuracy over recent years.However, there remain gaps in the public datasets.First, despite the ubiquity of cameras on mobile devices, there are few datasets recorded specifically with mobile phone cameras. Second, most datasets are relatively small and therefore are limited in diversity, both in appearance (e.g., skin tone), behaviors (e.g., motion) and environment (e.g., lighting conditions). In an effort to help the field advance, we present the Multi-domain Mobile Video Physiology Dataset (MMPD), comprising 11 hours of recordings from mobile phones of 33 subjects. The dataset is designed to capture videos with greater representation across skin tone, body motion, and lighting conditions. MMPD is comprehensive with eight descriptive labels and can be used in conjunction with the rPPG-toolbox. The reliability of the dataset is verified by mainstream unsupervised methods and neural methods. The GitHub repository of our dataset: https://github.com/THU-CS-PI/MMPD_rPPG_dataset."
1009,https://arxiv.org/abs/2210.09222,MMTSA: Multimodal Temporal Segment Attention Network for Efficient Human Activity Recognition,"Multimodal sensors (e.g., visual, non-visual, and wearable) provide complementary information to develop robust perception systems for recognizing activities. However, most existing algorithms use dense sampling and heterogeneous sub-network to extract unimodal features and fuse them at the end of their framework, which causes data redundancy, lack of complementary multimodal information and high computational cost. In this paper, we propose a new novel multimodal neural architecture based on RGB and IMU wearable sensors (e.g., accelerometer, gyroscope) for human activity recognition called Multimodal Temporal Segment Attention Network (MMTSA). MMTSA first employs a multimodal data isomorphism mechanism based on Gramian Angular Field (GAF) and then applies a novel multimodal sparse sampling method to reduce redundancy. Moreover, we propose an inter-segment attention module in MMTSA to fuse multimodal features effectively and efficiently. We demonstrate the importance of imu data imaging and attention mechanism in human activity recognition by rigorous evaluation on three public datasets, and achieve superior improvements ($11.13\%$ on the MMAct dataset) than the previous state-of-the-art methods. The code is available at: https://github.com/THU-CS-PI/MMTSA."
1010,https://arxiv.org/abs/2209.12018,Facilitating Self-monitored Physical Rehabilitation with Virtual Reality and Haptic feedback,"Physical rehabilitation is essential to recovery from joint replacement operations. As a representation, total knee arthroplasty (TKA) requires patients to conduct intensive physical exercises to regain the knee's range of motion and muscle strength. However, current joint replacement physical rehabilitation methods rely highly on therapists for supervision, and existing computer-assisted systems lack consideration for enabling self-monitoring, making at-home physical rehabilitation difficult. In this paper, we investigated design recommendations that would enable self-monitored rehabilitation through clinical observations and focus group interviews with doctors and therapists. With this knowledge, we further explored Virtual Reality(VR)-based visual presentation and supplemental haptic motion guidance features in our implementation VReHab, a self-monitored and multimodal physical rehabilitation system with VR and vibrotactile and pneumatic feedback in a TKA rehabilitation context. We found that the third point of view real-time reconstructed motion on a virtual avatar overlaid with the target pose effectively provides motion awareness and guidance while haptic feedback helps enhance users' motion accuracy and stability. Finally, we implemented \systemname to facilitate self-monitored post-operative exercises and validated its effectiveness through a clinical study with 10 patients."
1011,https://arxiv.org/abs/2204.12071,Modeling the Noticeability of User-Avatar Movement Inconsistency for Sense of Body Ownership Intervention,"An avatar mirroring the user's movement is commonly adopted in Virtual Reality(VR). Maintaining the user-avatar movement consistency provides the user a sense of body ownership and thus an immersive experience. However, breaking this consistency can enable new interaction functionalities, such as pseudo haptic feedback or input augmentation, at the expense of immersion. We propose to quantify the probability of users noticing the movement inconsistency while the inconsistency amplitude is being enlarged, which aims to guide the intervention of the users' sense of body ownership in VR. We applied angular offsets to the avatar's shoulder and elbow joints and recorded whether the user identified the inconsistency through a series of three user studies and built a statistical model based on the results. Results show that the noticeability of movement inconsistency increases roughly quadratically with the enlargement of offsets and the offsets at two joints negatively affect the probability distributions of each other. Leveraging the model, we implemented a technique that amplifies the user's arm movements with unnoticeable offsets and then evaluated implementations with different parameters(offset strength, offset distribution). Results show that the technique with medium-level and balanced-distributed offsets achieves the best overall performance. Finally, we demonstrated our model's extendability in interventions in the sense of body ownership with three VR applications including stroke rehabilitation, action game and widget arrangement."
1012,https://arxiv.org/abs/2203.10553,FaceOri: Tracking Head Position and Orientation Using Ultrasonic Ranging on Earphones,"Face orientation can often indicate users' intended interaction target. In this paper, we propose FaceOri, a novel face tracking technique based on acoustic ranging using earphones. FaceOri can leverage the speaker on a commodity device to emit an ultrasonic chirp, which is picked up by the set of microphones on the user's earphone, and then processed to calculate the distance from each microphone to the device. These measurements are used to derive the user's face orientation and distance with respect to the device. We conduct a ground truth comparison and user study to evaluate FaceOri's performance. The results show that the system can determine whether the user orients to the device at a 93.5% accuracy within a 1.5 meters range. Furthermore, FaceOri can continuously track the user's head orientation with a median absolute error of 10.9 mm in the distance, 3.7 degrees in yaw, and 5.8 degrees in pitch. FaceOri can allow for convenient hands-free control of devices and produce more intelligent context-aware interaction."
1013,https://arxiv.org/abs/2112.13156,Enabling Real-time On-chip Audio Super Resolution for Bone Conduction Microphones,"Voice communication using the air conduction microphone in noisy environments suffers from the degradation of speech audibility. Bone conduction microphones (BCM) are robust against ambient noises but suffer from limited effective bandwidth due to their sensing mechanism. Although existing audio super resolution algorithms can recover the high frequency loss to achieve high-fidelity audio, they require considerably more computational resources than available in low-power hearable devices. This paper proposes the first-ever real-time on-chip speech audio super resolution system for BCM. To accomplish this, we built and compared a series of lightweight audio super resolution deep learning models. Among all these models, ATS-UNet is the most cost-efficient because the proposed novel Audio Temporal Shift Module (ATSM) reduces the network's dimensionality while maintaining sufficient temporal features from speech audios. Then we quantized and deployed the ATS-UNet to low-end ARM micro-controller units for real-time embedded prototypes. Evaluation results show that our system achieved real-time inference speed on Cortex-M7 and higher quality than the baseline audio super resolution method. Finally, we conducted a user study with ten experts and ten amateur listeners to evaluate our method's effectiveness to human ears. Both groups perceived a significantly higher speech quality with our method when compared to the solutions with the original BCM or air conduction microphone with cutting-edge noise reduction algorithms."
1014,https://arxiv.org/abs/2106.00931,Understanding the Design Space of Mouth Microgestures,"As wearable devices move toward the face (i.e. smart earbuds, glasses), there is an increasing need to facilitate intuitive interactions with these devices. Current sensing techniques can already detect many mouth-based gestures; however, users' preferences of these gestures are not fully understood. In this paper, we investigate the design space and usability of mouth-based microgestures. We first conducted brainstorming sessions (N=16) and compiled an extensive set of 86 user-defined gestures. Then, with an online survey (N=50), we assessed the physical and mental demand of our gesture set and identified a subset of 14 gestures that can be performed easily and naturally. Finally, we conducted a remote Wizard-of-Oz usability study (N=11) mapping gestures to various daily smartphone operations under a sitting and walking context. From these studies, we develop a taxonomy for mouth gestures, finalize a practical gesture set for common applications, and provide design guidelines for future mouth-based gesture interactions."
1015,https://arxiv.org/abs/2105.05182,PTeacher: a Computer-Aided Personalized Pronunciation Training System with Exaggerated Audio-Visual Corrective Feedback,"Second language (L2) English learners often find it difficult to improve their pronunciations due to the lack of expressive and personalized corrective feedback. In this paper, we present Pronunciation Teacher (PTeacher), a Computer-Aided Pronunciation Training (CAPT) system that provides personalized exaggerated audio-visual corrective feedback for mispronunciations. Though the effectiveness of exaggerated feedback has been demonstrated, it is still unclear how to define the appropriate degrees of exaggeration when interacting with individual learners. To fill in this gap, we interview 100 L2 English learners and 22 professional native teachers to understand their needs and experiences. Three critical metrics are proposed for both learners and teachers to identify the best exaggeration levels in both audio and visual modalities. Additionally, we incorporate the personalized dynamic feedback mechanism given the English proficiency of learners. Based on the obtained insights, a comprehensive interactive pronunciation training course is designed to help L2 learners rectify mispronunciations in a more perceptible, understandable, and discriminative manner. Extensive user studies demonstrate that our system significantly promotes the learners' learning efficiency."
1016,https://arxiv.org/abs/2305.13796,SE-Bridge: Speech Enhancement with Consistent Brownian Bridge,"We propose SE-Bridge, a novel method for speech enhancement (SE). After recently applying the diffusion models to speech enhancement, we can achieve speech enhancement by solving a stochastic differential equation (SDE). Each SDE corresponds to a probabilistic flow ordinary differential equation (PF-ODE), and the trajectory of the PF-ODE solution consists of the speech states at different moments. Our approach is based on consistency model that ensure any speech states on the same PF-ODE trajectory, correspond to the same initial state. By integrating the Brownian Bridge process, the model is able to generate high-intelligibility speech samples without adversarial training. This is the first attempt that applies the consistency models to SE task, achieving state-of-the-art results in several metrics while saving 15 x the time required for sampling compared to the diffusion-based baseline. Our experiments on multiple datasets demonstrate the effectiveness of SE-Bridge in SE. Furthermore, we show through extensive experiments on downstream tasks, including Automatic Speech Recognition (ASR) and Speaker Verification (SV), that SE-Bridge can effectively support multiple downstream tasks."
1017,https://arxiv.org/abs/2303.14470,Compacting Binary Neural Networks by Sparse Kernel Selection,"Binary Neural Network (BNN) represents convolution weights with 1-bit values, which enhances the efficiency of storage and computation. This paper is motivated by a previously revealed phenomenon that the binary kernels in successful BNNs are nearly power-law distributed: their values are mostly clustered into a small number of codewords. This phenomenon encourages us to compact typical BNNs and obtain further close performance through learning non-repetitive kernels within a binary kernel subspace. Specifically, we regard the binarization process as kernel grouping in terms of a binary codebook, and our task lies in learning to select a smaller subset of codewords from the full codebook. We then leverage the Gumbel-Sinkhorn technique to approximate the codeword selection process, and develop the Permutation Straight-Through Estimator (PSTE) that is able to not only optimize the selection process end-to-end but also maintain the non-repetitive occupancy of selected codewords. Experiments verify that our method reduces both the model size and bit-wise computational costs, and achieves accuracy improvements compared with state-of-the-art BNNs under comparable budgets."
1018,https://arxiv.org/abs/2303.05240,Intriguing Property and Counterfactual Explanation of GAN for Remote Sensing Image Generation,"Generative adversarial networks (GANs) have achieved remarkable progress in the natural image field. However, when applying GANs in the remote sensing (RS) image generation task, an extraordinary phenomenon is observed: the GAN model is more sensitive to the size of training data for RS image generation than for natural image generation. In other words, the generation quality of RS images will change significantly with the number of training categories or samples per category. In this paper, we first analyze this phenomenon from two kinds of toy experiments and conclude that the amount of feature information contained in the GAN model decreases with reduced training data. Then we establish a structural causal model (SCM) of the data generation process and interpret the generated data as the counterfactuals. Based on this SCM, we theoretically prove that the quality of generated images is positively correlated with the amount of feature information. This provides insights for enriching the feature information learned by the GAN model during training. Consequently, we propose two innovative adjustment schemes, namely Uniformity Regularization (UR) and Entropy Regularization (ER), to increase the information learned by the GAN model at the distributional and sample levels, respectively. We theoretically and empirically demonstrate the effectiveness and versatility of our methods. Extensive experiments on three RS datasets and two natural datasets show that our methods outperform the well-established models on RS image generation tasks. The source code is available at https://github.com/rootSue/Causal-RSGAN."
1019,https://arxiv.org/abs/2302.10717,Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment,"In this paper, a novel robotic grasping system is established to automatically pick up objects in cluttered scenes. A composite robotic hand composed of a suction cup and a gripper is designed for grasping the object stably. The suction cup is used for lifting the object from the clutter first and the gripper for grasping the object accordingly. We utilize the affordance map to provide pixel-wise lifting point candidates for the suction cup. To obtain a good affordance map, the active exploration mechanism is introduced to the system. An effective metric is designed to calculate the reward for the current affordance map, and a deep Q-Network (DQN) is employed to guide the robotic hand to actively explore the environment until the generated affordance map is suitable for grasping. Experimental results have demonstrated that the proposed robotic grasping system is able to greatly increase the success rate of the robotic grasping in cluttered scenes."
1020,https://arxiv.org/abs/2302.05209,A Survey on Causal Reinforcement Learning,"While Reinforcement Learning (RL) achieves tremendous success in sequential decision-making problems of many domains, it still faces key challenges of data inefficiency and the lack of interpretability. Interestingly, many researchers have leveraged insights from the causality literature recently, bringing forth flourishing works to unify the merits of causality and address well the challenges from RL. As such, it is of great necessity and significance to collate these Causal Reinforcement Learning (CRL) works, offer a review of CRL methods, and investigate the potential functionality from causality toward RL. In particular, we divide existing CRL approaches into two categories according to whether their causality-based information is given in advance or not. We further analyze each category in terms of the formalization of different models, ranging from the Markov Decision Process (MDP), Partially Observed Markov Decision Process (POMDP), Multi-Arm Bandits (MAB), and Dynamic Treatment Regime (DTR). Moreover, we summarize the evaluation matrices and open sources while we discuss emerging applications, along with promising prospects for the future development of CRL."
1021,https://arxiv.org/abs/2301.08496,Introducing Expertise Logic into Graph Representation Learning from A Causal Perspective,"Benefiting from the injection of human prior knowledge, graphs, as derived discrete data, are semantically dense so that models can efficiently learn the semantic information from such data. Accordingly, graph neural networks (GNNs) indeed achieve impressive success in various fields. Revisiting the GNN learning paradigms, we discover that the relationship between human expertise and the knowledge modeled by GNNs still confuses researchers. To this end, we introduce motivating experiments and derive an empirical observation that the GNNs gradually learn human expertise in general domains. By further observing the ramifications of introducing expertise logic into graph representation learning, we conclude that leading the GNNs to learn human expertise can improve the model performance. Hence, we propose a novel graph representation learning method to incorporate human expert knowledge into GNN models. The proposed method ensures that the GNN model can not only acquire the expertise held by human experts but also engage in end-to-end learning from datasets. Plentiful experiments on the crafted and real-world domains support the consistent effectiveness of the proposed method."
1022,https://arxiv.org/abs/2301.08343,Tacchi: A Pluggable and Low Computational Cost Elastomer Deformation Simulator for Optical Tactile Sensors,"Simulation is widely applied in robotics research to save time and resources. There have been several works to simulate optical tactile sensors that leverage either a smoothing method or Finite Element Method (FEM). However, elastomer deformation physics is not considered in the former method, whereas the latter requires a massive amount of computational resources like a computer cluster. In this work, we propose a pluggable and low computational cost simulator using the Taichi programming language for simulating optical tactile sensors, named as Tacchi . It reconstructs elastomer deformation using particles, which allows deformed elastomer surfaces to be rendered into tactile images and reveals contact information without suffering from high computational costs. Tacchi facilitates creating realistic tactile images in simulation, e.g., ones that capture wear-and-tear defects on object surfaces. In addition, the proposed Tacchi can be integrated with robotics simulators for a robot system simulation. Experiment results showed that Tacchi can produce images with better similarity to real images and achieved higher Sim2Real accuracy compared to the existing methods. Moreover, it can be connected with MuJoCo and Gazebo with only the requirement of 1G memory space in GPU compared to a computer cluster applied for FEM. With Tacchi, physical robot simulation with optical tactile sensors becomes possible. All the materials in this paper are available at https://github.com/zixichen007115/Tacchi ."
1023,https://arxiv.org/abs/2212.11694,Timestamp-Supervised Action Segmentation from the Perspective of Clustering,"Video action segmentation under timestamp supervision has recently received much attention due to lower annotation costs. Most existing methods generate pseudo-labels for all frames in each video to train the segmentation model. However, these methods suffer from incorrect pseudo-labels, especially for the semantically unclear frames in the transition region between two consecutive actions, which we call ambiguous intervals. To address this issue, we propose a novel framework from the perspective of clustering, which includes the following two parts. First, pseudo-label ensembling generates incomplete but high-quality pseudo-label sequences, where the frames in ambiguous intervals have no pseudo-labels. Second, iterative clustering iteratively propagates the pseudo-labels to the ambiguous intervals by clustering, and thus updates the pseudo-label sequences to train the model. We further introduce a clustering loss, which encourages the features of frames within the same action segment more compact. Extensive experiments show the effectiveness of our method."
1024,https://arxiv.org/abs/2212.05767,"A Survey of Knowledge Graph Reasoning on Graph Types: Static, Dynamic, and Multimodal","Knowledge graph reasoning (KGR), aiming to deduce new facts from existing facts based on mined logic rules underlying knowledge graphs (KGs), has become a fast-growing research direction. It has been proven to significantly benefit the usage of KGs in many AI applications, such as question answering and recommendation systems, etc. According to the graph types, the existing KGR models can be roughly divided into three categories, i.e., static models, temporal models, and multi-modal models. The early works in this domain mainly focus on static KGR and tend to directly apply general knowledge graph embedding models to the reasoning task. However, these models are not suitable for more complex but practical tasks, such as inductive static KGR, temporal KGR, and multi-modal KGR. To this end, multiple works have been developed recently, but no survey papers and open-source repositories comprehensively summarize and discuss models in this important direction. To fill the gap, we conduct a survey for knowledge graph reasoning tracing from static to temporal and then to multi-modal KGs. Concretely, the preliminaries, summaries of KGR models, and typical datasets are introduced and discussed consequently. Moreover, we discuss the challenges and potential opportunities. The corresponding open-source repository is shared on GitHub: https://github.com/LIANGKE23/Awesome-Knowledge-Graph-Reasoning."
1025,https://arxiv.org/abs/2210.16805,SRTNet: Time Domain Speech Enhancement Via Stochastic Refinement,"Diffusion model, as a new generative model which is very popular in image generation and audio synthesis, is rarely used in speech enhancement. In this paper, we use the diffusion model as a module for stochastic refinement. We propose SRTNet, a novel method for speech enhancement via Stochastic Refinement in complete Time domain. Specifically, we design a joint network consisting of a deterministic module and a stochastic module, which makes up the ``enhance-and-refine'' paradigm. We theoretically demonstrate the feasibility of our method and experimentally prove that our method achieves faster training, faster sampling and higher quality. Our code and enhanced samples are available at https://github.com/zhibinQiu/SRTNet.git."
1026,https://arxiv.org/abs/2210.08537,Learning 6-DoF Task-oriented Grasp Detection via Implicit Estimation and Visual Affordance,"Currently, task-oriented grasp detection approaches are mostly based on pixel-level affordance detection and semantic segmentation. These pixel-level approaches heavily rely on the accuracy of a 2D affordance mask, and the generated grasp candidates are restricted to a small workspace. To mitigate these limitations, we first construct a novel affordance-based grasp dataset and propose a 6-DoF task-oriented grasp detection framework, which takes the observed object point cloud as input and predicts diverse 6-DoF grasp poses for different tasks. Specifically, our implicit estimation network and visual affordance network in this framework could directly predict coarse grasp candidates, and corresponding 3D affordance heatmap for each potential task, respectively. Furthermore, the grasping scores from coarse grasps are combined with heatmap values to generate more accurate and finer candidates. Our proposed framework shows significant improvements compared to baselines for existing and novel objects on our simulation dataset. Although our framework is trained based on the simulated objects and environment, the final generated grasp candidates can be accurately and stably executed in real robot experiments when the object is randomly placed on a support surface."
1027,https://arxiv.org/abs/2210.08349,When to Update Your Model: Constrained Model-based Reinforcement Learning,"Designing and analyzing model-based RL (MBRL) algorithms with guaranteed monotonic improvement has been challenging, mainly due to the interdependence between policy optimization and model learning. Existing discrepancy bounds generally ignore the impacts of model shifts, and their corresponding algorithms are prone to degrade performance by drastic model updating. In this work, we first propose a novel and general theoretical scheme for a non-decreasing performance guarantee of MBRL. Our follow-up derived bounds reveal the relationship between model shifts and performance improvement. These discoveries encourage us to formulate a constrained lower-bound optimization problem to permit the monotonicity of MBRL. A further example demonstrates that learning models from a dynamically-varying number of explorations benefit the eventual returns. Motivated by these analyses, we design a simple but effective algorithm CMLO (Constrained Model-shift Lower-bound Optimization), by introducing an event-triggered mechanism that flexibly determines when to update the model. Experiments show that CMLO surpasses other state-of-the-art methods and produces a boost when various policy optimization methods are employed."
1028,https://arxiv.org/abs/2210.01391,Bridged Transformer for Vision and Point Cloud 3D Object Detection,"3D object detection is a crucial research topic in computer vision, which usually uses 3D point clouds as input in conventional setups. Recently, there is a trend of leveraging multiple sources of input data, such as complementing the 3D point cloud with 2D images that often have richer color and fewer noises. However, due to the heterogeneous geometrics of the 2D and 3D representations, it prevents us from applying off-the-shelf neural networks to achieve multimodal fusion. To that end, we propose Bridged Transformer (BrT), an end-to-end architecture for 3D object detection. BrT is simple and effective, which learns to identify 3D and 2D object bounding boxes from both points and image patches. A key element of BrT lies in the utilization of object queries for bridging 3D and 2D spaces, which unifies different sources of data representations in Transformer. We adopt a form of feature aggregation realized by point-to-patch projections which further strengthen the correlations between images and points. Moreover, BrT works seamlessly for fusing the point cloud with multi-view images. We experimentally show that BrT surpasses state-of-the-art methods on SUN RGB-D and ScanNetV2 datasets."
1029,https://arxiv.org/abs/2210.01353,Pay Self-Attention to Audio-Visual Navigation,"Audio-visual embodied navigation, as a hot research topic, aims training a robot to reach an audio target using egocentric visual (from the sensors mounted on the robot) and audio (emitted from the target) input. The audio-visual information fusion strategy is naturally important to the navigation performance, but the state-of-the-art methods still simply concatenate the visual and audio features, potentially ignoring the direct impact of context. Moreover, the existing approaches requires either phase-wise training or additional aid (e.g. topology graph and sound semantics). Up till this date, the work that deals with the more challenging setup with moving target(s) is still rare. As a result, we propose an end-to-end framework FSAAVN (feature self-attention audio-visual navigation) to learn chasing after a moving audio target using a context-aware audio-visual fusion strategy implemented as a self-attention module. Our thorough experiments validate the superior performance (both quantitatively and qualitatively) of FSAAVN in comparison with the state-of-the-arts, and also provide unique insights about the choice of visual modalities, visual/audio encoder backbones and fusion patterns."
1030,https://arxiv.org/abs/2208.12681,Disentangle and Remerge: Interventional Knowledge Distillation for Few-Shot Object Detection from A Conditional Causal Perspective,"Few-shot learning models learn representations with limited human annotations, and such a learning paradigm demonstrates practicability in various tasks, e.g., image classification, object detection, etc. However, few-shot object detection methods suffer from an intrinsic defect that the limited training data makes the model cannot sufficiently explore semantic information. To tackle this, we introduce knowledge distillation to the few-shot object detection learning paradigm. We further run a motivating experiment, which demonstrates that in the process of knowledge distillation, the empirical error of the teacher model degenerates the prediction performance of the few-shot object detection model as the student. To understand the reasons behind this phenomenon, we revisit the learning paradigm of knowledge distillation on the few-shot object detection task from the causal theoretic standpoint, and accordingly, develop a Structural Causal Model. Following the theoretical guidance, we propose a backdoor adjustment-based knowledge distillation method for the few-shot object detection task, namely Disentangle and Remerge (D&R), to perform conditional causal intervention toward the corresponding Structural Causal Model. Empirically, the experiments on benchmarks demonstrate that D&R can yield significant performance boosts in few-shot object detection. Code is available at https://github.com/ZYN-1101/DandR.git."
1031,https://arxiv.org/abs/2208.08584,Robust Causal Graph Representation Learning against Confounding Effects,"The prevailing graph neural network models have achieved significant progress in graph representation learning. However, in this paper, we uncover an ever-overlooked phenomenon: the pre-trained graph representation learning model tested with full graphs underperforms the model tested with well-pruned graphs. This observation reveals that there exist confounders in graphs, which may interfere with the model learning semantic information, and current graph representation learning methods have not eliminated their influence. To tackle this issue, we propose Robust Causal Graph Representation Learning (RCGRL) to learn robust graph representations against confounding effects. RCGRL introduces an active approach to generate instrumental variables under unconditional moment restrictions, which empowers the graph representation learning model to eliminate confounders, thereby capturing discriminative information that is causally related to downstream predictions. We offer theorems and proofs to guarantee the theoretical effectiveness of the proposed approach. Empirically, we conduct extensive experiments on a synthetic dataset and multiple benchmark datasets. The results demonstrate that compared with state-of-the-art methods, RCGRL achieves better prediction performance and generalization ability."
1032,https://arxiv.org/abs/2207.07870,Scene Graph for Embodied Exploration in Cluttered Scenario,"The ability to handle objects in cluttered environment has been long anticipated by robotic community. However, most of works merely focus on manipulation instead of rendering hidden semantic information in cluttered objects. In this work, we introduce the scene graph for embodied exploration in cluttered scenarios to solve this problem. To validate our method in cluttered scenario, we adopt the Manipulation Question Answering (MQA) tasks as our test benchmark, which requires an embodied robot to have the active exploration ability and semantic understanding ability of vision and language.As a general solution framework to the task, we propose an imitation learning method to generate manipulations for exploration. Meanwhile, a VQA model based on dynamic scene graph is adopted to comprehend a series of RGB frames from wrist camera of manipulator along with every step of manipulation is conducted to answer questions in our framework.The experiments on of MQA dataset with different interaction requirements demonstrate that our proposed framework is effective for MQA task a representative of tasks in cluttered scenario."
1033,https://arxiv.org/abs/2206.11959,Similarity-aware Positive Instance Sampling for Graph Contrastive Pre-training,"Graph instance contrastive learning has been proved as an effective task for Graph Neural Network (GNN) pre-training. However, one key issue may seriously impede the representative power in existing works: Positive instances created by current methods often miss crucial information of graphs or even yield illegal instances (such as non-chemically-aware graphs in molecular generation). To remedy this issue, we propose to select positive graph instances directly from existing graphs in the training set, which ultimately maintains the legality and similarity to the target graphs. Our selection is based on certain domain-specific pair-wise similarity measurements as well as sampling from a hierarchical graph encoding similarity relations among graphs. Besides, we develop an adaptive node-level pre-training method to dynamically mask nodes to distribute them evenly in the graph. We conduct extensive experiments on $13$ graph classification and node classification benchmark datasets from various domains. The results demonstrate that the GNN models pre-trained by our strategies can outperform those trained-from-scratch models as well as the variants obtained by existing methods."
1034,https://arxiv.org/abs/2206.01724,SNAKE: Shape-aware Neural 3D Keypoint Field,"Detecting 3D keypoints from point clouds is important for shape reconstruction, while this work investigates the dual question: can shape reconstruction benefit 3D keypoint detection? Existing methods either seek salient features according to statistics of different orders or learn to predict keypoints that are invariant to transformation. Nevertheless, the idea of incorporating shape reconstruction into 3D keypoint detection is under-explored. We argue that this is restricted by former problem formulations. To this end, a novel unsupervised paradigm named SNAKE is proposed, which is short for shape-aware neural 3D keypoint field. Similar to recent coordinate-based radiance or distance field, our network takes 3D coordinates as inputs and predicts implicit shape indicators and keypoint saliency simultaneously, thus naturally entangling 3D keypoint detection and shape reconstruction. We achieve superior performance on various public benchmarks, including standalone object datasets ModelNet40, KeypointNet, SMPL meshes and scene-level datasets 3DMatch and Redwood. Intrinsic shape awareness brings several advantages as follows. (1) SNAKE generates 3D keypoints consistent with human semantic annotation, even without such supervision. (2) SNAKE outperforms counterparts in terms of repeatability, especially when the input point clouds are down-sampled. (3) the generated keypoints allow accurate geometric registration, notably in a zero-shot setting. Codes are available at https://github.com/zhongcl-thu/SNAKE"
1035,https://arxiv.org/abs/2204.08721,Multimodal Token Fusion for Vision Transformers,"Many adaptations of transformers have emerged to address the single-modal vision tasks, where self-attention modules are stacked to handle input sources like images. Intuitively, feeding multiple modalities of data to vision transformers could improve the performance, yet the inner-modal attentive weights may also be diluted, which could thus undermine the final performance. In this paper, we propose a multimodal token fusion method (TokenFusion), tailored for transformer-based vision tasks. To effectively fuse multiple modalities, TokenFusion dynamically detects uninformative tokens and substitutes these tokens with projected and aggregated inter-modal features. Residual positional alignment is also adopted to enable explicit utilization of the inter-modal alignments after fusion. The design of TokenFusion allows the transformer to learn correlations among multimodal features, while the single-modal transformer architecture remains largely intact. Extensive experiments are conducted on a variety of homogeneous and heterogeneous modalities and demonstrate that TokenFusion surpasses state-of-the-art methods in three typical vision tasks: multimodal image-to-image translation, RGB-depth semantic segmentation, and 3D object detection with point cloud and images. Our code is available at https://github.com/yikaiw/TokenFusion."
1036,https://arxiv.org/abs/2203.07988,Smoothing Matters: Momentum Transformer for Domain Adaptive Semantic Segmentation,"After the great success of Vision Transformer variants (ViTs) in computer vision, it has also demonstrated great potential in domain adaptive semantic segmentation. Unfortunately, straightforwardly applying local ViTs in domain adaptive semantic segmentation does not bring in expected improvement. We find that the pitfall of local ViTs is due to the severe high-frequency components generated during both the pseudo-label construction and features alignment for target domains. These high-frequency components make the training of local ViTs very unsmooth and hurt their transferability. In this paper, we introduce a low-pass filtering mechanism, momentum network, to smooth the learning dynamics of target domain features and pseudo labels. Furthermore, we propose a dynamic of discrepancy measurement to align the distributions in the source and target domains via dynamic weights to evaluate the importance of the samples. After tackling the above issues, extensive experiments on sim2real benchmarks show that the proposed method outperforms the state-of-the-art methods. Our codes are available at https://github.com/alpc91/TransDA"
1037,https://arxiv.org/abs/2203.06442,Equivariant Graph Mechanics Networks with Constraints,"Learning to reason about relations and dynamics over multiple interacting objects is a challenging topic in machine learning. The challenges mainly stem from that the interacting systems are exponentially-compositional, symmetrical, and commonly geometrically-constrained. Current methods, particularly the ones based on equivariant Graph Neural Networks (GNNs), have targeted on the first two challenges but remain immature for constrained systems. In this paper, we propose Graph Mechanics Network (GMN) which is combinatorially efficient, equivariant and constraint-aware. The core of GMN is that it represents, by generalized coordinates, the forward kinematics information (positions and velocities) of a structural object. In this manner, the geometrical constraints are implicitly and naturally encoded in the forward kinematics. Moreover, to allow equivariant message passing in GMN, we have developed a general form of orthogonality-equivariant functions, given that the dynamics of constrained systems are more complicated than the unconstrained counterparts. Theoretically, the proposed equivariant formulation is proved to be universally expressive under certain conditions. Extensive experiments support the advantages of GMN compared to the state-of-the-art GNNs in terms of prediction accuracy, constraint satisfaction and data efficiency on the simulated systems consisting of particles, sticks and hinges, as well as two real-world datasets for molecular dynamics prediction and human motion capture."
1038,https://arxiv.org/abs/2202.12796,Hybrid Robotic Grasping with a Soft Multimodal Gripper and a Deep Multistage Learning Scheme,"Grasping has long been considered an important and practical task in robotic manipulation. Yet achieving robust and efficient grasps of diverse objects is challenging, since it involves gripper design, perception, control and learning, etc. Recent learning-based approaches have shown excellent performance in grasping a variety of novel objects. However, these methods either are typically limited to one single grasping mode, or else more end effectors are needed to grasp various objects. In addition, gripper design and learning methods are commonly developed separately, which may not adequately explore the ability of a multimodal gripper. In this paper, we present a deep reinforcement learning (DRL) framework to achieve multistage hybrid robotic grasping with a new soft multimodal gripper. A soft gripper with three grasping modes (i.e., enveloping, sucking, and enveloping_then_sucking) can both deal with objects of different shapes and grasp more than one object simultaneously. We propose a novel hybrid grasping method integrated with the multimodal gripper to optimize the number of grasping actions. We evaluate the DRL framework under different scenarios (i.e., with different ratios of objects of two grasp types). The proposed algorithm is shown to reduce the number of grasping actions (i.e., enlarge the grasping efficiency, with maximum values of 161% in simulations and 154% in real-world experiments) compared to single grasping modes."
1039,https://arxiv.org/abs/2202.10910,Sound Adversarial Audio-Visual Navigation,"Audio-visual navigation task requires an agent to find a sound source in a realistic, unmapped 3D environment by utilizing egocentric audio-visual observations. Existing audio-visual navigation works assume a clean environment that solely contains the target sound, which, however, would not be suitable in most real-world applications due to the unexpected sound noise or intentional interference. In this work, we design an acoustically complex environment in which, besides the target sound, there exists a sound attacker playing a zero-sum game with the agent. More specifically, the attacker can move and change the volume and category of the sound to make the agent suffer from finding the sounding object while the agent tries to dodge the attack and navigate to the goal under the intervention. Under certain constraints to the attacker, we can improve the robustness of the agent towards unexpected sound attacks in audio-visual navigation. For better convergence, we develop a joint training mechanism by employing the property of a centralized critic with decentralized actors. Experiments on two real-world 3D scan datasets, Replica, and Matterport3D, verify the effectiveness and the robustness of the agent trained under our designed environment when transferred to the clean environment or the one containing sound attackers with random policy. Project: \url{https://yyf17.github.io/SAAVN}."
1040,https://arxiv.org/abs/2202.00448,Sim2Real Object-Centric Keypoint Detection and Description,"Keypoint detection and description play a central role in computer vision. Most existing methods are in the form of scene-level prediction, without returning the object classes of different keypoints. In this paper, we propose the object-centric formulation, which, beyond the conventional setting, requires further identifying which object each interest point belongs to. With such fine-grained information, our framework enables more downstream potentials, such as object-level matching and pose estimation in a clustered environment. To get around the difficulty of label collection in the real world, we develop a sim2real contrastive learning mechanism that can generalize the model trained in simulation to real-world applications. The novelties of our training method are three-fold: (i) we integrate the uncertainty into the learning framework to improve feature description of hard cases, e.g., less-textured or symmetric patches; (ii) we decouple the object descriptor into two output branches -- intra-object salience and inter-object distinctness, resulting in a better pixel-wise description; (iii) we enforce cross-view semantic consistency for enhanced robustness in representation learning. Comprehensive experiments on image matching and 6D pose estimation verify the encouraging generalization ability of our method from simulation to reality. Particularly for 6D pose estimation, our method significantly outperforms typical unsupervised/sim2real methods, achieving a closer gap with the fully supervised counterpart. Additional results and videos can be found at https://zhongcl-thu.github.io/rock/"
1041,https://arxiv.org/abs/2201.10788,Self-supervised 3D Semantic Representation Learning for Vision-and-Language Navigation,"In the Vision-and-Language Navigation task, the embodied agent follows linguistic instructions and navigates to a specific goal. It is important in many practical scenarios and has attracted extensive attention from both computer vision and robotics communities. However, most existing works only use RGB images but neglect the 3D semantic information of the scene. To this end, we develop a novel self-supervised training framework to encode the voxel-level 3D semantic reconstruction into a 3D semantic representation. Specifically, a region query task is designed as the pretext task, which predicts the presence or absence of objects of a particular class in a specific 3D region. Then, we construct an LSTM-based navigation model and train it with the proposed 3D semantic representations and BERT language features on vision-language pairs. Experiments show that the proposed approach achieves success rates of 68% and 66% on the validation unseen and test unseen splits of the R2R dataset respectively, which are superior to most of RGB-based methods utilizing vision-language transformers."
1042,https://arxiv.org/abs/2201.03812,Bootstrapping Informative Graph Augmentation via A Meta Learning Approach,"Recent works explore learning graph representations in a self-supervised manner. In graph contrastive learning, benchmark methods apply various graph augmentation approaches. However, most of the augmentation methods are non-learnable, which causes the issue of generating unbeneficial augmented graphs. Such augmentation may degenerate the representation ability of graph contrastive learning methods. Therefore, we motivate our method to generate augmented graph by a learnable graph augmenter, called MEta Graph Augmentation (MEGA). We then clarify that a ""good"" graph augmentation must have uniformity at the instance-level and informativeness at the feature-level. To this end, we propose a novel approach to learning a graph augmenter that can generate an augmentation with uniformity and informativeness. The objective of the graph augmenter is to promote our feature extraction network to learn a more discriminative feature representation, which motivates us to propose a meta-learning paradigm. Empirically, the experiments across multiple benchmark datasets demonstrate that MEGA outperforms the state-of-the-art methods in graph self-supervised learning tasks. Further experimental studies prove the effectiveness of different terms of MEGA."
1043,https://arxiv.org/abs/2112.02252,Channel Exchanging Networks for Multimodal and Multitask Dense Image Prediction,"Multimodal fusion and multitask learning are two vital topics in machine learning. Despite the fruitful progress, existing methods for both problems are still brittle to the same challenge -- it remains dilemmatic to integrate the common information across modalities (resp. tasks) meanwhile preserving the specific patterns of each modality (resp. task). Besides, while they are actually closely related to each other, multimodal fusion and multitask learning are rarely explored within the same methodological framework before. In this paper, we propose Channel-Exchanging-Network (CEN) which is self-adaptive, parameter-free, and more importantly, applicable for multimodal and multitask dense image prediction. At its core, CEN adaptively exchanges channels between subnetworks of different modalities. Specifically, the channel exchanging process is self-guided by individual channel importance that is measured by the magnitude of Batch-Normalization (BN) scaling factor during training. For the application of dense image prediction, the validity of CEN is tested by four different scenarios: multimodal fusion, cycle multimodal fusion, multitask learning, and multimodal multitask learning. Extensive experiments on semantic segmentation via RGB-D data and image translation through multi-domain input verify the effectiveness of CEN compared to state-of-the-art methods. Detailed ablation studies have also been carried out, which demonstrate the advantage of each component we propose. Our code is available at https://github.com/yikaiw/CEN."
1044,https://arxiv.org/abs/2110.09195,Sub-bit Neural Networks: Learning to Compress and Accelerate Binary Neural Networks,"In the low-bit quantization field, training Binary Neural Networks (BNNs) is the extreme solution to ease the deployment of deep models on resource-constrained devices, having the lowest storage cost and significantly cheaper bit-wise operations compared to 32-bit floating-point counterparts. In this paper, we introduce Sub-bit Neural Networks (SNNs), a new type of binary quantization design tailored to compress and accelerate BNNs. SNNs are inspired by an empirical observation, showing that binary kernels learnt at convolutional layers of a BNN model are likely to be distributed over kernel subsets. As a result, unlike existing methods that binarize weights one by one, SNNs are trained with a kernel-aware optimization framework, which exploits binary quantization in the fine-grained convolutional kernel space. Specifically, our method includes a random sampling step generating layer-specific subsets of the kernel space, and a refinement step learning to adjust these subsets of binary kernels via optimization. Experiments on visual recognition benchmarks and the hardware deployment on FPGA validate the great potentials of SNNs. For instance, on ImageNet, SNNs of ResNet-18/ResNet-34 with 0.56-bit weights achieve 3.13/3.33 times runtime speed-up and 1.8 times compression over conventional BNNs with moderate drops in recognition accuracy. Promising results are also obtained when applying SNNs to binarize both weights and activations. Our code is available at https://github.com/yikaiw/SNN."
1045,https://arxiv.org/abs/2109.10571,Audio-Visual Grounding Referring Expression for Robotic Manipulation,"Referring expressions are commonly used when referring to a specific target in people's daily dialogue. In this paper, we develop a novel task of audio-visual grounding referring expression for robotic manipulation. The robot leverages both the audio and visual information to understand the referring expression in the given manipulation instruction and the corresponding manipulations are implemented. To solve the proposed task, an audio-visual framework is proposed for visual localization and sound recognition. We have also established a dataset which contains visual data, auditory data and manipulation instructions for evaluation. Finally, extensive experiments are conducted both offline and online to verify the effectiveness of the proposed audio-visual framework. And it is demonstrated that the robot performs better with the audio-visual data than with only the visual data."
1046,https://arxiv.org/abs/2109.09531,Multi-Agent Embodied Visual Semantic Navigation with Scene Prior Knowledge,"In visual semantic navigation, the robot navigates to a target object with egocentric visual observations and the class label of the target is given. It is a meaningful task inspiring a surge of relevant research. However, most of the existing models are only effective for single-agent navigation, and a single agent has low efficiency and poor fault tolerance when completing more complicated tasks. Multi-agent collaboration can improve the efficiency and has strong application potentials. In this paper, we propose the multi-agent visual semantic navigation, in which multiple agents collaborate with others to find multiple target objects. It is a challenging task that requires agents to learn reasonable collaboration strategies to perform efficient exploration under the restrictions of communication bandwidth. We develop a hierarchical decision framework based on semantic mapping, scene prior knowledge, and communication mechanism to solve this task. The results of testing experiments in unseen scenes with both known objects and unknown objects illustrate the higher accuracy and efficiency of the proposed model compared with the single-agent model."
1047,https://arxiv.org/abs/2109.07872,Knowledge-based Embodied Question Answering,"In this paper, we propose a novel Knowledge-based Embodied Question Answering (K-EQA) task, in which the agent intelligently explores the environment to answer various questions with the knowledge. Different from explicitly specifying the target object in the question as existing EQA work, the agent can resort to external knowledge to understand more complicated question such as ""Please tell me what are objects used to cut food in the room?"", in which the agent must know the knowledge such as ""knife is used for cutting food"".
  To address this K-EQA problem, a novel framework based on neural program synthesis reasoning is proposed, where the joint reasoning of the external knowledge and 3D scene graph is performed to realize navigation and question answering. Especially, the 3D scene graph can provide the memory to store the visual information of visited scenes, which significantly improves the efficiency for the multi-turn question answering. Experimental results have demonstrated that the proposed framework is capable of answering more complicated and realistic questions in the embodied environment. The proposed method is also applicable to multi-agent scenarios."
1048,https://arxiv.org/abs/2108.05013,Elastic Tactile Simulation Towards Tactile-Visual Perception,"Tactile sensing plays an important role in robotic perception and manipulation tasks. To overcome the real-world limitations of data collection, simulating tactile response in a virtual environment comes as a desirable direction of robotic research. In this paper, we propose Elastic Interaction of Particles (EIP) for tactile simulation. Most existing works model the tactile sensor as a rigid multi-body, which is incapable of reflecting the elastic property of the tactile sensor as well as characterizing the fine-grained physical interaction between the two objects. By contrast, EIP models the tactile sensor as a group of coordinated particles, and the elastic property is applied to regulate the deformation of particles during contact. With the tactile simulation by EIP, we further propose a tactile-visual perception network that enables information fusion between tactile data and visual images. The perception network is based on a global-to-local fusion mechanism where multi-scale tactile features are aggregated to the corresponding local region of the visual modality with the guidance of tactile positions and directions. The fusion method exhibits superiority regarding the 3D geometric reconstruction task."
1049,https://arxiv.org/abs/2108.05009,Learning Deep Multimodal Feature Representation with Asymmetric Multi-layer Fusion,"We propose a compact and effective framework to fuse multimodal features at multiple layers in a single network. The framework consists of two innovative fusion schemes. Firstly, unlike existing multimodal methods that necessitate individual encoders for different modalities, we verify that multimodal features can be learnt within a shared single network by merely maintaining modality-specific batch normalization layers in the encoder, which also enables implicit fusion via joint feature representation learning. Secondly, we propose a bidirectional multi-layer fusion scheme, where multimodal features can be exploited progressively. To take advantage of such scheme, we introduce two asymmetric fusion operations including channel shuffle and pixel shift, which learn different fused features with respect to different fusion directions. These two operations are parameter-free and strengthen the multimodal feature interactions across channels as well as enhance the spatial feature discrimination within channels. We conduct extensive experiments on semantic segmentation and image translation tasks, based on three publicly available datasets covering diverse modalities. Results indicate that our proposed framework is general, compact and is superior to state-of-the-art fusion frameworks."
1050,https://arxiv.org/abs/2107.00511,TransSC: Transformer-based Shape Completion for Grasp Evaluation,"Currently, robotic grasping methods based on sparse partial point clouds have attained a great grasping performance on various objects while they often generate wrong grasping candidates due to the lack of geometric information on the object. In this work, we propose a novel and robust shape completion model (TransSC). This model has a transformer-based encoder to explore more point-wise features and a manifold-based decoder to exploit more object details using a partial point cloud as input.
  Quantitative experiments verify the effectiveness of the proposed shape completion network and demonstrate it outperforms existing methods. Besides, TransSC is integrated into a grasp evaluation network to generate a set of grasp candidates. The simulation experiment shows that TransSC improves the grasping generation result compared to the existing shape completion baselines. Furthermore, our robotic experiment shows that with TransSC the robot is more successful in grasping objects that are randomly placed on a support surface."
1051,https://arxiv.org/abs/2106.05530,Adversarial Option-Aware Hierarchical Imitation Learning,"It has been a challenge to learning skills for an agent from long-horizon unannotated demonstrations. Existing approaches like Hierarchical Imitation Learning(HIL) are prone to compounding errors or suboptimal solutions. In this paper, we propose Option-GAIL, a novel method to learn skills at long horizon. The key idea of Option-GAIL is modeling the task hierarchy by options and train the policy via generative adversarial optimization. In particular, we propose an Expectation-Maximization(EM)-style algorithm: an E-step that samples the options of expert conditioned on the current learned policy, and an M-step that updates the low- and high-level policies of agent simultaneously to minimize the newly proposed option-occupancy measurement between the expert and the agent. We theoretically prove the convergence of the proposed algorithm. Experiments show that Option-GAIL outperforms other counterparts consistently across a variety of tasks."
1052,https://arxiv.org/abs/2103.09693,A Robust Tube-Based Smooth-MPC for Robot Manipulator Planning,"Model Predictive Control (MPC) has shown the great performance of target optimization and constraint satisfaction. However, the heavy computation of the Optimal Control Problem (OCP) at each triggering instant brings the serious delay from state sampling to the control signals, which limits the applications of MPC in resource-limited robot manipulator systems over complicated tasks. In this paper, we propose a novel robust tube-based smooth-MPC strategy for nonlinear robot manipulator planning systems with disturbances and constraints. Based on piecewise linearization and state prediction, our control strategy improves the smoothness and optimizes the delay of the control process. By deducing the deviation of the real system states and the nominal system states, we can predict the next real state set at the current instant. And by using this state set as the initial condition, we can solve the next OCP ahead and store the optimal controls based on the nominal system states, which eliminates the delay. Furthermore, we linearize the nonlinear system with a given upper bound of error, reducing the complexity of the OCP and improving the response speed. Based on the theoretical framework of tube MPC, we prove that the control strategy is recursively feasible and closed-loop stable with the constraints and disturbances. Numerical simulations have verified the efficacy of the designed approach compared with the conventional MPC."
1053,https://arxiv.org/abs/2011.11528,Elastic Interaction of Particles for Robotic Tactile Simulation,"Tactile sensing plays an important role in robotic perception and manipulation. To overcome the real-world limitations of data collection, simulating tactile response in virtual environment comes as a desire direction of robotic research. Most existing works model the tactile sensor as a rigid multi-body, which is incapable of reflecting the elastic property of the tactile sensor as well as characterizing the fine-grained physical interaction between two objects. In this paper, we propose Elastic Interaction of Particles (EIP), a novel framework for tactile emulation. At its core, EIP models the tactile sensor as a group of coordinated particles, and the elastic theory is applied to regulate the deformation of particles during the contact process. The implementation of EIP is conducted from scratch, without resorting to any existing physics engine. Experiments to verify the effectiveness of our method have been carried out on two applications: robotic perception with tactile data and 3D geometric reconstruction by tactile-visual fusion. It is possible to open up a new vein for robotic tactile simulation, and contribute to various downstream robotic tasks."
1054,https://arxiv.org/abs/2011.08728,Fault-Aware Robust Control via Adversarial Reinforcement Learning,"Robots have limited adaptation ability compared to humans and animals in the case of damage. However, robot damages are prevalent in real-world applications, especially for robots deployed in extreme environments. The fragility of robots greatly limits their widespread application. We propose an adversarial reinforcement learning framework, which significantly increases robot robustness over joint damage cases in both manipulation tasks and locomotion tasks. The agent is trained iteratively under the joint damage cases where it has poor performance. We validate our algorithm on a three-fingered robot hand and a quadruped robot. Our algorithm can be trained only in simulation and directly deployed on a real robot without any fine-tuning. It also demonstrates exceeding success rates over arbitrary joint damage cases."
1055,https://arxiv.org/abs/2011.05005,Deep Multimodal Fusion by Channel Exchanging,"Deep multimodal fusion by using multiple sources of data for classification or regression has exhibited a clear advantage over the unimodal counterpart on various applications. Yet, current methods including aggregation-based and alignment-based fusion are still inadequate in balancing the trade-off between inter-modal fusion and intra-modal processing, incurring a bottleneck of performance improvement. To this end, this paper proposes Channel-Exchanging-Network (CEN), a parameter-free multimodal fusion framework that dynamically exchanges channels between sub-networks of different modalities. Specifically, the channel exchanging process is self-guided by individual channel importance that is measured by the magnitude of Batch-Normalization (BN) scaling factor during training. The validity of such exchanging process is also guaranteed by sharing convolutional filters yet keeping separate BN layers across modalities, which, as an add-on benefit, allows our multimodal architecture to be almost as compact as a unimodal network. Extensive experiments on semantic segmentation via RGB-D data and image translation through multi-domain input verify the effectiveness of our CEN compared to current state-of-the-art methods. Detailed ablation studies have also been carried out, which provably affirm the advantage of each component we propose. Our code is available at https://github.com/yikaiw/CEN."
1056,https://arxiv.org/abs/2011.03383,Adversarial Skill Learning for Robust Manipulation,"Deep reinforcement learning has made significant progress in robotic manipulation tasks and it works well in the ideal disturbance-free environment. However, in a real-world environment, both internal and external disturbances are inevitable, thus the performance of the trained policy will dramatically drop. To improve the robustness of the policy, we introduce the adversarial training mechanism to the robotic manipulation tasks in this paper, and an adversarial skill learning algorithm based on soft actor-critic (SAC) is proposed for robust manipulation. Extensive experiments are conducted to demonstrate that the learned policy is robust to internal and external disturbances. Additionally, the proposed algorithm is evaluated in both the simulation environment and on the real robotic platform."
1057,https://arxiv.org/abs/2010.11694,Unsupervised Representation Learning by InvariancePropagation,"Unsupervised learning methods based on contrastive learning have drawn increasing attention and achieved promising results. Most of them aim to learn representations invariant to instance-level variations, which are provided by different views of the same instance. In this paper, we propose Invariance Propagation to focus on learning representations invariant to category-level variations, which are provided by different instances from the same category. Our method recursively discovers semantically consistent samples residing in the same high-density regions in representation space. We demonstrate a hard sampling strategy to concentrate on maximizing the agreement between the anchor sample and its hard positive samples, which provide more intra-class variations to help capture more abstract invariance. As a result, with a ResNet-50 as the backbone, our method achieves 71.3% top-1 accuracy on ImageNet linear classification and 78.2% top-5 accuracy fine-tuning on only 1% labels, surpassing previous results. We also achieve state-of-the-art performance on other downstream tasks, including linear classification on Places205 and Pascal VOC, and transfer learning on small scale datasets."
1058,https://arxiv.org/abs/2008.09864,Tackling Over-Smoothing for General Graph Convolutional Networks,"Increasing the depth of GCN, which is expected to permit more expressivity, is shown to incur performance detriment especially on node classification. The main cause of this lies in over-smoothing. The over-smoothing issue drives the output of GCN towards a space that contains limited distinguished information among nodes, leading to poor expressivity. Several works on refining the architecture of deep GCN have been proposed, but it is still unknown in theory whether or not these refinements are able to relieve over-smoothing. In this paper, we first theoretically analyze how general GCNs act with the increase in depth, including generic GCN, GCN with bias, ResGCN, and APPNP. We find that all these models are characterized by a universal process: all nodes converging to a cuboid. Upon this theorem, we propose DropEdge to alleviate over-smoothing by randomly removing a certain number of edges at each training epoch. Theoretically, DropEdge either reduces the convergence speed of over-smoothing or relieves the information loss caused by dimension collapse. Experimental evaluations on simulated dataset have visualized the difference in over-smoothing between different GCNs. Moreover, extensive experiments on several real benchmarks support that DropEdge consistently improves the performance on a variety of both shallow and deep GCNs."
1059,https://arxiv.org/abs/2008.04872,A Boundary Based Out-of-Distribution Classifier for Generalized Zero-Shot Learning,"Generalized Zero-Shot Learning (GZSL) is a challenging topic that has promising prospects in many realistic scenarios. Using a gating mechanism that discriminates the unseen samples from the seen samples can decompose the GZSL problem to a conventional Zero-Shot Learning (ZSL) problem and a supervised classification problem. However, training the gate is usually challenging due to the lack of data in the unseen domain. To resolve this problem, in this paper, we propose a boundary based Out-of-Distribution (OOD) classifier which classifies the unseen and seen domains by only using seen samples for training. First, we learn a shared latent space on a unit hyper-sphere where the latent distributions of visual features and semantic attributes are aligned class-wisely. Then we find the boundary and the center of the manifold for each class. By leveraging the class centers and boundaries, the unseen samples can be separated from the seen samples. After that, we use two experts to classify the seen and unseen samples separately. We extensively validate our approach on five popular benchmark datasets including AWA1, AWA2, CUB, FLO and SUN. The experimental results demonstrate the advantages of our approach over state-of-the-art methods."
1060,https://arxiv.org/abs/2007.09558,Resolution Switchable Networks for Runtime Efficient Image Recognition,"We propose a general method to train a single convolutional neural network which is capable of switching image resolutions at inference. Thus the running speed can be selected to meet various computational resource limits. Networks trained with the proposed method are named Resolution Switchable Networks (RS-Nets). The basic training framework shares network parameters for handling images which differ in resolution, yet keeps separate batch normalization layers. Though it is parameter-efficient in design, it leads to inconsistent accuracy variations at different resolutions, for which we provide a detailed analysis from the aspect of the train-test recognition discrepancy. A multi-resolution ensemble distillation is further designed, where a teacher is learnt on the fly as a weighted ensemble over resolutions. Thanks to the ensemble and knowledge distillation, RS-Nets enjoy accuracy improvements at a wide range of resolutions compared with individually trained models. Extensive experiments on the ImageNet dataset are provided, and we additionally consider quantization problems. Code and models are available at https://github.com/yikaiw/RS-Nets."
1061,https://arxiv.org/abs/2004.14638,Towards Embodied Scene Description,"Embodiment is an important characteristic for all intelligent agents (creatures and robots), while existing scene description tasks mainly focus on analyzing images passively and the semantic understanding of the scenario is separated from the interaction between the agent and the environment. In this work, we propose the Embodied Scene Description, which exploits the embodiment ability of the agent to find an optimal viewpoint in its environment for scene description tasks. A learning framework with the paradigms of imitation learning and reinforcement learning is established to teach the intelligent agent to generate corresponding sensorimotor activities. The proposed framework is tested on both the AI2Thor dataset and a real world robotic platform demonstrating the effectiveness and extendability of the developed method."
1062,https://arxiv.org/abs/2003.05212,A Mobile Robot Hand-Arm Teleoperation System by Vision and IMU,"In this paper, we present a multimodal mobile teleoperation system that consists of a novel vision-based hand pose regression network (Transteleop) and an IMU-based arm tracking method. Transteleop observes the human hand through a low-cost depth camera and generates not only joint angles but also depth images of paired robot hand poses through an image-to-image translation process. A keypoint-based reconstruction loss explores the resemblance in appearance and anatomy between human and robotic hands and enriches the local features of reconstructed images. A wearable camera holder enables simultaneous hand-arm control and facilitates the mobility of the whole teleoperation system. Network evaluation results on a test dataset and a variety of complex manipulation tasks that go beyond simple pick-and-place operations show the efficiency and stability of our multimodal teleoperation system."
1063,https://arxiv.org/abs/2003.04641,MQA: Answering the Question via Robotic Manipulation,"In this paper, we propose a novel task, Manipulation Question Answering (MQA), where the robot performs manipulation actions to change the environment in order to answer a given question. To solve this problem, a framework consisting of a QA module and a manipulation module is proposed. For the QA module, we adopt the method for the Visual Question Answering (VQA) task. For the manipulation module, a Deep Q Network (DQN) model is designed to generate manipulation actions for the robot to interact with the environment. We consider the situation where the robot continuously manipulating objects inside a bin until the answer to the question is found. Besides, a novel dataset that contains a variety of object models, scenarios and corresponding question-answer pairs is established in a simulation environment. Extensive experiments have been conducted to validate the effectiveness of the proposed framework."
1064,https://arxiv.org/abs/2003.00839,Fabric Defect Detection Using Vision-Based Tactile Sensor,"This paper introduces a new type of system for fabric defect detection with the tactile inspection system. Different from existed visual inspection systems, the proposed system implements a vision-based tactile sensor. The tactile sensor, which mainly consists of a camera, four LEDs, and an elastic sensing layer, captures detailed information about fabric surface structure and ignores the color and pattern. Thus, the ambiguity between a defect and image background related to fabric color and pattern is avoided. To utilize the tactile sensor for fabric inspection, we employ intensity adjustment for image preprocessing, Residual Network with ensemble learning for detecting defects, and uniformity measurement for selecting ideal dataset for model training. An experiment is conducted to verify the performance of the proposed tactile system. The experimental results have demonstrated the feasibility of the proposed system, which performs well in detecting structural defects for various types of fabrics. In addition, the system does not require external light sources, which skips the process of setting up and tuning a lighting environment."
1065,https://arxiv.org/abs/2003.00342,Robust Robotic Pouring using Audition and Haptics,"Robust and accurate estimation of liquid height lies as an essential part of pouring tasks for service robots. However, vision-based methods often fail in occluded conditions while audio-based methods cannot work well in a noisy environment. We instead propose a multimodal pouring network (MP-Net) that is able to robustly predict liquid height by conditioning on both audition and haptics input. MP-Net is trained on a self-collected multimodal pouring dataset. This dataset contains 300 robot pouring recordings with audio and force/torque measurements for three types of target containers. We also augment the audio data by inserting robot noise. We evaluated MP-Net on our collected dataset and a wide variety of robot experiments. Both network training results and robot experiments demonstrate that MP-Net is robust against noise and changes to the task and environment. Moreover, we further combine the predicted height and force data to estimate the shape of the target container."
1066,https://arxiv.org/abs/2003.00273,Reusing Discriminators for Encoding: Towards Unsupervised Image-to-Image Translation,"Unsupervised image-to-image translation is a central task in computer vision. Current translation frameworks will abandon the discriminator once the training process is completed. This paper contends a novel role of the discriminator by reusing it for encoding the images of the target domain. The proposed architecture, termed as NICE-GAN, exhibits two advantageous patterns over previous approaches: First, it is more compact since no independent encoding component is required; Second, this plug-in encoder is directly trained by the adversary loss, making it more informative and trained more effectively if a multi-scale discriminator is applied. The main issue in NICE-GAN is the coupling of translation with discrimination along the encoder, which could incur training inconsistency when we play the min-max game via GAN. To tackle this issue, we develop a decoupled training strategy by which the encoder is only trained when maximizing the adversary loss while keeping frozen otherwise. Extensive experiments on four popular benchmarks demonstrate the superior performance of NICE-GAN over state-of-the-art methods in terms of FID, KID, and also human preference. Comprehensive ablation studies are also carried out to isolate the validity of each proposed component. Our codes are available at https://github.com/alpc91/NICE-GAN-pytorch."
1067,https://arxiv.org/abs/1911.07109,Reinforcement Learning from Imperfect Demonstrations under Soft Expert Guidance,"In this paper, we study Reinforcement Learning from Demonstrations (RLfD) that improves the exploration efficiency of Reinforcement Learning (RL) by providing expert demonstrations. Most of existing RLfD methods require demonstrations to be perfect and sufficient, which yet is unrealistic to meet in practice. To work on imperfect demonstrations, we first define an imperfect expert setting for RLfD in a formal way, and then point out that previous methods suffer from two issues in terms of optimality and convergence, respectively. Upon the theoretical findings we have derived, we tackle these two issues by regarding the expert guidance as a soft constraint on regulating the policy exploration of the agent, which eventually leads to a constrained optimization problem. We further demonstrate that such problem is able to be addressed efficiently by performing a local linear search on its dual form. Considerable empirical evaluations on a comprehensive collection of benchmarks indicate our method attains consistent improvement over other RLfD counterparts."
1068,https://arxiv.org/abs/1911.00886,Regularized Adversarial Sampling and Deep Time-aware Attention for Click-Through Rate Prediction,"Improving the performance of click-through rate (CTR) prediction remains one of the core tasks in online advertising systems. With the rise of deep learning, CTR prediction models with deep networks remarkably enhance model capacities. In deep CTR models, exploiting users' historical data is essential for learning users' behaviors and interests. As existing CTR prediction works neglect the importance of the temporal signals when embed users' historical clicking records, we propose a time-aware attention model which explicitly uses absolute temporal signals for expressing the users' periodic behaviors and relative temporal signals for expressing the temporal relation between items. Besides, we propose a regularized adversarial sampling strategy for negative sampling which eases the classification imbalance of CTR data and can make use of the strong guidance provided by the observed negative CTR samples. The adversarial sampling strategy significantly improves the training efficiency, and can be co-trained with the time-aware attention model seamlessly. Experiments are conducted on real-world CTR datasets from both in-station and out-station advertising places."
1069,https://arxiv.org/abs/1910.04417,Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement,"This paper studies Learning from Observations (LfO) for imitation learning with access to state-only demonstrations. In contrast to Learning from Demonstration (LfD) that involves both action and state supervision, LfO is more practical in leveraging previously inapplicable resources (e.g. videos), yet more challenging due to the incomplete expert guidance. In this paper, we investigate LfO and its difference with LfD in both theoretical and practical perspectives. We first prove that the gap between LfD and LfO actually lies in the disagreement of inverse dynamics models between the imitator and the expert, if following the modeling approach of GAIL. More importantly, the upper bound of this gap is revealed by a negative causal entropy which can be minimized in a model-free way. We term our method as Inverse-Dynamics-Disagreement-Minimization (IDDM) which enhances the conventional LfO method through further bridging the gap to LfD. Considerable empirical results on challenging benchmarks indicate that our method attains consistent improvements over other LfO counterparts."
1070,https://arxiv.org/abs/1909.07725,Deep Point-wise Prediction for Action Temporal Proposal,"Detecting actions in videos is an important yet challenging task. Previous works usually utilize (a) sliding window paradigms, or (b) per-frame action scoring and grouping to enumerate the possible temporal locations. Their performances are also limited to the designs of sliding windows or grouping strategies. In this paper, we present a simple and effective method for temporal action proposal generation, named Deep Point-wise Prediction (DPP). DPP simultaneously predicts the action existing possibility and the corresponding temporal locations, without the utilization of any handcrafted sliding window or grouping. The whole system is end-to-end trained with joint loss of temporal action proposal classification and location prediction. We conduct extensive experiments to verify its effectiveness, generality and robustness on standard THUMOS14 dataset. DPP runs more than 1000 frames per second, which largely satisfies the real-time requirement. The code is available at https://github.com/liluxuan1997/DPP."
1071,https://arxiv.org/abs/1904.11950,Attention-based Transfer Learning for Brain-computer Interface,"Different functional areas of the human brain play different roles in brain activity, which has not been paid sufficient research attention in the brain-computer interface (BCI) field. This paper presents a new approach for electroencephalography (EEG) classification that applies attention-based transfer learning. Our approach considers the importance of different brain functional areas to improve the accuracy of EEG classification, and provides an additional way to automatically identify brain functional areas associated with new activities without the involvement of a medical professional. We demonstrate empirically that our approach out-performs state-of-the-art approaches in the task of EEG classification, and the results of visualization indicate that our approach can detect brain functional areas related to a certain task."
1072,https://arxiv.org/abs/1904.03797,FoveaBox: Beyond Anchor-based Object Detector,"We present FoveaBox, an accurate, flexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predefined anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. In FoveaBox, an instance is assigned to adjacent feature levels to make the model more accurate.We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis. Without bells and whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO and Pascal VOC object detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters related to anchor boxes, which are often sensitive to the final detection performance. We believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection. The code has been made publicly available at https://github.com/taokong/FoveaBox ."
1073,https://arxiv.org/abs/1903.00650,Making Sense of Audio Vibration for Liquid Height Estimation in Robotic Pouring,"In this paper, we focus on the challenging perception problem in robotic pouring. Most of the existing approaches either leverage visual or haptic information. However, these techniques may suffer from poor generalization performances on opaque containers or concerning measuring precision. To tackle these drawbacks, we propose to make use of audio vibration sensing and design a deep neural network PouringNet to predict the liquid height from the audio fragment during the robotic pouring task. PouringNet is trained on our collected real-world pouring dataset with multimodal sensing data, which contains more than 3000 recordings of audio, force feedback, video and trajectory data of the human hand that performs the pouring task. Each record represents a complete pouring procedure. We conduct several evaluations on PouringNet with our dataset and robotic hardware. The results demonstrate that our PouringNet generalizes well across different liquid containers, positions of the audio receiver, initial liquid heights and types of liquid, and facilitates a more robust and accurate audio-based perception for robotic pouring."
1074,https://arxiv.org/abs/1901.06563,Consistent Optimization for Single-Shot Object Detection,"We present consistent optimization for single stage object detection. Previous works of single stage object detectors usually rely on the regular, dense sampled anchors to generate hypothesis for the optimization of the model. Through an examination of the behavior of the detector, we observe that the misalignment between the optimization target and inference configurations has hindered the performance improvement. We propose to bride this gap by consistent optimization, which is an extension of the traditional single stage detector's optimization strategy. Consistent optimization focuses on matching the training hypotheses and the inference quality by utilizing of the refined anchors during training. To evaluate its effectiveness, we conduct various design choices based on the state-of-the-art RetinaNet detector. We demonstrate it is the consistent optimization, not the architecture design, that yields the performance boosts. Consistent optimization is nearly cost-free, and achieves stable performance gains independent of the model capacities or input scales. Specifically, utilizing consistent optimization improves RetinaNet from 39.1 AP to 40.1 AP on COCO dataset without any bells or whistles, which surpasses the accuracy of all existing state-of-the-art one-stage detectors when adopting ResNet-101 as backbone. The code will be made available."
1075,https://arxiv.org/abs/1809.06268,Vision-based Teleoperation of Shadow Dexterous Hand using End-to-End Deep Neural Network,"In this paper, we present TeachNet, a novel neural network architecture for intuitive and markerless vision-based teleoperation of dexterous robotic hands. Robot joint angles are directly generated from depth images of the human hand that produce visually similar robot hand poses in an end-to-end fashion. The special structure of TeachNet, combined with a consistency loss function, handles the differences in appearance and anatomy between human and robotic hands. A synchronized human-robot training set is generated from an existing dataset of labeled depth images of the human hand and simulated depth images of a robotic hand. The final training set includes 400K pairwise depth images and joint angles of a Shadow C6 robotic hand. The network evaluation results verify the superiority of TeachNet, especially regarding the high-precision condition. Imitation experiments and grasp tasks teleoperated by novice users demonstrate that TeachNet is more reliable and faster than the state-of-the-art vision-based teleoperation method."
1076,https://arxiv.org/abs/1809.06267,PointNetGPD: Detecting Grasp Configurations from Point Sets,"In this paper, we propose an end-to-end grasp evaluation model to address the challenging problem of localizing robot grasp configurations directly from the point cloud. Compared to recent grasp evaluation metrics that are based on handcrafted depth features and a convolutional neural network (CNN), our proposed PointNetGPD is lightweight and can directly process the 3D point cloud that locates within the gripper for grasp evaluation. Taking the raw point cloud as input, our proposed grasp evaluation network can capture the complex geometric structure of the contact area between the gripper and the object even if the point cloud is very sparse. To further improve our proposed model, we generate a larger-scale grasp dataset with 350k real point cloud and grasps with the YCB object set for training. The performance of the proposed model is quantitatively measured both in simulation and on robotic hardware. Experiments on object grasping and clutter removal show that our proposed model generalizes well to novel objects and outperforms state-of-the-art methods. Code and video are available at \href{https://lianghongzhuo.github.io/PointNetGPD}{https://lianghongzhuo.github.io/PointNetGPD}"
1077,https://arxiv.org/abs/1808.07993,Deep Feature Pyramid Reconfiguration for Object Detection,"State-of-the-art object detectors usually learn multi-scale representations to get better results by employing feature pyramids. However, the current designs for feature pyramids are still inefficient to integrate the semantic information over different scales. In this paper, we begin by investigating current feature pyramids solutions, and then reformulate the feature pyramid construction as the feature reconfiguration process. Finally, we propose a novel reconfiguration architecture to combine low-level representations with high-level semantic features in a highly-nonlinear yet efficient way. In particular, our architecture which consists of global attention and local reconfigurations, is able to gather task-oriented features across different spatial locations and scales, globally and locally. Both the global attention and local reconfiguration are lightweight, in-place, and end-to-end trainable. Using this method in the basic SSD system, our models achieve consistent and significant boosts compared with the original model and its other variations, without losing real-time processing speed."
1078,https://arxiv.org/abs/1808.04443,Spatial and Spectral Features Fusion for EEG Classification during Motor Imagery in BCI,"Brain computer interface (BCI) is the only way for some special patients to communicate with the outside world and provide a direct control channel between brain and the external devices. As a non-invasive interface, the scalp electroencephalography (EEG) has a significant potential to be a major input signal for future BCI systems. Traditional methods only focus on a particular feature in the EEG signal, which limits the practical applications of EEG-based BCI. In this paper, we propose a algorithm for EEG classification with the ability to fuse multiple features. First, use the common spatial pattern (CSP) as the spatial feature and use wavelet coefficient as the spectral feature. Second, fuse these features with a fusion algorithm in orchestrate way to improve the accuracy of classification. Our algorithms are applied to the dataset IVa from BCI complete \uppercase\expandafter{\romannumeral3}. By analyzing the experimental results, it is possible to conclude that we can speculate that our algorithm perform better than traditional methods."
1079,https://arxiv.org/abs/1808.01974,A Survey on Deep Transfer Learning,"As a new classification platform, deep learning has recently received increasing attention from researchers and has been successfully applied to many domains. In some domains, like bioinformatics and robotics, it is very difficult to construct a large-scale well-annotated dataset due to the expense of data acquisition and costly annotation, which limits its development. Transfer learning relaxes the hypothesis that the training data must be independent and identically distributed (i.i.d.) with the test data, which motivates us to use transfer learning to solve the problem of insufficient training data. This survey focuses on reviewing the current researches of transfer learning by using deep neural network and its applications. We defined deep transfer learning, category and review the recent research works based on the techniques used in deep transfer learning."
1080,https://arxiv.org/abs/1808.01752,Deep Transfer Learning for EEG-based Brain Computer Interface,"The electroencephalography classifier is the most important component of brain-computer interface based systems. There are two major problems hindering the improvement of it. First, traditional methods do not fully exploit multimodal information. Second, large-scale annotated EEG datasets are almost impossible to acquire because biological data acquisition is challenging and quality annotation is costly. Herein, we propose a novel deep transfer learning approach to solve these two problems. First, we model cognitive events based on EEG data by characterizing the data using EEG optical flow, which is designed to preserve multimodal EEG information in a uniform representation. Second, we design a deep transfer learning framework which is suitable for transferring knowledge by joint training, which contains a adversarial network and a special loss function. The experiments demonstrate that our approach, when applied to EEG classification tasks, has many advantages, such as robustness and accuracy."
1081,https://arxiv.org/abs/1807.10641,Multimodal Classification with Deep Convolutional-Recurrent Neural Networks for Electroencephalography,"Electroencephalography (EEG) has become the most significant input signal for brain computer interface (BCI) based systems. However, it is very difficult to obtain satisfactory classification accuracy due to traditional methods can not fully exploit multimodal information. Herein, we propose a novel approach to modeling cognitive events from EEG data by reducing it to a video classification problem, which is designed to preserve the multimodal information of EEG. In addition, optical flow is introduced to represent the variant information of EEG. We train a deep neural network (DNN) with convolutional neural network (CNN) and recurrent neural network (RNN) for the EEG classification task by using EEG video and optical flow. The experiments demonstrate that our approach has many advantages, such as more robustness and more accuracy in EEG classification tasks. According to our approach, we designed a mixed BCI-based rehabilitation support system to help stroke patients perform some basic operations."
1082,https://arxiv.org/abs/1805.07252,Learning and Inferring Movement with Deep Generative Model,"Learning and inference movement is a very challenging problem due to its high dimensionality and dependency to varied environments or tasks. In this paper, we propose an effective probabilistic method for learning and inference of basic movements. The motion planning problem is formulated as learning on a directed graphic model and deep generative model is used to perform learning and inference from demonstrations. An important characteristic of this method is that it flexibly incorporates the task descriptors and context information for long-term planning and it can be combined with dynamic systems for robot control. The experimental validations on robotic approaching path planning tasks show the advantages over the base methods with limited training data."
1083,https://arxiv.org/abs/1805.04686,Task Transfer by Preference-Based Cost Learning,"The goal of task transfer in reinforcement learning is migrating the action policy of an agent to the target task from the source task. Given their successes on robotic action planning, current methods mostly rely on two requirements: exactly-relevant expert demonstrations or the explicitly-coded cost function on target task, both of which, however, are inconvenient to obtain in practice. In this paper, we relax these two strong conditions by developing a novel task transfer framework where the expert preference is applied as a guidance. In particular, we alternate the following two steps: Firstly, letting experts apply pre-defined preference rules to select related expert demonstrates for the target task. Secondly, based on the selection result, we learn the target cost function and trajectory distribution simultaneously via enhanced Adversarial MaxEnt IRL and generate more trajectories by the learned target distribution for the next preference selection. The theoretical analysis on the distribution learning and convergence of the proposed algorithm are provided. Extensive simulations on several benchmarks have been conducted for further verifying the effectiveness of the proposed method."
1084,https://arxiv.org/abs/1707.01691,RON: Reverse Connection with Objectness Prior Networks for Object Detection,"We present RON, an efficient and effective framework for generic object detection. Our motivation is to smartly associate the best of the region-based (e.g., Faster R-CNN) and region-free (e.g., SSD) methodologies. Under fully convolutional architecture, RON mainly focuses on two fundamental problems: (a) multi-scale object localization and (b) negative sample mining. To address (a), we design the reverse connection, which enables the network to detect objects on multi-levels of CNNs. To deal with (b), we propose the objectness prior to significantly reduce the searching space of objects. We optimize the reverse connection, objectness prior and object detector jointly by a multi-task loss function, thus RON can directly predict final detection results from all locations of various feature maps. Extensive experiments on the challenging PASCAL VOC 2007, PASCAL VOC 2012 and MS COCO benchmarks demonstrate the competitive performance of RON. Specifically, with VGG-16 and low resolution 384X384 input size, the network gets 81.3% mAP on PASCAL VOC 2007, 80.7% mAP on PASCAL VOC 2012 datasets. Its superiority increases when datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset. With 1.5G GPU memory at test phase, the speed of the network is 15 FPS, 3X faster than the Faster R-CNN counterpart."
1085,https://arxiv.org/abs/1702.05658,MAT: A Multimodal Attentive Translator for Image Captioning,"In this work we formulate the problem of image captioning as a multimodal translation task. Analogous to machine translation, we present a sequence-to-sequence recurrent neural networks (RNN) model for image caption generation. Different from most existing work where the whole image is represented by convolutional neural network (CNN) feature, we propose to represent the input image as a sequence of detected objects which feeds as the source sequence of the RNN model. In this way, the sequential representation of an image can be naturally translated to a sequence of words, as the target sequence of the RNN model. To represent the image in a sequential way, we extract the objects features in the image and arrange them in a order using convolutional neural networks. To further leverage the visual information from the encoded objects, a sequential attention layer is introduced to selectively attend to the objects that are related to generate corresponding words in the sentences. Extensive experiments are conducted to validate the proposed approach on popular benchmark dataset, i.e., MS COCO, and the proposed model surpasses the state-of-the-art methods in all metrics following the dataset splits of previous work. The proposed approach is also evaluated by the evaluation server of MS COCO captioning challenge, and achieves very competitive results, e.g., a CIDEr of 1.029 (c5) and 1.064 (c40)."
1086,https://arxiv.org/abs/1608.01059,Analyzing Linear Dynamical Systems: From Modeling to Coding and Learning,"Encoding time-series with Linear Dynamical Systems (LDSs) leads to rich models with applications ranging from dynamical texture recognition to video segmentation to name a few. In this paper, we propose to represent LDSs with infinite-dimensional subspaces and derive an analytic solution to obtain stable LDSs. We then devise efficient algorithms to perform sparse coding and dictionary learning on the space of infinite-dimensional subspaces. In particular, two solutions are developed to sparsely encode an LDS. In the first method, we map the subspaces into a Reproducing Kernel Hilbert Space (RKHS) and achieve our goal through kernel sparse coding. As for the second solution, we propose to embed the infinite-dimensional subspaces into the space of symmetric matrices and formulate the sparse coding accordingly in the induced space. For dictionary learning, we encode time-series by introducing a novel concept, namely the two-fold LDSs. We then make use of the two-fold LDSs to derive an analytical form for updating atoms of an LDS dictionary, i.e., each atom is an LDS itself. Compared to several baselines and state-of-the-art methods, the proposed methods yield higher accuracies in various classification tasks including video classification and tactile recognition."
1087,https://arxiv.org/abs/1604.00600,HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection,"Almost all of the current top-performing object detection networks employ region proposals to guide the search for object instances. State-of-the-art region proposal methods usually need several thousand proposals to get high recall, thus hurting the detection efficiency. Although the latest Region Proposal Network method gets promising detection accuracy with several hundred proposals, it still struggles in small-size object detection and precise localization (e.g., large IoU thresholds), mainly due to the coarseness of its feature maps. In this paper, we present a deep hierarchical network, namely HyperNet, for handling region proposal generation and object detection jointly. Our HyperNet is primarily based on an elaborately designed Hyper Feature which aggregates hierarchical feature maps first and then compresses them into a uniform space. The Hyper Features well incorporate deep but highly semantic, intermediate but really complementary, and shallow but naturally high-resolution features of the image, thus enabling us to construct HyperNet by sharing them both in generating proposals and detecting objects via an end-to-end joint training strategy. For the deep VGG16 model, our method achieves completely leading recall and state-of-the-art object detection accuracy on PASCAL VOC 2007 and 2012 using only 100 proposals per image. It runs with a speed of 5 fps (including all steps) on a GPU, thus having the potential for real-time processing."
1088,https://arxiv.org/abs/1403.1016,High-order S-Lemma with application to stability of a class of switched nonlinear systems,"This paper extends some results on the S-Lemma proposed by Yakubovich and uses the improved results to investigate the asymptotic stability of a class of switched nonlinear systems.
  Firstly, the strict S-Lemma is extended from quadratic forms to homogeneous functions with respect to any dilation, where the improved S-Lemma is named the strict homogeneous S-Lemma (the SHS-Lemma for short). In detail, this paper indicates that the strict S-Lemma does not necessarily hold for homogeneous functions that are not quadratic forms, and proposes a necessary and sufficient condition under which the SHS-Lemma holds.
  It is well known that a switched linear system with two sub-systems admits a Lyapunov function with homogeneous derivative (LFHD for short), if and only if it has a convex combination of the vector fields of its two sub-systems that admits a LFHD. In this paper, it is shown that this conclusion does not necessarily hold for a general switched nonlinear system with two sub-systems, and gives a necessary and sufficient condition under which the conclusion holds for a general switched nonlinear system with two sub-systems. It is also shown that for a switched nonlinear system with three or more sub-systems, the ""if"" part holds, but the ""only if"" part may not.
  At last, the S-Lemma is extended from quadratic polynomials to polynomials of degree more than $2$ under some mild conditions, and the improved results are called the homogeneous S-Lemma (the HS-Lemma for short) and the non-homogeneous S-Lemma (the NHS-Lemma for short), respectively.
  Besides, some examples and counterexamples are given to illustrate the main results."
1089,https://arxiv.org/abs/2105.10484,A General Method For Automatic Discovery of Powerful Interactions In Click-Through Rate Prediction,"Modeling powerful interactions is a critical challenge in Click-through rate (CTR) prediction, which is one of the most typical machine learning tasks in personalized advertising and recommender systems. Although developing hand-crafted interactions is effective for a small number of datasets, it generally requires laborious and tedious architecture engineering for extensive scenarios. In recent years, several neural architecture search (NAS) methods have been proposed for designing interactions automatically. However, existing methods only explore limited types and connections of operators for interaction generation, leading to low generalization ability. To address these problems, we propose a more general automated method for building powerful interactions named AutoPI. The main contributions of this paper are as follows: AutoPI adopts a more general search space in which the computational graph is generalized from existing network connections, and the interactive operators in the edges of the graph are extracted from representative hand-crafted works. It allows searching for various powerful feature interactions to produce higher AUC and lower Logloss in a wide variety of applications. Besides, AutoPI utilizes a gradient-based search strategy for exploration with a significantly low computational cost. Experimentally, we evaluate AutoPI on a diverse suite of benchmark datasets, demonstrating the generalizability and efficiency of AutoPI over hand-crafted architectures and state-of-the-art NAS algorithms."
1090,https://arxiv.org/abs/2105.02409,Multimedia Edge Computing,"In this paper, we investigate the recent studies on multimedia edge computing, from sensing not only traditional visual/audio data but also individuals' geographical preference and mobility behaviors, to performing distributed machine learning over such data using the joint edge and cloud infrastructure and using evolutional strategies like reinforcement learning and online learning at edge devices to optimize the quality of experience for multimedia services at the last mile proactively. We provide both a retrospective view of recent rapid migration (resp. merge) of cloud multimedia to (resp. and) edge-aware multimedia and insights on the fundamental guidelines for designing multimedia edge computing strategies that target satisfying the changing demand of quality of experience. By showing the recent research studies and industrial solutions, we also provide future directions towards high-quality multimedia services over edge computing."
1091,https://arxiv.org/abs/2005.12788,Self-play Reinforcement Learning for Video Transmission,"Video transmission services adopt adaptive algorithms to ensure users' demands. Existing techniques are often optimized and evaluated by a function that linearly combines several weighted metrics. Nevertheless, we observe that the given function fails to describe the requirement accurately. Thus, such proposed methods might eventually violate the original needs. To eliminate this concern, we propose \emph{Zwei}, a self-play reinforcement learning algorithm for video transmission tasks. Zwei aims to update the policy by straightforwardly utilizing the actual requirement. Technically, Zwei samples a number of trajectories from the same starting point and instantly estimates the win rate w.r.t the competition outcome. Here the competition result represents which trajectory is closer to the assigned requirement. Subsequently, Zwei optimizes the strategy by maximizing the win rate. To build Zwei, we develop simulation environments, design adequate neural network models, and invent training methods for dealing with different requirements on various video transmission scenarios. Trace-driven analysis over two representative tasks demonstrates that Zwei optimizes itself according to the assigned requirement faithfully, outperforming the state-of-the-art methods under all considered scenarios."
1092,https://arxiv.org/abs/2005.12657,Continual Local Training for Better Initialization of Federated Models,"Federated learning (FL) refers to the learning paradigm that trains machine learning models directly in the decentralized systems consisting of smart edge devices without transmitting the raw data, which avoids the heavy communication costs and privacy concerns. Given the typical heterogeneous data distributions in such situations, the popular FL algorithm \emph{Federated Averaging} (FedAvg) suffers from weight divergence and thus cannot achieve a competitive performance for the global model (denoted as the \emph{initial performance} in FL) compared to centralized methods. In this paper, we propose the local continual training strategy to address this problem. Importance weights are evaluated on a small proxy dataset on the central server and then used to constrain the local training. With this additional term, we alleviate the weight divergence and continually integrate the knowledge on different local clients into the global model, which ensures a better generalization ability. Experiments on various FL settings demonstrate that our method significantly improves the initial performance of federated models with few extra communication costs."
1093,https://arxiv.org/abs/1910.10986,Adversarial Feature Alignment: Avoid Catastrophic Forgetting in Incremental Task Lifelong Learning,"Human beings are able to master a variety of knowledge and skills with ongoing learning. By contrast, dramatic performance degradation is observed when new tasks are added to an existing neural network model. This phenomenon, termed as \emph{Catastrophic Forgetting}, is one of the major roadblocks that prevent deep neural networks from achieving human-level artificial intelligence. Several research efforts, e.g. \emph{Lifelong} or \emph{Continual} learning algorithms, have been proposed to tackle this problem. However, they either suffer from an accumulating drop in performance as the task sequence grows longer, or require to store an excessive amount of model parameters for historical memory, or cannot obtain competitive performance on the new tasks. In this paper, we focus on the incremental multi-task image classification scenario. Inspired by the learning process of human students, where they usually decompose complex tasks into easier goals, we propose an adversarial feature alignment method to avoid catastrophic forgetting. In our design, both the low-level visual features and high-level semantic features serve as soft targets and guide the training process in multiple stages, which provide sufficient supervised information of the old tasks and help to reduce forgetting. Due to the knowledge distillation and regularization phenomenons, the proposed method gains even better performance than finetuning on the new tasks, which makes it stand out from other methods. Extensive experiments in several typical lifelong learning scenarios demonstrate that our method outperforms the state-of-the-art methods in both accuracies on new tasks and performance preservation on old tasks."
1094,https://arxiv.org/abs/1910.08234,Federated Learning with Unbiased Gradient Aggregation and Controllable Meta Updating,"Federated learning (FL) aims to train machine learning models in the decentralized system consisting of an enormous amount of smart edge devices. Federated averaging (FedAvg), the fundamental algorithm in FL settings, proposes on-device training and model aggregation to avoid the potential heavy communication costs and privacy concerns brought by transmitting raw data. However, through theoretical analysis we argue that 1) the multiple steps of local updating will result in gradient biases and 2) there is an inconsistency between the expected target distribution and the optimization objectives following the training paradigm in FedAvg. To tackle these problems, we first propose an unbiased gradient aggregation algorithm with the keep-trace gradient descent and the gradient evaluation strategy. Then we introduce an additional controllable meta updating procedure with a small set of data samples, indicating the expected target distribution, to provide a clear and consistent optimization objective. Both the two improvements are model- and task-agnostic and can be applied individually or together. Experimental results demonstrate that the proposed methods are faster in convergence and achieve higher accuracy with different network architectures in various FL settings."
1095,https://arxiv.org/abs/1908.05891,Federated Learning with Additional Mechanisms on Clients to Reduce Communication Costs,"Federated learning (FL) enables on-device training over distributed networks consisting of a massive amount of modern smart devices, such as smartphones and IoT (Internet of Things) devices. However, the leading optimization algorithm in such settings, i.e., federated averaging (FedAvg), suffers from heavy communication costs and the inevitable performance drop, especially when the local data is distributed in a non-IID way. To alleviate this problem, we propose two potential solutions by introducing additional mechanisms to the on-device training.
  The first (FedMMD) is adopting a two-stream model with the MMD (Maximum Mean Discrepancy) constraint instead of a single model in vanilla FedAvg to be trained on devices. Experiments show that the proposed method outperforms baselines, especially in non-IID FL settings, with a reduction of more than 20% in required communication rounds.
  The second is FL with feature fusion (FedFusion). By aggregating the features from both the local and global models, we achieve higher accuracy at fewer communication costs. Furthermore, the feature fusion modules offer better initialization for newly incoming clients and thus speed up the process of convergence. Experiments in popular FL scenarios show that our FedFusion outperforms baselines in both accuracy and generalization ability while reducing the number of required communication rounds by more than 60%."
1096,https://arxiv.org/abs/1908.02270,Comyco: Quality-Aware Adaptive Video Streaming via Imitation Learning,"Learning-based Adaptive Bit Rate~(ABR) method, aiming to learn outstanding strategies without any presumptions, has become one of the research hotspots for adaptive streaming. However, it typically suffers from several issues, i.e., low sample efficiency and lack of awareness of the video quality information. In this paper, we propose Comyco, a video quality-aware ABR approach that enormously improves the learning-based methods by tackling the above issues. Comyco trains the policy via imitating expert trajectories given by the instant solver, which can not only avoid redundant exploration but also make better use of the collected samples. Meanwhile, Comyco attempts to pick the chunk with higher perceptual video qualities rather than video bitrates. To achieve this, we construct Comyco's neural network architecture, video datasets and QoE metrics with video quality features. Using trace-driven and real-world experiments, we demonstrate significant improvements of Comyco's sample efficiency in comparison to prior work, with 1700x improvements in terms of the number of samples required and 16x improvements on training time required. Moreover, results illustrate that Comyco outperforms previously proposed methods, with the improvements on average QoE of 7.5% - 16.79%. Especially, Comyco also surpasses state-of-the-art approach Pensieve by 7.37% on average video quality under the same rebuffering time."
1097,https://arxiv.org/abs/1905.06650,Reactive Video Caching via long-short-term fusion approach,"Video caching has been a basic network functionality in today's network architectures. Although the abundance of caching replacement algorithms has been proposed recently, these methods all suffer from a key limitation: due to their immature rules, inaccurate feature engineering or unresponsive model update, they cannot strike a balance between the long-term history and short-term sudden events. To address this concern, we propose LA-E2, a long-short-term fusion caching replacement approach, which is based on a learning-aided exploration-exploitation process. Specifically, by effectively combining the deep neural network (DNN) based prediction with the online exploitation-exploration process through a \emph{top-k} method, LA-E2 can both make use of the historical information and adapt to the constantly changing popularity responsively. Through the extensive experiments in two real-world datasets, we show that LA-E2 can achieve state-of-the-art performance and generalize well. Especially when the cache size is small, our approach can outperform the baselines by 17.5\%-68.7\% higher in total hit rate."
1098,https://arxiv.org/abs/1811.06166,Tiyuntsong: A Self-Play Reinforcement Learning Approach for ABR Video Streaming,"Existing reinforcement learning~(RL)-based adaptive bitrate~(ABR) approaches outperform the previous fixed control rules based methods by improving the Quality of Experience~(QoE) score, as the QoE metric can hardly provide clear guidance for optimization, finally resulting in the unexpected strategies. In this paper, we propose \emph{Tiyuntsong}, a self-play reinforcement learning approach with generative adversarial network~(GAN)-based method for ABR video streaming. Tiyuntsong learns strategies automatically by training two agents who are competing against each other. Note that the competition results are determined by a set of rules rather than a numerical QoE score that allows clearer optimization objectives. Meanwhile, we propose GAN Enhancement Module to extract hidden features from the past status for preserving the information without the limitations of sequence lengths. Using testbed experiments, we show that the utilization of GAN significantly improves the Tiyuntsong's performance. By comparing the performance of ABRs, we observe that Tiyuntsong also betters existing ABR algorithms in the underlying metrics."
1099,https://arxiv.org/abs/1805.09249,Multi-User Cooperative Mobile Video Streaming: Performance Analysis and Online Mechanism Design,"Adaptive bitrate streaming enables video users to adapt their playing bitrates to the real-time network conditions, hence achieving the desirable quality-of-experience (QoE). In a multi-user wireless scenario, however, existing single-user based bitrate adaptation methods may fail to provide the desirable QoE, due to lack of consideration of multi-user interactions (such as the multi-user interferences and network congestion). In this work, we propose a novel user cooperation framework based on user-provided networking for multi-user mobile video streaming over wireless cellular networks. The framework enables nearby mobile video users to crowdsource their cellular links and resources for cooperative video streaming. We first analyze the social welfare performance bound of the proposed cooperative streaming system by introducing a virtual time-slotted system. Then, we design a low complexity Lyapunov-based online algorithm, which can be implemented in an online and distributed manner without the complete future and global network information. Numerical results show that the proposed online algorithm achieves an average 97% of the theoretical maximum social welfare. We further conduct experiments with real data traces, to compare our proposed online algorithm with the existing online algorithms in the literature. Experiment results show that our algorithm outperforms the existing algorithms in terms of both the achievable bitrate (with an average gain of 20% - 30%) and social welfare (with an average gain of 10% - 50%)."
1100,https://arxiv.org/abs/1805.08008,Performance Bound Analysis for Crowdsourced Mobile Video Streaming,"Adaptive bitrate (ABR) streaming enables video users to adapt the playing bitrate to the real-time network conditions to achieve the desirable quality of experience (QoE). In this work, we propose a novel crowdsourced streaming framework for multi-user ABR video streaming over wireless networks. This framework enables the nearby mobile video users to crowdsource their radio links and resources for cooperative video streaming. We focus on analyzing the social welfare performance bound of the proposed crowdsourced streaming system. Directly solving this bound is challenging due to the asynchronous operations of users. To this end, we introduce a virtual time-slotted system with the synchronized operations, and formulate the associated social welfare optimization problem as a linear programming. We show that the optimal social welfare performance of the virtual system provides effective upper-bound and lower-bound for the optimal performance (bound) of the original asynchronous system, hence characterizes the feasible performance region of the proposed crowdsourced streaming system. The performance bounds derived in this work can serve as a benchmark for the future online algorithm design and incentive mechanism design."
1101,https://arxiv.org/abs/1805.02482,QARC: Video Quality Aware Rate Control for Real-Time Video Streaming via Deep Reinforcement Learning,"Due to the fluctuation of throughput under various network conditions, how to choose a proper bitrate adaptively for real-time video streaming has become an upcoming and interesting issue. Recent work focuses on providing high video bitrates instead of video qualities. Nevertheless, we notice that there exists a trade-off between sending bitrate and video quality, which motivates us to focus on how to get a balance between them. In this paper, we propose QARC (video Quality Awareness Rate Control), a rate control algorithm that aims to have a higher perceptual video quality with possibly lower sending rate and transmission latency. Starting from scratch, QARC uses deep reinforcement learning(DRL) algorithm to train a neural network to select future bitrates based on previously observed network status and past video frames, and we design a neural network to predict future perceptual video quality as a vector for taking the place of the raw picture in the DRL's inputs. We evaluate QARC over a trace-driven emulation. As excepted, QARC betters existing approaches."
1102,https://arxiv.org/abs/1805.00619,Delay-Constrained Rate Control for Real-Time Video Streaming with Bounded Neural Network,"Rate control is widely adopted during video streaming to provide both high video qualities and low latency under various network conditions. However, despite that many work have been proposed, they fail to tackle one major problem: previous methods determine a future transmission rate as a single for value which will be used in an entire time-slot, while real-world network conditions, unlike lab setup, often suffer from rapid and stochastic changes, resulting in the failures of predictions.
  In this paper, we propose a delay-constrained rate control approach based on end-to-end deep learning. The proposed model predicts future bit rate not as a single value, but as possible bit rate ranges using target delay gradient, with which the transmission delay is guaranteed. We collect a large scale of real-world live streaming data to train our model, and as a result, it automatically learns the correlation between throughput and target delay gradient. We build a testbed to evaluate our approach. Compared with the state-of-the-art methods, our approach demonstrates a better performance in bandwidth utilization. In all considered scenarios, a range based rate control approach outperforms the one without range by 19% to 35% in average QoE improvement."
1103,https://arxiv.org/abs/1709.00273,When Data Sponsoring Meets Edge Caching: A Game-Theoretic Analysis,"Data sponsoring is a widely-used incentive method in today's cellular networks, where video content providers (CPs) cover part or all of the cellular data cost for mobile users so as to attract more video users and increase data traffic. In the forthcoming 5G cellular networks, edge caching is emerging as a promising technique to deliver videos with lower cost and higher quality. The key idea is to cache video contents on edge networks (e.g., femtocells and WiFi access points) in advance and deliver the cached contents to local video users directly (without involving cellular data cost for users). In this work, we aim to study how the edge caching will affect the CP's data sponsoring strategy as well as the users' behaviors and the data market. Specifically, we consider a single CP who offers both the edge caching service and the data sponsoring service to a set of heterogeneous mobile video users (with different mobility and video request patterns). We formulate the interactions of the CP and the users as a two-stage Stackelberg game, where the CP (leader) determines the budgets (efforts) for both services in Stage I, and the users (followers) decide whether and which service(s) they would like to subscribe to. We analyze the sub-game perfect equilibrium (SPE) of the proposed game systematically. Our analysis and experimental results show that by introducing the edge caching, the CP can increase his revenue by 105%."
1104,https://arxiv.org/abs/1703.06648,Multi-Dimensional Auction Mechanisms for Crowdsourced Mobile Video Streaming,"Crowdsourced mobile video streaming enables nearby mobile video users to aggregate network resources to improve their video streaming performances. However, users are often selfish and may not be willing to cooperate without proper incentives. Designing an incentive mechanism for such a scenario is challenging due to the users' asynchronous downloading behaviors and their private valuations for multi-bitrate coded videos. In this work, we propose both single-object and multi-object multi-dimensional auction mechanisms, through which users sell the opportunities for downloading single and multiple video segments with multiple bitrates, respectively. Both auction mechanisms can achieves truthfulness (i.e, truthful private information revelation) and efficiency (i.e., social welfare maximization). Simulations with real traces show that crowdsourced mobile streaming facilitated by the auction mechanisms outperforms noncooperative stream ing by 48.6% (on average) in terms of social welfare. To evaluate the real-world performance, we also construct a demo system for crowdsourced mobile streaming and implement our proposed auction mechanism. Experiments over the demo system further show that those users who provide resources to others and those users who receive helps can increase their welfares by 15.5% and 35.4% (on average) via cooperation, respectively."
1105,https://arxiv.org/abs/1703.03530,Towards Wi-Fi AP-Assisted Content Prefetching for On-Demand TV Series: A Reinforcement Learning Approach,"The emergence of smart Wi-Fi APs (Access Point), which are equipped with huge storage space, opens a new research area on how to utilize these resources at the edge network to improve users' quality of experience (QoE) (e.g., a short startup delay and smooth playback). One important research interest in this area is content prefetching, which predicts and accurately fetches contents ahead of users' requests to shift the traffic away during peak periods. However, in practice, the different video watching patterns among users, and the varying network connection status lead to the time-varying server load, which eventually makes the content prefetching problem challenging. To understand this challenge, this paper first performs a large-scale measurement study on users' AP connection and TV series watching patterns using real-traces. Then, based on the obtained insights, we formulate the content prefetching problem as a Markov Decision Process (MDP). The objective is to strike a balance between the increased prefetching&storage cost incurred by incorrect prediction and the reduced content download delay because of successful prediction. A learning-based approach is proposed to solve this problem and another three algorithms are adopted as baselines. In particular, first, we investigate the performance lower bound by using a random algorithm, and the upper bound by using an ideal offline approach. Then, we present a heuristic algorithm as another baseline. Finally, we design a reinforcement learning algorithm that is more practical to work in the online manner. Through extensive trace-based experiments, we demonstrate the performance gain of our design. Remarkably, our learning-based algorithm achieves a better precision and hit ratio (e.g., 80%) with about 70% (resp. 50%) cost saving compared to the random (resp. heuristic) algorithm."
1106,https://arxiv.org/abs/1611.00211,Joint Optimization of Data Sponsoring and Edge Caching for Mobile Video Delivery,"In this work, we study the joint optimization of edge caching and data sponsoring for a video content provider (CP), aiming at reducing the content delivery cost and increasing the CP's revenue. Specifically, we formulate the joint optimization problem as a two-stage decision problem for the CP. In Stage I, the CP determines the edge caching policy (for a relatively long time period). In Stage II, the CP decides the real-time data sponsoring strategy for each content request within the period. We first propose a Lyapunov-based online sponsoring strategy in Stage II, which reaches 90% of the offline maximum performance (benchmark). We then solve the edge caching problem in Stage I based on the online sponsoring strategy proposed in Stage II, and show that the optimal caching policy depends on the aggregate user request for each content in each location. Simulations show that such a joint optimization can increase the CP's revenue by 30%~100%, comparing with the purely data sponsoring (i.e., without edge caching)."
1107,https://arxiv.org/abs/1607.01261,Dynamic Flow Scheduling Strategy in Multihoming Video CDNs,"Multihoming for a video Content Delivery Network (CDN) allows edge peering servers to deliver video chunks through different Internet Service Providers (ISPs), to achieve an improved quality of service (QoS) for video streaming users. However, since traditional strategies for a multihoming video CDN are simply designed according to static rules, e.g., simply sending traffic via a ISP which is the same as the ISP of client, they fail to dynamically allocate resources among different ISPs over time. In this paper, we perform measurement studies to demonstrate that such static allocation mechanism is inefficient to make full utilization of multiple ISPs' resources. To address this problem, we propose a dynamic flow scheduling strategy for multihoming video CDN. The challenge is to find the control parameters that can guide the ISP selection when performing flow scheduling. Using a data-driven approach, we find factors that have a major impact on the performance improvement in the dynamic flow scheduling. We further utilize an information gain approach to generate parameter combinations that can be used to guide the flow scheduling, i.e., to determine the ISP each request should be responded by. Our evaluation results demonstrate that our design effectively performs the flow scheduling. In particular, our design yields near optimal performance in a simulation of real-world multihoming setup."
1108,https://arxiv.org/abs/1607.01172,A Measurement Study of TCP Performance for Chunk Delivery in DASH,"Dynamic Adaptive Streaming over HTTP (DASH) has emerged as an increasingly popular paradigm for video streaming [13], in which a video is segmented into many chunks delivered to users by HTTP request/response over Transmission Control Protocol (TCP) con- nections. Therefore, it is intriguing to study the performance of strategies implemented in conventional TCPs, which are not dedicated for video streaming, e.g., whether chunks are efficiently delivered when users per- form interactions with the video players. In this paper, we conduct mea- surement studies on users chunk requesting traces in DASH from a rep- resentative video streaming provider, to investigate users behaviors in DASH, and TCP-connection-level traces from CDN servers, to investi- gate the performance of TCP for DASH. By studying how video chunks are delivered in both the slow start and congestion avoidance phases, our observations have revealed the performance characteristics of TCP for DASH as follows: (1) Request patterns in DASH have a great impact on the performance of TCP variations including cubic; (2) Strategies in conventional TCPs may cause user perceived quality degradation in DASH streaming; (3) Potential improvement to TCP strategies for better delivery in DASH can be further explored."
1109,https://arxiv.org/abs/1607.01159,Towards Network-Failure-Tolerant Content Delivery for Web Content,"Popularly used to distribute a variety of multimedia content items in today Internet, HTTP-based web content delivery still suffers from various content delivery failures. Hindered by the expensive deployment cost, the conventional CDN can not deploy as many edge servers as possible to successfully deliver content items to all users under these delivery failures. In this paper, we propose a joint CDN and peer-assisted web content delivery framework to address the delivery failure problem. Different from conventional peer-assisted approaches for web content delivery, which mainly focus on alleviating the CDN servers bandwidth load, we study how to use a browser-based peer-assisted scheme, namely WebRTC, to resolve content delivery failures. To this end, we carry out large-scale measurement studies on how users access and view webpages. Our measurement results demonstrate the challenges (e.g., peers stay on a webpage extremely short) that can not be directly solved by conventional P2P strategies, and some important webpage viewing patterns. Due to these unique characteristics, WebRTC peers open up new possibilities for helping the web content delivery, coming with the problem of how to utilize the dynamic resources efficiently. We formulate the peer selection that is the critical strategy in our framework, as an optimization problem, and design a heuristic algorithm based on the measurement insights to solve it. Our simulation experiments driven by the traces from Tencent QZone demonstrate the effectiveness of our design: compared with non-peer-assisted strategy and random peer selection strategy, our design significantly improves the successful relay ratio of web content items under network failures, e.g., our design improves the content download ratio up to 60% even when users located in a particular region (e.g., city) where none can connect to the regional CDN server."
1110,https://arxiv.org/abs/1606.04195,Social- and Mobility-Aware Device-to-Device Content Delivery,"Mobile online social network services have seen a rapid increase, in which the huge amount of user-generated social media contents propagating between users via social connections has significantly challenged the traditional content delivery paradigm: First, replicating all of the contents generated by users to edge servers that well ""fit"" the receivers becomes difficult due to the limited bandwidth and storage capacities. Motivated by device-to-device (D2D) communication that allows users with smart devices to transfer content directly, we propose replicating bandwidth-intensive social contents in a device-to-device manner. Based on large-scale measurement studies on social content propagation and user mobility patterns in edge-network regions, we observe that (1) Device-to-device replication can significantly help users download social contents from nearby neighboring peers; (2) Both social propagation and mobility patterns affect how contents should be replicated; (3) The replication strategies depend on regional characteristics ({\em e.g.}, how users move across regions).
  Using these measurement insights, we propose a joint \emph{propagation- and mobility-aware} content replication strategy for edge-network regions, in which social contents are assigned to users in edge-network regions according to a joint consideration of social graph, content propagation and user mobility. We formulate the replication scheduling as an optimization problem and design distributed algorithm only using historical, local and partial information to solve it. Trace-driven experiments further verify the superiority of our proposal: compared with conventional pure movement-based and popularity-based approach, our design can significantly ($2-4$ times) improve the amount of social contents successfully delivered by device-to-device replication."
1111,https://arxiv.org/abs/1605.07705,Understanding Content Placement Strategies in Smartrouter-based Peer CDN for Video Streaming,"Recent years have witnessed a new video delivery paradigm: smartrouter-based peer video content delivery network, which is enabled by smartrouters deployed at users' homes. ChinaCache (one of the largest CDN providers in China) and Youku (a video provider using smartrouters to assist video delivery) announced their cooperation in 2015, to create a new paradigm of content delivery based on householders' network resources. This new paradigm is different from the conventional peer-to-peer (P2P) approach, because millions of dedicated smartrouters are operated by the centralized video service providers in a coordinative manner. Thus it is intriguing to study the content placement strategies used in a smartrouter-based content delivery system, as well as its potential impact on the content delivery ecosystem. In this paper, we carry out measurement studies of Youku's peer video CDN, who has deployed over 300K smartrouter devices for its video delivery. In our measurement studies, 104K videos were investigated and 4TB traffic has been analyzed, over controlled smartrouter nodes and players. Our measurement insights are as follows. First, a global content replication strategy is essential for the peer CDN systems. Second, such peer CDN deployment itself can form an effective sub-system for end-to-end QoS monitoring, which can be used for fine-grained request redirection (e.g., user-level) and content replication. We also show our analysis on the performance limitations and propose potential improvements to the peer CDN systems."
1112,https://arxiv.org/abs/1605.07704,Understanding the Smartrouter-based Peer CDN for Video Streaming,"Recent years have witnessed a new video delivery paradigm: smartrouter-based video delivery network, which is enabled by smartrouters deployed at users' homes, together with the conventional video servers deployed in the datacenters. Recently, ChinaCache, a large content delivery network (CDN) provider, and Youku, a video service provider using smartrouters to assist video delivery, announced their cooperation to create a new paradigm of content delivery based on householders' network resources. This new paradigm is different from the conventional peer-to-peer (P2P) approach, because such dedicated smartrouters are inherently operated by the centralized video service providers in a coordinative manner. It is intriguing to study the strategies, performance and potential impact on the content delivery ecosystem of such peer CDN systems. In this paper, we study the Youku peer CDN, which has deployed over 300K smartrouter devices for its video streaming. In our measurement, 78K videos were investigated and 3TB traffic has been analyzed, over controlled routers and players. Our contributions are the following measurement insights. First, a global replication and caching strategy is essential for the peer CDN systems, and proactively scheduling replication and caching on a daily basis can guarantee their performance. Second, such peer CDN deployment can itself form an effective Quality of Service (QoS) monitoring sub-system, which can be used for fine-grained user request redirection. We also provide our analysis on the performance issues and potential improvements to the peer CDN systems."
1113,https://arxiv.org/abs/1502.03190,MAP: Microblogging Assisted Profiling of TV Shows,"Online microblogging services that have been increasingly used by people to share and exchange information, have emerged as a promising way to profiling multimedia contents, in a sense to provide users a socialized abstraction and understanding of these contents. In this paper, we propose a microblogging profiling framework, to provide a social demonstration of TV shows. Challenges for this study lie in two folds: First, TV shows are generally offline, i.e., most of them are not originally from the Internet, and we need to create a connection between these TV shows with online microblogging services; Second, contents in a microblogging service are extremely noisy for video profiling, and we need to strategically retrieve the most related information for the TV show profiling.To address these challenges, we propose a MAP, a microblogging-assisted profiling framework, with contributions as follows: i) We propose a joint user and content retrieval scheme, which uses information about both actors and topics of a TV show to retrieve related microblogs; ii) We propose a social-aware profiling strategy, which profiles a video according to not only its content, but also the social relationship of its microblogging users and its propagation in the social network; iii) We present some interesting analysis, based on our framework to profile real-world TV shows."
1114,https://arxiv.org/abs/1502.02226,Dispersing Instant Social Video Service Across Multiple Clouds,"Instant social video sharing which combines the online social network and user-generated short video streaming services, has become popular in today's Internet. Cloud-based hosting of such instant social video contents has become a norm to serve the increasing users with user-generated contents. A fundamental problem of cloud-based social video sharing service is that users are located globally, who cannot be served with good service quality with a single cloud provider. In this paper, we investigate the feasibility of dispersing instant social video contents to multiple cloud providers. The challenge is that inter-cloud social \emph{propagation} is indispensable with such multi-cloud social video hosting, yet such inter-cloud traffic incurs substantial operational cost. We analyze and formulate the multi-cloud hosting of an instant social video system as an optimization problem. We conduct large-scale measurement studies to show the characteristics of instant social video deployment, and demonstrate the trade-off between satisfying users with their ideal cloud providers, and reducing the inter-cloud data propagation. Our measurement insights of the social propagation allow us to propose a heuristic algorithm with acceptable complexity to solve the optimization problem, by partitioning a propagation-weighted social graph in two phases: a preference-aware initial cloud provider selection and a propagation-aware re-hosting. Our simulation experiments driven by real-world social network traces show the superiority of our design."
1115,https://arxiv.org/abs/1403.0316,Cross-Scale Cost Aggregation for Stereo Matching,"Human beings process stereoscopic correspondence across multiple scales. However, this bio-inspiration is ignored by state-of-the-art cost aggregation methods for dense stereo correspondence. In this paper, a generic cross-scale cost aggregation framework is proposed to allow multi-scale interaction in cost aggregation. We firstly reformulate cost aggregation from a unified optimization perspective and show that different cost aggregation methods essentially differ in the choices of similarity kernels. Then, an inter-scale regularizer is introduced into optimization and solving this new optimization problem leads to the proposed framework. Since the regularization term is independent of the similarity kernel, various cost aggregation methods can be integrated into the proposed general framework. We show that the cross-scale framework is important as it effectively and efficiently expands state-of-the-art cost aggregation methods and leads to significant improvements, when evaluated on Middlebury, KITTI and New Tsukuba datasets."
1116,https://arxiv.org/abs/1402.2020,Binary Stereo Matching,"In this paper, we propose a novel binary-based cost computation and aggregation approach for stereo matching problem. The cost volume is constructed through bitwise operations on a series of binary strings. Then this approach is combined with traditional winner-take-all strategy, resulting in a new local stereo matching algorithm called binary stereo matching (BSM). Since core algorithm of BSM is based on binary and integer computations, it has a higher computational efficiency than previous methods. Experimental results on Middlebury benchmark show that BSM has comparable performance with state-of-the-art local stereo methods in terms of both quality and speed. Furthermore, experiments on images with radiometric differences demonstrate that BSM is more robust than previous methods under these changes, which is common under real illumination."
1117,https://arxiv.org/abs/2305.08322,C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models,"New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C-Eval comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-Eval is accompanied by C-Eval Hard, a subset of very challenging subjects in C-Eval that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-Eval, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over 60%, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-Eval will help analyze important strengths and shortcomings of foundation models, and foster their development and growth for Chinese users."
1118,https://arxiv.org/abs/2305.01624,UNTER: A Unified Knowledge Interface for Enhancing Pre-trained Language Models,"Recent research demonstrates that external knowledge injection can advance pre-trained language models (PLMs) in a variety of downstream NLP tasks. However, existing knowledge injection methods are either applicable to structured knowledge or unstructured knowledge, lacking a unified usage. In this paper, we propose a UNified knowledge inTERface, UNTER, to provide a unified perspective to exploit both structured knowledge and unstructured knowledge. In UNTER, we adopt the decoder as a unified knowledge interface, aligning span representations obtained from the encoder with their corresponding knowledge. This approach enables the encoder to uniformly invoke span-related knowledge from its parameters for downstream applications. Experimental results show that, with both forms of knowledge injected, UNTER gains continuous improvements on a series of knowledge-driven NLP tasks, including entity typing, named entity recognition and relation extraction, especially in low-resource scenarios."
1119,https://arxiv.org/abs/2304.11029,CLaMP: Contrastive Language-Music Pre-training for Cross-Modal Symbolic Music Information Retrieval,"We introduce CLaMP: Contrastive Language-Music Pre-training, which learns cross-modal representations between natural language and symbolic music using a music encoder and a text encoder trained jointly with a contrastive loss. To pre-train CLaMP, we collected a large dataset of 1.4 million music-text pairs. It employed text dropout as a data augmentation technique and bar patching to efficiently represent music data which reduces sequence length to less than 10\%. In addition, we developed a masked music model pre-training objective to enhance the music encoder's comprehension of musical context and structure. CLaMP integrates textual information to enable semantic search and zero-shot classification for symbolic music, surpassing the capabilities of previous models. To support the evaluation of semantic search and music classification, we publicly release WikiMusicText (WikiMT), a dataset of 1010 lead sheets in ABC notation, each accompanied by a title, artist, genre, and description. In comparison to state-of-the-art models that require fine-tuning, zero-shot CLaMP demonstrated comparable or superior performance on score-oriented datasets. Our models and code are available at https://github.com/microsoft/muzic/tree/main/clamp."
1120,https://arxiv.org/abs/2301.02884,TunesFormer: Forming Tunes with Control Codes,"In recent years, deep learning techniques have been applied to music generation systems with promising results. However, one of the main challenges in this field has been the lack of annotated datasets, making it difficult for models to learn musical forms in compositions. To address this issue, we present TunesFormer, a Transformer-based melody generation system that is trained on a large dataset of 285,449 ABC tunes. By utilizing specific symbols commonly found in ABC notation to indicate section boundaries, TunesFormer can understand and generate melodies with given musical forms based on control codes. Our objective evaluations demonstrate the effectiveness of the control codes in achieving controlled musical forms, and subjective experiments show that the generated melodies are of comparable quality to human compositions. Our results also provide insights into the optimal placement of control codes and their impact on the generated melodies. TunesFormer presents a promising approach for generating melodies with desired musical forms through the use of deep learning techniques."
1121,https://arxiv.org/abs/2212.09387,Prompt Gating: A Parameter Efficient Tuning Method for Zero-Shot Multi-Source Translation,"Multi-source translation (MST), which typically receives multiple source sentences of the same meaning in different languages, has been shown superior to single-source translation. As the quantity of multi-source parallel data is limited, taking full advantage of single-source data and limited multi-source data to make models perform well when receiving as many as possible sources remains a challenge. Unlike previous work mostly devoted to supervised scenarios, we focus on zero-shot MST: expecting models to be able to process unseen combinations of multiple sources, e.g., unseen language combinations, during inference. We propose a simple yet effective parameter efficient method, named Prompt Gating, which appends prompts to the model inputs and attaches gates on the extended hidden states for each encoder layer. It shows strong zero-shot transferability (+9.0 BLEU points maximally) and remarkable compositionality (+15.6 BLEU points maximally) on MST, and also shows its superiorities over baselines on lexically constrained translation."
1122,https://arxiv.org/abs/2212.09097,Continually Learning from Existing Models: Knowledge Accumulation for Neural Machine Translation,"Although continually extending an existing NMT model to new domains or languages has attracted intensive interest in recent years, the equally valuable problem of continually improving a given NMT model in its domain by leveraging knowledge from an unlimited number of existing NMT models is not explored yet. To facilitate the study, we propose a formal definition for the problem named knowledge accumulation for NMT (KA-NMT) with corresponding datasets and evaluation metrics and develop a novel method for KA-NMT. We investigate a novel knowledge detection algorithm to identify beneficial knowledge from existing models at token level, and propose to learn from beneficial knowledge and learn against other knowledge simultaneously to improve learning efficiency. To alleviate catastrophic forgetting, we further propose to transfer knowledge from previous to current version of the given model. Extensive experiments show that our proposed method significantly and consistently outperforms representative baselines under homogeneous, heterogeneous, and malicious model settings for different language pairs."
1123,https://arxiv.org/abs/2211.11216,Exploring the Efficacy of Pre-trained Checkpoints in Text-to-Music Generation Task,"Benefiting from large-scale datasets and pre-trained models, the field of generative models has recently gained significant momentum. However, most datasets for symbolic music are very small, which potentially limits the performance of data-driven multimodal models. An intuitive solution to this problem is to leverage pre-trained models from other modalities (e.g., natural language) to improve the performance of symbolic music-related multimodal tasks. In this paper, we carry out the first study of generating complete and semantically consistent symbolic music scores from text descriptions, and explore the efficacy of using publicly available checkpoints (i.e., BERT, GPT-2, and BART) for natural language processing in the task of text-to-music generation. Our experimental results show that the improvement from using pre-trained checkpoints is statistically significant in terms of BLEU score and edit distance similarity. We analyse the capabilities and limitations of our model to better understand the potential of language-music models."
1124,https://arxiv.org/abs/2211.07164,Evade the Trap of Mediocrity: Promoting Diversity and Novelty in Text Generation via Concentrating Attention,"Recently, powerful Transformer architectures have proven superior in generating high-quality sentences. Nevertheless, these models tend to produce dull high-frequency phrases, severely hurting the diversity and novelty of generated text. In this work, we dig into the intrinsic mechanism of this problem and found that sparser attention values in Transformer could improve diversity. To understand such a phenomenon, we first conduct both empirical and theoretical analysis and then attribute it to representation degeneration caused by the attentive mixture of the hidden states during training. We term this process the Trap of Mediocrity. To escape from such a trap, we introduce a novel attention regularization loss to control the sharpness of the attention distribution, which is transparent to model structures and can be easily implemented within 20 lines of python code. We prove that this method could be mathematically regarded as learning a Bayesian approximation of posterior attention. Experiments show that our method improved the diversity and novelty of the generated text while maintaining comparable quality on a variety of conditional and unconditional generation tasks."
1125,https://arxiv.org/abs/2210.12409,Recurrence Boosts Diversity! Revisiting Recurrent Latent Variable in Transformer-Based Variational AutoEncoder for Diverse Text Generation,"Variational Auto-Encoder (VAE) has been widely adopted in text generation. Among many variants, recurrent VAE learns token-wise latent variables with each conditioned on the preceding ones, which captures sequential variability better in the era of RNN. However, it is unclear how to incorporate such recurrent dynamics into the recently dominant Transformer due to its parallelism. In this work, we propose TRACE, a Transformer-based recurrent VAE structure. TRACE imposes recurrence on segment-wise latent variables with arbitrarily separated text segments and constructs the posterior distribution with residual parameterization. Besides, we design an acceleration method by approximating idempotent matrices, which allows parallelism while maintaining the conditional dependence of latent variables. We demonstrate that TRACE could enhance the entanglement of each segment and preceding latent variables and deduce a non-zero lower bound of the KL term, providing a theoretical guarantee of generation diversity. Experiments on two unconditional and one conditional generation tasks show that TRACE achieves significantly improved diversity while maintaining satisfactory generation quality."
1126,https://arxiv.org/abs/2209.06496,CCOM-HuQin: an Annotated Multimodal Chinese Fiddle Performance Dataset,"HuQin is a family of traditional Chinese bowed string instruments. Playing techniques(PTs) embodied in various playing styles add abundant emotional coloring and aesthetic feelings to HuQin performance. The complex applied techniques make HuQin music a challenging source for fundamental MIR tasks such as pitch analysis, transcription and score-audio alignment. In this paper, we present a multimodal performance dataset of HuQin music that contains audio-visual recordings of 11,992 single PT clips and 57 annotated musical pieces of classical excerpts. We systematically describe the HuQin PT taxonomy based on musicological theory and practical use cases. Then we introduce the dataset creation methodology and highlight the annotation principles featuring PTs. We analyze the statistics in different aspects to demonstrate the variety of PTs played in HuQin subcategories and perform preliminary experiments to show the potential applications of the dataset in various MIR tasks and cross-cultural music studies. Finally, we propose future work to be extended on the dataset."
1127,https://arxiv.org/abs/2207.06130,Fuse It More Deeply! A Variational Transformer with Layer-Wise Latent Variable Inference for Text Generation,"The past several years have witnessed Variational Auto-Encoder's superiority in various text generation tasks. However, due to the sequential nature of the text, auto-regressive decoders tend to ignore latent variables and then reduce to simple language models, known as the KL vanishing problem, which would further deteriorate when VAE is combined with Transformer-based structures. To ameliorate this problem, we propose DELLA, a novel variational Transformer framework. DELLA learns a series of layer-wise latent variables with each inferred from those of lower layers and tightly coupled with the hidden states by low-rank tensor product. In this way, DELLA forces these posterior latent variables to be fused deeply with the whole computation path and hence incorporate more information. We theoretically demonstrate that our method can be regarded as entangling latent variables to avoid posterior information decrease through layers, enabling DELLA to get higher non-zero KL values even without any annealing or thresholding tricks. Experiments on four unconditional and three conditional generation tasks show that DELLA could better alleviate KL vanishing and improve both quality and diversity compared to several strong baselines."
1128,https://arxiv.org/abs/2205.11255,A Template-based Method for Constrained Neural Machine Translation,"Machine translation systems are expected to cope with various types of constraints in many practical scenarios. While neural machine translation (NMT) has achieved strong performance in unconstrained cases, it is non-trivial to impose pre-specified constraints into the translation process of NMT models. Although many approaches have been proposed to address this issue, most existing methods can not satisfy the following three desiderata at the same time: (1) high translation quality, (2) high match accuracy, and (3) low latency. In this work, we propose a template-based method that can yield results with high translation quality and match accuracy and the inference speed of our method is comparable with unconstrained NMT models. Our basic idea is to rearrange the generation of constrained and unconstrained tokens through a template. Our method does not require any changes in the model architecture and the decoding algorithm. Experimental results show that the proposed template-based approach can outperform several representative baselines in both lexically and structurally constrained translation tasks."
1129,https://arxiv.org/abs/2205.06036,Efficient and Training-Free Control of Language Generation,"In recent years, there has been a growing interest in the development of language models capable of generating text with controllable attributes. While several approaches have been proposed, many of these methods require condition-specific data or significant computational resources. In this study, we propose a novel method called Gamma Sampling, which enables controllable language generation without the need for any training data and maintains a fast generation speed. Gamma Sampling incorporates attribute-related information into the sampling process, effectively guiding the language model to produce text with desired attributes. Our experimental results demonstrate that Gamma Sampling, when applied to GPT2, outperforms representative baselines in terms of diversity, attribute relevance, and overall quality of the generated samples."
1130,https://arxiv.org/abs/2205.05448,Symphony Generation with Permutation Invariant Language Model,"In this work, we propose a permutation invariant language model, SymphonyNet, as a solution for symbolic symphony music generation. We propose a novel Multi-track Multi-instrument Repeatable (MMR) representation for symphonic music and model the music sequence using a Transformer-based auto-regressive language model with specific 3-D positional embedding. To overcome length overflow when modeling extra-long symphony tokens, we also propose a modified Byte Pair Encoding algorithm (Music BPE) for music tokens and introduce a novel linear transformer decoder architecture as a backbone. Meanwhile, we train the decoder to learn automatic orchestration as a joint task by masking instrument information from the input. We also introduce a large-scale symbolic symphony dataset for the advance of symphony generation research. Empirical results show that the proposed approach can generate coherent, novel, complex and harmonious symphony as a pioneer solution for multi-track multi-instrument symbolic music generation."
1131,https://arxiv.org/abs/2202.08423,Chord-Conditioned Melody Harmonization with Controllable Harmonicity,"Melody harmonization has long been closely associated with chorales composed by Johann Sebastian Bach. Previous works rarely emphasised chorale generation conditioned on chord progressions, and there has been a lack of focus on assistive compositional tools. In this paper, we first designed a music representation that encoded chord symbols for chord conditioning, and then proposed DeepChoir, a melody harmonization system that can generate a four-part chorale for a given melody conditioned on a chord progression. With controllable harmonicity, users can control the extent of harmonicity for generated chorales. Experimental results reveal the effectiveness of the music representation and the controllability of DeepChoir."
1132,https://arxiv.org/abs/2112.15043,YACLC: A Chinese Learner Corpus with Multidimensional Annotation,"Learner corpus collects language data produced by L2 learners, that is second or foreign-language learners. This resource is of great relevance for second language acquisition research, foreign-language teaching, and automatic grammatical error correction. However, there is little focus on learner corpus for Chinese as Foreign Language (CFL) learners. Therefore, we propose to construct a large-scale, multidimensional annotated Chinese learner corpus. To construct the corpus, we first obtain a large number of topic-rich texts generated by CFL learners. Then we design an annotation scheme including a sentence acceptability score as well as grammatical error and fluency-based corrections. We build a crowdsourcing platform to perform the annotation effectively (https://yaclc.wenmind.net). We name the corpus YACLC (Yet Another Chinese Learner Corpus) and release it as part of the CUGE benchmark (http://cuge.baai.ac.cn). By analyzing the original sentences and annotations in the corpus, we found that YACLC has a considerable size and very high annotation quality. We hope this corpus can further enhance the studies on Chinese International Education and Chinese automatic grammatical error correction."
1133,https://arxiv.org/abs/2112.11122,Generating Chords from Melody with Flexible Harmonic Rhythm and Controllable Harmonic Density,"Melody harmonization, i.e., generating a chord progression for a user-given melody, remains a challenging task to this day. A chord progression must not only be in harmony with the melody, but its harmonic rhythm is also interdependent on the melodic rhythm. Although previous neural network-based systems can effectively generate a chord progression for a melody, few studies have addressed controllable melody harmonization, and there has been a lack of focus on generating flexible harmonic rhythms. In this paper, we propose AutoHarmonizer, a harmonic density-controllable melody harmonization system with flexible harmonic rhythm. This system supports 1,462 chord types and can generate denser or sparser chord progressions for a given melody. Experimental results demonstrate the diversity of harmonic rhythms in the AutoHarmonizer-generated chord progressions and the effectiveness of controllable harmonic density."
1134,https://arxiv.org/abs/2109.06067,Packed Levitated Marker for Entity and Relation Extraction,"Recent entity and relation extraction works focus on investigating how to obtain a better span representation from the pre-trained encoder. However, a major limitation of existing works is that they ignore the interrelation between spans (pairs). In this work, we propose a novel span representation approach, named Packed Levitated Markers (PL-Marker), to consider the interrelation between the spans (pairs) by strategically packing the markers in the encoder. In particular, we propose a neighborhood-oriented packing strategy, which considers the neighbor spans integrally to better model the entity boundary information. Furthermore, for those more complicated span pair classification tasks, we design a subject-oriented packing strategy, which packs each subject and all its objects to model the interrelation between the same-subject span pairs. The experimental results show that, with the enhanced marker feature, our model advances baselines on six NER benchmarks, and obtains a 4.1%-4.3% strict relation F1 improvement with higher speed over previous state-of-the-art models on ACE04 and ACE05."
1135,https://arxiv.org/abs/2108.12108,Lingxi: A Diversity-aware Chinese Modern Poetry Generation System,"Poetry generation has been a difficult task in natural language processing. Unlike plain neural text generation tasks, poetry has a high requirement for novelty, since an easily-understood sentence with too many high frequency words might not be considered as poetic, while adequately ambiguous sentences with low frequency words can possibly be novel and creative. Inspired by this, we present Lingxi, a diversity-aware Chinese modern poetry generation system. We propose nucleus sampling with randomized head (NS-RH) algorithm, which randomizes the high frequency part (""head"") of the predicted distribution, in order to emphasize on the ""comparatively low frequency"" words. The proposed algorithm can significantly increase the novelty of generated poetry compared with traditional sampling methods. The permutation of distribution is controllable by tuning the filtering parameter that determines the ""head"" to permutate, achieving diversity-aware sampling. We find that even when a large portion of filtered vocabulary is randomized, it can actually generate fluent poetry but with notably higher novelty. We also propose a semantic-similarity-based rejection sampling algorithm, which creates longer and more informative context on the basis of the short input poetry title while maintaining high semantic similarity to the title, alleviating the off-topic issue."
1136,https://arxiv.org/abs/2106.13627,Language Models are Good Translators,"Recent years have witnessed the rapid advance in neural machine translation (NMT), the core of which lies in the encoder-decoder architecture. Inspired by the recent progress of large-scale pre-trained language models on machine translation in a limited scenario, we firstly demonstrate that a single language model (LM4MT) can achieve comparable performance with strong encoder-decoder NMT models on standard machine translation benchmarks, using the same training data and similar amount of model parameters. LM4MT can also easily utilize source-side texts as additional supervision. Though modeling the source- and target-language texts with the same mechanism, LM4MT can provide unified representations for both source and target sentences, which can better transfer knowledge across languages. Extensive experiments on pivot-based and zero-shot translation tasks show that LM4MT can outperform the encoder-decoder NMT model by a large margin."
1137,https://arxiv.org/abs/2106.08582,Alternated Training with Synthetic and Authentic Data for Neural Machine Translation,"While synthetic bilingual corpora have demonstrated their effectiveness in low-resource neural machine translation (NMT), adding more synthetic data often deteriorates translation performance. In this work, we propose alternated training with synthetic and authentic data for NMT. The basic idea is to alternate synthetic and authentic corpora iteratively during training. Compared with previous work, we introduce authentic data as guidance to prevent the training of NMT models from being disturbed by noisy synthetic data. Experiments on Chinese-English and German-English translation tasks show that our approach improves the performance over several strong baselines. We visualize the BLEU landscape to further investigate the role of authentic and synthetic data during alternated training. From the visualization, we find that authentic data helps to direct the NMT model parameters towards points with higher BLEU scores and leads to consistent translation performance improvement."
1138,https://arxiv.org/abs/2106.03297,On the Language Coverage Bias for Neural Machine Translation,"Language coverage bias, which indicates the content-dependent differences between sentence pairs originating from the source and target languages, is important for neural machine translation (NMT) because the target-original training data is not well exploited in current practice. By carefully designing experiments, we provide comprehensive analyses of the language coverage bias in the training data, and find that using only the source-original data achieves comparable performance with using full training data. Based on these observations, we further propose two simple and effective approaches to alleviate the language coverage bias problem through explicitly distinguishing between the source- and target-original training data, which consistently improve the performance over strong baselines on six WMT20 translation tasks. Complementary to the translationese effect, language coverage bias provides another explanation for the performance drop caused by back-translation. We also apply our approach to both back- and forward-translation and find that mitigating the language coverage bias can improve the performance of both the two representative data augmentation methods and their tagged variants."
1139,https://arxiv.org/abs/2106.01979,CCPM: A Chinese Classical Poetry Matching Dataset,"Poetry is one of the most important art forms of human languages. Recently many studies have focused on incorporating some linguistic features of poetry, such as style and sentiment, into its understanding or generation system. However, there is no focus on understanding or evaluating the semantics of poetry. Therefore, we propose a novel task to assess a model's semantic understanding of poetry by poem matching. Specifically, this task requires the model to select one line of Chinese classical poetry among four candidates according to the modern Chinese translation of a line of poetry. To construct this dataset, we first obtain a set of parallel data of Chinese classical poetry and modern Chinese translation. Then we retrieve similar lines of poetry with the lines in a poetry corpus as negative choices. We name the dataset Chinese Classical Poetry Matching Dataset (CCPM) and release it at https://github.com/THUNLP-AIPoet/CCPM. We hope this dataset can further enhance the study on incorporating deep semantics into the understanding and generation system of Chinese classical poetry. We also preliminarily run two variants of BERT on this dataset as the baselines for this dataset."
1140,https://arxiv.org/abs/2105.14809,Transfer Learning for Sequence Generation: from Single-source to Multi-source,"Multi-source sequence generation (MSG) is an important kind of sequence generation tasks that takes multiple sources, including automatic post-editing, multi-source translation, multi-document summarization, etc. As MSG tasks suffer from the data scarcity problem and recent pretrained models have been proven to be effective for low-resource downstream tasks, transferring pretrained sequence-to-sequence models to MSG tasks is essential. Although directly finetuning pretrained models on MSG tasks and concatenating multiple sources into a single long sequence is regarded as a simple method to transfer pretrained models to MSG tasks, we conjecture that the direct finetuning method leads to catastrophic forgetting and solely relying on pretrained self-attention layers to capture cross-source information is not sufficient. Therefore, we propose a two-stage finetuning method to alleviate the pretrain-finetune discrepancy and introduce a novel MSG model with a fine encoder to learn better representations in MSG tasks. Experiments show that our approach achieves new state-of-the-art results on the WMT17 APE task and multi-source translation task using the WMT14 test set. When adapted to document-level translation, our framework outperforms strong baselines significantly."
1141,https://arxiv.org/abs/2105.11618,TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference,"Existing pre-trained language models (PLMs) are often computationally expensive in inference, making them impractical in various resource-limited real-world applications. To address this issue, we propose a dynamic token reduction approach to accelerate PLMs' inference, named TR-BERT, which could flexibly adapt the layer number of each token in inference to avoid redundant calculation. Specially, TR-BERT formulates the token reduction process as a multi-step token selection problem and automatically learns the selection strategy via reinforcement learning. The experimental results on several downstream NLP tasks show that TR-BERT is able to speed up BERT by 2-5 times to satisfy various performance demands. Moreover, TR-BERT can also achieve better performance with less computation in a suite of long-text tasks since its token-level layer number adaption greatly accelerates the self-attention operation in PLMs. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/TR-BERT."
1142,https://arxiv.org/abs/2105.06679,Dynamic Multi-Branch Layers for On-Device Neural Machine Translation,"With the rapid development of artificial intelligence (AI), there is a trend in moving AI applications, such as neural machine translation (NMT), from cloud to mobile devices. Constrained by limited hardware resources and battery, the performance of on-device NMT systems is far from satisfactory. Inspired by conditional computation, we propose to improve the performance of on-device NMT systems with dynamic multi-branch layers. Specifically, we design a layer-wise dynamic multi-branch network with only one branch activated during training and inference. As not all branches are activated during training, we propose shared-private reparameterization to ensure sufficient training for each branch. At almost the same computational cost, our method achieves improvements of up to 1.7 BLEU points on the WMT14 English-German translation task and 1.8 BLEU points on the WMT20 Chinese-English translation task over the Transformer model, respectively. Compared with a strong baseline that also uses multiple branches, the proposed method is up to 1.5 times faster with the same number of parameters."
1143,https://arxiv.org/abs/2105.04443,Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction,"Grammatical Error Correction (GEC) aims to correct writing errors and help language learners improve their writing skills. However, existing GEC models tend to produce spurious corrections or fail to detect lots of errors. The quality estimation model is necessary to ensure learners get accurate GEC results and avoid misleading from poorly corrected sentences. Well-trained GEC models can generate several high-quality hypotheses through decoding, such as beam search, which provide valuable GEC evidence and can be used to evaluate GEC quality. However, existing models neglect the possible GEC evidence from different hypotheses. This paper presents the Neural Verification Network (VERNet) for GEC quality estimation with multiple hypotheses. VERNet establishes interactions among hypotheses with a reasoning graph and conducts two kinds of attention mechanisms to propagate GEC evidence to verify the quality of generated hypotheses. Our experiments on four GEC datasets show that VERNet achieves state-of-the-art grammatical error detection performance, achieves the best quality estimation results, and significantly improves GEC performance by reranking hypotheses. All data and source codes are available at https://github.com/thunlp/VERNet."
1144,https://arxiv.org/abs/2103.07656,Optimal Embedding Calibration for Symbolic Music Similarity,"In natural language processing (NLP), the semantic similarity task requires large-scale, high-quality human-annotated labels for fine-tuning or evaluation. By contrast, in cases of music similarity, such labels are expensive to collect and largely dependent on the annotator's artistic preferences. Recent research has demonstrated that embedding calibration technique can greatly increase semantic similarity performance of the pre-trained language model without fine-tuning. However, it is yet unknown which calibration method is the best and how much performance improvement can be achieved. To address these issues, we propose using composer information to construct labels for automatically evaluating music similarity. Under this paradigm, we discover the optimal combination of embedding calibration which achieves superior metrics than the baseline methods."
1145,https://arxiv.org/abs/2103.07649,Improving Diversity of Neural Text Generation via Inverse Probability Weighting,"The neural text generation suffers from the text degeneration issue such as repetition. Traditional stochastic sampling methods only focus on truncating the unreliable ""tail"" of the distribution, and do not address the ""head"" part, which we show might contain tedious or even repetitive candidates with high probability that lead to repetition loops. They also do not consider the issue that human text does not always favor high-probability words. Inspired by these, in this work we propose a heuristic sampling method. We propose to use interquartile range of the predicted distribution to determine the ""head"" part, then permutate and rescale the ""head"" with inverse probability. This aims at decreasing the probability for the tedious and possibly repetitive candidates with higher probability, and increasing the probability for the rational but more surprising candidates with lower probability. The proposed algorithm provides a reasonable permutation on the predicted distribution which enhances diversity without compromising rationality of the distribution. We use pre-trained language model to compare our algorithm with traditional methods. Results show that our algorithm can effectively increase the diversity of generated samples while achieving close resemblance to human text."
1146,https://arxiv.org/abs/2012.15515,"Neural Machine Translation: A Review of Methods, Resources, and Tools","Machine translation (MT) is an important sub-field of natural language processing that aims to translate natural languages using computers. In recent years, end-to-end neural machine translation (NMT) has achieved great success and has become the new mainstream method in practical MT systems. In this article, we first provide a broad review of the methods for NMT and focus on methods relating to architectures, decoding, and data augmentation. Then we summarize the resources and tools that are useful for researchers. Finally, we conclude with a discussion of possible future research directions."
1147,https://arxiv.org/abs/2012.13568,Towards a Universal Continuous Knowledge Base,"In artificial intelligence (AI), knowledge is the information required by an intelligent system to accomplish tasks. While traditional knowledge bases use discrete, symbolic representations, detecting knowledge encoded in the continuous representations learned from data has received increasing attention recently. In this work, we propose a method for building a continuous knowledge base (CKB) that can store knowledge imported from multiple, diverse neural networks. The key idea of our approach is to define an interface for each neural network and cast knowledge transferring as a function simulation problem. Experiments on text classification show promising results: the CKB imports knowledge from a single model and then exports the knowledge to a new model, achieving comparable performance with the original model. More interesting, we import the knowledge from multiple models to the knowledge base, from which the fused knowledge is exported back to a single model, achieving a higher accuracy than the original model. With the CKB, it is also easy to achieve knowledge distillation and transfer learning. Our work opens the door to building a universal continuous knowledge base to collect, store, and organize all continuous knowledge encoded in various neural networks trained for different AI tasks."
1148,https://arxiv.org/abs/2012.07162,Mask-Align: Self-Supervised Neural Word Alignment,"Word alignment, which aims to align translationally equivalent words between source and target sentences, plays an important role in many natural language processing tasks. Current unsupervised neural alignment methods focus on inducing alignments from neural machine translation models, which does not leverage the full context in the target sequence. In this paper, we propose Mask-Align, a self-supervised word alignment model that takes advantage of the full context on the target side. Our model masks out each target token and predicts it conditioned on both source and the remaining target tokens. This two-step process is based on the assumption that the source token contributing most to recovering the masked target token should be aligned. We also introduce an attention variant called leaky attention, which alleviates the problem of unexpected high cross-attention weights on special tokens such as periods. Experiments on four language pairs show that our model outperforms previous unsupervised neural aligners and obtains new state-of-the-art results."
1149,https://arxiv.org/abs/2011.10369,ONION: A Simple and Effective Defense Against Textual Backdoor Attacks,"Backdoor attacks are a kind of emergent training-time threat to deep neural networks (DNNs). They can manipulate the output of DNNs and possess high insidiousness. In the field of natural language processing, some attack methods have been proposed and achieve very high attack success rates on multiple popular models. Nevertheless, there are few studies on defending against textual backdoor attacks. In this paper, we propose a simple and effective textual backdoor defense named ONION, which is based on outlier word detection and, to the best of our knowledge, is the first method that can handle all the textual backdoor attack situations. Experiments demonstrate the effectiveness of our model in defending BiLSTM and BERT against five different backdoor attacks. All the code and data of this paper can be obtained at https://github.com/thunlp/ONION."
1150,https://arxiv.org/abs/2011.05268,Towards Interpretable Natural Language Understanding with Explanations as Latent Variables,"Recently generating natural language explanations has shown very promising results in not only offering interpretable explanations but also providing additional information and supervision for prediction. However, existing approaches usually require a large set of human annotated explanations for training while collecting a large set of explanations is not only time consuming but also expensive. In this paper, we develop a general framework for interpretable natural language understanding that requires only a small set of human annotated explanations for training. Our framework treats natural language explanations as latent variables that model the underlying reasoning process of a neural model. We develop a variational EM framework for optimization where an explanation generation module and an explanation-augmented prediction module are alternatively optimized and mutually enhance each other. Moreover, we further propose an explanation-based self-training method under this framework for semi-supervised learning. It alternates between assigning pseudo-labels to unlabeled data and generating new explanations to iteratively improve each other. Experiments on two natural language understanding tasks demonstrate that our framework can not only make effective predictions in both supervised and semi-supervised settings, but also generate good natural language explanation."
1151,https://arxiv.org/abs/2011.03888,Denoising Relation Extraction from Document-level Distant Supervision,"Distant supervision (DS) has been widely used to generate auto-labeled data for sentence-level relation extraction (RE), which improves RE performance. However, the existing success of DS cannot be directly transferred to the more challenging document-level relation extraction (DocRE), since the inherent noise in DS may be even multiplied in document level and significantly harm the performance of RE. To address this challenge, we propose a novel pre-trained model for DocRE, which denoises the document-level DS data via multiple pre-training tasks. Experimental results on the large-scale DocRE benchmark show that our model can capture useful information from noisy DS data and achieve promising results."
1152,https://arxiv.org/abs/2011.03770,Know What You Don't Need: Single-Shot Meta-Pruning for Attention Heads,"Deep pre-trained Transformer models have achieved state-of-the-art results over a variety of natural language processing (NLP) tasks. By learning rich language knowledge with millions of parameters, these models are usually overparameterized and significantly increase the computational overhead in applications. It is intuitive to address this issue by model compression. In this work, we propose a method, called Single-Shot Meta-Pruning, to compress deep pre-trained Transformers before fine-tuning. Specifically, we focus on pruning unnecessary attention heads adaptively for different downstream tasks. To measure the informativeness of attention heads, we train our Single-Shot Meta-Pruner (SMP) with a meta-learning paradigm aiming to maintain the distribution of text representations after pruning. Compared with existing compression methods for pre-trained models, our method can reduce the overhead of both fine-tuning and inference. Experimental results show that our pruner can selectively prune 50% of attention heads with little impact on the performance on downstream tasks and even provide better text representations. The source code will be released in the future."
1153,https://arxiv.org/abs/2010.08091,"PiRhDy: Learning Pitch-, Rhythm-, and Dynamics-aware Embeddings for Symbolic Music","Definitive embeddings remain a fundamental challenge of computational musicology for symbolic music in deep learning today. Analogous to natural language, music can be modeled as a sequence of tokens. This motivates the majority of existing solutions to explore the utilization of word embedding models to build music embeddings. However, music differs from natural languages in two key aspects: (1) musical token is multi-faceted -- it comprises of pitch, rhythm and dynamics information; and (2) musical context is two-dimensional -- each musical token is dependent on both melodic and harmonic contexts. In this work, we provide a comprehensive solution by proposing a novel framework named PiRhDy that integrates pitch, rhythm, and dynamics information seamlessly. PiRhDy adopts a hierarchical strategy which can be decomposed into two steps: (1) token (i.e., note event) modeling, which separately represents pitch, rhythm, and dynamics and integrates them into a single token embedding; and (2) context modeling, which utilizes melodic and harmonic knowledge to train the token embedding. A thorough study was made on each component and sub-strategy of PiRhDy. We further validate our embeddings in three downstream tasks -- melody completion, accompaniment suggestion, and genre classification. Results indicate a significant advancement of the neural approach towards symbolic music as well as PiRhDy's potential as a pretrained tool for a broad range of symbolic music applications."
1154,https://arxiv.org/abs/2010.01923,Learning from Context or Names? An Empirical Study on Neural Relation Extraction,"Neural models have achieved remarkable success on relation extraction (RE) benchmarks. However, there is no clear understanding which type of information affects existing RE models to make decisions and how to further improve the performance of these models. To this end, we empirically study the effect of two main information sources in text: textual context and entity mentions (names). We find that (i) while context is the main source to support the predictions, RE models also heavily rely on the information from entity mentions, most of which is type information, and (ii) existing datasets may leak shallow heuristics via entity mentions and thus contribute to the high performance on RE benchmarks. Based on the analyses, we propose an entity-masked contrastive pre-training framework for RE to gain a deeper understanding on both textual context and type information while avoiding rote memorization of entities or use of superficial cues in mentions. We carry out extensive experiments to support our views, and show that our framework can improve the effectiveness and robustness of neural models in different RE scenarios. All the code and datasets are released at https://github.com/thunlp/RE-Context-or-Names."
1155,https://arxiv.org/abs/2009.13964,CokeBERT: Contextual Knowledge Selection and Embedding towards Enhanced Pre-Trained Language Models,"Several recent efforts have been devoted to enhancing pre-trained language models (PLMs) by utilizing extra heterogeneous knowledge in knowledge graphs (KGs) and achieved consistent improvements on various knowledge-driven NLP tasks. However, most of these knowledge-enhanced PLMs embed static sub-graphs of KGs (""knowledge context""), regardless of that the knowledge required by PLMs may change dynamically according to specific text (""textual context""). In this paper, we propose a novel framework named Coke to dynamically select contextual knowledge and embed knowledge context according to textual context for PLMs, which can avoid the effect of redundant and ambiguous knowledge in KGs that cannot match the input text. Our experimental results show that Coke outperforms various baselines on typical knowledge-driven NLP tasks, indicating the effectiveness of utilizing dynamic knowledge context for language understanding. Besides the performance improvements, the dynamically selected knowledge in Coke can describe the semantics of text-related knowledge in a more interpretable form than the conventional PLMs. Our source code and datasets will be available to provide more details for Coke."
1156,https://arxiv.org/abs/2009.09226,Knowledge Transfer via Pre-training for Recommendation: A Review and Prospect,"Recommender systems aim to provide item recommendations for users, and are usually faced with data sparsity problem (e.g., cold start) in real-world scenarios. Recently pre-trained models have shown their effectiveness in knowledge transfer between domains and tasks, which can potentially alleviate the data sparsity problem in recommender systems. In this survey, we first provide a review of recommender systems with pre-training. In addition, we show the benefits of pre-training to recommender systems through experiments. Finally, we discuss several promising directions for future research for recommender systems with pre-training."
1157,https://arxiv.org/abs/2009.09192,Learning to Attack: Towards Textual Adversarial Attacking in Real-world Situations,"Adversarial attacking aims to fool deep neural networks with adversarial examples. In the field of natural language processing, various textual adversarial attack models have been proposed, varying in the accessibility to the victim model. Among them, the attack models that only require the output of the victim model are more fit for real-world situations of adversarial attacking. However, to achieve high attack performance, these models usually need to query the victim model too many times, which is neither efficient nor viable in practice. To tackle this problem, we propose a reinforcement learning based attack model, which can learn from attack history and launch attacks more efficiently. In experiments, we evaluate our model by attacking several state-of-the-art models on the benchmark datasets of multiple tasks including sentiment analysis, text classification and natural language inference. Experimental results demonstrate that our model consistently achieves both better attack performance and higher efficiency than recently proposed baseline methods. We also find our attack model can bring more robustness improvement to the victim model by adversarial training. All the code and data of this paper will be made public."
1158,https://arxiv.org/abs/2009.09191,OpenAttack: An Open-source Textual Adversarial Attack Toolkit,"Textual adversarial attacking has received wide and increasing attention in recent years. Various attack models have been proposed, which are enormously distinct and implemented with different programming frameworks and settings. These facts hinder quick utilization and fair comparison of attack models. In this paper, we present an open-source textual adversarial attack toolkit named OpenAttack to solve these issues. Compared with existing other textual adversarial attack toolkits, OpenAttack has its unique strengths in support for all attack types, multilinguality, and parallel processing. Currently, OpenAttack includes 15 typical attack models that cover all attack types. Its highly inclusive modular design not only supports quick utilization of existing attack models, but also enables great flexibility and extensibility. OpenAttack has broad uses including comparing and evaluating attack models, measuring robustness of a model, assisting in developing new attack models, and adversarial training. Source code and documentation can be obtained at https://github.com/thunlp/OpenAttack."
1159,https://arxiv.org/abs/2009.05817,Country Image in COVID-19 Pandemic: A Case Study of China,"Country image has a profound influence on international relations and economic development. In the worldwide outbreak of COVID-19, countries and their people display different reactions, resulting in diverse perceived images among foreign public. Therefore, in this study, we take China as a specific and typical case and investigate its image with aspect-based sentiment analysis on a large-scale Twitter dataset. To our knowledge, this is the first study to explore country image in such a fine-grained way. To perform the analysis, we first build a manually-labeled Twitter dataset with aspect-level sentiment annotations. Afterward, we conduct the aspect-based sentiment analysis with BERT to explore the image of China. We discover an overall sentiment change from non-negative to negative in the general public, and explain it with the increasing mentions of negative ideology-related aspects and decreasing mentions of non-negative fact-based aspects. Further investigations into different groups of Twitter users, including U.S. Congress members, English media, and social bots, reveal different patterns in their attitudes toward China. This study provides a deeper understanding of the changing image of China in COVID-19 pandemic. Our research also demonstrates how aspect-based sentiment analysis can be applied in social science researches to deliver valuable insights."
1160,https://arxiv.org/abs/2304.13596,Video Frame Interpolation with Densely Queried Bilateral Correlation,"Video Frame Interpolation (VFI) aims to synthesize non-existent intermediate frames between existent frames. Flow-based VFI algorithms estimate intermediate motion fields to warp the existent frames. Real-world motions' complexity and the reference frame's absence make motion estimation challenging. Many state-of-the-art approaches explicitly model the correlations between two neighboring frames for more accurate motion estimation. In common approaches, the receptive field of correlation modeling at higher resolution depends on the motion fields estimated beforehand. Such receptive field dependency makes common motion estimation approaches poor at coping with small and fast-moving objects. To better model correlations and to produce more accurate motion fields, we propose the Densely Queried Bilateral Correlation (DQBC) that gets rid of the receptive field dependency problem and thus is more friendly to small and fast-moving objects. The motion fields generated with the help of DQBC are further refined and up-sampled with context features. After the motion fields are fixed, a CNN-based SynthNet synthesizes the final interpolated frame. Experiments show that our approach enjoys higher accuracy and less inference time than the state-of-the-art. Source code is available at https://github.com/kinoud/DQBC."
1161,https://arxiv.org/abs/2302.07084,Towards Lightweight and Automated Representation Learning System for Networks,"We propose LIGHTNE 2.0, a cost-effective, scalable, automated, and high-quality network embedding system that scales to graphs with hundreds of billions of edges on a single machine. In contrast to the mainstream belief that distributed architecture and GPUs are needed for large-scale network embedding with good quality, we prove that we can achieve higher quality, better scalability, lower cost, and faster runtime with shared-memory, CPU-only architecture. LIGHTNE 2.0 combines two theoretically grounded embedding methods NetSMF and ProNE. We introduce the following techniques to network embedding for the first time: (1) a newly proposed downsampling method to reduce the sample complexity of NetSMF while preserving its theoretical advantages; (2) a high-performance parallel graph processing stack GBBS to achieve high memory efficiency and scalability; (3) sparse parallel hash table to aggregate and maintain the matrix sparsifier in memory; (4) a fast randomized singular value decomposition (SVD) enhanced by power iteration and fast orthonormalization to improve vanilla randomized SVD in terms of both efficiency and effectiveness; (5) Intel MKL for proposed fast randomized SVD and spectral propagation; and (6) a fast and lightweight AutoML library FLAML for automated hyperparameter tuning. Experimental results show that LIGHTNE 2.0 can be up to 84X faster than GraphVite, 30X faster than PBG and 9X faster than NetSMF while delivering better performance. LIGHTNE 2.0 can embed very large graph with 1.7 billion nodes and 124 billion edges in half an hour on a CPU server, while other baselines cannot handle very large graphs of this scale."
1162,https://arxiv.org/abs/2301.13442,Scaling laws for single-agent reinforcement learning,"Recent work has shown that, in generative modeling, cross-entropy loss improves smoothly with model size and training compute, following a power law plus constant scaling law. One challenge in extending these results to reinforcement learning is that the main performance objective of interest, mean episode return, need not vary smoothly. To overcome this, we introduce *intrinsic performance*, a monotonic function of the return defined as the minimum compute required to achieve the given return across a family of models of different sizes. We find that, across a range of environments, intrinsic performance scales as a power law in model size and environment interactions. Consequently, as in generative modeling, the optimal model size scales as a power law in the training compute budget. Furthermore, we study how this relationship varies with the environment and with other properties of the training setup. In particular, using a toy MNIST-based environment, we show that varying the ""horizon length"" of the task mostly changes the coefficient but not the exponent of this relationship."
1163,https://arxiv.org/abs/2211.16776,From Coarse to Fine: Hierarchical Pixel Integration for Lightweight Image Super-Resolution,"Image super-resolution (SR) serves as a fundamental tool for the processing and transmission of multimedia data. Recently, Transformer-based models have achieved competitive performances in image SR. They divide images into fixed-size patches and apply self-attention on these patches to model long-range dependencies among pixels. However, this architecture design is originated for high-level vision tasks, which lacks design guideline from SR knowledge. In this paper, we aim to design a new attention block whose insights are from the interpretation of Local Attribution Map (LAM) for SR networks. Specifically, LAM presents a hierarchical importance map where the most important pixels are located in a fine area of a patch and some less important pixels are spread in a coarse area of the whole image. To access pixels in the coarse area, instead of using a very large patch size, we propose a lightweight Global Pixel Access (GPA) module that applies cross-attention with the most similar patch in an image. In the fine area, we use an Intra-Patch Self-Attention (IPSA) module to model long-range pixel dependencies in a local patch, and then a $3\times3$ convolution is applied to process the finest details. In addition, a Cascaded Patch Division (CPD) strategy is proposed to enhance perceptual quality of recovered images. Extensive experiments suggest that our method outperforms state-of-the-art lightweight SR methods by a large margin. Code is available at https://github.com/passerer/HPINet."
1164,https://arxiv.org/abs/2211.14313,AICOM-MP: an AI-based Monkeypox Detector for Resource-Constrained Environments,"Under the Autonomous Mobile Clinics (AMCs) initiative, we are developing, open sourcing, and standardizing health AI technologies to enable healthcare access in least developed countries (LDCs). We deem AMCs as the next generation of health care delivery platforms, whereas health AI engines are applications on these platforms, similar to how various applications expand the usage scenarios of smart phones. Facing the recent global monkeypox outbreak, in this article, we introduce AICOM-MP, an AI-based monkeypox detector specially aiming for handling images taken from resource-constrained devices. Compared to existing AI-based monkeypox detectors, AICOM-MP has achieved state-of-the-art (SOTA) performance. We have hosted AICOM-MP as a web service to allow universal access to monkeypox screening technology. We have also open sourced both the source code and the dataset of AICOM-MP to allow health AI professionals to integrate AICOM-MP into their services. Also, through the AICOM-MP project, we have generalized a methodology of developing health AI technologies for AMCs to allow universal access even in resource-constrained environments."
1165,https://arxiv.org/abs/2210.16771,Parameter-Efficient Tuning Makes a Good Classification Head,"In recent years, pretrained models revolutionized the paradigm of natural language understanding (NLU), where we append a randomly initialized classification head after the pretrained backbone, e.g. BERT, and finetune the whole model. As the pretrained backbone makes a major contribution to the improvement, we naturally expect a good pretrained classification head can also benefit the training. However, the final-layer output of the backbone, i.e. the input of the classification head, will change greatly during finetuning, making the usual head-only pretraining (LP-FT) ineffective. In this paper, we find that parameter-efficient tuning makes a good classification head, with which we can simply replace the randomly initialized heads for a stable performance gain. Our experiments demonstrate that the classification head jointly pretrained with parameter-efficient tuning consistently improves the performance on 9 tasks in GLUE and SuperGLUE."
1166,https://arxiv.org/abs/2207.00737,Brief Industry Paper: The Necessity of Adaptive Data Fusion in Infrastructure-Augmented Autonomous Driving System,"This paper is the first to provide a thorough system design overview along with the fusion methods selection criteria of a real-world cooperative autonomous driving system, named Infrastructure-Augmented Autonomous Driving or IAAD. We present an in-depth introduction of the IAAD hardware and software on both road-side and vehicle-side computing and communication platforms. We extensively characterize the IAAD system in the context of real-world deployment scenarios and observe that the network condition that fluctuates along the road is currently the main technical roadblock for cooperative autonomous driving. To address this challenge, we propose new fusion methods, dubbed ""inter-frame fusion"" and ""planning fusion"" to complement the current state-of-the-art ""intra-frame fusion"". We demonstrate that each fusion method has its own benefit and constraint."
1167,https://arxiv.org/abs/2206.11795,Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos,"Pretraining on noisy, internet-scale datasets has been heavily studied as a technique for training models with broad, general capabilities for text, images, and other modalities. However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way. We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos. Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data -- here, online videos of people playing Minecraft -- from which we can then train a general behavioral prior. Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-shot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning. For many tasks our models exhibit human-level performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish."
1168,https://arxiv.org/abs/2206.08610,Masked Autoencoders for Generic Event Boundary Detection CVPR'2022 Kinetics-GEBD Challenge,"Generic Event Boundary Detection (GEBD) tasks aim at detecting generic, taxonomy-free event boundaries that segment a whole video into chunks. In this paper, we apply Masked Autoencoders to improve algorithm performance on the GEBD tasks. Our approach mainly adopted the ensemble of Masked Autoencoders fine-tuned on the GEBD task as a self-supervised learner with other base models. Moreover, we also use a semi-supervised pseudo-label method to take full advantage of the abundant unlabeled Kinetics-400 data while training. In addition, we propose a soft-label method to partially balance the positive and negative samples and alleviate the problem of ambiguous labeling in this task. Lastly, a tricky segmentation alignment policy is implemented to refine boundaries predicted by our models to more accurate locations. With our approach, we achieved 85.94% on the F1-score on the Kinetics-GEBD test set, which improved the F1-score by 2.31% compared to the winner of the 2021 Kinetics-GEBD Challenge. Our code is available at https://github.com/ContentAndMaterialPortrait/MAE-GEBD."
1169,https://arxiv.org/abs/2205.15868,CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers,"Large-scale pretrained transformers have created milestones in text (GPT-3) and text-to-image (DALL-E and CogView) generation. Its application to video generation is still facing many challenges: The potential huge computation cost makes the training from scratch unaffordable; The scarcity and weak relevance of text-video datasets hinder the model understanding complex movement semantics. In this work, we present 9B-parameter transformer CogVideo, trained by inheriting a pretrained text-to-image model, CogView2. We also propose multi-frame-rate hierarchical training strategy to better align text and video clips. As (probably) the first open-source large-scale pretrained text-to-video model, CogVideo outperforms all publicly available models at a large margin in machine and human evaluations."
1170,https://arxiv.org/abs/2205.14403,Rethinking the Setting of Semi-supervised Learning on Graphs,"We argue that the present setting of semisupervised learning on graphs may result in unfair comparisons, due to its potential risk of over-tuning hyper-parameters for models. In this paper, we highlight the significant influence of tuning hyper-parameters, which leverages the label information in the validation set to improve the performance. To explore the limit of over-tuning hyperparameters, we propose ValidUtil, an approach to fully utilize the label information in the validation set through an extra group of hyper-parameters. With ValidUtil, even GCN can easily get high accuracy of 85.8% on Cora.
  To avoid over-tuning, we merge the training set and the validation set and construct an i.i.d. graph benchmark (IGB) consisting of 4 datasets. Each dataset contains 100 i.i.d. graphs sampled from a large graph to reduce the evaluation variance. Our experiments suggest that IGB is a more stable benchmark than previous datasets for semisupervised learning on graphs."
1171,https://arxiv.org/abs/2205.10475,DeepStruct: Pretraining of Language Models for Structure Prediction,"We introduce a method for improving the structural understanding abilities of language models. Unlike previous approaches that finetune the models with task-specific augmentation, we pretrain language models on a collection of task-agnostic corpora to generate structures from text. Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks. We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretraining with the task-specific training sets. We show that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that we evaluate."
1172,https://arxiv.org/abs/2205.05675,NTIRE 2022 Challenge on Efficient Super-Resolution: Methods and Results,"This paper reviews the NTIRE 2022 challenge on efficient single image super-resolution with focus on the proposed solutions and results. The task of the challenge was to super-resolve an input image with a magnification factor of $\times$4 based on pairs of low and corresponding high resolution images. The aim was to design a network for single image super-resolution that achieved improvement of efficiency measured according to several metrics including runtime, parameters, FLOPs, activations, and memory consumption while at least maintaining the PSNR of 29.00dB on DIV2K validation set. IMDN is set as the baseline for efficiency measurement. The challenge had 3 tracks including the main track (runtime), sub-track one (model complexity), and sub-track two (overall performance). In the main track, the practical runtime performance of the submissions was evaluated. The rank of the teams were determined directly by the absolute value of the average runtime on the validation set and test set. In sub-track one, the number of parameters and FLOPs were considered. And the individual rankings of the two metrics were summed up to determine a final ranking in this track. In sub-track two, all of the five metrics mentioned in the description of the challenge including runtime, parameter count, FLOPs, activations, and memory consumption were considered. Similar to sub-track one, the rankings of five metrics were summed up to determine a final ranking. The challenge had 303 registered participants, and 43 teams made valid submissions. They gauge the state-of-the-art in efficient single image super-resolution."
1173,https://arxiv.org/abs/2204.14217,CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers,"The development of the transformer-based text-to-image models are impeded by its slow generation and complexity for high-resolution images. In this work, we put forward a solution based on hierarchical transformers and local parallel auto-regressive generation. We pretrain a 6B-parameter transformer with a simple and flexible self-supervised task, Cross-modal general language model (CogLM), and finetune it for fast super-resolution. The new text-to-image system, CogView2, shows very competitive generation compared to concurrent state-of-the-art DALL-E-2, and naturally supports interactive text-guided editing on images."
1174,https://arxiv.org/abs/2204.08397,Fast and Memory-Efficient Network Towards Efficient Image Super-Resolution,"Runtime and memory consumption are two important aspects for efficient image super-resolution (EISR) models to be deployed on resource-constrained devices. Recent advances in EISR exploit distillation and aggregation strategies with plenty of channel split and concatenation operations to make full use of limited hierarchical features. In contrast, sequential network operations avoid frequently accessing preceding states and extra nodes, and thus are beneficial to reducing the memory consumption and runtime overhead. Following this idea, we design our lightweight network backbone by mainly stacking multiple highly optimized convolution and activation layers and decreasing the usage of feature fusion. We propose a novel sequential attention branch, where every pixel is assigned an important factor according to local and global contexts, to enhance high-frequency details. In addition, we tailor the residual block for EISR and propose an enhanced residual block (ERB) to further accelerate the network inference. Finally, combining all the above techniques, we construct a fast and memory-efficient network (FMEN) and its small version FMEN-S, which runs 33% faster and reduces 74% memory consumption compared with the state-of-the-art EISR model: E-RFDN, the champion in AIM 2020 efficient super-resolution challenge. Besides, FMEN-S achieves the lowest memory consumption and the second shortest runtime in NTIRE 2022 challenge on efficient super-resolution. Code is available at https://github.com/NJU-Jet/FMEN."
1175,https://arxiv.org/abs/2203.11480,WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models,"Compared with the domain-specific model, the vision-language pre-training models (VLPMs) have shown superior performance on downstream tasks with fast fine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with a uniform transformers stack architecture and large amounts of image-text paired data, achieving remarkable results on downstream tasks such as image-text reference(IR and TR), vision question answering (VQA) and image captioning (IC) etc. During the training phase, VLPMs are always fed with a combination of multiple public datasets to meet the demand of large-scare training data. However, due to the unevenness of data distribution including size, task type and quality, using the mixture of multiple datasets for model training can be problematic. In this work, we introduce a large-scale multi-modal corpora named WuDaoMM, totally containing more than 650M image-text pairs. Specifically, about 600 million pairs of data are collected from multiple webpages in which image and caption present weak correlation, and the other 50 million strong-related image-text pairs are collected from some high-quality graphic websites. We also release a base version of WuDaoMM with 5 million strong-correlated image-text pairs, which is sufficient to support the common cross-modal model pre-training. Besides, we trained both an understanding and a generation vision-language (VL) model to test the dataset effectiveness. The results show that WuDaoMM can be applied as an efficient dataset for VLPMs, especially for the model in text-to-image generation task. The data is released at https://data.wudaoai.cn"
1176,https://arxiv.org/abs/2203.11011,Reinforced MOOCs Concept Recommendation in Heterogeneous Information Networks,"Massive open online courses (MOOCs), which offer open access and widespread interactive participation through the internet, are quickly becoming the preferred method for online and remote learning. Several MOOC platforms offer the service of course recommendation to users, to improve the learning experience of users. Despite the usefulness of this service, we consider that recommending courses to users directly may neglect their varying degrees of expertise. To mitigate this gap, we examine an interesting problem of concept recommendation in this paper, which can be viewed as recommending knowledge to users in a fine-grained way. We put forward a novel approach, termed HinCRec-RL, for Concept Recommendation in MOOCs, which is based on Heterogeneous Information Networks and Reinforcement Learning. In particular, we propose to shape the problem of concept recommendation within a reinforcement learning framework to characterize the dynamic interaction between users and knowledge concepts in MOOCs. Furthermore, we propose to form the interactions among users, courses, videos, and concepts into a heterogeneous information network (HIN) to learn the semantic user representations better. We then employ an attentional graph neural network to represent the users in the HIN, based on meta-paths. Extensive experiments are conducted on a real-world dataset collected from a Chinese MOOC platform, XuetangX, to validate the efficacy of our proposed HinCRec-RL. Experimental results and analysis demonstrate that our proposed HinCRec-RL performs well when comparing with several state-of-the-art models."
1177,https://arxiv.org/abs/2202.13296,Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering,"Recent works on knowledge base question answering (KBQA) retrieve subgraphs for easier reasoning. A desired subgraph is crucial as a small one may exclude the answer but a large one might introduce more noises. However, the existing retrieval is either heuristic or interwoven with the reasoning, causing reasoning on the partial subgraphs, which increases the reasoning bias when the intermediate supervision is missing. This paper proposes a trainable subgraph retriever (SR) decoupled from the subsequent reasoning process, which enables a plug-and-play framework to enhance any subgraph-oriented KBQA model. Extensive experiments demonstrate SR achieves significantly better retrieval and QA performance than existing retrieval methods. Via weakly supervised pre-training as well as the end-to-end fine-tuning, SRl achieves new state-of-the-art performance when combined with NSM, a subgraph-oriented reasoner, for embedding-based KBQA methods."
1178,https://arxiv.org/abs/2201.12565,Active IRS Aided Multiple Access for Energy-Constrained IoT Systems,"We investigate the fundamental multiple access (MA) scheme in an active intelligent reflecting surface (IRS) aided energy-constrained Internet-of-Things (IoT) system, where an active IRS is deployed to assist the uplink transmission from multiple IoT devices to an access point (AP). Our goal is to maximize the sum throughput by optimizing the IRS beamforming vectors across time and resource allocation. To this end, we first study two typical active IRS aided MA schemes, namely time division multiple access (TDMA) and non-orthogonal multiple access (NOMA), by analytically comparing their achievable sum throughput and proposing corresponding algorithms. Interestingly, we prove that given only one available IRS beamforming vector, the NOMA-based scheme generally achieves a larger throughput than the TDMA-based scheme, whereas the latter can potentially outperform the former if multiple IRS beamforming vectors are available to harness the favorable time selectivity of the IRS. To strike a flexible balance between the system performance and the associated signaling overhead incurred by more IRS beamforming vectors, we then propose a general hybrid TDMA-NOMA scheme with user grouping, where the devices in the same group transmit simultaneously via NOMA while devices in different groups occupy orthogonal time slots. By controlling the number of groups, the hybrid TDMA-NOMA scheme is applicable for any given number of IRS beamforming vectors available. Despite of the non-convexity of the considered optimization problem, we propose an efficient algorithm based on alternating optimization. Simulation results illustrate the practical superiorities of the active IRS over the passive IRS in terms of the coverage extension and supporting multiple energy-limited devices, and demonstrate the effectiveness of our proposed hybrid MA scheme for flexibly balancing the performance-cost tradeoff."
1179,https://arxiv.org/abs/2111.13905,AdaDM: Enabling Normalization for Image Super-Resolution,"Normalization like Batch Normalization (BN) is a milestone technique to normalize the distributions of intermediate layers in deep learning, enabling faster training and better generalization accuracy. However, in fidelity image Super-Resolution (SR), it is believed that normalization layers get rid of range flexibility by normalizing the features and they are simply removed from modern SR networks. In this paper, we study this phenomenon quantitatively and qualitatively. We found that the standard deviation of the residual feature shrinks a lot after normalization layers, which causes the performance degradation in SR networks. Standard deviation reflects the amount of variation of pixel values. When the variation becomes smaller, the edges will become less discriminative for the network to resolve. To address this problem, we propose an Adaptive Deviation Modulator (AdaDM), in which a modulation factor is adaptively predicted to amplify the pixel deviation. For better generalization performance, we apply BN in state-of-the-art SR networks with the proposed AdaDM. Meanwhile, the deviation amplification strategy in AdaDM makes the edge information in the feature more distinguishable. As a consequence, SR networks with BN and our AdaDM can get substantial performance improvements on benchmark datasets. Extensive experiments have been conducted to show the effectiveness of our method."
1180,https://arxiv.org/abs/2111.10772,Network representation learning: A macro and micro view,"Graph is a universe data structure that is widely used to organize data in real-world. Various real-word networks like the transportation network, social and academic network can be represented by graphs. Recent years have witnessed the quick development on representing vertices in the network into a low-dimensional vector space, referred to as network representation learning. Representation learning can facilitate the design of new algorithms on the graph data. In this survey, we conduct a comprehensive review of current literature on network representation learning. Existing algorithms can be categorized into three groups: shallow embedding models, heterogeneous network embedding models, graph neural network based models. We review state-of-the-art algorithms for each category and discuss the essential differences between these algorithms. One advantage of the survey is that we systematically study the underlying theoretical foundations underlying the different categories of algorithms, which offers deep insights for better understanding the development of the network representation learning field."
1181,https://arxiv.org/abs/2111.07658,Calculating Question Similarity is Enough: A New Method for KBQA Tasks,"Knowledge Base Question Answering (KBQA) aims to answer natural language questions with the help of an external knowledge base. The core idea is to find the link between the internal knowledge behind questions and known triples of the knowledge base. Traditional KBQA task pipelines contain several steps, including entity recognition, entity linking, answering selection, etc. In this kind of pipeline methods, errors in any procedure will inevitably propagate to the final prediction. To address this challenge, this paper proposes a Corpus Generation - Retrieve Method (CGRM) with Pre-training Language Model (PLM) for the KBQA task. The major novelty lies in the design of the new method, wherein our approach, the knowledge enhanced T5 (kT5) model aims to generate natural language QA pairs based on Knowledge Graph triples and directly solve the QA by retrieving the synthetic dataset. The new method can extract more information about the entities from PLM to improve accuracy and simplify the processes. We test our method on NLPCC-ICCPOL 2016 KBQA dataset, and the results show that our method improves the performance of KBQA and the out straight-forward method is competitive with the state-of-the-art."
1182,https://arxiv.org/abs/2110.12782,NetMF+: Network Embedding Based on Fast and Effective Single-Pass Randomized Matrix Factorization,"In this work, we propose NetMF+, a fast, memory-efficient, scalable, and effective network embedding algorithm developed for a single machine with CPU only. NetMF+ is based on the theoretically grounded embedding method NetMF and leverages the theories from randomized matrix factorization to learn embedding efficiently. We firstly propose a fast randomized eigen-decomposition algorithm for the modified Laplacian matrix. Then, sparse-sign randomized single-pass singular value decomposition (SVD) is utilized to avoid constructing dense matrix and generate promising embedding. To enhance the performance of embedding, we apply spectral propagation in NetMF+. Finally, A high-performance parallel graph processing stack GBBS is used to achieve memory-efficiency. Experiment results show that NetMF+ can learn a powerful embedding from a network with more than 10^11 edges within 1.5 hours at lower memory cost than state-of-the-art methods. The result on ClueWeb with 0.9 billion vertices and 75 billion edges shows that NetMF+ saves more than half of the memory and runtime than the state-of-the-art and has better performance. The source code of NetMF+ will be publicly available after the anonymous peer review."
1183,https://arxiv.org/abs/2110.07602,P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks,"Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning \cite{li2021prefix,qin2021learning} optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.Our code and data are released at https://github.com/THUDM/P-tuning-v2."
1184,https://arxiv.org/abs/2109.12742,FewNLU: Benchmarking State-of-the-Art Methods for Few-Shot Natural Language Understanding,"The few-shot natural language understanding (NLU) task has attracted much recent attention. However, prior methods have been evaluated under a disparate set of protocols, which hinders fair comparison and measuring progress of the field. To address this issue, we introduce an evaluation framework that improves previous evaluation procedures in three key aspects, i.e., test performance, dev-test correlation, and stability. Under this new evaluation framework, we re-evaluate several state-of-the-art few-shot methods for NLU tasks. Our framework reveals new insights: (1) both the absolute performance and relative gap of the methods were not accurately estimated in prior literature; (2) no single method dominates most tasks with consistent performance; (3) improvements of some methods diminish with a larger pretrained model; and (4) gains from different methods are often complementary and the best combined model performs close to a strong fully-supervised baseline. We open-source our toolkit, FewNLU, that implements our evaluation framework along with a number of state-of-the-art methods."
1185,https://arxiv.org/abs/2109.11171,Zero-Shot Information Extraction as a Unified Text-to-Triple Translation,"We cast a suite of information extraction tasks into a text-to-triple translation framework. Instead of solving each task relying on task-specific datasets and models, we formalize the task as a translation between task-specific input text and output triples. By taking the task-specific input, we enable a task-agnostic translation by leveraging the latent knowledge that a pre-trained language model has about the task. We further demonstrate that a simple pre-training task of predicting which relational information corresponds to which input text is an effective way to produce task-specific outputs. This enables the zero-shot transfer of our framework to downstream tasks. We study the zero-shot performance of this framework on open information extraction (OIE2016, NYT, WEB, PENN), relation classification (FewRel and TACRED), and factual probe (Google-RE and T-REx). The model transfers non-trivially to most tasks and is often competitive with a fully supervised method without the need for any task-specific training. For instance, we significantly outperform the F1 score of the supervised open information extraction without needing to use its training set."
1186,https://arxiv.org/abs/2108.08188,Airborne Quantum Key Distribution with Boundary Layer Effects,"Airborne quantum key distribution (QKD) is now becoming a flexible bond between terrestrial fiber and satellite, which is an efficient solution to establish a mobile, on-demand, and real-time coverage quantum network. Furthermore, When the aircraft is flying at a high speed, usually larger than 0.3 Ma, the produced boundary layer will impair the performance of aircraft-based QKD. The boundary layer would introduce random wavefront aberration, jitter and extra intensity attenuation to the transmitted photons. However, previous airborne QKD implementations only considered the influences from atmospheric turbulence and molecular scattering, but ignored the boundary layer effects. In this article, we propose a detailed performance evaluation scheme of airborne QKD with boundary layer effects and estimate the overall photon transmission efficiency, quantum bit error rate and final secure key rate. Through simulations and modeling, in our proposed airborne QKD scenario, the boundary layer would introduce 3.5dB loss to the transmitted photons and decrease 70.7% of the secure key rate, which shows that the aero-optical effects caused by the boundary layer can not be ignored. With tolerated quantum bit error rate set to 10%, the suggested quantum communication azimuth angle between the aircraft and the ground station is within 60 degrees. Moreover, the optimal beacon laser module and adaptive optics module are suggested to be employed to improve the performance of airborne QKD system. Our detailed airborne QKD evaluation study can be performed to the future airborne quantum communication designs."
1187,https://arxiv.org/abs/2108.07435,Modeling Protein Using Large-scale Pretrain Language Model,"Protein is linked to almost every life process. Therefore, analyzing the biological structure and property of protein sequences is critical to the exploration of life, as well as disease detection and drug discovery. Traditional protein analysis methods tend to be labor-intensive and time-consuming. The emergence of deep learning models makes modeling data patterns in large quantities of data possible. Interdisciplinary researchers have begun to leverage deep learning methods to model large biological datasets, e.g. using long short-term memory and convolutional neural network for protein sequence classification. After millions of years of evolution, evolutionary information is encoded in protein sequences. Inspired by the similarity between natural language and protein sequences, we use large-scale language models to model evolutionary-scale protein sequences, encoding protein biology information in representation. Significant improvements are observed in both token-level and sequence-level tasks, demonstrating that our large-scale model can accurately capture evolution information from pretraining on evolutionary-scale individual sequences. Our code and model are available at https://github.com/THUDM/ProteinLM."
1188,https://arxiv.org/abs/2108.06332,FlipDA: Effective and Robust Data Augmentation for Few-Shot Learning,"Most previous methods for text data augmentation are limited to simple tasks and weak baselines. We explore data augmentation on hard tasks (i.e., few-shot natural language understanding) and strong baselines (i.e., pretrained models with over one billion parameters). Under this setting, we reproduced a large number of previous augmentation methods and found that these methods bring marginal gains at best and sometimes degrade the performance much. To address this challenge, we propose a novel data augmentation method FlipDA that jointly uses a generative model and a classifier to generate label-flipped data. Central to the idea of FlipDA is the discovery that generating label-flipped data is more crucial to the performance than generating label-preserved data. Experiments show that FlipDA achieves a good tradeoff between effectiveness and robustness -- it substantially improves many tasks while not negatively affecting the others."
1189,https://arxiv.org/abs/2107.03374,Evaluating Large Language Models Trained on Code,"We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics."
1190,https://arxiv.org/abs/2106.14876,"Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft","An important challenge in reinforcement learning is training agents that can solve a wide variety of tasks. If tasks depend on each other (e.g. needing to learn to walk before learning to run), curriculum learning can speed up learning by focusing on the next best task to learn. We explore curriculum learning in a complex, visual domain with many hard exploration challenges: Minecraft. We find that learning progress (defined as a change in success probability of a task) is a reliable measure of learnability for automatically constructing an effective curriculum. We introduce a learning-progress based curriculum and test it on a complex reinforcement learning problem (called ""Simon Says"") where an agent is instructed to obtain a desired goal item. Many of the required skills depend on each other. Experiments demonstrate that: (1) a within-episode exploration bonus for obtaining new items improves performance, (2) dynamically adjusting this bonus across training such that it only applies to items the agent cannot reliably obtain yet further increases performance, (3) the learning-progress based curriculum elegantly follows the learning curve of the agent, and (4) when the learning-progress based curriculum is combined with the dynamic exploration bonus it learns much more efficiently and obtains far higher performance than uniform baselines. These results suggest that combining intra-episode and across-training exploration bonuses with learning progress creates a promising method for automated curriculum generation, which may substantially increase our ability to train more capable, generally intelligent agents."
1191,https://arxiv.org/abs/2106.11534,"Turing Award elites revisited: patterns of productivity, collaboration, authorship and impact","The Turing Award is recognized as the most influential and prestigious award in the field of computer science(CS). With the rise of the science of science (SciSci), a large amount of bibliographic data has been analyzed in an attempt to understand the hidden mechanism of scientific evolution. These include the analysis of the Nobel Prize, including physics, chemistry, medicine, etc. In this article, we extract and analyze the data of 72 Turing Award laureates from the complete bibliographic data, fill the gap in the lack of Turing Award analysis, and discover the development characteristics of computer science as an independent discipline. First, we show most Turing Award laureates have long-term and high-quality educational backgrounds, and more than 61% of them have a degree in mathematics, which indicates that mathematics has played a significant role in the development of computer science. Secondly, the data shows that not all scholars have high productivity and high h-index; that is, the number of publications and h-index is not the leading indicator for evaluating the Turing Award. Third, the average age of awardees has increased from 40 to around 70 in recent years. This may be because new breakthroughs take longer, and some new technologies need time to prove their influence. Besides, we have also found that in the past ten years, international collaboration has experienced explosive growth, showing a new paradigm in the form of collaboration. It is also worth noting that in recent years, the emergence of female winners has also been eye-catching. Finally, by analyzing the personal publication records, we find that many people are more likely to publish high-impact articles during their high-yield periods."
1192,https://arxiv.org/abs/2106.10405,Cascaded Channel Estimation for RIS Assisted mmWave MIMO Transmissions,"Channel estimation is challenging for the reconfigurable intelligence surface (RIS) assisted millimeter wave (mmWave) communications. Since the number of coefficients of the cascaded channels in such systems is closely dependent on the product of the number of base station antennas and the number of RIS elements, the pilot overhead would be prohibitively high. In this letter, we propose a cascaded channel estimation framework for an RIS assisted mmWave multiple-input multiple-output system, where the wideband effect on transmission model is considered. Then, we transform the wideband channel estimation into a parameter recovery problem and use a few pilot symbols to detect the channel parameters by the Newtonized orthogonal matching pursuit algorithm. Moreover, the Cramer-Rao lower bound on the channel estimation is introduced. Numerical results show the effectiveness of the proposed channel estimation scheme."
1193,https://arxiv.org/abs/2106.00958,A Generalizable Approach to Learning Optimizers,"A core issue with learning to optimize neural networks has been the lack of generalization to real world problems. To address this, we describe a system designed from a generalization-first perspective, learning to update optimizer hyperparameters instead of model parameters directly using novel features, actions, and a reward function. This system outperforms Adam at all neural network tasks including on modalities not seen during training. We achieve 2x speedups on ImageNet, and a 2.5x speedup on a language modeling task using over 5 orders of magnitude more compute than the training tasks."
1194,https://arxiv.org/abs/2105.14211,M6-UFC: Unifying Multi-Modal Controls for Conditional Image Synthesis via Non-Autoregressive Generative Transformers,"Conditional image synthesis aims to create an image according to some multi-modal guidance in the forms of textual descriptions, reference images, and image blocks to preserve, as well as their combinations. In this paper, instead of investigating these control signals separately, we propose a new two-stage architecture, M6-UFC, to unify any number of multi-modal controls. In M6-UFC, both the diverse control signals and the synthesized image are uniformly represented as a sequence of discrete tokens to be processed by Transformer. Different from existing two-stage autoregressive approaches such as DALL-E and VQGAN, M6-UFC adopts non-autoregressive generation (NAR) at the second stage to enhance the holistic consistency of the synthesized image, to support preserving specified image blocks, and to improve the synthesis speed. Further, we design a progressive algorithm that iteratively improves the non-autoregressively generated image, with the help of two estimators developed for evaluating the compliance with the controls and evaluating the fidelity of the synthesized image, respectively. Extensive experiments on a newly collected large-scale clothing dataset M2C-Fashion and a facial dataset Multi-Modal CelebA-HQ verify that M6-UFC can synthesize high-fidelity images that comply with flexible multi-modal controls."
1195,https://arxiv.org/abs/2105.13290,CogView: Mastering Text-to-Image Generation via Transformers,"Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E."
1196,https://arxiv.org/abs/2105.09750,Anchor-based Plain Net for Mobile Image Super-Resolution,"Along with the rapid development of real-world applications, higher requirements on the accuracy and efficiency of image super-resolution (SR) are brought forward. Though existing methods have achieved remarkable success, the majority of them demand plenty of computational resources and large amount of RAM, and thus they can not be well applied to mobile device. In this paper, we aim at designing efficient architecture for 8-bit quantization and deploy it on mobile device. First, we conduct an experiment about meta-node latency by decomposing lightweight SR architectures, which determines the portable operations we can utilize. Then, we dig deeper into what kind of architecture is beneficial to 8-bit quantization and propose anchor-based plain net (ABPN). Finally, we adopt quantization-aware training strategy to further boost the performance. Our model can outperform 8-bit quantized FSRCNN by nearly 2dB in terms of PSNR, while satisfying realistic needs at the same time. Code is avaliable at https://github.com/NJU- Jet/SR_Mobile_Quantization."
1197,https://arxiv.org/abs/2103.16045,The Matter of Time -- A General and Efficient System for Precise Sensor Synchronization in Robotic Computing,"Time synchronization is a critical task in robotic computing such as autonomous driving. In the past few years, as we developed advanced robotic applications, our synchronization system has evolved as well. In this paper, we first introduce the time synchronization problem and explain the challenges of time synchronization, especially in robotic workloads. Summarizing these challenges, we then present a general hardware synchronization system for robotic computing, which delivers high synchronization accuracy while maintaining low energy and resource consumption. The proposed hardware synchronization system is a key building block in our future robotic products."
1198,https://arxiv.org/abs/2103.13262,FastMoE: A Fast Mixture-of-Expert Training System,"Mixture-of-Expert (MoE) presents a strong potential in enlarging the size of language model to trillions of parameters. However, training trillion-scale MoE requires algorithm and system co-design for a well-tuned high performance distributed training system. Unfortunately, the only existing platform that meets the requirements strongly depends on Google's hardware (TPU) and software (Mesh Tensorflow) stack, and is not open and available to the public, especially GPU and PyTorch communities.
  In this paper, we present FastMoE, a distributed MoE training system based on PyTorch with common accelerators. The system provides a hierarchical interface for both flexible model design and easy adaption to different applications, such as Transformer-XL and Megatron-LM. Different from direct implementation of MoE models using PyTorch, the training speed is highly optimized in FastMoE by sophisticated high-performance acceleration skills. The system supports placing different experts on multiple GPUs across multiple nodes, enabling enlarging the number of experts linearly against the number of GPUs. The source of FastMoE is available at https://github.com/laekov/fastmoe under Apache-2 license."
1199,https://arxiv.org/abs/2103.10685,Controllable Generation from Pre-trained Language Models via Inverse Prompting,"Large-scale pre-trained language models have demonstrated strong capabilities of generating realistic text. However, it remains challenging to control the generation results. Previous approaches such as prompting are far from sufficient, which limits the usage of language models. To tackle this challenge, we propose an innovative method, inverse prompting, to better control text generation. The core idea of inverse prompting is to use generated text to inversely predict the prompt during beam search, which enhances the relevance between the prompt and the generated text and provides better controllability. Empirically, we pre-train a large-scale Chinese language model to perform a systematic study using human evaluation on the tasks of open-domain poem generation and open-domain long-form question answering. Our results show that our proposed method substantially outperforms the baselines and that our generation quality is close to human performance on some of the tasks.
  Narrators can try our poem generation demo at https://pretrain.aminer.cn/apps/poetry.html, while our QA demo can be found at https://pretrain.aminer.cn/app/qa. For researchers, the code is provided in https://github.com/THUDM/InversePrompting."
1200,https://arxiv.org/abs/2103.10385,"GPT Understands, Too","While GPTs with traditional fine-tuning fail to achieve strong results on natural language understanding (NLU), we show that GPTs can be better than or comparable to similar-sized BERTs on NLU tasks with a novel method P-tuning -- which employs trainable continuous prompt embeddings. On the knowledge probing (LAMA) benchmark, the best GPT recovers 64\% (P@1) of world knowledge without any additional text provided during test time, which substantially improves the previous best by 20+ percentage points. On the SuperGlue benchmark, GPTs achieve comparable and sometimes better performance to similar-sized BERTs in supervised learning. Importantly, we find that P-tuning also improves BERTs' performance in both few-shot and supervised settings while largely reducing the need for prompt engineering. Consequently, P-tuning outperforms the state-of-the-art approaches on the few-shot SuperGlue benchmark."
1201,https://arxiv.org/abs/2103.10360,GLM: General Language Model Pretraining with Autoregressive Blank Infilling,"There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25x parameters of BERT Large , demonstrating its generalizability to different downstream tasks."
1202,https://arxiv.org/abs/2103.02176,Towards Fully Intelligent Transportation through Infrastructure-Vehicle Cooperative Autonomous Driving: Challenges and Opportunities,"The infrastructure-vehicle cooperative autonomous driving approach depends on the cooperation between intelligent roads and intelligent vehicles. This approach is not only safer but also more economical compared to the traditional on-vehicle-only autonomous driving approach. In this paper, we introduce our real-world deployment experiences of cooperative autonomous driving, and delve into the details of new challenges and opportunities. Specifically, based on our progress towards commercial deployment, we follow a three-stage development roadmap of the cooperative autonomous driving approach:infrastructure-augmented autonomous driving (IAAD), infrastructure-guided autonomous driving (IGAD), and infrastructure-planned autonomous driving (IPAD)."
1203,https://arxiv.org/abs/2103.00823,M6: A Chinese Multimodal Pretrainer,"In this work, we construct the largest dataset for multimodal pretraining in Chinese, which consists of over 1.9TB images and 292GB texts that cover a wide range of domains. We propose a cross-modal pretraining method called M6, referring to Multi-Modality to Multi-Modality Multitask Mega-transformer, for unified pretraining on the data of single modality and multiple modalities. We scale the model size up to 10 billion and 100 billion parameters, and build the largest pretrained model in Chinese. We apply the model to a series of downstream applications, and demonstrate its outstanding performance in comparison with strong baselines. Furthermore, we specifically design a downstream task of text-guided image generation, and show that the finetuned M6 can create high-quality images with high resolution and abundant details."
1204,https://arxiv.org/abs/2012.11336,CODE: Contrastive Pre-training with Adversarial Fine-tuning for Zero-shot Expert Linking,"Expert finding, a popular service provided by many online websites such as Expertise Finder, LinkedIn, and AMiner, is beneficial to seeking candidate qualifications, consultants, and collaborators. However, its quality is suffered from lack of ample sources of expert information. This paper employs AMiner as the basis with an aim at linking any external experts to the counterparts on AMiner. As it is infeasible to acquire sufficient linkages from arbitrary external sources, we explore the problem of zero-shot expert linking. In this paper, we propose CODE, which first pre-trains an expert linking model by contrastive learning on AMiner such that it can capture the representation and matching patterns of experts without supervised signals, then it is fine-tuned between AMiner and external sources to enhance the models transferability in an adversarial manner. For evaluation, we first design two intrinsic tasks, author identification and paper clustering, to validate the representation and matching capability endowed by contrastive learning. Then the final external expert linking performance on two genres of external sources also implies the superiority of the adversarial fine-tuning method. Additionally, we show the online deployment of CODE, and continuously improve its online performance via active learning."
1205,https://arxiv.org/abs/2012.01353,Eudoxus: Characterizing and Accelerating Localization in Autonomous Machines,"We develop and commercialize autonomous machines, such as logistic robots and self-driving cars, around the globe. A critical challenge to our -- and any -- autonomous machine is accurate and efficient localization under resource constraints, which has fueled specialized localization accelerators recently. Prior acceleration efforts are point solutions in that they each specialize for a specific localization algorithm. In real-world commercial deployments, however, autonomous machines routinely operate under different environments and no single localization algorithm fits all the environments. Simply stacking together point solutions not only leads to cost and power budget overrun, but also results in an overly complicated software stack.
  This paper demonstrates our new software-hardware co-designed framework for autonomous machine localization, which adapts to different operating scenarios by fusing fundamental algorithmic primitives. Through characterizing the software framework, we identify ideal acceleration candidates that contribute significantly to the end-to-end latency and/or latency variation. We show how to co-design a hardware accelerator to systematically exploit the parallelisms, locality, and common building blocks inherent in the localization framework. We build, deploy, and evaluate an FPGA prototype on our next-generation self-driving cars. To demonstrate the flexibility of our framework, we also instantiate another FPGA prototype targeting drones, which represent mobile autonomous machines. We achieve about 2x speedup and 4x energy reduction compared to widely-deployed, optimized implementations on general-purpose platforms."
1206,https://arxiv.org/abs/2011.06277,On Designing Computing Systems for Autonomous Vehicles: a PerceptIn Case Study,"PerceptIn develops and commercializes autonomous vehicles for micromobility around the globe. This paper makes a holistic summary of PerceptIn's development and operating experiences. This paper provides the business tale behind our product, and presents the development of the computing system for our vehicles. We illustrate the design decision made for the computing system, and show the advantage of offloading localization workloads onto an FPGA platform."
1207,https://arxiv.org/abs/2010.05233,An Energy-Efficient High Definition Map Data Distribution Mechanism for Autonomous Driving,"Autonomous Driving is now the promising future of transportation. As one basis for autonomous driving, High Definition Map (HD map) provides high-precision descriptions of the environment, therefore it enables more accurate perception and localization while improving the efficiency of path planning. However, an extremely large amount of map data needs to be transmitted during driving, thus posing great challenge for real-time and safety requirements for autonomous driving. To this end, we first demonstrate how the existing data distribution mechanism can support HD map services. Furthermore, considering the constraints of vehicle power, vehicle speed, base station bandwidth, etc., we propose a HD map data distribution mechanism on top of Vehicle-to-Infrastructure (V2I) data transmission. By this mechanism, the map provision task is allocated to the selected RSU nodes and transmits proportionate HD map data cooperatively. Their works on map data loading aims to provide in-time HD map data service with optimized in-vehicle energy consumption. Finally, we model the selection of RSU nodes into a partial knapsack problem and propose a greedy strategy-based data transmission algorithm. Experimental results confirm that within limited energy consumption, the proposed mechanism can ensure HD map data service by coordinating multiple RSUs with the shortest data transmission time."
1208,https://arxiv.org/abs/2009.11551,Residual Feature Distillation Network for Lightweight Image Super-Resolution,"Recent advances in single image super-resolution (SISR) explored the power of convolutional neural network (CNN) to achieve a better performance. Despite the great success of CNN-based methods, it is not easy to apply these methods to edge devices due to the requirement of heavy computation. To solve this problem, various fast and lightweight CNN models have been proposed. The information distillation network is one of the state-of-the-art methods, which adopts the channel splitting operation to extract distilled features. However, it is not clear enough how this operation helps in the design of efficient SISR models. In this paper, we propose the feature distillation connection (FDC) that is functionally equivalent to the channel splitting operation while being more lightweight and flexible. Thanks to FDC, we can rethink the information multi-distillation network (IMDN) and propose a lightweight and accurate SISR model called residual feature distillation network (RFDN). RFDN uses multiple feature distillation connections to learn more discriminative feature representations. We also propose a shallow residual block (SRB) as the main building block of RFDN so that the network can benefit most from residual learning while still being lightweight enough. Extensive experimental results show that the proposed RFDN achieve  a better trade-off against the state-of-the-art methods in terms of performance and model complexity. Moreover, we propose an enhanced RFDN (E-RFDN) and won the first place in the AIM 2020 efficient super-resolution challenge. Code will be available at https://github.com/njulj/RFDN."
1209,https://arxiv.org/abs/2009.06943,AIM 2020 Challenge on Efficient Super-Resolution: Methods and Results,"This paper reviews the AIM 2020 challenge on efficient single image super-resolution with focus on the proposed solutions and results. The challenge task was to super-resolve an input image with a magnification factor x4 based on a set of prior examples of low and corresponding high resolution images. The goal is to devise a network that reduces one or several aspects such as runtime, parameter count, FLOPs, activations, and memory consumption while at least maintaining PSNR of MSRResNet. The track had 150 registered participants, and 25 teams submitted the final results. They gauge the state-of-the-art in efficient single image super-resolution."
1210,https://arxiv.org/abs/2009.06034,A Survey of FPGA-Based Robotic Computing,"Recent researches on robotics have shown significant improvement, spanning from algorithms, mechanics to hardware architectures. Robotics, including manipulators, legged robots, drones, and autonomous vehicles, are now widely applied in diverse scenarios. However, the high computation and data complexity of robotic algorithms pose great challenges to its applications. On the one hand, CPU platform is flexible to handle multiple robotic tasks. GPU platform has higher computational capacities and easy-touse development frameworks, so they have been widely adopted in several applications. On the other hand, FPGA-based robotic accelerators are becoming increasingly competitive alternatives, especially in latency-critical and power-limited scenarios. With specialized designed hardware logic and algorithm kernels, FPGA-based accelerators can surpass CPU and GPU in performance and energy efficiency. In this paper, we give an overview of previous work on FPGA-based robotic accelerators covering different stages of the robotic system pipeline. An analysis of software and hardware optimization techniques and main technical issues is presented, along with some commercial and space applications, to serve as a guide for future work."
1211,https://arxiv.org/abs/2008.02464,A Matrix Chernoff Bound for Markov Chains and Its Application to Co-occurrence Matrices,"We prove a Chernoff-type bound for sums of matrix-valued random variables sampled via a regular (aperiodic and irreducible) finite Markov chain. Specially, consider a random walk on a regular Markov chain and a Hermitian matrix-valued function on its state space. Our result gives exponentially decreasing bounds on the tail distributions of the extreme eigenvalues of the sample mean matrix. Our proof is based on the matrix expander (regular undirected graph) Chernoff bound [Garg et al. STOC '18] and scalar Chernoff-Hoeffding bounds for Markov chains [Chung et al. STACS '12].
  Our matrix Chernoff bound for Markov chains can be applied to analyze the behavior of co-occurrence statistics for sequential data, which have been common and important data signals in machine learning. We show that given a regular Markov chain with $n$ states and mixing time $τ$, we need a trajectory of length $O(τ(\log{(n)}+\log{(τ)})/ε^2)$ to achieve an estimator of the co-occurrence matrix with error bound $ε$. We conduct several experiments and the experimental results are consistent with the exponentially fast convergence rate from theoretical analysis. Our result gives the first bound on the convergence rate of the co-occurrence matrix and the first sample complexity analysis in graph representation learning."
1212,https://arxiv.org/abs/2007.02086,"A framework for constructing a huge name disambiguation dataset: algorithms, visualization and human collaboration","We present a manually-labeled Author Name Disambiguation(AND) Dataset called WhoisWho, which consists of 399,255 documents and 45,187 distinct authors with 421 ambiguous author names. To label such a great amount of AND data of high accuracy, we propose a novel annotation framework where the human and computer collaborate efficiently and precisely. Within the framework, we also propose an inductive disambiguation model to classify whether two documents belong to the same author. We evaluate the proposed method and other state-of-the-art disambiguation methods on WhoisWho. The experiment results show that: (1) Our model outperforms other disambiguation algorithms on this challenging benchmark. (2) The AND problem still remains largely unsolved and requires more in-depth research. We believe that such a large-scale benchmark would bring great value for the author name disambiguation task. We also conduct several experiments to prove our annotation framework could assist annotators to make accurate results efficiently and eliminate wrong label problems made by human annotators effectively."
1213,https://arxiv.org/abs/2006.13257,Attentional Graph Convolutional Networks for Knowledge Concept Recommendation in MOOCs in a Heterogeneous View,"Massive open online courses are becoming a modish way for education, which provides a large-scale and open-access learning opportunity for students to grasp the knowledge. To attract students' interest, the recommendation system is applied by MOOCs providers to recommend courses to students. However, as a course usually consists of a number of video lectures, with each one covering some specific knowledge concepts, directly recommending courses overlook students'interest to some specific knowledge concepts. To fill this gap, in this paper, we study the problem of knowledge concept recommendation. We propose an end-to-end graph neural network-based approach calledAttentionalHeterogeneous Graph Convolutional Deep Knowledge Recommender(ACKRec) for knowledge concept recommendation in MOOCs. Like other recommendation problems, it suffers from sparsity issues. To address this issue, we leverage both content information and context information to learn the representation of entities via graph convolution network. In addition to students and knowledge concepts, we consider other types of entities (e.g., courses, videos, teachers) and construct a heterogeneous information network to capture the corresponding fruitful semantic relationships among different types of entities and incorporate them into the representation learning process. Specifically, we use meta-path on the HIN to guide the propagation of students' preferences. With the help of these meta-paths, the students' preference distribution with respect to a candidate knowledge concept can be captured. Furthermore, we propose an attention mechanism to adaptively fuse the context information from different meta-paths, in order to capture the different interests of different students. The promising experiment results show that the proposedACKRecis able to effectively recommend knowledge concepts to students pursuing online learning in MOOCs."
1214,https://arxiv.org/abs/2006.11153,Spectral-Energy Efficiency Trade-off-based Beamforming Design for MISO Non-Orthogonal Multiple Access Systems,"Energy efficiency (EE) and spectral efficiency (SE) are two of the key performance metrics in future wireless networks, covering both design and operational requirements. For previous conventional resource allocation techniques, these two performance metrics have been considered in isolation, resulting in severe performance degradation in either of these metrics. Motivated by this problem, in this paper, we propose a novel beamforming design that jointly considers the trade-off between the two performance metrics in a multiple-input single-output non-orthogonal multiple access system. In particular, we formulate a joint SE-EE based design as a multi-objective optimization (MOO) problem to achieve a good tradeoff between the two performance metrics. However, this MOO problem is not mathematically tractable and, thus, it is difficult to determine a feasible solution due to the conflicting objectives, where both need to be simultaneously optimized. To overcome this issue, we exploit a priori articulation scheme combined with the weighted sum approach. Using this, we reformulate the original MOO problem as a conventional single objective optimization (SOO) problem. In doing so, we develop an iterative algorithm to solve this non-convex SOO problem using the sequential convex approximation technique. Simulation results are provided to demonstrate the advantages and effectiveness of the proposed approach over the available beamforming designs."
1215,https://arxiv.org/abs/2006.08218,Self-supervised Learning: Generative or Contrastive,"Deep supervised learning has achieved great success in the last decade. However, its deficiencies of dependence on manual labels and vulnerability to attacks have driven people to explore a better solution. As an alternative, self-supervised learning attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further investigate related theoretical analysis work to provide deeper thoughts on how self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided."
1216,https://arxiv.org/abs/2006.05817,A Data Streaming Process Framework for Autonomous Driving By Edge,"In recent years, with the rapid development of sensing technology and the Internet of Things (IoT), sensors play increasingly important roles in traffic control, medical monitoring, industrial production and etc. They generated high volume of data in a streaming way that often need to be processed in real time. Therefore, streaming data computing technology plays an indispensable role in the real-time processing of sensor data in high throughput but low latency. In view of the above problems, the proposed framework is implemented on top of Spark Streaming, which builds up a gray model based traffic flow monitor, a traffic prediction orientated prediction layer and a fuzzy control based Batch Interval dynamic adjustment layer for Spark Streaming. It could forecast the variation of sensors data arrive rate, make streaming Batch Interval adjustment in advance and implement real-time streaming process by edge. Therefore, it can realize the monitor and prediction of the data flow changes of the autonomous driving vehicle sensor data in geographical coverage of edge computing node area, meanwhile minimize the end-to-end latency but satisfy the application throughput requirements. The experiments show that it can predict short-term traffic with no more than 4% relative error in a whole day. By making batch consuming rate close to data generating rate, it can maintain system stability well even when arrival data rate changes rapidly. The Batch Interval can be converged to a suitable value in two minutes when data arrival rate is doubled. Compared with vanilla version Spark Streaming, where there has serious task accumulation and introduces large delay, it can reduce 35% latency by squeezing Batch Interval when data arrival rate is low; it also can significantly improve system throughput by only at most 25% Batch Interval increase when data arrival rate is high."
1217,https://arxiv.org/abs/2005.14256,Attention: to Better Stand on the Shoulders of Giants,"Science of science (SciSci) is an emerging discipline wherein science is used to study the structure and evolution of science itself using large data sets. The increasing availability of digital data on scholarly outcomes offers unprecedented opportunities to explore SciSci. In the progress of science, the previously discovered knowledge principally inspires new scientific ideas, and citation is a reasonably good reflection of this cumulative nature of scientific research. The researches that choose potentially influential references will have a lead over the emerging publications. Although the peer review process is the mainly reliable way of predicting a paper's future impact, the ability to foresee the lasting impact based on citation records is increasingly essential in the scientific impact analysis in the era of big data. This paper develops an attention mechanism for the long-term scientific impact prediction and validates the method based on a real large-scale citation data set. The results break conventional thinking. Instead of accurately simulating the original power-law distribution, emphasizing the limited attention can better stand on the shoulders of giants."
1218,https://arxiv.org/abs/2005.09863,Understanding Negative Sampling in Graph Representation Learning,"Graph representation learning has been extensively studied in recent years. Despite its potential in generating continuous embeddings for various networks, both the effectiveness and efficiency to infer high-quality representations toward large corpus of nodes are still challenging. Sampling is a critical point to achieve the performance goals. Prior arts usually focus on sampling positive node pairs, while the strategy for negative sampling is left insufficiently explored. To bridge the gap, we systematically analyze the role of negative sampling from the perspectives of both objective and risk, theoretically demonstrating that negative sampling is as important as positive sampling in determining the optimization objective and the resulted variance. To the best of our knowledge, we are the first to derive the theory and quantify that the negative sampling distribution should be positively but sub-linearly correlated to their positive sampling distribution. With the guidance of the theory, we propose MCNS, approximating the positive distribution with self-contrast approximation and accelerating negative sampling by Metropolis-Hastings. We evaluate our method on 5 datasets that cover extensive downstream graph learning tasks, including link prediction, node classification and personalized recommendation, on a total of 19 experimental settings. These relatively comprehensive experimental results demonstrate its robustness and superiorities."
1219,https://arxiv.org/abs/2005.09347,Controllable Multi-Interest Framework for Recommendation,"Recently, neural networks have been widely used in e-commerce recommender systems, owing to the rapid development of deep learning. We formalize the recommender system as a sequential recommendation problem, intending to predict the next items that the user might be interacted with. Recent works usually give an overall embedding from a user's behavior sequence. However, a unified user embedding cannot reflect the user's multiple interests during a period. In this paper, we propose a novel controllable multi-interest framework for the sequential recommendation, called ComiRec. Our multi-interest module captures multiple interests from user behavior sequences, which can be exploited for retrieving candidate items from the large-scale item pool. These items are then fed into an aggregation module to obtain the overall recommendation. The aggregation module leverages a controllable factor to balance the recommendation accuracy and diversity. We conduct experiments for the sequential recommendation on two real-world datasets, Amazon and Taobao. Experimental results demonstrate that our framework achieves significant improvements over state-of-the-art models. Our framework has also been successfully deployed on the offline Alibaba distributed cloud platform."
1220,https://arxiv.org/abs/2004.05783,A positivity-preserving conservative Semi-Lagrangian Multi-moment Global Transport Model on the Cubed Sphere,"A positivity-preserving conservative semi-Lagrangian transport model by multi-moment finite volume method has been developed on the cubed-sphere grid. In this paper, two kinds of moments, i.e. point values (PV moment) at cell boundaries and volume integrated average (VIA) value, are defined within a single cell. The PV moment is updated by a conventional semi-Lagrangian method, while the VIA moment is cast by the flux form formulation that assures the exact numerical conservation. Different from the spatial approximation used in CSL2 (conservative semi-Lagrangian scheme with second order polynomial function) scheme, a monotonic rational function which can effectively remove non-physical oscillations and preserve the shape, is reconstructed in a single cell by the PV moment and VIA moment. The resulting scheme is inherently conservative and can allow a CFL number larger than one. Moreover, the scheme uses only one cell for spatial reconstruction, which is very easy for practical implementation. The proposed model is evaluated by several widely used benchmark tests on cubed-sphere geometry. Numerical results show that the proposed transport model can effectively remove unphysical oscillations compared with the CSL2 scheme and preserve the numerical non-negativity, and it has the potential to transport the tracers accurately in real atmospheric model."
1221,https://arxiv.org/abs/2003.10149,Modelling High-Order Social Relations for Item Recommendation,"The prevalence of online social network makes it compulsory to study how social relations affect user choice. However, most existing methods leverage only first-order social relations, that is, the direct neighbors that are connected to the target user. The high-order social relations, e.g., the friends of friends, which very informative to reveal user preference, have been largely ignored. In this work, we focus on modeling the indirect influence from the high-order neighbors in social networks to improve the performance of item recommendation. Distinct from mainstream social recommenders that regularize the model learning with social relations, we instead propose to directly factor social relations in the predictive model, aiming at learning better user embeddings to improve recommendation. To address the challenge that high-order neighbors increase dramatically with the order size, we propose to recursively ""propagate"" embeddings along the social network, effectively injecting the influence of high-order neighbors into user representation. We conduct experiments on two real datasets of Yelp and Douban to verify our High-Order Social Recommender (HOSR) model. Empirical results show that our HOSR significantly outperforms recent graph regularization-based recommenders NSCR and IF-BPR+, and graph convolutional network-based social influence prediction model DeepInf, achieving new state-of-the-arts of the task."
1222,https://arxiv.org/abs/2003.02196,Resource Allocation Technique for Hybrid TDMA-NOMA System with Opportunistic Time Assignment,"In this paper, we develop a resource allocation technique for a hybrid time division multiple access (TDMA) - non-orthogonal multiple access (NOMA) system with opportunistic time assignment. In particular, the available transmission time is divided into several time-slots, through which multiple users are served by exploiting power-domain NOMA. To fully exploit underlying benefits of this hybrid TDMA-NOMA system, we utilize the available resources efficiently by jointly allocating transmit power and time-slots to several groups of users in the system. Furthermore, these resources are allocated to maximize minimum rate of the users in the system. However, this max-min resource allocation problem is non-convex due to coupled design parameters of time and power allocations. Hence, we exploit a novel second-order cone formulation to overcome this non-convexity issue and develop an iterative algorithm to realize a solution to the original max-min problem. Simulation results show that this joint resource allocation technique has a considerable performance enhancement in terms of both minimum achieved rate and overall system throughput compared to that of the conventional resource allocation technique where equal time-slots are assigned to the groups of users."
1223,https://arxiv.org/abs/2001.08096,Autonomous Last-mile Delivery Vehicles in Complex Traffic Environments,"E-commerce has evolved with the digital technology revolution over the years. Last-mile logistics service contributes a significant part of the e-commerce experience. In contrast to the traditional last-mile logistics services, smart logistics service with autonomous driving technologies provides a promising solution to reduce the delivery cost and to improve efficiency. However, the traffic conditions in complex traffic environments, such as those in China, are more challenging compared to those in well-developed countries. Many types of moving objects (such as pedestrians, bicycles, electric bicycles, and motorcycles, etc.) share the road with autonomous vehicles, and their behaviors are not easy to track and predict. This paper introduces a technical solution from JD.com, a leading E-commerce company in China, to the autonomous last-mile delivery in complex traffic environments. Concretely, the methodologies in each module of our autonomous vehicles are presented, together with safety guarantee strategies. Up to this point, JD.com has deployed more than 300 self-driving vehicles for trial operations in tens of provinces of China, with an accumulated 715,819 miles and up to millions of on-road testing hours."
1224,https://arxiv.org/abs/1912.08976,A multi-label classification method using a hierarchical and transparent representation for paper-reviewer recommendation,"Paper-reviewer recommendation task is of significant academic importance for conference chairs and journal editors. How to effectively and accurately recommend reviewers for the submitted papers is a meaningful and still tough task. In this paper, we propose a Multi-Label Classification method using a hierarchical and transparent Representation named Hiepar-MLC. Further, we propose a simple multi-label-based reviewer assignment MLBRA strategy to select the appropriate reviewers. It is interesting that we also explore the paper-reviewer recommendation in the coarse-grained granularity."
1225,https://arxiv.org/abs/1912.06680,Dota 2 with Large Scale Deep Reinforcement Learning,"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task."
1226,https://arxiv.org/abs/2305.06542,Modeling opinion polarization under perception bias,"Social networks have provided a platform for the effective exchange of ideas or opinions but also served as a hotbed of polarization. While much research attempts to explore different causes of opinion polarization, the effect of perception bias caused by the network structure itself is largely understudied. To this end, we propose a threshold model that simulates the evolution of opinions by taking into account the perception bias, which is the gap between global information and locally available information from the neighborhood within networks. Our findings suggest that polarization occurs when the collective stubbornness of the population exceeds a critical value which is largely affected by the perception bias. In addition, as the level of stubbornness grows, the occurrence of first-order and second-order phase transitions between consensus and polarization becomes more prevalent, and the types of these phase transitions rely on the initial proportion of active opinions. Notably, for regular network structures, a step-wise pattern emerges that corresponds to various levels of polarization and is strongly associated with the formation of echo chambers. Our research presents a valuable framework for investigating the connection between perception bias and opinion polarization and provides valuable insights for mitigating polarization in the context of biased information."
1227,https://arxiv.org/abs/2305.04169,Hexagonal close-packed polar-skyrmion lattice in ultrathin ferroelectric PbTiO3 films,"Polar skyrmions are topologically stable, swirling polarization textures with particle-like characteristics, which hold promise for next-generation, nanoscale logic and memory. While understanding of how to create ordered polar skyrmion lattice structures and how such structure respond to applied electric fields, temperature, and film thickness remains elusive. Here, using phase-field simulations, the evolution of polar topology and the emergence of a phase transition to a hexagonal close-packed skyrmion lattice is explored through the construction of a temperature-electric field phase diagram for ultrathin ferroelectric PbTiO3 films. The hexagonal-lattice skyrmion crystal can be stabilized under application of an external, out-of-plane electric field which carefully adjusts the delicate interplay of elastic, electrostatic, and gradient energies. In addition, the lattice constants of the polar skyrmion crystals are found to increase with film thickness, consistent with expectation from Kittel law. Our studies pave the way for the development of novel ordered condensed matter phases assembled from topological polar textures and related emergent properties in nanoscale ferroelectrics."
1228,https://arxiv.org/abs/2302.13068,Classifying solutions of ${\rm SU}(n+1)$ Toda system around a singular source via Fuchsian equations,"Let $n$ be a positive integer, $γ_1>-1,\cdots,γ_n>-1$, $D=\{z\in {\Bbb C}:|z|<1\}$, and $(a_{ij})_{n\times n}$ be the Cartan matrix of $\frak{su}(n+1)$. By using the Fuchsian equation of $(n+1)$th order around a singular source of ${\rm SU}(n+1)$ Toda system discovered by Lin-Wei-Ye ($\textit{Invent Math}$, $\textbf{190}$(1):169-207, 2012), we describe precisely a solution $(u_1,\cdots, u_n)$ to the ${\rm SU}(n+1)$ Toda system \begin{equation*}
  \begin{cases}
  \frac{\partial^2 u_i}{\partial z\partial \bar z}+\sum_{j=1}^n a_{ij} e^{u_j}&=πγ_iδ_0\,\,{\rm on}\,\, D\\
  \frac{\sqrt{-1}}{2}\,\int_{D\backslash \{0\}} e^{u_{i} }{\rm d}z\wedge {\rm d}\bar z &< \infty
  \end{cases}
  \quad \text{for all}\quad i=1,\cdots, n \end{equation*} in terms of some $(n+1)$ holomorphic functions satisfying the normalized condition. Moreover, we show that for each $1\leq i\leq n$, $0$ is the cone singularity with angle $2π(1+γ_i)$ for metric $e^{u_i}|{\rm d}z|^2$ on $D\backslash\{0\}$, whose restriction near $0$ could be characterized by some $(n-1)$ holomorphic functions non-vanishing at $0$."
1229,https://arxiv.org/abs/2211.15141,Solutions of the ${\rm SU}(n+1)$ Toda system from meromorphic functions,"We consider the ${\rm SU}(n+1)$ Toda system on a simply connected domain $Ω$ in ${\Bbb C}$, the $n=1$ case of which coincides with the Liouville equation $Δu+8e^u=0$. A classical result by Liouville says that a solution of this equation on $Ω$ can be represented by some non-degenerate meromorphic function on $Ω$. We construct a family of solutions parameterized by ${\rm PSL}(n+1,\,{\Bbb C})/{\rm PSU}(n+1)$ for the ${\rm SU}(n+1)$ Toda system from such a meromorphic function on $Ω$, which generalizes the result of Liouville. As an application, we find a new class of solvable ${\rm SU}(n+1)$ Toda systems with singular sources via cone spherical metrics on compact Riemann surfaces."
1230,https://arxiv.org/abs/2211.11929,The structure of CSC-1 reducible conformal metrics on a compact Riemann surface with finite conical singularities,"We study the structure of CSC-1 reducible conformal metrics on a compact Riemann surface with finite conical singularities. We prove that any compact Riemann surface with a CSC-1 reducible conformal metric of finite conical singularities can be divided into a finite number of pieces by cutting along geodesics where each piece is isometric to some football. This allows us to study the existence of CSC-1 reducible conformal metrics with finite conical singularities on a compact Riemann surface. As an application, we give an angle condition of the existence of CSC-1 reducible conformal metrics with finite conical singularities on a compact Riemann surface."
1231,https://arxiv.org/abs/2211.04679,A new look at the temperature-dependent properties of the antiferroelectric model PbZrO3: an effective Hamiltonian study,"A novel atomistic effective Hamiltonian scheme, incorporating an original and simple bilinear energetic coupling, is developed and used to investigate the temperature dependent physical properties of the prototype antiferroelectric PbZrO3 (PZO) system. This scheme reproduces very well the known experimental hallmarks of the complex Pbam orthorhombic phase at low temperatures and the cubic paraelectric state of Pm 3m symmetry at high temperatures. Unexpectedly, it further predicts a novel intermediate state also of Pbam symmetry, but in which anti-phase oxygen octahedral tiltings have vanished with respect to the Pbam ground state. Interestingly, such new state exhibits a large dielectric response and thermal expansion that remarkably agree with previous experimental observations and the x-ray experiments we performed. We also conducted direct first-principles calculations at 0K which further support such low energy phase. Within this fresh framework, a re-examination of the properties of PZO is thus called for."
1232,https://arxiv.org/abs/2210.15591,Spin-current driven Dzyaloshinskii-Moriya interaction in the multiferroic BiFeO3 from first-principles,"The electrical control of magnons opens up new ways to transport and process information for logic devices. In magnetoelectrical multiferroics, the Dzyaloshinskii-Moriya (DM) interaction directly allow for such a control and, hence, is of major importance. We determine the origin and the strength of the (converse) spin current DM interaction in the R3c bulk phase of the multiferroic BiFeO3 based on density functional theory. Our data supports only the existence of one DM interaction contribution originating from the spin current model. By exploring then magnon dispersion in the full Brillouin Zone, we show that the exchange is isotropic, but the DM interaction and anisotropy prefer any propagation and any magnetization direction within the full (111) plane. Our work emphasizes the significance of the asymmetric potential induced by the spin current over the structural asymmetry induced by the anionic octahedron in multiferroics such as BiFeO3."
1233,https://arxiv.org/abs/2210.09700,A note on the Hurwitz problem and cone spherical metrics,"We are motivated by cone spherical metrics on compact Riemann surfaces of positive genus to solve a special case of the Hurwitz problem. Precisely speaking, letting $d,\,g$ and $\ell$ be three positive integers and $Λ$ be the following collection of $(\ell+2)$ partitions of a positive integer $d$: \[(a_1,\cdots, a_p),\,(b_1,\cdots, b_q),\,(m_1+1,1,\cdots,1),\cdots, (m_{\ell}+1,1,\cdots,1),\] where $(m_1,\cdots, m_{\ell})$ is a partition of $p+q-2+2g$, we prove that there exists a branched cover from some compact Riemann surface of genus $g$ to the Riemann sphere ${\Bbb P}^1$ with branch data $Λ$. An analogue for the genus-zero case was found by the first two authors ({\it Algebra Colloq.} {\bf 27} (2020), no. 2, 231-246), who were stimulated by such metrics on ${\Bbb P}^1$ and conjectured the veracity of the above statement there."
1234,https://arxiv.org/abs/2210.04138,Origin of negative electrocaloric effect in Pnma-type antiferroelectric perovskites,"Anomalous electrocaloric effect (ECE) with decreasing temperature upon application of an electric field is known to occur in antiferroelectrics (AFEs), and previous understanding refers to the field-induced canting of electric dipoles if there is no phase transitions. Here, we use a first-principle-based method to study the ECE in Nd-substituted BiFeO3 (BNFO) perovskite solid solutions, which has the Pnma-type AFE ground state. We demonstrate another scenario to achieve and explain anomalous ECE, emphasizing that explicit consideration of octahedral tiltings is indispensable for a correct understanding. This mechanism may be general for AFEs for which the antipolar mode is not the primary order parameter. We also find that the negative ECE can reach a large magnitude in BNFO."
1235,https://arxiv.org/abs/2208.00207,LRIP-Net: Low-Resolution Image Prior based Network for Limited-Angle CT Reconstruction,"In the practical applications of computed tomography imaging, the projection data may be acquired within a limited-angle range and corrupted by noises due to the limitation of scanning conditions. The noisy incomplete projection data results in the ill-posedness of the inverse problems. In this work, we theoretically verify that the low-resolution reconstruction problem has better numerical stability than the high-resolution problem. In what follows, a novel low-resolution image prior based CT reconstruction model is proposed to make use of the low-resolution image to improve the reconstruction quality. More specifically, we build up a low-resolution reconstruction problem on the down-sampled projection data, and use the reconstructed low-resolution image as prior knowledge for the original limited-angle CT problem. We solve the constrained minimization problem by the alternating direction method with all subproblems approximated by the convolutional neural networks. Numerical experiments demonstrate that our double-resolution network outperforms both the variational method and popular learning-based reconstruction methods on noisy limited-angle reconstruction problems."
1236,https://arxiv.org/abs/2207.08069,Unusual slow energy relaxation induced by mobile discrete breathers in one-dimensional lattices with next-nearest-neighbor coupling,"We study the energy relaxation process in one-dimensional (1D) lattices with next-nearest-neighbor (NNN) couplings. This relaxation is produced by adding damping (absorbing conditions) to the boundary (free-end) of the lattice. Compared to the 1D lattices with on-site potentials, the properties of discrete breathers (DBs) that are spatially localized intrinsic modes are quite unusual with the NNN couplings included, i.e., these DBs are mobile, and thus they can interact with both the phonons and the boundaries of the lattice. For the interparticle interactions of harmonic and Fermi-Pasta-Ulam-Tsingou-$β$ (FPUT-$β$) types, we find two crossovers of relaxation in general, i.e., a first crossover from the stretched-exponential to the regular exponential relaxation occurring in a short timescale, and a further crossover from the exponential to the power-law relaxation taking place in a long timescale. The first and second relaxations are universal, but the final power-law relaxation is strongly influenced by the properties of DBs, e.g. the scattering processes of DBs with phonons and boundaries in the FPUT-$β$ type systems make the power-law decay relatively faster than that in the counterparts of the harmonic type systems under the same coupling. Our results present new information and insights for understanding the slow energy relaxation in cooling the lattices."
1237,https://arxiv.org/abs/2206.05782,DSCA: A Dual-Stream Network with Cross-Attention on Whole-Slide Image Pyramids for Cancer Prognosis,"The cancer prognosis on gigapixel Whole-Slide Images (WSIs) has always been a challenging task. To further enhance WSI visual representations, existing methods have explored image pyramids, instead of single-resolution images, in WSIs. In spite of this, they still face two major problems: high computational cost and the unnoticed semantical gap in multi-resolution feature fusion. To tackle these problems, this paper proposes to efficiently exploit WSI pyramids from a new perspective, the dual-stream network with cross-attention (DSCA). Our key idea is to utilize two sub-streams to process the WSI patches with two resolutions, where a square pooling is devised in a high-resolution stream to significantly reduce computational costs, and a cross-attention-based method is proposed to properly handle the fusion of dual-stream features. We validate our DSCA on three publicly-available datasets with a total number of 3,101 WSIs from 1,911 patients. Our experiments and ablation studies verify that (i) the proposed DSCA could outperform existing state-of-the-art methods in cancer prognosis, by an average C-Index improvement of around 4.6%; (ii) our DSCA network is more efficient in computation -- it has more learnable parameters (6.31M vs. 860.18K) but less computational costs (2.51G vs. 4.94G), compared to a typical existing multi-resolution network. (iii) the key components of DSCA, dual-stream and cross-attention, indeed contribute to our model's performance, gaining an average C-Index rise of around 2.0% while maintaining a relatively-small computational load. Our DSCA could serve as an alternative and effective tool for WSI-based cancer prognosis."
1238,https://arxiv.org/abs/2201.05816,A Critical Analysis of Image-based Camera Pose Estimation Techniques,"Camera, and associated with its objects within the field of view, localization could benefit many computer vision fields, such as autonomous driving, robot navigation, and augmented reality (AR). In this survey, we first introduce specific application areas and the evaluation metrics for camera localization pose according to different sub-tasks (learning-based 2D-2D task, feature-based 2D-3D task, and 3D-3D task). Then, we review common methods for structure-based camera pose estimation approaches, absolute pose regression and relative pose regression approaches by critically modelling the methods to inspire further improvements in their algorithms such as loss functions, neural network structures. Furthermore, we summarise what are the popular datasets used for camera localization and compare the quantitative and qualitative results of these methods with detailed performance metrics. Finally, we discuss future research possibilities and applications."
1239,https://arxiv.org/abs/2112.15356,OpenQA: Hybrid QA System Relying on Structured Knowledge Base as well as Non-structured Data,"Search engines based on keyword retrieval can no longer adapt to the way of information acquisition in the era of intelligent Internet of Things due to the return of keyword related Internet pages. How to quickly, accurately and effectively obtain the information needed by users from massive Internet data has become one of the key issues urgently needed to be solved. We propose an intelligent question-answering system based on structured KB and unstructured data, called OpenQA, in which users can give query questions and the model can quickly give accurate answers back to users. We integrate KBQA structured question answering based on semantic parsing and deep representation learning, and two-stage unstructured question answering based on retrieval and neural machine reading comprehension into OpenQA, and return the final answer with the highest probability through the Transformer answer selection module in OpenQA. We carry out preliminary experiments on our constructed dataset, and the experimental results prove the effectiveness of the proposed intelligent question answering system. At the same time, the core technology of each module of OpenQA platform is still in the forefront of academic hot spots, and the theoretical essence and enrichment of OpenQA will be further explored based on these academic hot spots."
1240,https://arxiv.org/abs/2112.06863,3+1 Dimension Schwinger Pair Production with Quantum Computers,"Real-time quantum simulation of quantum field theory in (3+1)D requires large quantum computing resources. With a few-qubit quantum computer, we develop a novel algorithm and experimentally study the Schwinger effect, the electron-positron pair production in a strong electric field, in (3+1)D. The resource reduction is achieved by treating the electric field as a background field, working in Fourier space transverse to the electric field direction, and considering parity symmetry, such that we successfully map the three spatial dimension problems into one spatial dimension problems. We observe that the rate of pair production of electrons and positrons is consistent with the theoretical predication of the Schwinger effect. Our work paves the way towards exploring quantum simulation of quantum field theory beyond one spatial dimension."
1241,https://arxiv.org/abs/2111.10171,Structural Origin of Boson Peak in Glasses,"Boson peak, the excess low energy excitations in the terahertz regime, is one of the most unique features of disordered systems and has been linked to many anomalous properties of glass materials. The nature and structural origin of the boson peak remain elusive and have been debated for more than a half century mainly due to the lack of real-time and real-space experimental insights of the dynamic phenomenon. In this work we employed femtosecond MeV ultrafast electron diffraction to characterize the atomic dynamics of metallic glasses in real time. The experiment reveals collective atomic oscillations, presented in elastic electron scattering and atomic pair distribution functions, within the boson peak frequency range of 1.0-1.8 THz in both reciprocal and real space. It was found that the oscillation frequency has reciprocal dependence on interatomic pair distances and the corresponding wave velocity experimentally affirms the transverse acoustic wave nature of the boson peak. The observed strong correlation between THz acoustic vibrations and coherent electron scattering provides compelling evidence that the boson peak originates from the collective transverse vibrational modes of structurally ordered atoms in the disordered system."
1242,https://arxiv.org/abs/2111.07591,Arthur packets for quasisplit $GSp(2n)$ and $GO(2n)$ over a $p$-adic field,We construct the Arthur packets for symplectic and even orthogonal similitude groups over a $p$-adic field and show that they are stable and satisfy the twisted endoscopic character relations.
1243,https://arxiv.org/abs/2111.06250,A flexible film thermocouple temperature sensor,"This article introduces a thin-film thermocouple temperature sensor with symmetrical electrode structure. It uses PI film as a flexible substrate. Cu film and CuNi film made by MEMS manufacturing process as positive and negative electrodes. The device itself has the advantages of miniature, bendable and fast response speed. To reduce the film resistance value. Conducting metal film thickness and sputtering substrate temperature optimization experiments. The critical dimensions of Cu/CuNi film are 650nm and 400nm. The best sputtering substrate temperature for Cu/CuNi films is 100 degrees Celsius and 150 degrees Celsius. Testing the adhesion of thin film thermocouples using the peel-off method. The test result is 9.4N. Finally, the film thermocouple temperature sensor is subjected to a temperature static calibration experiment. The result shows that the actual potential difference error is within 1 degrees Celsius. It belongs to the second class standard in the formulation of thermocouple standards in my country. Through curve fitting, the corresponding relationship between temperature and potential difference is more accurate."
1244,https://arxiv.org/abs/2111.03835,Design of an Adaptive Surface for Space-Reconfigurable Reflectors using Auxetic Lattice Skin,"The effect of Poisson's ratio to the reflector reshaping is investigated through mechanical study of reconfigurable reflectors in this paper. The value of Poisson's ratio corresponding to the minimum deforming stress is given and an auxetic lattice is proposed for the reflector surface. The parameters of the auxetic lattice are investigated for vary Poisson's ratio. A case of reconfigurable reflector is studied, the curvature change and strain are calculated by surface geometry analysis, and the negative Poisson's ratio is established for vary thickness. According to RMS calculation by the FEM structure analysis, the thickness can finally be established."
1245,https://arxiv.org/abs/2111.02707,Phase-transition-induced giant Thomson effect for thermoelectric cooling,"The Seebeck and Peltier effects have been widely studied and used in various thermoelectric technologies, including thermal energy harvesting and solid-state heat pumps. However, basic and applied studies on the Thomson effect, another fundamental thermoelectric effect in conductors, are limited despite the fact that the Thomson effect allows electronic cooling through the application of a temperature gradient bias rather than the construction of junction structures. In this article, we report the observation of a giant Thomson effect that appears owing to magnetic phase transitions. The Thomson coefficient of FeRh-based alloys reaches large values approaching $-$1,000 $μ$VK$^{-1}$ around room temperature because of the steep temperature dependence of the Seebeck coefficient associated with the antiferromagnetic-ferromagnetic phase transition. The Thomson coefficient is several orders of magnitude larger than the Seebeck coefficient of the alloys. Using the active thermography technique, we demonstrate that the Thomson cooling can be much larger than Joule heating in the same material even in a nearly steady state. The operation temperature of the giant Thomson effect in the FeRh-based alloys can be tuned over a wide range by applying an external magnetic field or by slightly changing the composition. Our findings provide a new direction in the materials science of thermoelectrics and pave the way for thermal management applications using the Thomson effect."
1246,https://arxiv.org/abs/2111.00412,A capacitive flexible tactile sensor,"In this paper, a capacitive flexible tactile sensor was designed to measure the pressure of objects based on MEMS technology. This sensor is a structure of a 4x4 array, with metal Ag as the capacitive electrode, which forms the tactile sensing unit of the sensor. The structure of capacitive flexible tactile sensor was designed and an experimental platform was established to test the performance. The tests show that when the thickness of the intermediate layer is 2 mm and the density is medium, the sensor's sensitivity is the best while the time of both the response and the rebound is fast."
1247,https://arxiv.org/abs/2111.00385,Geometry reconfiguration of a reflector with an auxetic material surface,"Mechanically reconfigurable reflectors can modify their geometries to realize large angle beam scanning. This paper studies a method for reconfiguring a reflector geometry from a standard parabolic shape to offset parabolic shapes with varies offset angles. To fulfil the geometry control, the coordinate change of the nodes on the reflector surface are studied with the coordinate transformation method. To accommodate the large deformation of the reflector surface, a negative Poisson's ratio (NPR) material is applied to the reconfigurable reflector."
1248,https://arxiv.org/abs/2107.10473,A Novel Strategy for GaN-on-Diamond Device with a High Thermal Boundary Conductance,"To achieve high device performance and high reliability for the gallium nitride (GaN)-based high electron mobility transistors (HEMTs), efficient heat dissipation is important but remains challenging. Enormous efforts have been made to transfer a GaN device layer onto a diamond substrate with a high thermal conductivity by bonding. In this work, two GaN-diamond bonded composites are prepared via modified surface activated bonding (SAB) at room temperature with silicon interlayers of different thicknesses (15 nm and 22 nm). Before and after post-annealing process at 800 oC, thermal boundary conductance (TBC) across the bonded interface including the interlayer and the stress of GaN layer are investigated by time-domain thermoreflectance and Raman spectroscopy, respectively. After bonding, the 15 nm Si interlayer achieved a higher TBC. The post-annealing significantly increased the TBC of both interfaces, while the TBC of 22 nm silicon interlayer increased greater and became higher than that of 15 nm. Detailed investigation of the microstructure and composition of the interfaces were carried out to understand the difference in interfacial thermal conduction. The obtained stress was no more than 230 MPa for both before and after the annealing, and this high thermal stability of the bonded composites indicates that the room temperature bonding can realize a GaN-on-diamond template suitable for further epitaxial growth or device process. This work brings a novel strategy of SAB followed by high-temperature annealing to fabricate a GaN-on-diamond device with a high TBC."
1249,https://arxiv.org/abs/2107.05002,Improving Low-resource Reading Comprehension via Cross-lingual Transposition Rethinking,"Extractive Reading Comprehension (ERC) has made tremendous advances enabled by the availability of large-scale high-quality ERC training data. Despite of such rapid progress and widespread application, the datasets in languages other than high-resource languages such as English remain scarce. To address this issue, we propose a Cross-Lingual Transposition ReThinking (XLTT) model by modelling existing high-quality extractive reading comprehension datasets in a multilingual environment. To be specific, we present multilingual adaptive attention (MAA) to combine intra-attention and inter-attention to learn more general generalizable semantic and lexical knowledge from each pair of language families. Furthermore, to make full use of existing datasets, we adopt a new training framework to train our model by calculating task-level similarities between each existing dataset and target dataset. The experimental results show that our XLTT model surpasses six baselines on two multilingual ERC benchmarks, especially more effective for low-resource languages with 3.9 and 4.1 average improvement in F1 and EM, respectively."
1250,https://arxiv.org/abs/2107.04880,PatentMiner: Patent Vacancy Mining via Context-enhanced and Knowledge-guided Graph Attention,"Although there are a small number of work to conduct patent research by building knowledge graph, but without constructing patent knowledge graph using patent documents and combining latest natural language processing methods to mine hidden rich semantic relationships in existing patents and predict new possible patents. In this paper, we propose a new patent vacancy prediction approach named PatentMiner to mine rich semantic knowledge and predict new potential patents based on knowledge graph (KG) and graph attention mechanism. Firstly, patent knowledge graph over time (e.g. year) is constructed by carrying out named entity recognition and relation extrac-tion from patent documents. Secondly, Common Neighbor Method (CNM), Graph Attention Networks (GAT) and Context-enhanced Graph Attention Networks (CGAT) are proposed to perform link prediction in the constructed knowledge graph to dig out the potential triples. Finally, patents are defined on the knowledge graph by means of co-occurrence relationship, that is, each patent is represented as a fully connected subgraph containing all its entities and co-occurrence relationships of the patent in the knowledge graph; Furthermore, we propose a new patent prediction task which predicts a fully connected subgraph with newly added prediction links as a new pa-tent. The experimental results demonstrate that our proposed patent predic-tion approach can correctly predict new patents and Context-enhanced Graph Attention Networks is much better than the baseline. Meanwhile, our proposed patent vacancy prediction task still has significant room to im-prove."
1251,https://arxiv.org/abs/2105.15033,DiaKG: an Annotated Diabetes Dataset for Medical Knowledge Graph Construction,"Knowledge Graph has been proven effective in modeling structured information and conceptual knowledge, especially in the medical domain. However, the lack of high-quality annotated corpora remains a crucial problem for advancing the research and applications on this task. In order to accelerate the research for domain-specific knowledge graphs in the medical domain, we introduce DiaKG, a high-quality Chinese dataset for Diabetes knowledge graph, which contains 22,050 entities and 6,890 relations in total. We implement recent typical methods for Named Entity Recognition and Relation Extraction as a benchmark to evaluate the proposed dataset thoroughly. Empirical results show that the DiaKG is challenging for most existing methods and further analysis is conducted to discuss future research direction for improvements. We hope the release of this dataset can assist the construction of diabetes knowledge graphs and facilitate AI-based applications."
1252,https://arxiv.org/abs/2105.14880,A Multilingual Modeling Method for Span-Extraction Reading Comprehension,"Span-extraction reading comprehension models have made tremendous advances enabled by the availability of large-scale, high-quality training datasets. Despite such rapid progress and widespread application, extractive reading comprehension datasets in languages other than English remain scarce, and creating such a sufficient amount of training data for each language is costly and even impossible. An alternative to creating large-scale high-quality monolingual span-extraction training datasets is to develop multilingual modeling approaches and systems which can transfer to the target language without requiring training data in that language. In this paper, in order to solve the scarce availability of extractive reading comprehension training data in the target language, we propose a multilingual extractive reading comprehension approach called XLRC by simultaneously modeling the existing extractive reading comprehension training data in a multilingual environment using self-adaptive attention and multilingual attention. Specifically, we firstly construct multilingual parallel corpora by translating the existing extractive reading comprehension datasets (i.e., CMRC 2018) from the target language (i.e., Chinese) into different language families (i.e., English). Secondly, to enhance the final target representation, we adopt self-adaptive attention (SAA) to combine self-attention and inter-attention to extract the semantic relations from each pair of the target and source languages. Furthermore, we propose multilingual attention (MLA) to learn the rich knowledge from various language families. Experimental results show that our model outperforms the state-of-the-art baseline (i.e., RoBERTa_Large) on the CMRC 2018 task, which demonstrate the effectiveness of our proposed multi-lingual modeling approach and show the potentials in multilingual NLP tasks."
1253,https://arxiv.org/abs/2104.13525,Effect of biaxial strain and hydrostatic pressure on the magnetic properties of bilayer CrI3,"Two-dimensional van der Waals magnetic materials are intriguing for applications in the future spintronics devices, so it is crucial to explore strategy to control the magnetic properties. Here, we carried out first-principles calculations and Monte Carlo simulations to investigate the effect of biaxial strain and hydrostatic pressure on the magnetic properties of the bilayer CrI3. We found that the magnetic anisotropy, intralayer and interlayer exchange interactions, and Curie temperature can be tuned by biaxial strain and hydrostatic pressure. Large compressive biaxial strain may induce a ferromagnetic-to-antiferromagnetic transition of both CrI3 layers. The hydrostatic pressure could enhance the intralayer exchange interaction significantly and hence largely boost the Curie temperature. The effect of the biaxial strain and hydrostatic pressure revealed in the bilayer CrI3 may be generalized to other two-dimensional magnetic materials."
1254,https://arxiv.org/abs/2104.09825,Isolation of the Cuspidal Spectrum: the Function Field Case,"Isolating cuspidal automorphic representations from the whole automorphic spectrum is a basic problem in the trace formula approach. For example, matrix coefficients of supercupidal representations can be used as test functions for this, which kills the continuous spectrum, but also a large class of cuspidal automorphic representations. For the case of number fields, multipliers of the Schwartz algebra is used in the recent work [3] to isolate all cuspidal spectrum which provide enough test functions and suitable for the comparison of orbital integrals. These multipliers are then applied to the proof of the Gan-Gross-Prasad conjecture for unitary groups [3,2]. In this article, we prove similar result on isolating the cuspidal spectrum in [3] for the function field case."
1255,https://arxiv.org/abs/2103.15300,Global L-packets of quasisplit $GSp(2n)$ and $GO(2n)$,This is a sequel to [Xu18] on the $L$-packets of quasisplit general symplectic and even orthogonal groups. We show the existence of global $L$-packets and establish the functoriality of endoscopic transfer for them in many cases.
1256,https://arxiv.org/abs/2103.10238,Two-dimensional multiferroic metal with voltage-tunable magnetization and metallicity,"We design a multiferroic metal that combines seemingly incompatible ferromagnetism, ferroelectricity, and metallicity by hole doping a two-dimensional (2D) ferroelectric with high density of states near the Fermi level. The strong magnetoelectric effect is demonstrated in hole-doped and arsenic-doped monolayer α-In2Se3 using first-principles calculations. Taking advantage of the oppositely charged surfaces created by an out-of-plane polarization, the 2D magnetization and metallicity can be electrically switched on and off in an asymmetrically doped monolayer. The substitutional arsenic defect pair exhibits an intriguing electric field-tunable charge disproportionation process accompanied with an on-off switch of local magnetic moments. The charge ordering process can be controlled by tuning the relative strength of on-site Coulomb repulsion and defect dipole-polarization coupling via strain engineering. Our design principle relying on no transition metal broadens the materials design space for 2D multiferroic metals."
1257,https://arxiv.org/abs/2102.13526,Electrocaloric effects in multiferroics,"An atomistic effective Hamiltonian is used to compute electrocaloric (EC) effects in rare-earth substituted BiFeO$_{3}$ multiferroics. A phenomenological model is then developed to interpret these computations, with this model indicating that the EC coefficient is the sum of two terms, that involve electric quantities (polarization, dielectric response), the antiferromagnetic order parameter, and the coupling between polarization and antiferromagnetic order. The first one depends on the polarization and dielectric susceptibility, has the analytical form previously demonstrated for ferroelectrics, and is thus enhanced at the ferroelectric Curie temperature. The second one explicitly involves the dielectric response, the magnetic order parameter and a specific magnetoelectric coupling, and generates a peak of the EC response at the Néel temperature. These atomistic results and phenomenological model may be put in use to optimize EC coefficients."
1258,https://arxiv.org/abs/2102.13296,Ultrahigh energy storage density in epitaxial AlN/ScN superlattices,"Dielectric and antiferroelectric materials are particularly promising for high-power energy-storage applications. However, relatively low energy density greatly hinders their usage in storage technologies. Here, we report first-principles-based calculations predicting that epitaxial and initially non-polar AlN/ScN superlattices can achieve an ultrahigh energy density of up to 200 J/cm$^{\textrm{3}}$, accompanied by an ideal efficiency of 100%. We also show that high energy density requires the system being neither too close nor too far from a ferroelectric phase transition under zero electric field. A phenomenological model is further proposed to rationalize such striking features."
1259,https://arxiv.org/abs/2102.08856,A re-examination of antiferroelectric PbZrO$_3$ and PbHfO$_3$: an 80-atom $Pnam$ structure,"First principles density functional theory (DFT) simulations of antiferroelectric (AFE) PbZrO$_3$ and PbHfO$_3$ reveal a dynamical instability in the phonon spectra of their purported low temperature $Pbam$ ground states. This instability doubles the $c$-axis of $Pbam$ and condenses five new small amplitude phonon modes giving rise to an 80-atom $Pnam$ structure. Compared with $Pbam$, the stability of this structure is slightly enhanced and highly reproducible as demonstrated through using different DFT codes and different treatments of electronic exchange & correlation interactions. This suggests that $Pnam$ is a new candidate for the low temperature ground state of both materials. With this finding, we bring parity between the AFE archetypes and recent observations of a very similar AFE phase in doped or electrostatically engineered BiFeO$_3$."
1260,https://arxiv.org/abs/2102.00558,Super-R BiFeO$_3$: Epitaxial stabilization of a low-symmetry phase with giant electromechanical response,"Piezoelectrics interconvert mechanical energy and electric charge and are widely used in actuators and sensors. The best performing materials are ferroelectrics at a morphotropic phase boundary (MPB), where several phases can intimately coexist. Switching between these phases by electric field produces a large electromechanical response. In the ferroelectric BiFeO$_3$, strain can be used to create an MPB-like phase mixture and thus to generate large electric field dependent strains. However, this enhanced response occurs at localized, randomly positioned regions of the film, which potentially complicates nanodevice design. Here, we use epitaxial strain and orientation engineering in tandem - anisotropic epitaxy - to craft a hitherto unavailable low-symmetry phase of BiFeO$_3$ which acts as a structural bridge between the rhombohedral-like and tetragonal-like polymorphs. Interferometric displacement sensor measurements and first-principle calculations reveal that under external electric bias, this phase undergoes a transition to the tetragonal-like polymorph, generating a piezoelectric response enhanced by over 200%, and associated giant field-induced reversible strain. These results offer a new route to engineer giant electromechanical properties in thin films, with broader perspectives for other functional oxide systems."
1261,https://arxiv.org/abs/2101.01601,Bilateral Grid Learning for Stereo Matching Networks,"Real-time performance of stereo matching networks is important for many applications, such as automatic driving, robot navigation and augmented reality (AR). Although significant progress has been made in stereo matching networks in recent years, it is still challenging to balance real-time performance and accuracy. In this paper, we present a novel edge-preserving cost volume upsampling module based on the slicing operation in the learned bilateral grid. The slicing layer is parameter-free, which allows us to obtain a high quality cost volume of high resolution from a low-resolution cost volume under the guide of the learned guidance map efficiently. The proposed cost volume upsampling module can be seamlessly embedded into many existing stereo matching networks, such as GCNet, PSMNet, and GANet. The resulting networks are accelerated several times while maintaining comparable accuracy. Furthermore, we design a real-time network (named BGNet) based on this module, which outperforms existing published real-time deep stereo matching networks, as well as some complex networks on the KITTI stereo datasets. The code is available at https://github.com/YuhuaXu/BGNet."
1262,https://arxiv.org/abs/2101.01396,Scalable monolayer-functionalized nanointerface for thermal conductivity enhancement in copper/diamond composite,"Aiming at developing high thermal conductivity copper/diamond composite, an unconventional approach applying self-assembled monolayer (SAM) prior to the high-temperature sintering of copper/diamond composite was utilized to enhance the thermal boundary conductance (TBC) between copper and diamond. The enhancement was first systematically confirmed on a model interface system by detailed SAM morphology characterization and TBC measurements. TBC significantly depends on the SAM coverage and ordering, and the formation of high-quality SAM promoted the TBC to 73 MW/m^2-K from 27 MW/m^2-K, the value without SAM. With the help of molecular dynamics simulations, the TBC enhancement was identified to be determined by the number of SAM bridges and the overlap of vibrational density of states. The diamond particles of 210 {\micro\metre} in size were simultaneously functionalized by SAM with the condition giving the highest TBC in the model system and sintered together with the copper to fabricate isotropic copper/diamond composite of 50% volume fraction. The measured thermal conductivity marked 711 W/m-K at room temperature, the highest value among the ones with similar diamond-particles volume fraction and size. This work demonstrates a novel strategy to enhance the thermal conductivity of composite materials by SAM functionalization."
1263,https://arxiv.org/abs/2011.14558,Above-room-temperature giant thermal conductivity switching in spintronic multilayer,"Thermal switching provides an effective way for active heat flow control, which has recently attracted increasing attention in terms of nanoscale thermal management technologies. In magnetic and spintronic materials, the thermal conductivity depends on the magnetization configuration: this is the magneto-thermal resistance effect. Here we show that an epitaxial Cu/Co$_{50}$Fe$_{50}$ multilayer film exhibits giant magnetic-field-induced modulation of the cross-plane thermal conductivity. The magneto-thermal resistance ratio for the Cu/Co$_{50}$Fe$_{50}$ multilayer reaches 150% at room temperature, which is much larger than the previous record high. Although the ratio decreases with increasing the temperature, the giant magneto-thermal resistance effect of ~100% still appears up to 400 K. The magnetic field dependence of the thermal conductivity of the Cu/Co$_{50}$Fe$_{50}$ multilayer was observed to be about twice greater than that of the cross-plane electrical conductivity. The observation of the giant magneto-thermal resistance effect clarifies a potential of spintronic multilayers as thermal switching devices."
1264,https://arxiv.org/abs/2011.10962,The characteristic cycles and semi-canonical bases on type $A$ quiver variety,"In this article we study a conjecture of Geiss-Leclerc-Schr{ö}er, which is an analogue of a classical conjecture of Lusztig in the Weyl group case. It concerns the relation between canonical basis and semi-canonical basis through the characteristic cycles. We formulate an approach to this conjecture and prove it for type $A_2$ quiver. In general type A case, we reduce the conjecture to show that certain nearby cycles have vanishing Euler characteristic."
1265,https://arxiv.org/abs/2010.15239,A Cloud-Based Energy Management Strategy for Hybrid Electric City Bus Considering Real-Time Passenger Load Prediction,"Electric city bus gains popularity in recent years for its low greenhouse gas emission, low noise level, etc. Different from a passenger car, the weight of a city bus varies significantly with different amounts of onboard passengers. After analyzing the importance of battery aging and passenger load effects on an optimal energy management strategy, this study introduces the passenger load prediction into the hybrid-electric city buses energy management problem, which is not well studied in the existing literature. The average model, Decision Tree, Gradient Boost Decision Tree, and Neural Networks models are compared in the passenger load prediction. The Gradient Boost Decision Tree model is selected due to its best accuracy and high stability. Given the predicted passenger load, a dynamic programming algorithm determines the optimal power demand for supercapacitor and battery by optimizing the battery aging and energy usage leveraging cloud techniques. Then, rule extraction is conducted on dynamic programming results, and the rule is real-time loaded to the vehicle onboard controller to handle prediction errors and uncertainties. The proposed cloud-based Dynamic Programming and rule extraction framework with the passenger load prediction show 4% and 11% lower bus operating costs in off-peak and peak hours, respectively. The operating cost by the proposed framework is less than 1% of the dynamic programming with the true passenger load information."
1266,https://arxiv.org/abs/2010.14575,Learning Time Reduction Using Warm Start Methods for a Reinforcement Learning Based Supervisory Control in Hybrid Electric Vehicle Applications,"Reinforcement Learning (RL) is widely utilized in the field of robotics, and as such, it is gradually being implemented in the Hybrid Electric Vehicle (HEV) supervisory control. Even though RL exhibits excellent performance in terms of fuel consumption minimization in simulation, the large learning iteration number needs a long learning time, making it hardly applicable in real-world vehicles. In addition, the fuel consumption of initial learning phases is much worse than baseline controls. This study aims to reduce the learning iterations of Q-learning in HEV application and improve fuel consumption in initial learning phases utilizing warm start methods. Different from previous studies, which initiated Q-learning with zero or random Q values, this study initiates the Q-learning with different supervisory controls (i.e., Equivalent Consumption Minimization Strategy control and heuristic control), and detailed analysis is given. The results show that the proposed warm start Q-learning requires 68.8% fewer iterations than cold start Q-learning. The trained Q-learning is validated in two different driving cycles, and the results show 10-16% MPG improvement when compared to Equivalent Consumption Minimization Strategy control. Furthermore, real-time feasibility is analyzed, and the guidance of vehicle implementation is provided. The results of this study can be used to facilitate the deployment of RL in vehicle supervisory control applications."
1267,https://arxiv.org/abs/2010.14115,Energy Consumption and Battery Aging Minimization Using a Q-learning Strategy for a Battery/Ultracapacitor Electric Vehicle,"Propulsion system electrification revolution has been undergoing in the automotive industry. The electrified propulsion system improves energy efficiency and reduces the dependence on fossil fuel. However, the batteries of electric vehicles experience degradation process during vehicle operation. Research considering both battery degradation and energy consumption in battery/ supercapacitor electric vehicles is still lacking. This study proposes a Q-learning-based strategy to minimize battery degradation and energy consumption. Besides Q-learning, two heuristic energy management methods are also proposed and optimized using Particle Swarm Optimization algorithm. A vehicle propulsion system model is first presented, where the severity factor battery degradation model is considered and experimentally validated with the help of Genetic Algorithm. In the results analysis, Q-learning is first explained with the optimal policy map after learning. Then, the result from a vehicle without ultracapacitor is used as the baseline, which is compared with the results from the vehicle with ultracapacitor using Q-learning, and two heuristic methods as the energy management strategies. At the learning and validation driving cycles, the results indicate that the Q-learning strategy slows down the battery degradation by 13-20% and increases the vehicle range by 1.5-2% compared with the baseline vehicle without ultracapacitor."
1268,https://arxiv.org/abs/2010.07700,A gradient model for the spatial patterns of cities,"The dynamics of city's spatial structures are determined by the coupling of functional components (such as restaurants and shops) and human beings within the city. Yet, there still lacks mechanism models to quantify the spatial distribution of functional components. Here, we establish a gradient model to simulate the density curves of multiple types of components based on the equilibria of gravitational and repulsive forces along the urban-rural gradient. The forces from city center to components are determined by both the city's attributes (land rent, population and people's environmental preferences) and the components attributes (supply capacity, product transportability and environmental impacts). The simulation for the distribution curves of 22 types of components on the urban-rural gradient are a good fit for the real-world data in cities. Based on the 4 typical types of components, the model reveals a bottom-up self-organizing mechanism that is, the patterns in city development are determined by the economic, ecological, and social attributes of both cities and components. Based on the mechanism, we predict the distribution curves of many types of components along with the development of cities. The model provides a general tool for analyzing the distribution of objects on the gradients."
1269,https://arxiv.org/abs/2009.13471,Neutrino Decoherence in Simple Open Quantum Systems,"Neutrinos lose coherence as they propagate, which leads to the fading away of oscillations. In this work, we model neutrino decoherence induced in open quantum systems from their interaction with the environment. We first present two different models in the quantum mechanical framework, in which the environment is modeled as forced harmonic oscillators with white noise interactions, or two-level systems with stochastic phase kicks. We then look at the decoherence process in the quantum field theoretic framework induced by elastic scatterings with environmental particles. The exponential decay is obtained as a common feature for all models, which shows the universality of the decoherence processes. We discuss connections to the GKSL master equation approach and give a clear physical meaning of the Lindblad operators. We demonstrate that the universality of exponential decay of coherence is based on the Born-Markov approximation. The models in this work are suitable to be extended to describe real physical processes that could be non-Markovian."
1270,https://arxiv.org/abs/2006.07883,Ferroelastic-switching-driven colossal shear strain and piezoelectricity in a hybrid ferroelectric,"Materials that can produce large controllable strains are widely used in shape memory devices, actuators and sensors. Great efforts have been made to improve the strain outputs of various material systems. Among them, ferroelastic transitions underpin giant reversible strains in electrically-driven ferro/piezoelectrics and thermally- or magneticallydriven shape memory alloys. However, large-strain ferroelastic switching in conventional ferroelectrics is very challenging while magnetic and thermal controls are not desirable for applications. Here, we demonstrate an unprecedentedly large shear strain up to 21.5 % in a hybrid ferroelectric, C6H5N(CH3)3CdCl3. The strain response is about two orders of magnitude higher than those of top-performing conventional ferroelectric polymers and oxides. It is achieved via inorganic bond switching and facilitated by the structural confinement of the large organic moieties, which prevents the undesired 180-degree polarization switching. Furthermore, Br substitution can effectively soften the bonds and result in giant shear piezoelectric coefficient (d35 ~ 4800 pm/V) in Br-rich end of the solid solution, C6H5N(CH3)3CdBr3xCl3(1-x). The superior electromechanical properties of the compounds promise their potential in lightweight and high energy density devices, and the strategy described here should inspire the development of next-generation piezoelectrics and electroactive materials based on hybrid ferroelectrics."
1271,https://arxiv.org/abs/2004.02513,Singular hyperbolic metrics and negative subharmonic functions,"We propose a conjecture that the monodromy group of a singular hyperbolic metric on a non-hyperbolic Riemann surface is {\it Zariski dense} in ${\rm PSL}(2,\,{\Bbb R})$. By using meromorphic differentials and affine connections, we obtain an evidence of the conjecture that the monodromy group of the singular hyperbolic metric can not be contained in four classes of one-dimensional Lie subgroups of ${\rm PSL}(2,\,{\Bbb R})$. Moreover, we confirm the conjecture if the Riemann surface is either one of the once punctured Riemann sphere, the twice punctured Riemann sphere, a once punctured torus and a compact Riemann surface."
1272,https://arxiv.org/abs/2003.05000,PAS: Prediction-based Adaptive Sleeping for Environment Monitoring in Sensor Networks,"Energy efficiency has proven to be an important factor dominating the working period of WSN surveillance systems. Intensive studies have been done to provide energy efficient power management mechanisms. In this paper, we present PAS, a Prediction-based Adaptive Sleeping mechanism for environment monitoring sensor networks to conserve energy. PAS focuses on the diffusion stimulus (DS) scenario, which is very common and important in the application of environment monitoring. Different with most of previous works, PAS explores the features of DS spreading process to obtain higher energy efficiency. In PAS, sensors determine their sleeping schedules based on the observed emergency of DS spreading. While sensors near the DS boundary stay awake to accurately capture the possible stimulus arrival, the far away sensors turn into sleeping mode to conserve energy. Simulation experiment shows that PAS largely reduces the energy cost without decreasing system performance"
1273,https://arxiv.org/abs/2001.08872,Irreducible cone spherical metrics and stable extensions of two line bundles,"A cone spherical metric is called irreducible if any developing map of the metric does not have monodromy in ${\rm U(1)}$. By using the theory of indigenous bundles, we construct on a compact Riemann surface $X$ of genus $g_X \geq 1$ a canonical surjective map from the moduli space of stable extensions of two line bundles to that of irreducible metrics with cone angles in $2 π\mathbb{Z}_{>1}$, which is generically injective in the algebro-geometric sense as $g_X \geq 2$. As an application, we prove the following two results about irreducible metrics:
  $\bullet$ as $g_X \geq 2$ and $d$ is even and greater than $12g_X - 7$, the effective divisors of degree $d$ which could be represented by irreducible metrics form an arcwise connected Borel subset of Hausdorff dimension $\geq 2(d+3-3g_X)$ in ${\rm Sym}^d(X)$;
  $\bullet$ as $g_X \geq 1$, for almost every effective divisor $D$ of degree odd and greater than $2g_X-2$ on $X$, there exist finitely many cone spherical metrics representing $D$."
1274,https://arxiv.org/abs/2001.04019,Tribimaximal Mixing in the $SU(5) \times \mathcal{T}_{13}$ Texture,"We extend the recently proposed $SU(5) \times \mathcal{T}_{13}$ model for the asymmetric texture to the up-type quark and seesaw sectors. The hierarchical up-type quark masses are generated from higher-dimensional operators involving family-singlet Higgses, gauge-singlet familons, and vectorlike messengers. The complex-tribimaximal (TBM) seesaw mixing arises from the vacuum structure of a minimal number of familons, resulting in an alignment between the Yukawa and Majorana matrices of the seesaw formula. Introducing four right-handed neutrinos, normal ordering of the light neutrino masses is obtained, with $m_{ν_1} = 27.6\ \mathrm{meV}$, $m_{ν_2} = 28.9\ \mathrm{meV}$ and $m_{ν_3} = 57.8\ \mathrm{meV}$. Their sum almost saturates Planck's cosmological upper bound ($120$ $\text{meV}$). The right-handed neutrino masses are expressed in terms of two parameters for a particular choice of familon vacuum alignment. We predict the $\require{cancel}\cancel{CP}$ Jarlskog-Greenberg invariant to be $|\mathcal{J}| = 0.028$, consistent with the current PDG estimate, and Majorana invariants $|\mathcal{I}_1| = 0.106$ and $|\mathcal{I}_2| = 0.011$. A sign ambiguity in the model parameters leads to two possibilities for the invariant mass parameter $|m_{ββ}|$: $13.02$ or $25.21$ $\text{meV}$, both within an order of magnitude of the most rigorous experimental upper limit ($61$--$165$ $\text{meV}$)."
1275,https://arxiv.org/abs/1912.01187,The Gauss-Bonnet formula of a conical metric on a compact Riemann surface,We prove a generalization of the classical Gauss-Bonnet formula for a conical metric on a compact Riemann surface provided that the Gaussian curvature is Lebesgue integrable with respect to the area form of the metric. We also construct explicitly some conical metrics whose curvature is not integrable.
1276,https://arxiv.org/abs/1910.10203,Pattern Formation in a Coupled Membrane-Bulk Reaction-Diffusion Model for Intracellular Polarization and Oscillations,"Reaction-diffusion systems have been widely used to study spatio-temporal phenomena in cell biology, such as cell polarization. Coupled bulk-surface models naturally include compartmentalization of cytosolic and membrane-bound polarity molecules. Here we study the distribution of the polarity protein Cdc42 in a mass-conserved membrane-bulk model, and explore the effects of diffusion and spatial dimensionality on spatio-temporal pattern formation. We first analyze a 1-D model for Cdc42 oscillations in fission yeast, consisting of two diffusion equations in the bulk domain coupled to nonlinear ODEs for binding kinetics at each end of the cell. In 1-D, our analysis reveals the existence of symmetric and asymmetric steady states, as well as anti-phase relaxation oscillations typical of slow-fast systems. We then extend our analysis to a 2-D model with circular bulk geometry, for which species can either diffuse inside the cell or become bound to the membrane and undergo a nonlinear reaction-diffusion process. We also consider a nonlocal system of PDEs approximating the dynamics of the 2-D membrane-bulk model in the limit of fast bulk diffusion. In all three model variants we find that mass conservation selects perturbations of spatial modes that simply redistribute mass. In 1-D, only anti-phase oscillations between the two ends of the cell can occur, and in-phase oscillations are excluded. In higher dimensions, no radially symmetric oscillations are observed. Instead, the only instabilities are symmetry-breaking, either corresponding to stationary Turing instabilities, leading to the formation of stationary patterns, or to oscillatory Turing instabilities, leading to traveling and standing waves. Codimension-two Bogdanov--Takens bifurcations occur when the two distinct instabilities coincide, causing traveling waves to slow down and to eventually become stationary patterns."
1277,https://arxiv.org/abs/1909.02536,Uncompensated Polarization in Incommensurate Modulations of Perovskite Antiferroelectrics,"Complex polar structures of incommensurate modulations (ICMs) are revealed in chemically modified PbZrO$_3$ perovskite antiferroelectrics using advanced transmission electron microscopy techniques. The Pb-cation displacements, previously assumed to arrange in a fully-compensated antiparallel fashion, are found to be either antiparallel but with different magnitudes, or in a nearly orthogonal arrangement in adjacent stripes in the ICMs. Ab initio calculations corroborate the low-energy state of these arrangements. Our discovery corrects the atomic understanding of ICMs in PbZrO$_3$-based perovskite antiferroelectrics."
1278,https://arxiv.org/abs/1909.00546,Spectral properties of reducible conical metrics,"We show that the monodromy of a spherical conical metric is reducible if and only if it has a real-valued eigenfunction with eigenvalue 2 in the holomorphic extension of the associated Laplace--Beltrami operator. Such an eigenfunction produces a meromorphic vector field, which is then related to the developing maps of the conical metric. We also give a lower bound of the first nonzero eigenvalue, and a complete classification of the eigenspace dimension depending on the monodromy. This paper can be seen as a new connection between the complex analysis method and the PDE approach in the study of spherical conical metrics."
1279,https://arxiv.org/abs/1908.04989,Isolated singularities of flat metrics on Riemann surfaces,"Robert Bryant (Theorie des varietes minimales et applications, 1988, 154: 321-347) proved that an isolated singularity of a conformal metric of positive constant curvature on a Riemann surface is a conical one. Using Complex Analysis, we find all of the local models for an isolated singularity of a flat metric whose area satisfies some polynomial growth condition near the singularity. In particular, we show that an isolated singularity of a flat metric with finite area is also a conical one."
1280,https://arxiv.org/abs/1907.10698,Stitching an Asymmetric Texture with $\mathcal{T}_{13} \times \mathcal{Z}_5$ Family Symmetry,"We propose $\mathcal{T}_{13} = \mathcal{Z}_{13} \rtimes \mathcal{Z}_3$ as the underlying non-Abelian discrete family symmetry of the asymmetric texture presented in arXiv:1805.10684 [hep-ph]. Its mod 13 arithmetic distinguishes each Yukawa matrix element of the texture. We construct a model of effective interactions that singles out the asymmetry and equates, without fine-tuning, the products of down-quark and charged-lepton masses at a GUT-like scale."
1281,https://arxiv.org/abs/1906.07522,Characterizing isolated singularities of conformal hyperbolic metrics,"We find the explicit local models of isolated singularities of conformal hyperbolic metrics by Complex Analysis, which is interesting in its own and could potentially be extended to high-dimensional case."
1282,https://arxiv.org/abs/1905.12035,Atomic nonaffinity as a predictor of plasticity in amorphous solids,"Structural heterogeneity of amorphous solids present difficult challenges that stymie the prediction of plastic events, which are intimately connected to their mechanical behavior. Based on a perturbation analysis of the potential energy landscape, we derive the atomic nonaffinity as an indicator with intrinsic orientation, which quantifies the contribution of an individual atom to the total nonaffine modulus of the system. We find that the atomic nonaffinity can efficiently characterize the locations of the shear transformation zones, with a predicative capacity comparable to the best indicators. More importantly, the atomic nonaffinity, combining the sign of third order derivative of energy with respect to coordinates, reveals an intrinsic softest shear orientation. By analyzing the angle between this orientation and the shear loading direction, it is possible to predict the protocol-dependent response of plastic events. Employing the new method, the distribution of orientations of shear transformation zones in a model two-dimensional amorphous solids can be measured. The resulting plastic events can be understood from a simple model of independent plastic events occurring at variously oriented shear transformation zones. These results shed light on the characterization and prediction of the mechanical response of amorphous solids."
1283,https://arxiv.org/abs/1905.04171,High Thermal Boundary Conductance across Bonded Heterogeneous GaN-SiC Interfaces,"GaN-based HEMTs have the potential to be widely used in high-power and high-frequency electronics while their maximum output powers are limited by high channel temperature induced by near-junction Joule-heating, which degrades device performance and reliability. Increasing the TBC between GaN and SiC will aid in the heat dissipation of GaN-on-SiC power devices, taking advantage of the high thermal conductivity of the SiC substrate. However, a good understanding of the TBC of this technically important interface is still lacking due to the complicated nature of interfacial heat transport. In this work, a lattice-mismatch-insensitive surface activated bonding method is used to bond GaN directly to SiC and thus eliminating the AlN layer altogether. This allows for the direct integration of high quality GaN layers with SiC to create a high thermal boundary conductance interface. TDTR is used to measure the thermal properties of the GaN thermal conductivity and GaN-SiC TBC. The measured GaN thermal conductivity is larger than that of GaN grown by MBE on SiC, showing the impact of reducing the dislocations in the GaN near the interface. High GaN-SiC TBC is observed for the bonded GaN-SiC interfaces, especially for the annealed interface whose TBC (230 MW/m2-K) is close to the highest values ever reported. To understand the structure-thermal property relation, STEM and EELS are used to characterize the interface structure. The results show that, for the as-bonded sample, there exists an amorphous layer near the interface for the as bonded samples. This amorphous layer is crystallized upon annealing, leading to the high TBC found in our work. Our work paves the way for thermal transport across bonded interfaces, which will impact real-world applications of semiconductor integration and packaging."
1284,https://arxiv.org/abs/1905.01408,Model reconstruction from temporal data for coupled oscillator networks,"In a complex system, the interactions between individual agents often lead to emergent collective behavior like spontaneous synchronization, swarming, and pattern formation. The topology of the network of interactions can have a dramatic influence over those dynamics. In many studies, researchers start with a specific model for both the intrinsic dynamics of each agent and the interaction network, and attempt to learn about the dynamics that can be observed in the model. Here we consider the inverse problem: given the dynamics of a system, can one learn about the underlying network? We investigate arbitrary networks of coupled phase-oscillators whose dynamics are characterized by synchronization. We demonstrate that, given sufficient observational data on the transient evolution of each oscillator, one can use machine learning methods to reconstruct the interaction network and simultaneously identify the parameters of a model for the intrinsic dynamics of the oscillators and their coupling."
1285,https://arxiv.org/abs/1904.02397,Convergence analysis of beetle antennae search algorithm and its applications,"The beetle antennae search algorithm was recently proposed and investigated for solving global optimization problems. Although the performance of the algorithm and its variants were shown to be better than some existing meta-heuristic algorithms, there is still a lack of convergence analysis. In this paper, we provide theoretical analysis on the convergence of the beetle antennae search algorithm. We test the performance of the BAS algorithm via some representative benchmark functions. Meanwhile, some applications of the BAS algorithm are also presented."
1286,https://arxiv.org/abs/1903.09436,Nonarchimedean components of non-endoscopic automorphic representations for quasisplit $Sp(N)$ and $O(N)$,"Arthur classified the discrete automorphic representations of symplectic and orthogonal groups over a number field by that of the general linear groups. In this classification, those that are not from endoscopic lifting correspond to pairs $(φ, b)$, where $φ$ is an irreducible unitary cuspidal automorphic representation of some general linear group and $b$ is an integer. In this paper, we study the local components of these automorphic representations at a nonarchimedean place, and we give a complete description of them in terms of their Langlands parameters."
1287,https://arxiv.org/abs/1903.06319,Stitching Videos from a Fisheye Lens Camera and a Wide-Angle Lens Camera for Telepresence Robots,"Many telepresence robots are equipped with a forward-facing camera for video communication and a downward-facing camera for navigation. In this paper, we propose to stitch videos from the FF-camera with a wide-angle lens and the DF-camera with a fisheye lens for telepresence robots. We aim at providing more compact and efficient visual feedback for the user interface of telepresence robots with user-friendly interactive experiences. To this end, we present a multi-homography-based video stitching method which stitches videos from a wide-angle camera and a fisheye camera. The method consists of video image alignment, seam cutting, and image blending. We directly align the wide-angle video image and the fisheye video image based on the multi-homography alignment without calibration, distortion correction, and unwarping procedures. Thus, we can obtain a stitched video with shape preservation in the non-overlapping regions and alignment in the overlapping area for telepresence. To alleviate ghosting effects caused by moving objects and/or moving cameras during telepresence robot driving, an optimal seam is found for aligned video composition, and the optimal seam will be updated in subsequent frames, considering spatial and temporal coherence. The final stitched video is created by image blending based on the optimal seam. We conducted a user study to demonstrate the effectiveness of our method and the superiority of telepresence robots with a stitched video as visual feedback."
1288,https://arxiv.org/abs/1902.08880,Order-disorder transition in the prototypical antiferroelectric PbZrO$_3$,"The prototypical antiferroelectric PbZrO$_3$ has several unsettled questions, such as the nature of the antiferroelectric transition, possible intermediate phase and the microscopic origin of the Pbam ground state. Using first principles, we show that no phonon becomes truly soft at the cubic-to-Pbam transition temperature, and the order-disorder character of this transition is clearly demonstrated based on molecular dynamics simulations and potential energy surfaces. The out-of-phase octahedral tilting is an important degree of freedom, which can collaborate with other phonon distortions and form a complex energy landscape with multiple minima. Candidates of the possible intermediate phase are suggested based on the calculated kinetic barriers between energy minima, and the development of a first-principles-based effective Hamiltonian. The use of this latter scheme further reveals that specific bi-linear interactions between local dipoles and octahedral tiltings play a major role in the formation of the Pbam ground state, which contrasts with most of the previous explanations."
1289,https://arxiv.org/abs/1902.04065,Singularities of the Moduli Space of n Unordered Points on the Riemann Sphere,We classify the finite groups associated to the singularities of the moduli space of $n/ge5$ unordered points on the Riemann sphere. We also realize the classification by an algorithm.
1290,https://arxiv.org/abs/1901.10774,Constructing Strebel differentials via Belyi maps on the Riemann sphere,"In this manuscript, by using Belyi maps and dessin d'enfants, we construct some concrete examples of Strebel differentials with four double poles on the Riemann sphere. As an application, we could give some explicit cone spherical metrics on the Riemann sphere."
1291,https://arxiv.org/abs/1901.09352,Magic numbers in polymer phase separation -- the importance of being rigid,"Cells possess non-membrane-bound bodies, many of which are now understood as phase-separated condensates. One class of such condensates is composed of two polymer species, where each consists of repeated binding sites that interact in a one-to-one fashion with the binding sites of the other polymer. Previous biologically-motivated modeling of such a two-component system surprisingly revealed that phase separation is suppressed for certain combinations of numbers of binding sites. This phenomenon, dubbed the ""magic-number effect"", occurs if the two polymers can form fully-bonded small oligomers by virtue of the number of binding sites in one polymer being an integer multiple of the number of binding sites of the other. Here we use lattice-model simulations and analytical calculations to show that this magic-number effect can be greatly enhanced if one of the polymer species has a rigid shape that allows for multiple distinct bonding conformations. Moreover, if one species is rigid, the effect is robust over a much greater range of relative concentrations of the two species. Our findings advance our understanding of the fundamental physics of two-component polymer-based phase-separation and suggest implications for biological and synthetic systems."
1292,https://arxiv.org/abs/1812.11719,Locally Removable Singularities for Kähler Metrics with Constant Holomorphic Sectional Curvature,"Let $n\ge 2$ be an integer, and $B^{n}\subset \mathbb{C}^{n}$ the unit ball. Let $K\subset B^{n}$ be a compact subset such that $B^n\setminus K$ is connected, or $K=\{z=(z_1,\cdots, z_n)|z_1=z_2=0\}\subset \mathbb{C}^{n}$. By the theory of developing maps, we prove that a Kähler metric on $B^{n}\setminus K$ with constant holomorphic sectional curvature uniquely extends to $B^{n}$."
1293,https://arxiv.org/abs/1812.03162,A reciprocal branching problem for automorphic representations and global Vogan packets,"Let $G$ be a group and $H$ be a subgroup of $G$. The classical branching rule (or symmetry breaking) asks: For an irreducible representation $π$ of $G$, determine the occurrence of an irreducible representation $σ$ of $H$ in the restriction of $π$ to $H$. The reciprocal branching problem of this classical branching problem is to ask: For an irreducible representation $σ$ of $H$, find an irreducible representation $π$ of $G$ such that $σ$ occurs in the restriction of $π$ to $H$. For automorphic representations of classical groups, the branching problem has been addressed by the well-known global Gan-Gross-Prasad conjecture. In this paper, we investigate the reciprocal branching problem for automorphic representations of special orthogonal groups using the twisted automorphic descent method as developed in [JZ15]. The method may be applied to other classical groups as well."
1294,https://arxiv.org/abs/1812.03157,On top Fourier coefficients of certain automorphic representations of GLn,"In this paper, we study top Fourier coefficients of certain automorphic representations of $\mathrm{GL}_n(\mathbb{A})$. In particular, we prove a conjecture of Jiang on top Fourier coefficients of isobaric automorphic representations of $\mathrm{GL}_n(\mathbb{A})$ of form $$ Δ(τ_1, b_1) \boxplus Δ(τ_2, b_2) \boxplus \cdots \boxplus Δ(τ_r, b_r)\,, $$ where $Δ(τ_i,b_i)$'s are Speh representations in the discrete spectrum of $\mathrm{GL}_{a_ib_i}(\mathbb{A})$ with $τ_i$'s being unitary cuspidal representations of $\mathrm{GL}_{a_i}(\mathbb{A})$, and $n = \sum_{i=1}^r a_ib_i$. Endoscopic lifting images of the discrete spectrum of classical groups form a special class of such representations. The result of this paper will facilitate the study of automorphic forms of classical groups occurring in the discrete spectrum."
1295,https://arxiv.org/abs/1811.09574,Magnetic Interactions in BiFeO$_3$: a First-Principles Study,"First-principles calculations, in combination with the four-state energy mapping method, are performed to extract the magnetic interaction parameters of multiferroic BiFeO$_3$. Such parameters include the symmetric exchange (SE) couplings and the Dzyaloshinskii-Moriya (DM) interactions up to second nearest neighbors, as well as the single ion anisotropy (SIA). All magnetic parameters are obtained not only for the $R3c$ structural ground state, but also for the $R3m$ and $R\bar{3}c$ phases in order to determine the effects of ferroelectricity and antiferrodistortion distortions, respectively, on these magnetic parameters. In particular, two different second-nearest neighbor couplings are identified and their origins are discussed in details. Moreover, Monte-Carlo (MC) simulations using a magnetic Hamiltonian incorporating these first-principles-derived interaction parameters are further performed. They result (i) not only in the accurate prediction of the spin-canted G-type antiferromagnetic structure and of the known magnetic cycloid propagating along a $<$1$\bar{1}$0$>$ direction, as well as their unusual characteristics (such as a weak magnetization and spin-density-waves, respectively); (ii) but also in the finding of another cycloidal state of low-energy and that awaits to be experimentally confirmed. Turning on and off the different magnetic interaction parameters in the MC simulations also reveal the precise role of each of them on magnetism."
1296,https://arxiv.org/abs/1808.04106,Cone spherical metrics and stable vector bundles,"Cone spherical metrics are conformal metrics with constant curvature one and finitely many conical singularities on compact Riemann surfaces. A cone spherical metric is called irreducible if each developing map of the metric does not have monodromy lying in ${\rm U(1)}$. We establish on compact Riemann surfaces of positive genera a correspondence between irreducible cone spherical metrics with cone angles being integral multiples of $2π$ and line subbundles of rank two stable vector bundles. Then we are motivated by it to prove a theorem of Lange-type that there always exists a stable extension of $L^*$ by $L$, for $L$ being a line bundle of negative degree on each compact Riemann surface of genus greater than one. At last, as an application of these two results, we obtain a new class of irreducible spherical metrics with cone angles being integral multiples of $2π$ on each compact Riemann surface of genus greater than one"
1297,https://arxiv.org/abs/1807.08550,"Special Kähler structures, cubic differentials and hyperbolic metrics","We obtain necessary conditions for the existence of special Kähler structures with isolated singularities on compact Riemann surfaces. We prove that these conditions are also sufficient in the case of the Riemann sphere and, moreover, we determine the whole moduli space of special Kähler structures with fixed singularities. The tool we develop for this aim is a correspondence between special Kähler structures and pairs consisting of a cubic differential and a hyperbolic metric."
1298,https://arxiv.org/abs/1805.10684,An Asymmetric TBM Texture,"We construct a texture where the Seesaw matrix is diagonalized by the TriBiMaximal (TBM) matrix with a phase. All CKM and PMNS angles are within their pdg values, and the mass relations of quarks and charged leptons extrapolated to GUT scale are satisfied, including the Gatto relation. The novel ingredient is the asymmetry of the down-quark and charged lepton Yukawa matrices. Explaining the reactor angle requires a $\require{cancel} \cancel {CP}$ phase in the TBM matrix, resulting in the Jarlskog-Greenberg invariant at $|J|=0.028$, albeit with an undetermined sign. While $SO(10)$ restrains the right-handed neutrino Majorana matrix, the neutrino masses are left undetermined."
1299,https://arxiv.org/abs/1712.06974,Strain-induced effects in the electronic and spin properties of a monolayer of ferromagnetic GdAg2,"We report on the structural, electronic and magnetic properties of a monolayer of GdAg2, forming a moiré pattern on Ag(111). Combining scanning tunneling microscopy and ab-initio spin-polarized calculations, we show that the electronic band structure can be shifted linearly via thermal dependent strain of the intra-layer atomic distance in a range between 1-7%, leading to lateral hetero-structuring. Furthermore, the coupling of the incommensurable GdAg2 alloy layer to the Ag(111) substrate leads to spatially varying atomic relaxation causing subsurface layer buckling, texturing of the electronic and spin properties, and inhomogeneity of the magnetic anisotropy energy across the layer. These results provide perspectives for a control of electronic properties and magnetic ordering in atomically-thin layers."
1300,https://arxiv.org/abs/1711.01018,Isolated singularities of conformal hyperbolic metrics,J. Nitsche proved that an isolated singularity of a conformal hyperbolic metric is either a conical singularity or a cusp one. We prove by developing map that there exists a complex coordinate $z$ centered at the singularity where the metric has the expression of either $\displaystyle{\frac{4α^2\vert z \vert^{2α-2}}{(1-\vert z \vert ^{2α})^2}\vert \mathrm{d} z \vert^2}$ with $α>0$ or $\displaystyle{\vert z \vert ^{-2}\big(\ln|z|\big)^{-2}|dz|^{2}}$.
1301,https://arxiv.org/abs/1710.04205,Raising Awareness of Conveyed Personality In Social Media Traces,"Users' persistent social media contents like posts on Facebook Timeline are presented as an ""exhibition"" about the person to others, and managing these exhibitional contents for impression management needs intentional and manual efforts. To raise awareness of and facilitate impression management around past contents, we developed a prototype called PersonalityInsight. The system employs computational psycho-linguistic analysis to help users visualize the way their past text posts might convey impressions of their personality and allowed users to modify their posts based on these visualizations. We conducted a user study to evaluate the design; users overall found that such a tool raised awareness of the fact and the ways personality might be conveyed through their past content as one aspect of impression management, but that it needs design improvement to offer action-able suggestions for content modification, as well as careful thinking about impression management as one of many values people have about their digital past."
1302,https://arxiv.org/abs/1709.03112,Bounded Projective Functions and Hyperbolic Metrics with Isolated Singularities,"We establish a correspondence on a Riemann surface between hyperbolic metrics with isolated singularities and bounded projective functions whose Schwarzian derivatives have at most double poles and whose monodromies lie in ${\rm PSU}(1,\,1)$. As an application, we construct explicitly a new class of hyperbolic metrics with countably many singularities on the unit disc."
1303,https://arxiv.org/abs/1708.06535,Drawing cone spherical metrics via Strebel differentials,"Cone spherical metrics are conformal metrics with constant curvature one and finitely many conical singularities on compact Riemann surfaces. By using Strebel differentials as a bridge, we construct a new class of cone spherical metrics on compact Riemann surfaces by drawing on the surfaces some class of connected metric ribbon graphs."
1304,https://arxiv.org/abs/1708.03745,Directgeneration of eye-safe single-and dual-vortexlasersvia off-axis pumping of the active medium,"A simple and high-efficiency method for direct generation of all-solid-state single- and dual-vortex lasers operating at eye-safe wavelengths is reported in a compact diode-end-pumped Nd:YAG laser cavity. By off-axis pumping of the Nd:YAG in two orthogonal directions, the original cavity mode, fundamental Gaussian mode (TEM00), is directly transformed into first-order Laguerre-Gaussian (LG01) mode with single- or dual-vortex structures depending on the degree of the off-axis pumping. Moreover, the single- and dual-vortex lasers can be produced in simultaneous eye-safe dual-wavelength operation with the aid of an intracavity etalon. At the same time, a novel method to determine the handedness of achieved vortex lasers by using a plane-concave mirror with suitable coating is also proposed. This method can be perfectly applied to generate vortex lasers at other emissions bands for various applications by using different laser gain mediums. This work paves a general way to simple and efficient realization and determination of vortex lasers."
1305,https://arxiv.org/abs/1705.01885,"Arthur packets for $p$-adic groups by way of microlocal vanishing cycles of perverse sheaves, with examples","In this article we propose a geometric description of Arthur packets for $p$-adic groups using vanishing cycles of perverse sheaves. Our approach is inspired by the 1992 book by Adams, Barbasch and Vogan on the Langlands classification of admissible representations of real groups and follows the direction indicated by Vogan in his 1993 paper on the Langlands correspondence. Using vanishing cycles, we introduce and study a functor from the category of equivariant perverse sheaves on the moduli space of certain Langlands parameters to local systems on the regular part of the conormal bundle for this variety. In this article we establish the main properties of this functor and show that it plays the role of microlocalization in the work of Adams, Barbasch and Vogan. We use this to define ABV-packets for pure rational forms of $p$-adic groups and propose a geometric description of the transfer coefficients that appear in Arthur's main local result in the endoscopic classification of representations. This article includes conjectures modelled on Vogan's work, especially the prediction that Arthur packets are ABV-packets for $p$-adic groups. We gather evidence for these conjectures by verifying them in numerous examples."
1306,https://arxiv.org/abs/1704.07583,Moduli Spaces of Unordered $n\ge5$ Points on the Riemann Sphere and Their Singularities,"For $n\ge5$, it is well known that the moduli space $\mathfrak{M_{0,\:n}}$ of unordered $n$ points on the Riemann sphere is a quotient space of the Zariski open set $K_n$ of $\mathbb C^{n-3}$ by an $S_n$ action. The stabilizers of this $S_n$ action at certain points of this Zariski open set $K_n$ correspond to the groups fixing the sets of $n$ points on the Riemann sphere. Let $α$ be a subset of $n$ distinct points on the Riemann sphere. We call the group of all linear fractional transformations leaving $α$ invariant the stabilizer of $α$, which is finite by observation. For each non-trivial finite subgroup $G$ of the group ${\rm PSL}(2,{\Bbb C})$ of linear fractional transformations, we give the necessary and sufficient condition for finite subsets of the Riemann sphere under which the stabilizers of them are conjugate to $G$. We also prove that there does exist some finite subset of the Riemann sphere whose stabilizer coincides with $G$. Next we obtain the irreducible decompositions of the representations of the stabilizers on the tangent spaces at the singularities of $\mathfrak{M_{0,\:n}}$. At last, on $\mathfrak{M_{0,\:5}}$ and $\mathfrak{M_{0,\:6}}$, we work out explicitly the singularities and the representations of their stabilizers on the tangent spaces at them."
1307,https://arxiv.org/abs/1703.08065,Robustness of Maximum Correntropy Estimation Against Large Outliers,"The maximum correntropy criterion (MCC) has recently been successfully applied in robust regression, classification and adaptive filtering, where the correntropy is maximized instead of minimizing the well-known mean square error (MSE) to improve the robustness with respect to outliers (or impulsive noises). Considerable efforts have been devoted to develop various robust adaptive algorithms under MCC, but so far little insight has been gained as to how the optimal solution will be affected by outliers. In this work, we study this problem in the context of parameter estimation for a simple linear errors-in-variables (EIV) model where all variables are scalar. Under certain conditions, we derive an upper bound on the absolute value of the estimation error and show that the optimal solution under MCC can be very close to the true value of the unknown parameter even with outliers (whose values can be arbitrarily large) in both input and output variables. Illustrative examples are presented to verify and clarify the theory."
1308,https://arxiv.org/abs/1703.01867,Light Attenuation Length of High Quality Linear Alkyl Benzene as Liquid Scintillator Solvent for the JUNO Experiment,"The Jiangmen Underground Neutrino Observatory (JUNO) is a multipurpose neutrino experiment with a 20 kt liquid scintillator detector designed to determine the neutrino mass hierarchy, and measure the neutrino oscillation parameters. Linear alkyl benzene (LAB) will be used as the solvent for the liquid scintillation system in the central detector of JUNO. For this purpose, we have prepared LAB samples, and have measured their light attenuation lengths, with one achieving a length of 25.8 m, comparable to the diameter of the JUNO detector."
1309,https://arxiv.org/abs/1701.02249,Photostrictive two-dimensional materials in the monochalcogenide family,"Photostriction is predicted for SnS and SnSe monolayers, two-dimensional ferroelectrics with rectangular unit cells (the lattice vector $\mathbf{a}_1$ is larger than $\mathbf{a}_2$) and an intrinsic dipole moment parallel to $\mathbf{a}_1$. Photostriction in these two-dimensional materials is found to be induced by a screened electric polarization in the photoexcited electronic state (i.e., a converse piezoelectric effect) that leads to a compression of $a_1$ and a comparatively smaller increase of $a_2$ for a reduced unit cell area. The structural change documented here is ten times larger than that observed in BiFeO$_3$, making monochalcogenide monolayers an ultimate platform for this effect. This structural modification should be observable under experimentally feasible densities of photexcited carriers on samples that have been grown already, having a potential usefulness for light-induced, remote mechano-opto-electronic applications."
1310,https://arxiv.org/abs/1608.07526,Maximum Correntropy Unscented Filter,"The unscented transformation (UT) is an efficient method to solve the state estimation problem for a non-linear dynamic system, utilizing a derivative-free higher-order approximation by approximating a Gaussian distribution rather than approximating a non-linear function. Applying the UT to a Kalman filter type estimator leads to the well-known unscented Kalman filter (UKF). Although the UKF works very well in Gaussian noises, its performance may deteriorate significantly when the noises are non-Gaussian, especially when the system is disturbed by some heavy-tailed impulsive noises. To improve the robustness of the UKF against impulsive noises, a new filter for nonlinear systems is proposed in this work, namely the maximum correntropy unscented filter (MCUF). In MCUF, the UT is applied to obtain the prior estimates of the state and covariance matrix, and a robust statistical linearization regression based on the maximum correntropy criterion (MCC) is then used to obtain the posterior estimates of the state and covariance. The satisfying performance of the new algorithm is confirmed by two illustrative examples."
1311,https://arxiv.org/abs/1608.04269,Surface Reactivity Enhancement by O2 Dissociation on a Single-layer MgO Film Deposited on Metal Substrate,"Improving reactivity on an insulating surface is crucial due to their important applications in surface catalytic reactions. In this work, we carried out first-principles calculations to investigate the adsorption of O2 on a single-layer MgO(100) film deposited on metal substrate. The adsorption configurations, reaction pathways, molecular dynamics simulations, and electronic properties are reported. We reveal that O2 can completely dissociate on the surface, which is in sharp contrast to that on MgO(100) films thicker than one monolayer. The dissociated O2 tends to penetrate into the interfacial region, behaving like a switch to trigger subsequent chemical reactions. As an example, the interplay between water and the interfacial oxygen results in the formation of hydroxyl radicals. This study paves an avenue to accomplish the desired surface catalytic reactions, especially those involving oxygen."
1312,https://arxiv.org/abs/1608.00441,"Kernel Risk-Sensitive Loss: Definition, Properties and Application to Robust Adaptive Filtering","Nonlinear similarity measures defined in kernel space, such as correntropy, can extract higher-order statistics of data and offer potentially significant performance improvement over their linear counterparts especially in non-Gaussian signal processing and machine learning. In this work, we propose a new similarity measure in kernel space, called the kernel risk-sensitive loss (KRSL), and provide some important properties. We apply the KRSL to adaptive filtering and investigate the robustness, and then develop the MKRSL algorithm and analyze the mean square convergence performance. Compared with correntropy, the KRSL can offer a more efficient performance surface, thereby enabling a gradient based method to achieve faster convergence speed and higher accuracy while still maintaining the robustness to outliers. Theoretical analysis results and superior performance of the new algorithm are confirmed by simulation."
1313,https://arxiv.org/abs/1606.04655,Jenkins-Strebel Differentials on the Riemann Sphere with Four Simple Poles,"A celebrated and deep theorem in the theory of Riemann surfaces states the existence and uniqueness of the Jenkins-Strebel differentials on a Riemann surface under some conditions, but the proof is non-constructive and examples are difficult to find. This paper deals with an example of a simple case, namely Jenkins-Strebel differentials on the Riemann sphere with four fixed simple poles. We will give explicit expressions of these Jenkins-Strebel differentials by means of the Weierstrass $\wp$ function and expose a simple algorithm determining the correspondence between these differentials and some classes of simple closed curves on the Riemann sphere with four points removed."
1314,https://arxiv.org/abs/1603.07716,A combinatorial solution to Mœglin's parametrization of Arthur packets for p-adic quasisplit $Sp(N)$ and $O(N)$,"We develop a general procedure to study the combinatorial structure of Arthur packets for $p$-adic quasisplit $Sp(N)$ and $O(N)$ following the works of Mœglin. This allows us to answer many delicate questions concerning the Arthur packets of these groups, for example the size of the packets."
1315,https://arxiv.org/abs/1603.07175,On Ambrosetti-Malchiodi-Ni conjecture on two-dimensional smooth bounded domains,"We consider the problem $$
  ε^2 Δu-V(y)u+u^p\,=\,0,~~u>0~~\quad\mbox{in}\quadΩ,~~\quad\frac {\partial u}{\partial ν}\,=\,0\quad\mbox{on}~~~\partial Ω, $$ where $Ω$ is a bounded domain in $\mathbb R^2$ with smooth boundary, the exponent $p>1$, $ε>0$ is a small parameter, $V$ is a uniformly positive, smooth potential on $\barΩ$, and $ν$ denotes the outward normal of $\partial Ω$. Let $Γ$ be a curve intersecting orthogonally with $\partial Ω$ at exactly two points and dividing $Ω$ into two parts. Moreover, $Γ$ satisfies stationary and non-degeneracy conditions with respect to the functional $\int_ΓV^σ$, where $σ=\frac {p+1}{p-1}-\frac 12$. We prove the existence of a solution $u_ε$ concentrating along the whole of $Γ$, exponentially small in $ε$ at any positive distance from it, provided that $ε$ is small and away from certain critical numbers. In particular, this establishes the validity of the two dimensional case of a conjecture by A. Ambrosetti, A. Malchiodi and W.-M. Ni(p.327, [4])."
1316,https://arxiv.org/abs/1603.03831,Non-monotonic anisotropy in charge conduction induced by antiferrodistortive transition in metallic SrTiO$_{3}$,"Cubic SrTiO$_{3}$ becomes tetragonal below 105 K. The antiferrodistortive (AFD) distortion leads to clockwise and counter-clockwise rotation of adjacent TiO$_{6}$ octahedra. This insulator becomes a metal upon the introduction of extremely low concentration of n-type dopants. However, signatures of the structural phase transition in charge conduction have remained elusive. Employing the Montgomery technique, we succeed in resolving the anisotropy of charge conductivity induced by the AFD transition, in the presence of different types of dopants. We find that the slight lattice distortion ($<6 \times 10^{-4}$) gives rise to a twenty percent anisotropy in charge conductivity, in agreement with the expectations of band calculations. Application of uniaxial strain amplifies the detectable anisotropy by disfavoring one of the three possible tetragonal domains. In contrast with all other known anisotropic Fermi liquids, the anisotropy has opposite signs for elastic and inelastic scattering. Increasing the concentration of dopants leads to a drastic shift in the temperature of the AFD transition either upward or downward. The latter result puts strong constraints on any hypothetical role played by the AFD soft mode in the formation of Cooper pairs and the emergence of superconductivity in SrTiO$_3$."
1317,https://arxiv.org/abs/1601.06005,A Low-Cost Tele-Presence Wheelchair System,"This paper presents the architecture and implementation of a tele-presence wheelchair system based on tele-presence robot, intelligent wheelchair, and touch screen technologies. The tele-presence wheelchair system consists of a commercial electric wheelchair, an add-on tele-presence interaction module, and a touchable live video image based user interface (called TIUI). The tele-presence interaction module is used to provide video-chatting for an elderly or disabled person with the family members or caregivers, and also captures the live video of an environment for tele-operation and semi-autonomous navigation. The user interface developed in our lab allows an operator to access the system anywhere and directly touch the live video image of the wheelchair to push it as if he/she did it in the presence. This paper also discusses the evaluation of the user experience."
1318,https://arxiv.org/abs/1512.05835,A multiferroic on the brink: uncovering the nuances of strain-induced transitions in BiFeO$_3$,"Bismuth ferrite (BiFeO$_3$) is one of the very few known single-phase multiferroic materials. While the bulk compound is rhombohedral (R), the discovery of an epitaxial strain-induced structural transition into a so-called 'super tetragonal-phase' (T-phase) in this material incited a flurry of research activity focused on gaining an understanding of this phase transition and its possible functionalities. This metastable phase of BiFeO$_3$ is also multiferroic, with giant ferroelectric polarization and coexisting antiferromagnetic order, but above all it is the strain relaxation-induced phase mixtures and their outstanding piezoelectric and magnetoelectric responses which continue to intrigue and motivate the physicist and materials scientist communities. Here, we review the research into the T-phase and mixed-phase BiFeO$_3$ system. We begin with a brief summary of the history of the T-phase and an analysis of the structure of the various phases reported in the literature. We then address important questions regarding the symmetry and octahedral rotation patterns and the (as yet underexplored) important role of chemistry in the formation of the metastable T-phase. We follow by describing the phase transitions in this material, and how these may hold promise for large magnetoelectric responses. Finally we point out some experimental challenges inherent to the study of such a system, and potential pathways for how they may be overcome. It is our intention with this work to highlight important issues that, in our opinion, should be carefully considered by the community in order to use this fascinating materials system for a new paradigm of functionality."
1319,https://arxiv.org/abs/1512.04334,Telepresence Interaction by Touching Live Video Images,"This paper presents a telepresence interaction framework based on touchscreen and telepresence-robot technologies. The core of the framework is a new user interface, Touchable live video Image based User Interface, called TIUI. The TIUI allows a remote operator to not just drive the telepresence robot but operate and interact with real objects by touching their live video images on a pad with finger touch gestures. We implemented a telepresence interaction system which is composed of a telepresence robot and tele-interactive objects located in a local space, the TIUI of a pad located in a remote space, and the wireless networks connecting the two spaces. Our system can be a perfect embodiment of a remote operator to do most of daily living tasks, such as opening a door, drawing a curtain, pushing a wheelchair, and other like tasks. The evaluation and demonstration results show the effectiveness and promising applications of our system."
1320,https://arxiv.org/abs/2305.04212,Numerical simulation of a rotating magnetic sail for space applications,"The Magnetic Sail is a space propulsion system that utilizes the interaction between solar wind particles and an artificial dipole magnetic field generated by a spacecraft's coil to produce thrust without the need for additional plasma or propellant. To reduce the size of the sail while improving the efficiency of capturing solar wind, a new type of rotating magnetic sail with an initial rotation speed is proposed. This study evaluates the thrust characteristics, attitude, and size design factors of a rotating magnetic sail using a 3-D single-component particle numerical simulation. The results show that an increase in rotational speed significantly increases the thrust of the rotating magnetic sail. The thrust is most significant when the magnetic moment of the sail is parallel to the direction of particle velocity. The study also found that the potential for the application of the rotating magnetic sail is greatest in orbits with high-density and low-speed space plasma environments. It suggests that a rotating magnetic sail with a magnetic moment (Mm) of 10^3-10^4 Am^2 operating at an altitude of 400 km in Low Earth Orbit (LEO) can achieve a similar thrust level to that of a rotating magnetic sail operating at 1 AU (astronomical unit) of 10^7-10^8 Am^2."
1321,https://arxiv.org/abs/2304.02127,A Bayesian Collocation Integral Method for Parameter Estimation in Ordinary Differential Equations,"Inferring the parameters of ordinary differential equations (ODEs) from noisy observations is an important problem in many scientific fields. Currently, most parameter estimation methods that bypass numerical integration tend to rely on basis functions or Gaussian processes to approximate the ODE solution and its derivatives. Due to the sensitivity of the ODE solution to its derivatives, these methods can be hindered by estimation error, especially when only sparse time-course observations are available. We present a Bayesian collocation framework that operates on the integrated form of the ODEs and also avoids the expensive use of numerical solvers. Our methodology has the capability to handle general nonlinear ODE systems. We demonstrate the accuracy of the proposed method through a simulation study, where the estimated parameters and recovered system trajectories are compared with other recent methods. A real data example is also provided."
1322,https://arxiv.org/abs/2108.09513,A Hard Label Black-box Adversarial Attack Against Graph Neural Networks,"Graph Neural Networks (GNNs) have achieved state-of-the-art performance in various graph structure related tasks such as node classification and graph classification. However, GNNs are vulnerable to adversarial attacks. Existing works mainly focus on attacking GNNs for node classification; nevertheless, the attacks against GNNs for graph classification have not been well explored.
  In this work, we conduct a systematic study on adversarial attacks against GNNs for graph classification via perturbing the graph structure. In particular, we focus on the most challenging attack, i.e., hard label black-box attack, where an attacker has no knowledge about the target GNN model and can only obtain predicted labels through querying the target model.To achieve this goal, we formulate our attack as an optimization problem, whose objective is to minimize the number of edges to be perturbed in a graph while maintaining the high attack success rate. The original optimization problem is intractable to solve, and we relax the optimization problem to be a tractable one, which is solved with theoretical convergence guarantee. We also design a coarse-grained searching algorithm and a query-efficient gradient computation algorithm to decrease the number of queries to the target GNN model. Our experimental results on three real-world datasets demonstrate that our attack can effectively attack representative GNNs for graph classification with less queries and perturbations. We also evaluate the effectiveness of our attack under two defenses: one is well-designed adversarial graph detector and the other is that the target GNN model itself is equipped with a defense to prevent adversarial graph generation. Our experimental results show that such defenses are not effective enough, which highlights more advanced defenses."
1323,https://arxiv.org/abs/2104.06348,Optimal Multi-Manipulator Arm Placement for Maximal Dexterity during Robotics Surgery,"Robot arm placements are oftentimes a limitation in surgical preoperative procedures, relying on trained staff to evaluate and decide on the optimal positions for the arms. Given new and different patient anatomies, it can be challenging to make an informed choice, leading to more frequently colliding arms or limited manipulator workspaces. In this paper, we develop a method to generate the optimal manipulator base positions for the multi-port da Vinci surgical system that minimizes self-collision and environment-collision, and maximizes the surgeon's reachability inside the patient. Scoring functions are defined for each criterion so that they may be optimized over. Since for multi-manipulator setups, a large number of free parameters are available to adjust the base positioning of each arm, a challenge becomes how one can expediently assess possible setups. We thus also propose methods that perform fast queries of each measure with the use of a proxy collision-checker. We then develop an optimization method to determine the optimal position using the scoring functions. We evaluate the optimality of the base positions for the robot arms on canonical trajectories, and show that the solution yielded by the optimization program can satisfy each criterion. The metrics and optimization strategy are generalizable to other surgical robotic platforms so that patient-side manipulator positioning may be optimized and solved."
1324,https://arxiv.org/abs/2101.02644,Data Poisoning Attacks to Deep Learning Based Recommender Systems,"Recommender systems play a crucial role in helping users to find their interested information in various web services such as Amazon, YouTube, and Google News. Various recommender systems, ranging from neighborhood-based, association-rule-based, matrix-factorization-based, to deep learning based, have been developed and deployed in industry. Among them, deep learning based recommender systems become increasingly popular due to their superior performance.
  In this work, we conduct the first systematic study on data poisoning attacks to deep learning based recommender systems. An attacker's goal is to manipulate a recommender system such that the attacker-chosen target items are recommended to many users. To achieve this goal, our attack injects fake users with carefully crafted ratings to a recommender system. Specifically, we formulate our attack as an optimization problem, such that the injected ratings would maximize the number of normal users to whom the target items are recommended. However, it is challenging to solve the optimization problem because it is a non-convex integer programming problem. To address the challenge, we develop multiple techniques to approximately solve the optimization problem. Our experimental results on three real-world datasets, including small and large datasets, show that our attack is effective and outperforms existing attacks. Moreover, we attempt to detect fake users via statistical analysis of the rating patterns of normal and fake users. Our results show that our attack is still effective and outperforms existing attacks even if such a detector is deployed."
1325,https://arxiv.org/abs/2010.11397,When Machine Learning Meets Congestion Control: A Survey and Comparison,"Machine learning (ML) has seen a significant surge and uptake across many diverse applications. The high flexibility, adaptability and computing capabilities it provides extends traditional approaches used in multiple fields including network operation and management. Numerous surveys have explored ML in the context of networking, such as traffic engineering, performance optimization and network security. Many ML approaches focus on clustering, classification, regression and reinforcement learning (RL). The innovation of this research and contribution of this paper lies in the detailed summary and comparison of learning-based congestion control (CC) approaches. Compared with traditional CC algorithms which are typically rule-based, capabilities to learn from historical experience are highly desirable. From the literature, it is observed that RL is a crucial trend among learning-based CC algorithms. In this paper, we explore the performance of RL-based CC algorithms and present current problems with RL-based CC algorithms. We outline challenges and trends related to learning-based CC algorithms."
1326,https://arxiv.org/abs/1910.03835,Interpreting Deep Learning-Based Networking Systems,"While many deep learning (DL)-based networking systems have demonstrated superior performance, the underlying Deep Neural Networks (DNNs) remain blackboxes and stay uninterpretable for network operators. The lack of interpretability makes DL-based networking systems prohibitive to deploy in practice. In this paper, we propose Metis, a framework that provides interpretability for two general categories of networking problems spanning local and global control. Accordingly, Metis introduces two different interpretation methods based on decision tree and hypergraph, where it converts DNN policies to interpretable rule-based controllers and highlight critical components based on analysis over hypergraph. We evaluate Metis over several state-of-the-art DL-based networking systems and show that Metis provides human-readable interpretations while preserving nearly no degradation in performance. We further present four concrete use cases of Metis, showcasing how Metis helps network operators to design, debug, deploy, and ad-hoc adjust DL-based networking systems."
1327,https://arxiv.org/abs/1908.04374,Two Dimensional Router: Design and Implementation,"Higher dimensional classification has attracted more attentions with increasing demands for more flexible services in the Internet. In this paper, we present the design and implementation of a two dimensional router (TwoD router), that makes forwarding decisions based on both destination and source addresses. This TwoD router is also a key element in our current effort towards two dimensional IP routing. With one more dimension, the forwarding table will grow explosively given a straightforward implementation. As a result, it is impossible to fit the forwarding table to the current TCAM, which is the de facto standard despite its limited capacity. To solve the explosion problem, we propose a forwarding table structure with a novel separation of TCAM and SRAM. As such, we move the redundancies in expensive TCAM to cheaper SRAM, while the lookup speed is comparable with conventional routers. We also design the incremental update algorithms that minimize the number of accesses to memory. We evaluate our design with a real implementation on a commercial router, Bit-Engine 12004, with real data sets. Our design does not need new devices, which is favorable for adoption. The results also show that the performance of our TwoD router is promising."
1328,https://arxiv.org/abs/2304.06908,Generating Adversarial Examples with Better Transferability via Masking Unimportant Parameters of Surrogate Model,"Deep neural networks (DNNs) have been shown to be vulnerable to adversarial examples. Moreover, the transferability of the adversarial examples has received broad attention in recent years, which means that adversarial examples crafted by a surrogate model can also attack unknown models. This phenomenon gave birth to the transfer-based adversarial attacks, which aim to improve the transferability of the generated adversarial examples. In this paper, we propose to improve the transferability of adversarial examples in the transfer-based attack via masking unimportant parameters (MUP). The key idea in MUP is to refine the pretrained surrogate models to boost the transfer-based attack. Based on this idea, a Taylor expansion-based metric is used to evaluate the parameter importance score and the unimportant parameters are masked during the generation of adversarial examples. This process is simple, yet can be naturally combined with various existing gradient-based optimizers for generating adversarial examples, thus further improving the transferability of the generated adversarial examples. Extensive experiments are conducted to validate the effectiveness of the proposed MUP-based methods."
1329,https://arxiv.org/abs/2303.03617,Computing Effective Resistances on Large Graphs Based on Approximate Inverse of Cholesky Factor,"Effective resistance, which originates from the field of circuits analysis, is an important graph distance in spectral graph theory. It has found numerous applications in various areas, such as graph data mining, spectral graph sparsification, circuits simulation, etc. However, computing effective resistances accurately can be intractable and we still lack efficient methods for estimating effective resistances on large graphs. In this work, we propose an efficient algorithm to compute effective resistances on general weighted graphs, based on a sparse approximate inverse technique. Compared with a recent competitor, the proposed algorithm shows several hundreds of speedups and also one to two orders of magnitude improvement in the accuracy of results. Incorporating the proposed algorithm with the graph sparsification based power grid (PG) reduction framework, we develop a fast PG reduction method, which achieves an average 6.4X speedup in the reduction time without loss of reduction accuracy. In the applications of power grid transient analysis and DC incremental analysis, the proposed method enables 1.7X and 2.5X speedup of overall time compared to using the PG reduction based on accurate effective resistances, without increase in the error of solution."
1330,https://arxiv.org/abs/2206.08316,Boosting the Adversarial Transferability of Surrogate Model with Dark Knowledge,"Deep neural networks (DNNs) for image classification are known to be vulnerable to adversarial examples. And, the adversarial examples have transferability, which means an adversarial example for a DNN model can fool another black-box model with a non-trivial probability. This gave birth of the transfer-based adversarial attack where the adversarial examples generated by a pretrained or known model (called surrogate model) are used to conduct black-box attack. There are some work on how to generate the adversarial examples from a given surrogate model to achieve better transferability. However, training a special surrogate model to generate adversarial examples with better transferability is relatively under-explored. In this paper, we propose a method of training a surrogate model with abundant dark knowledge to boost the adversarial transferability of the adversarial examples generated by the surrogate model. This trained surrogate model is named dark surrogate model (DSM), and the proposed method to train DSM consists of two key components: a teacher model extracting dark knowledge and providing soft labels, and the mixing augmentation skill which enhances the dark knowledge of training data. Extensive experiments have been conducted to show that the proposed method can substantially improve the adversarial transferability of surrogate model across different architectures of surrogate model and optimizers for generating adversarial examples. We also show that the proposed method can be applied to other scenarios of transfer-based attack that contain dark knowledge, like face verification."
1331,https://arxiv.org/abs/2206.06223,Pursuing More Effective Graph Spectral Sparsifiers via Approximate Trace Reduction,"Spectral graph sparsification aims to find ultra-sparse subgraphs which can preserve spectral properties of original graphs. In this paper, a new spectral criticality metric based on trace reduction is first introduced for identifying spectrally important off-subgraph edges. Then, a physics-inspired truncation strategy and an approach using approximate inverse of Cholesky factor are proposed to compute the approximate trace reduction efficiently. Combining them with the iterative densification scheme in \cite{feng2019grass} and the strategy of excluding spectrally similar off-subgraph edges in \cite{fegrass}, we develop a highly effective graph sparsification algorithm. The proposed method has been validated with various kinds of graphs. Experimental results show that it always produces sparsifiers with remarkably better quality than the state-of-the-art GRASS \cite{feng2019grass} in same computational cost, enabling more than 40% time reduction for preconditioned iterative equation solver on average. In the applications of power grid transient analysis and spectral graph partitioning, the derived iterative solver shows 3.3X or more advantages on runtime and memory cost, over the approach based on direct sparse solver."
1332,https://arxiv.org/abs/2107.06511,CNN-Cap: Effective Convolutional Neural Network Based Capacitance Models for Full-Chip Parasitic Extraction,"Accurate capacitance extraction is becoming more important for designing integrated circuits under advanced process technology. The pattern matching based full-chip extraction methodology delivers fast computational speed, but suffers from large error, and tedious efforts on building capacitance models of the increasing structure patterns. In this work, we propose an effective method for building convolutional neural network (CNN) based capacitance models (called CNN-Cap) for two-dimensional (2-D) structures in full-chip capacitance extraction. With a novel grid-based data representation, the proposed method is able to model the pattern with a variable number of conductors, so that largely reduce the number of patterns. Based on the ability of ResNet architecture on capturing spatial information and the proposed training skills, the obtained CNN-Cap exhibits much better performance over the multilayer perception neural network based capacitance model while being more versatile. Extensive experiments on a 55nm and a 15nm process technologies have demonstrated that the error of total capacitance produced with CNN-Cap is always within 1.3% and the error of produced coupling capacitance is less than 10% in over 99.5% probability. CNN-Cap runs more than 4000X faster than 2-D field solver on a GPU server, while it consumes negligible memory compared to the look-up table based capacitance model."
1333,https://arxiv.org/abs/2012.02006,AugSplicing: Synchronized Behavior Detection in Streaming Tensors,"How can we track synchronized behavior in a stream of time-stamped tuples, such as mobile devices installing and uninstalling applications in the lockstep, to boost their ranks in the app store? We model such tuples as entries in a streaming tensor, which augments attribute sizes in its modes over time. Synchronized behavior tends to form dense blocks (i.e. subtensors) in such a tensor, signaling anomalous behavior, or interesting communities. However, existing dense block detection methods are either based on a static tensor, or lack an efficient algorithm in a streaming setting. Therefore, we propose a fast streaming algorithm, AugSplicing, which can detect the top dense blocks by incrementally splicing the previous detection with the incoming ones in new tuples, avoiding re-runs over all the history data at every tracking time step. AugSplicing is based on a splicing condition that guides the algorithm (Section 4). Compared to the state-of-the-art methods, our method is (1) effective to detect fraudulent behavior in installing data of real-world apps and find a synchronized group of students with interesting features in campus Wi-Fi data; (2) robust with splicing theory for dense block detection; (3) streaming and faster than the existing streaming algorithm, with closely comparable accuracy."
1334,https://arxiv.org/abs/2009.02251,Efficient Model-Based Collaborative Filtering with Fast Adaptive PCA,"A model-based collaborative filtering (CF) approach utilizing fast adaptive randomized singular value decomposition (SVD) is proposed for the matrix completion problem in recommender system. Firstly, a fast adaptive PCA frameworkis presented which combines the fixed-precision randomized matrix factorization algorithm [1] and accelerating skills for handling large sparse data. Then, a novel termination mechanism for the adaptive PCA is proposed to automatically determine a number of latent factors for achieving the near optimal prediction accuracy during the subsequent model-based CF. The resulted CF approach has good accuracy while inheriting high runtime efficiency. Experiments on real data show that, the proposed adaptive PCA is up to 2.7X and 6.7X faster than the original fixed-precision SVD approach [1] and svds in Matlab repsectively, while preserving accuracy. The proposed model-based CF approach is able to efficiently process the MovieLens data with 20M ratings and exhibits more than 10X speedup over the regularized matrix factorization based approach [2] and the fast singular value thresholding approach [3] with comparable or better accuracy. It also owns the advantage of parameter free. Compared with the deep-learning-based CF approach, the proposed approach is much more computationally efficient, with just marginal performance loss."
1335,https://arxiv.org/abs/2007.04118,RobFR: Benchmarking Adversarial Robustness on Face Recognition,"Face recognition (FR) has recently made substantial progress and achieved high accuracy on standard benchmarks. However, it has raised security concerns in enormous FR applications because deep CNNs are unusually vulnerable to adversarial examples, and it is still lack of a comprehensive robustness evaluation before a FR model is deployed in safety-critical scenarios. To facilitate a better understanding of the adversarial vulnerability on FR, we develop an adversarial robustness evaluation library on FR named \textbf{RobFR}, which serves as a reference for evaluating the robustness of downstream tasks. Specifically, RobFR involves 15 popular naturally trained FR models, 9 models with representative defense mechanisms and 2 commercial FR API services, to perform the robustness evaluation by using various adversarial attacks as an important surrogate. The evaluations are conducted under diverse adversarial settings in terms of dodging and impersonation, $\ell_2$ and $\ell_\infty$, as well as white-box and black-box attacks. We further propose a landmark-guided cutout (LGC) attack method to improve the transferability of adversarial examples for black-box attacks by considering the special characteristics of FR. Based on large-scale evaluations, the commercial FR API services fail to exhibit acceptable performance on robustness evaluation, and we also draw several important conclusions for understanding the adversarial robustness of FR models and providing insights for the design of robust FR models. RobFR is open-source and maintains all extendable modules, i.e., \emph{Datasets}, \emph{FR Models}, \emph{Attacks\&Defenses}, and \emph{Evaluations} at \url{https://github.com/ShawnXYang/Face-Robustness-Benchmark}, which will be continuously updated to promote future research on robust FR."
1336,https://arxiv.org/abs/2003.09615,DP-Net: Dynamic Programming Guided Deep Neural Network Compression,"In this work, we propose an effective scheme (called DP-Net) for compressing the deep neural networks (DNNs). It includes a novel dynamic programming (DP) based algorithm to obtain the optimal solution of weight quantization and an optimization process to train a clustering-friendly DNN. Experiments showed that the DP-Net allows larger compression than the state-of-the-art counterparts while preserving accuracy. The largest 77X compression ratio on Wide ResNet is achieved by combining DP-Net with other compression techniques. Furthermore, the DP-Net is extended for compressing a robust DNN model with negligible accuracy loss. At last, a custom accelerator is designed on FPGA to speed up the inference computation with DP-Net."
1337,https://arxiv.org/abs/2001.00360,Kernelized Support Tensor Train Machines,"Tensor, a multi-dimensional data structure, has been exploited recently in the machine learning community. Traditional machine learning approaches are vector- or matrix-based, and cannot handle tensorial data directly. In this paper, we propose a tensor train (TT)-based kernel technique for the first time, and apply it to the conventional support vector machine (SVM) for image classification. Specifically, we propose a kernelized support tensor train machine that accepts tensorial input and preserves the intrinsic kernel property. The main contributions are threefold. First, we propose a TT-based feature mapping procedure that maintains the TT structure in the feature space. Second, we demonstrate two ways to construct the TT-based kernel function while considering consistency with the TT inner product and preservation of information. Third, we show that it is possible to apply different kernel functions on different data modes. In principle, our method tensorizes the standard SVM on its input structure and kernel mapping scheme. Extensive experiments are performed on real-world tensor data, which demonstrates the superiority of the proposed scheme under few-sample high-dimensional inputs."
1338,https://arxiv.org/abs/1908.02721,Faster Tensor Train Decomposition for Sparse Data,"In recent years, the application of tensors has become more widespread in fields that involve data analytics and numerical computation. Due to the explosive growth of data, low-rank tensor decompositions have become a powerful tool to harness the notorious curse of dimensionality. The main forms of tensor decomposition include CP decomposition, Tucker decomposition, tensor train (TT) decomposition, etc. Each of the existing TT decomposition algorithms, including the TT-SVD and randomized TT-SVD, is successful in the field, but neither can both accurately and efficiently decompose large-scale sparse tensors. Based on previous research, this paper proposes a new quasi-best fast TT decomposition algorithm for large-scale sparse tensors with proven correctness and the upper bound of its complexity is derived. In numerical experiments, we verify that the proposed algorithm can decompose sparse tensors faster than the TT-SVD, and have more speed, precision and versatility than randomized TT-SVD, and it can be used to decomposes arbitrary high-dimensional tensor without losing efficiency when the number of non-zero elements is limited. The new algorithm implements a large-scale sparse matrix TT decomposition that was previously unachievable, enabling tensor decomposition based algorithms to be applied in larger-scale scenarios."
1339,https://arxiv.org/abs/1812.05306,Optimal Algorithm for Profiling Dynamic Arrays with Finite Values,"How can one quickly answer the most and top popular objects at any time, given a large log stream in a system of billions of users? It is equivalent to find the mode and top-frequent elements in a dynamic array corresponding to the log stream. However, most existing work either restrain the dynamic array within a sliding window, or do not take advantages of only one element can be added or removed in a log stream. Therefore, we propose a profiling algorithm, named S-Profile, which is of $O(1)$ time complexity for every updating of the dynamic array, and optimal in terms of computational complexity. With the profiling results, answering the queries on the statistics of dynamic array becomes trivial and fast. With the experiments of various settings of dynamic arrays, our accurate S-Profile algorithm outperforms the well-known methods, showing at least 2X speedup to the heap based approach and 13X or larger speedup to the balanced tree based approach."
1340,https://arxiv.org/abs/1810.06860,Faster Matrix Completion Using Randomized SVD,"Matrix completion is a widely used technique for image inpainting and personalized recommender system, etc. In this work, we focus on accelerating the matrix completion using faster randomized singular value decomposition (rSVD). Firstly, two fast randomized algorithms (rSVD-PI and rSVD- BKI) are proposed for handling sparse matrix. They make use of an eigSVD procedure and several accelerating skills. Then, with the rSVD-BKI algorithm and a new subspace recycling technique, we accelerate the singular value thresholding (SVT) method in [1] to realize faster matrix completion. Experiments show that the proposed rSVD algorithms can be 6X faster than the basic rSVD algorithm [2] while keeping same accuracy. For image inpainting and movie-rating estimation problems, the proposed accelerated SVT algorithm consumes 15X and 8X less CPU time than the methods using svds and lansvd respectively, without loss of accuracy."
1341,https://arxiv.org/abs/1810.06825,Fast Randomized PCA for Sparse Data,"Principal component analysis (PCA) is widely used for dimension reduction and embedding of real data in social network analysis, information retrieval, and natural language processing, etc. In this work we propose a fast randomized PCA algorithm for processing large sparse data. The algorithm has similar accuracy to the basic randomized SVD (rPCA) algorithm (Halko et al., 2011), but is largely optimized for sparse data. It also has good flexibility to trade off runtime against accuracy for practical usage. Experiments on real data show that the proposed algorithm is up to 9.1X faster than the basic rPCA algorithm without accuracy loss, and is up to 20X faster than the svds in Matlab with little error. The algorithm computes the first 100 principal components of a large information retrieval data with 12,869,521 persons and 323,899 keywords in less than 400 seconds on a 24-core machine, while all conventional methods fail due to the out-of-memory issue."
1342,https://arxiv.org/abs/1807.10119,A Unified Approximation Framework for Compressing and Accelerating Deep Neural Networks,"Deep neural networks (DNNs) have achieved significant success in a variety of real world applications, i.e., image classification. However, tons of parameters in the networks restrict the efficiency of neural networks due to the large model size and the intensive computation. To address this issue, various approximation techniques have been investigated, which seek for a light weighted network with little performance degradation in exchange of smaller model size or faster inference. Both low-rankness and sparsity are appealing properties for the network approximation. In this paper we propose a unified framework to compress the convolutional neural networks (CNNs) by combining these two properties, while taking the nonlinear activation into consideration. Each layer in the network is approximated by the sum of a structured sparse component and a low-rank component, which is formulated as an optimization problem. Then, an extended version of alternating direction method of multipliers (ADMM) with guaranteed convergence is presented to solve the relaxed optimization problem. Experiments are carried out on VGG-16, AlexNet and GoogLeNet with large image classification datasets. The results outperform previous work in terms of accuracy degradation, compression rate and speedup ratio. The proposed method is able to remarkably compress the model (with up to 4.9x reduction of parameters) at a cost of little loss or without loss on accuracy."
1343,https://arxiv.org/abs/1804.06128,Fast and Accurate Tensor Completion with Total Variation Regularized Tensor Trains,"We propose a new tensor completion method based on tensor trains. The to-be-completed tensor is modeled as a low-rank tensor train, where we use the known tensor entries and their coordinates to update the tensor train. A novel tensor train initialization procedure is proposed specifically for image and video completion, which is demonstrated to ensure fast convergence of the completion algorithm. The tensor train framework is also shown to easily accommodate Total Variation and Tikhonov regularization due to their low-rank tensor train representations. Image and video inpainting experiments verify the superiority of the proposed scheme in terms of both speed and scalability, where a speedup of up to 155X is observed compared to state-of-the-art tensor completion methods at a similar accuracy. Moreover, we demonstrate the proposed scheme is especially advantageous over existing algorithms when only tiny portions (say, 1%) of the to-be-completed images/videos are known."
1344,https://arxiv.org/abs/1707.07803,Computing low-rank approximations of large-scale matrices with the Tensor Network randomized SVD,"We propose a new algorithm for the computation of a singular value decomposition (SVD) low-rank approximation of a matrix in the Matrix Product Operator (MPO) format, also called the Tensor Train Matrix format. Our tensor network randomized SVD (TNrSVD) algorithm is an MPO implementation of the randomized SVD algorithm that is able to compute dominant singular values and their corresponding singular vectors. In contrast to the state-of-the-art tensor-based alternating least squares SVD (ALS-SVD) and modified alternating least squares SVD (MALS-SVD) matrix approximation methods, TNrSVD can be up to 17 times faster while achieving the same accuracy. In addition, our TNrSVD algorithm also produces accurate approximations in particular cases where both ALS-SVD and MALS-SVD fail to converge. We also propose a new algorithm for the fast conversion of a sparse matrix into its corresponding MPO form, which is up to 509 times faster than the standard Tensor Train SVD (TT-SVD) method while achieving machine precision accuracy. The efficiency and accuracy of both algorithms are demonstrated in numerical experiments."
1345,https://arxiv.org/abs/1704.07669,Single-Pass PCA of Large High-Dimensional Data,"Principal component analysis (PCA) is a fundamental dimension reduction tool in statistics and machine learning. For large and high-dimensional data, computing the PCA (i.e., the singular vectors corresponding to a number of dominant singular values of the data matrix) becomes a challenging task. In this work, a single-pass randomized algorithm is proposed to compute PCA with only one pass over the data. It is suitable for processing extremely large and high-dimensional data stored in slow memory (hard disk) or the data generated in a streaming fashion. Experiments with synthetic and real data validate the algorithm's accuracy, which has orders of magnitude smaller error than an existing single-pass algorithm. For a set of high-dimensional data stored as a 150 GB file, the proposed algorithm is able to compute the first 50 principal components in just 24 minutes on a typical 24-core computer, with less than 1 GB memory cost."
1346,https://arxiv.org/abs/1704.05528,A Fast Implementation of Singular Value Thresholding Algorithm using Recycling Rank Revealing Randomized Singular Value Decomposition,"In this paper, we present a fast implementation of the Singular Value Thresholding (SVT) algorithm for matrix completion. A rank-revealing randomized singular value decomposition (R3SVD) algorithm is used to adaptively carry out partial singular value decomposition (SVD) to fast approximate the SVT operator given a desired, fixed precision. We extend the R3SVD algorithm to a recycling rank revealing randomized singular value decomposition (R4SVD) algorithm by reusing the left singular vectors obtained from the previous iteration as the approximate basis in the current iteration, where the computational cost for partial SVD at each SVT iteration is significantly reduced. A simulated annealing style cooling mechanism is employed to adaptively adjust the low-rank approximation precision threshold as SVT progresses. Our fast SVT implementation is effective in both large and small matrices, which is demonstrated in matrix completion applications including image recovery and movie recommendation system."
1347,https://arxiv.org/abs/1606.09402,Efficient Randomized Algorithms for the Fixed-Precision Low-Rank Matrix Approximation,"Randomized algorithms for low-rank matrix approximation are investigated, with the emphasis on the fixed-precision problem and computational efficiency for handling large matrices. The algorithms are based on the so-called QB factorization, where Q is an orthonormal matrix. Firstly, a mechanism for calculating the approximation error in Frobenius norm is proposed, which enables efficient adaptive rank determination for large and/or sparse matrix. It can be combined with any QB-form factorization algorithm in which B's rows are incrementally generated. Based on the blocked randQB algorithm by P.-G. Martinsson and S. Voronin, this results in an algorithm called randQB EI. Then, we further revise the algorithm to obtain a pass-efficient algorithm, randQB FP, which is mathematically equivalent to the existing randQB algorithms and also suitable for the fixed-precision problem. Especially, randQB FP can serve as a single-pass algorithm for calculating leading singular values, under certain condition. With large and/or sparse test matrices, we have empirically validated the merits of the proposed techniques, which exhibit remarkable speedup and memory saving over the blocked randQB algorithm. We have also demonstrated that the single-pass algorithm derived by randQB FP is much more accurate than an existing single-pass algorithm. And with data from a scenic image and an information retrieval application, we have shown the advantages of the proposed algorithms over the adaptive range finder algorithm for solving the fixed-precision problem."
1348,https://arxiv.org/abs/1605.08134,A Rank Revealing Randomized Singular Value Decomposition (R3SVD) Algorithm for Low-rank Matrix Approximations,"In this paper, we present a Rank Revealing Randomized Singular Value Decomposition (R3SVD) algorithm to incrementally construct a low-rank approximation of a potentially large matrix while adaptively estimating the appropriate rank that can capture most of the actions of the matrix. Starting from a low-rank approximation with an initial guessed rank, R3SVD adopts an orthogonal Gaussian sampling approach to obtain the dominant subspace within the leftover space, which is used to add up to the existing low-rank approximation. Orthogonal Gaussian sampling is repeated until an appropriate low-rank approximation with satisfactory accuracy, measured by the overall energy percentage of the original matrix, is obtained. While being a fast algorithm, R3SVD is also a memory-aware algorithm where the computational process can be decomposed into a series of sampling tasks that use constant amount of memory. Numerical examples in image compression and matrix completion are used to demonstrate the effectiveness of R3SVD in low-rank approximation."
1349,https://arxiv.org/abs/1511.04515,An Algorithmic Framework for Efficient Large-Scale Circuit Simulation Using Exponential Integrators,"We propose an efficient algorithmic framework for time domain circuit simulation using exponential integrator. This work addresses several critical issues exposed by previous matrix exponential based circuit simulation research, and makes it capable of simulating stiff nonlinear circuit system at a large scale. In this framework, the system's nonlinearity is treated with exponential Rosenbrock-Euler formulation. The matrix exponential and vector product is computed using invert Krylov subspace method. Our proposed method has several distinguished advantages over conventional formulations (e.g., the well-known backward Euler with Newton-Raphson method). The matrix factorization is performed only for the conductance/resistance matrix G, without being performed for the combinations of the capacitance/inductance matrix C and matrix G, which are used in traditional implicit formulations. Furthermore, due to the explicit nature of our formulation, we do not need to repeat LU decompositions when adjusting the length of time steps for error controls. Our algorithm is better suited to solving tightly coupled post-layout circuits in the pursuit for full-chip simulation. Our experimental results validate the advantages of our framework."
1350,https://arxiv.org/abs/1505.06699,Simulation Algorithms with Exponential Integration for Time-Domain Analysis of Large-Scale Power Delivery Networks,"We design an algorithmic framework using matrix exponentials for time-domain simulation of power delivery network (PDN). Our framework can reuse factorized matrices to simulate the large-scale linear PDN system with variable stepsizes. In contrast, current conventional PDN simulation solvers have to use fixed step-size approach in order to reuse factorized matrices generated by the expensive matrix decomposition. Based on the proposed exponential integration framework, we design a PDN solver R-MATEX with the flexible time-stepping capability. The key operation of matrix exponential and vector product (MEVP) is computed by the rational Krylov subspace method.
  To further improve the runtime, we also propose a distributed computing framework DR-MATEX. DR-MATEX reduces Krylov subspace generations caused by frequent breakpoints from a large number of current sources during simulation. By virtue of the superposition property of linear system and scaling invariance property of Krylov subspace, DR-MATEX can divide the whole simulation task into subtasks based on the alignments of breakpoints among those sources. The subtasks are processed in parallel at different computing nodes without any communication during the computation of transient simulation. The final result is obtained by summing up the partial results among all the computing nodes after they finish the assigned subtasks. Therefore, our computation model belongs to the category known as Embarrassingly Parallel model.
  Experimental results show R-MATEX and DR-MATEX can achieve up to around 14.4X and 98.0X runtime speedups over traditional trapezoidal integration based solver with fixed timestep approach."
1351,https://arxiv.org/abs/1012.4182,The Future of Social Experimenting,Recent lab experiments by Traulsen et al. for the spatial prisoner's dilemma suggest that exploratory behavior of human subjects prevents cooperation through neighborhood interactions over experimentally accessible time spans. This indicates that new theoretical and experimental efforts are needed to explore the mechanisms underlying a number of famous puzzles in the social sciences.
1352,https://arxiv.org/abs/0903.4054,The outbreak of cooperation among success-driven individuals under noisy conditions,"According to Thomas Hobbes' Leviathan [1651; 2008 (Touchstone, New York), English Ed], ""the life of man [is] solitary, poor, nasty, brutish, and short,"" and it would need powerful social institutions to establish social order. In reality, however, social cooperation can also arise spontaneously, based on local interactions rather than centralized control. The self-organization of cooperative behavior is particularly puzzling for social dilemmas related to sharing natural resources or creating common goods. Such situations are often described by the prisoner's dilemma. Here, we report the sudden outbreak of predominant cooperation in a noisy world dominated by selfishness and defection, when individuals imitate superior strategies and show success-driven migration. In our model, individuals are unrelated, and do not inherit behavioral traits. They defect or cooperate selfishly when the opportunity arises, and they do not know how often they will interact or have interacted with someone else. Moreover, our individuals have no reputation mechanism to form friendship networks, nor do they have the option of voluntary interaction or costly punishment. Therefore, the outbreak of prevailing cooperation, when directed motion is integrated in a game-theoretical model, is remarkable, particularly when random strategy mutations and random relocations challenge the formation and survival of cooperative clusters. Our results suggest that mobility is significant for the evolution of social order, and essential for its stabilization and maintenance."
1353,https://arxiv.org/abs/0903.0987,Game Theoretical Interactions of Moving Agents,"Game theory has been one of the most successful quantitative concepts to describe social interactions, their strategical aspects, and outcomes. Among the payoff matrix quantifying the result of a social interaction, the interaction conditions have been varied, such as the number of repeated interactions, the number of interaction partners, the possibility to punish defective behavior etc. While an extension to spatial interactions has been considered early on such as in the ""game of life"", recent studies have focussed on effects of the structure of social interaction networks.
  However, the possibility of individuals to move and, thereby, evade areas with a high level of defection, and to seek areas with a high level of cooperation, has not been fully explored so far. This contribution presents a model combining game theoretical interactions with success-driven motion in space, and studies the consequences that this may have for the degree of cooperation and the spatio-temporal dynamics in the population. It is demonstrated that the combination of game theoretical interactions with motion gives rise to many self-organized behavioral patterns on an aggregate level, which can explain a variety of empirically observed social behaviors."
1354,https://arxiv.org/abs/0708.3282,Modeling Crowd Turbulence by Many-Particle Simulations,"A recent study [D. Helbing, A. Johansson and H. Z. Al-Abideen, {\it Phys. Rev. E} 75, 046109 (2007)] has revealed a ""turbulent"" state of pedestrian flows, which is characterized by sudden displacements and causes the falling and trampling of people. However, turbulent crowd motion is not reproduced well by current many-particle models due to their insufficient representation of the local interactions in areas of extreme densities. In this contribution, we extend the repulsive force term of the social force model to reproduce crowd turbulence. We perform numerical simulations of pedestrians moving through a bottleneck area with this new model. The transitions from laminar to stop-and-go and turbulent flows are observed. The empirical features characterizing crowd turbulence, such as the structure function and the probability density function of velocity increments are reproduced well, i.e. they are well compatible with an analysis of video data during the annual Muslim pilgrimage."
1355,https://arxiv.org/abs/2301.07482,ReFresh: Reducing Memory Access from Exploiting Stable Historical Embeddings for Graph Neural Network Training,"A key performance bottleneck when training graph neural network (GNN) models on large, real-world graphs is loading node features onto a GPU. Due to limited GPU memory, expensive data movement is necessary to facilitate the storage of these features on alternative devices with slower access (e.g. CPU memory). Moreover, the irregularity of graph structures contributes to poor data locality which further exacerbates the problem. Consequently, existing frameworks capable of efficiently training large GNN models usually incur a significant accuracy degradation because of the inevitable shortcuts involved. To address these limitations, we instead propose ReFresh, a general-purpose GNN mini-batch training framework that leverages a historical cache for storing and reusing GNN node embeddings instead of re-computing them through fetching raw features at every iteration. Critical to its success, the corresponding cache policy is designed, using a combination of gradient-based and staleness criteria, to selectively screen those embeddings which are relatively stable and can be cached, from those that need to be re-computed to reduce estimation errors and subsequent downstream accuracy loss. When paired with complementary system enhancements to support this selective historical cache, ReFresh is able to accelerate the training speed on large graph datasets such as ogbn-papers100M and MAG240M by 4.6x up to 23.6x and reduce the memory access by 64.5% (85.7% higher than a raw feature cache), with less than 1% influence on test accuracy."
1356,https://arxiv.org/abs/2208.02025,OLLIE: Derivation-based Tensor Program Optimizer,"Boosting the runtime performance of deep neural networks (DNNs) is critical due to their wide adoption in real-world tasks. Existing approaches to optimizing the tensor algebra expression of a DNN only consider expressions representable by a fixed set of predefined operators, missing possible optimization opportunities between general expressions. We propose OLLIE, the first derivation-based tensor program optimizer. OLLIE optimizes tensor programs by leveraging transformations between general tensor algebra expressions, enabling a significantly larger expression search space that includes those supported by prior work as special cases. OLLIE uses a hybrid derivation-based optimizer that effectively combines explorative and guided derivations to quickly discover highly optimized expressions. Evaluation on seven DNNs shows that OLLIE can outperform existing optimizers by up to 2.73$\times$ (1.46$\times$ on average) on an A100 GPU and up to 2.68$\times$ (1.51$\times$) on a V100 GPU, respectively."
1357,https://arxiv.org/abs/2202.07628,Suppressing ZZ Crosstalk of Quantum Computers through Pulse and Scheduling Co-Optimization,"Noise is a significant obstacle to quantum computing, and $ZZ$ crosstalk is one of the most destructive types of noise affecting superconducting qubits. Previous approaches to suppressing $ZZ$ crosstalk have mainly relied on specific chip design that can complicate chip fabrication and aggravate decoherence. To some extent, special chip design can be avoided by relying on pulse optimization to suppress $ZZ$ crosstalk. However, existing approaches are non-scalable, as their required time and memory grow exponentially with the number of qubits involved. To address the above problems, we propose a scalable approach by co-optimizing pulses and scheduling. We optimize pulses to offer an ability to suppress $ZZ$ crosstalk surrounding a gate, and then design scheduling strategies to exploit this ability and achieve suppression across the whole circuit. A main advantage of such co-optimization is that it does not require special hardware support. Besides, we implement our approach as a general framework that is compatible with different pulse optimization methods. We have conducted extensive evaluations by simulation and on a real quantum computer. Simulation results show that our proposal can improve the fidelity of quantum computing on $4{\sim}12$ qubits by up to $81\times$ ($11\times$ on average). Ramsey experiments on a real quantum computer also demonstrate that our method can eliminate the effect of $ZZ$ crosstalk to a great extent."
1358,https://arxiv.org/abs/2106.06889,G-TADOC: Enabling Efficient GPU-Based Text Analytics without Decompression,"Text analytics directly on compression (TADOC) has proven to be a promising technology for big data analytics. GPUs are extremely popular accelerators for data analytics systems. Unfortunately, no work so far shows how to utilize GPUs to accelerate TADOC. We describe G-TADOC, the first framework that provides GPU-based text analytics directly on compression, effectively enabling efficient text analytics on GPUs without decompressing the input data. G-TADOC solves three major challenges. First, TADOC involves a large amount of dependencies, which makes it difficult to exploit massive parallelism on a GPU. We develop a novel fine-grained thread-level workload scheduling strategy for GPU threads, which partitions heavily-dependent loads adaptively in a fine-grained manner. Second, in developing G-TADOC, thousands of GPU threads writing to the same result buffer leads to inconsistency while directly using locks and atomic operations lead to large synchronization overheads. We develop a memory pool with thread-safe data structures on GPUs to handle such difficulties. Third, maintaining the sequence information among words is essential for lossless compression. We design a sequence-support strategy, which maintains high GPU parallelism while ensuring sequence information. Our experimental evaluations show that G-TADOC provides 31.1x average speedup compared to state-of-the-art TADOC."
1359,https://arxiv.org/abs/2009.10955,GraphPi: High Performance Graph Pattern Matching through Effective Redundancy Elimination,"Graph pattern matching, which aims to discover structural patterns in graphs, is considered one of the most fundamental graph mining problems in many real applications. Despite previous efforts, existing systems face two main challenges. First, inherent symmetry existing in patterns can introduce a large amount of redundant computation. Second, different matching orders for a pattern have significant performance differences and are quite hard to predict. When these factors are mixed, this problem becomes extremely complicated. High efficient pattern matching remains an open problem currently. To address these challenges, we propose GraphPi, a high performance distributed pattern matching system. GraphPi utilizes a new algorithm based on 2-cycles in group theory to generate multiple sets of asymmetric restrictions, where each set can eliminate redundant computation completely. We further design an accurate performance model to determine the optimal matching order and asymmetric restriction set for efficient pattern matching. We evaluate GraphPi on Tianhe-2A supercomputer. Results show that GraphPi outperforms the state-ofthe-art system, by up to 105X for 6 real-world graph datasets on a single node. We also scale GraphPi to 1,024 computing nodes (24,576 cores)."
1360,https://arxiv.org/abs/2009.01692,ScalAna: Automating Scaling Loss Detection with Graph Analysis,"Scaling a parallel program to modern supercomputers is challenging due to inter-process communication, Amdahl's law, and resource contention. Performance analysis tools for finding such scaling bottlenecks either base on profiling or tracing. Profiling incurs low overheads but does not capture detailed dependencies needed for root-cause analysis. Tracing collects all information at prohibitive overheads. In this work, we design ScalAna that uses static analysis techniques to achieve the best of both worlds - it enables the analyzability of traces at a cost similar to profiling. ScalAna first leverages static compiler techniques to build a Program Structure Graph, which records the main computation and communication patterns as well as the program's control structures. At runtime, we adopt lightweight techniques to collect performance data according to the graph structure and generate a Program Performance Graph. With this graph, we propose a novel approach, called backtracking root cause detection, which can automatically and efficiently detect the root cause of scaling loss. We evaluate ScalAna with real applications. Results show that our approach can effectively locate the root cause of scaling loss for real applications and incurs 1.73% overhead on average for up to 2,048 processes. We achieve up to 11.11% performance improvement by fixing the root causes detected by ScalAna on 2,048 processes."
1361,https://arxiv.org/abs/1705.05541,Algorithm-Directed Crash Consistence in Non-Volatile Memory for HPC,"Fault tolerance is one of the major design goals for HPC. The emergence of non-volatile memories (NVM) provides a solution to build fault tolerant HPC. Data in NVM-based main memory are not lost when the system crashes because of the non-volatility nature of NVM. However, because of volatile caches, data must be logged and explicitly flushed from caches into NVM to ensure consistence and correctness before crashes, which can cause large runtime overhead.
  In this paper, we introduce an algorithm-based method to establish crash consistence in NVM for HPC applications. We slightly extend application data structures or sparsely flush cache blocks, which introduce ignorable runtime overhead. Such extension or cache flushing allows us to use algorithm knowledge to \textit{reason} data consistence or correct inconsistent data when the application crashes. We demonstrate the effectiveness of our method for three algorithms, including an iterative solver, dense matrix multiplication, and Monte-Carlo simulation. Based on comprehensive performance evaluation on a variety of test environments, we demonstrate that our approach has very small runtime overhead (at most 8.2\% and less than 3\% in most cases), much smaller than that of traditional checkpoint, while having the same or less recomputation cost."
1362,https://arxiv.org/abs/2012.02371,Scale-aware Insertion of Virtual Objects in Monocular Videos,"In this paper, we propose a scale-aware method for inserting virtual objects with proper sizes into monocular videos. To tackle the scale ambiguity problem of geometry recovery from monocular videos, we estimate the global scale objects in a video with a Bayesian approach incorporating the size priors of objects, where the scene objects sizes should strictly conform to the same global scale and the possibilities of global scales are maximized according to the size distribution of object categories. To do so, we propose a dataset of sizes of object categories: Metric-Tree, a hierarchical representation of sizes of more than 900 object categories with the corresponding images. To handle the incompleteness of objects recovered from videos, we propose a novel scale estimation method that extracts plausible dimensions of objects for scale optimization. Experiments have shown that our method for scale estimation performs better than the state-of-the-art methods, and has considerable validity and robustness for different video scenes. Metric-Tree has been made available at: https://metric-tree.github.io"
1363,https://arxiv.org/abs/1606.00103,A Comparative Study of Algorithms for Realtime Panoramic Video Blending,"Unlike image blending algorithms, video blending algorithms have been little studied. In this paper, we investigate 6 popular blending algorithms---feather blending, multi-band blending, modified Poisson blending, mean value coordinate blending, multi-spline blending and convolution pyramid blending. We consider in particular realtime panoramic video blending, a key problem in various virtual reality tasks. To evaluate the performance of the 6 algorithms on this problem, we have created a video benchmark of several videos captured under various conditions. We analyze the time and memory needed by the above 6 algorithms, for both CPU and GPU implementations (where readily parallelizable). The visual quality provided by these algorithms is also evaluated both objectively and subjectively. The video benchmark and algorithm implementations are publicly available."
1364,https://arxiv.org/abs/2305.16998,A Tale of Two Approximations: Tightening Over-Approximation for DNN Robustness Verification via Under-Approximation,"The robustness of deep neural networks (DNNs) is crucial to the hosting system's reliability and security. Formal verification has been demonstrated to be effective in providing provable robustness guarantees. To improve its scalability, over-approximating the non-linear activation functions in DNNs by linear constraints has been widely adopted, which transforms the verification problem into an efficiently solvable linear programming problem. Many efforts have been dedicated to defining the so-called tightest approximations to reduce overestimation imposed by over-approximation. In this paper, we study existing approaches and identify a dominant factor in defining tight approximation, namely the approximation domain of the activation function. We find out that tight approximations defined on approximation domains may not be as tight as the ones on their actual domains, yet existing approaches all rely only on approximation domains. Based on this observation, we propose a novel dual-approximation approach to tighten over-approximations, leveraging an activation function's underestimated domain to define tight approximation bounds. We implement our approach with two complementary algorithms based respectively on Monte Carlo simulation and gradient descent into a tool called DualApp. We assess it on a comprehensive benchmark of DNNs with different architectures. Our experimental results show that DualApp significantly outperforms the state-of-the-art approaches with 100% - 1000% improvement on the verified robustness ratio and 10.64% on average (up to 66.53%) on the certified lower bound."
1365,https://arxiv.org/abs/2305.16599,Bridging the Domain Gaps in Context Representations for k-Nearest Neighbor Neural Machine Translation,"$k$-Nearest neighbor machine translation ($k$NN-MT) has attracted increasing attention due to its ability to non-parametrically adapt to new translation domains. By using an upstream NMT model to traverse the downstream training corpus, it is equipped with a datastore containing vectorized key-value pairs, which are retrieved during inference to benefit translation. However, there often exists a significant gap between upstream and downstream domains, which hurts the retrieval accuracy and the final translation quality. To deal with this issue, we propose a novel approach to boost the datastore retrieval of $k$NN-MT by reconstructing the original datastore. Concretely, we design a reviser to revise the key representations, making them better fit for the downstream domain. The reviser is trained using the collected semantically-related key-queries pairs, and optimized by two proposed losses: one is the key-queries semantic distance ensuring each revised key representation is semantically related to its corresponding queries, and the other is an L2-norm loss encouraging revised key representations to effectively retain the knowledge learned by the upstream NMT model. Extensive experiments on domain adaptation tasks demonstrate that our method can effectively boost the datastore retrieval and translation quality of $k$NN-MT.\footnote{Our code is available at \url{https://github.com/DeepLearnXMU/RevisedKey-knn-mt}.}"
1366,https://arxiv.org/abs/2305.16023,NaSGEC: a Multi-Domain Chinese Grammatical Error Correction Dataset from Native Speaker Texts,"We introduce NaSGEC, a new dataset to facilitate research on Chinese grammatical error correction (CGEC) for native speaker texts from multiple domains. Previous CGEC research primarily focuses on correcting texts from a single domain, especially learner essays. To broaden the target domain, we annotate multiple references for 12,500 sentences from three native domains, i.e., social media, scientific writing, and examination. We provide solid benchmark results for NaSGEC by employing cutting-edge CGEC models and different training data. We further perform detailed analyses of the connections and gaps between our domains from both empirical and statistical views. We hope this work can inspire future studies on an important but under-explored direction--cross-domain GEC."
1367,https://arxiv.org/abs/2305.15889,Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization,"Domain generalization (DG) is a prevalent problem in real-world applications, which aims to train well-generalized models for unseen target domains by utilizing several source domains. Since domain labels, i.e., which domain each data point is sampled from, naturally exist, most DG algorithms treat them as a kind of supervision information to improve the generalization performance. However, the original domain labels may not be the optimal supervision signal due to the lack of domain heterogeneity, i.e., the diversity among domains. For example, a sample in one domain may be closer to another domain, its original label thus can be the noise to disturb the generalization learning. Although some methods try to solve it by re-dividing domains and applying the newly generated dividing pattern, the pattern they choose may not be the most heterogeneous due to the lack of the metric for heterogeneity. In this paper, we point out that domain heterogeneity mainly lies in variant features under the invariant learning framework. With contrastive learning, we propose a learning potential-guided metric for domain heterogeneity by promoting learning variant features. Then we notice the differences between seeking variance-based heterogeneity and training invariance-based generalizable model. We thus propose a novel method called Heterogeneity-based Two-stage Contrastive Learning (HTCL) for the DG task. In the first stage, we generate the most heterogeneous dividing pattern with our contrastive metric. In the second stage, we employ an invariance-aimed contrastive learning by re-building pairs with the stable relation hinted by domains and classes, which better utilizes generated domain labels for generalization learning. Extensive experiments show HTCL better digs heterogeneity and yields great generalization performance."
1368,https://arxiv.org/abs/2305.15668,FedHC: A Scalable Federated Learning Framework for Heterogeneous and Resource-Constrained Clients,"Federated Learning (FL) is a distributed learning paradigm that empowers edge devices to collaboratively learn a global model leveraging local data. Simulating FL on GPU is essential to expedite FL algorithm prototyping and evaluations. However, current FL frameworks overlook the disparity between algorithm simulation and real-world deployment, which arises from heterogeneous computing capabilities and imbalanced workloads, thus misleading evaluations of new algorithms. Additionally, they lack flexibility and scalability to accommodate resource-constrained clients. In this paper, we present FedHC, a scalable federated learning framework for heterogeneous and resource-constrained clients. FedHC realizes system heterogeneity by allocating a dedicated and constrained GPU resource budget to each client, and also simulates workload heterogeneity in terms of framework-provided runtime. Furthermore, we enhance GPU resource utilization for scalable clients by introducing a dynamic client scheduler, process manager, and resource-sharing mechanism. Our experiments demonstrate that FedHC has the capability to capture the influence of various factors on client execution time. Moreover, despite resource constraints for each client, FedHC achieves state-of-the-art efficiency compared to existing frameworks without limits. When subjecting existing frameworks to the same resource constraints, FedHC achieves a 2.75x speedup. Code has been released on https://github.com/if-lab-repository/FedHC."
1369,https://arxiv.org/abs/2305.15273,Revisiting Token Dropping Strategy in Efficient BERT Pretraining,"Token dropping is a recently-proposed strategy to speed up the pretraining of masked language models, such as BERT, by skipping the computation of a subset of the input tokens at several middle layers. It can effectively reduce the training time without degrading much performance on downstream tasks. However, we empirically find that token dropping is prone to a semantic loss problem and falls short in handling semantic-intense tasks. Motivated by this, we propose a simple yet effective semantic-consistent learning method (ScTD) to improve the token dropping. ScTD aims to encourage the model to learn how to preserve the semantic information in the representation space. Extensive experiments on 12 tasks show that, with the help of our ScTD, token dropping can achieve consistent and significant performance gains across all task types and model sizes. More encouragingly, ScTD saves up to 57% of pretraining time and brings up to +1.56% average improvement over the vanilla token dropping."
1370,https://arxiv.org/abs/2305.13653,RaSa: Relation and Sensitivity Aware Representation Learning for Text-based Person Search,"Text-based person search aims to retrieve the specified person images given a textual description. The key to tackling such a challenging task is to learn powerful multi-modal representations. Towards this, we propose a Relation and Sensitivity aware representation learning method (RaSa), including two novel tasks: Relation-Aware learning (RA) and Sensitivity-Aware learning (SA). For one thing, existing methods cluster representations of all positive pairs without distinction and overlook the noise problem caused by the weak positive pairs where the text and the paired image have noise correspondences, thus leading to overfitting learning. RA offsets the overfitting risk by introducing a novel positive relation detection task (i.e., learning to distinguish strong and weak positive pairs). For another thing, learning invariant representation under data augmentation (i.e., being insensitive to some transformations) is a general practice for improving representation's robustness in existing methods. Beyond that, we encourage the representation to perceive the sensitive transformation by SA (i.e., learning to detect the replaced words), thus promoting the representation's robustness. Experiments demonstrate that RaSa outperforms existing state-of-the-art methods by 6.94%, 4.45% and 15.35% in terms of Rank@1 on CUHK-PEDES, ICFG-PEDES and RSTPReid datasets, respectively. Code is available at: https://github.com/Flame-Chasers/RaSa."
1371,https://arxiv.org/abs/2305.12964,Text-based Person Search without Parallel Image-Text Data,"Text-based person search (TBPS) aims to retrieve the images of the target person from a large image gallery based on a given natural language description. Existing methods are dominated by training models with parallel image-text pairs, which are very costly to collect. In this paper, we make the first attempt to explore TBPS without parallel image-text data ($μ$-TBPS), in which only non-parallel images and texts, or even image-only data, can be adopted. Towards this end, we propose a two-stage framework, generation-then-retrieval (GTR), to first generate the corresponding pseudo text for each image and then perform the retrieval in a supervised manner. In the generation stage, we propose a fine-grained image captioning strategy to obtain an enriched description of the person image, which firstly utilizes a set of instruction prompts to activate the off-the-shelf pretrained vision-language model to capture and generate fine-grained person attributes, and then converts the extracted attributes into a textual description via the finetuned large language model or the hand-crafted template. In the retrieval stage, considering the noise interference of the generated texts for training model, we develop a confidence score-based training scheme by enabling more reliable texts to contribute more during the training. Experimental results on multiple TBPS benchmarks (i.e., CUHK-PEDES, ICFG-PEDES and RSTPReid) show that the proposed GTR can achieve a promising performance without relying on parallel image-text data."
1372,https://arxiv.org/abs/2305.12945,ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination,"As ChatGPT and GPT-4 spearhead the development of Large Language Models (LLMs), more researchers are investigating their performance across various tasks. But more research needs to be done on the interpretability capabilities of LLMs, that is, the ability to generate reasons after an answer has been given. Existing explanation datasets are mostly English-language general knowledge questions, which leads to insufficient thematic and linguistic diversity. To address the language bias and lack of medical resources in generating rationales QA datasets, we present ExplainCPE (over 7k instances), a challenging medical benchmark in Simplified Chinese. We analyzed the errors of ChatGPT and GPT-4, pointing out the limitations of current LLMs in understanding text and computational reasoning. During the experiment, we also found that different LLMs have different preferences for in-context learning. ExplainCPE presents a significant challenge, but its potential for further investigation is promising, and it can be used to evaluate the ability of a model to generate explanations. AI safety and trustworthiness need more attention, and this work makes the first step to explore the medical interpretability of LLMs.The dataset is available at https://github.com/HITsz-TMG/ExplainCPE."
1373,https://arxiv.org/abs/2305.12839,CopyNE: Better Contextual ASR by Copying Named Entities,"Recent years have seen remarkable progress in automatic speech recognition (ASR). However, traditional token-level ASR models have struggled with accurately transcribing entities due to the problem of homophonic and near-homophonic tokens. This paper introduces a novel approach called CopyNE, which uses a span-level copying mechanism to improve ASR in transcribing entities. CopyNE can copy all tokens of an entity at once, effectively avoiding errors caused by homophonic or near-homophonic tokens that occur when predicting multiple tokens separately. Experiments on Aishell and ST-cmds datasets demonstrate that CopyNE achieves significant reductions in character error rate (CER) and named entity CER (NE-CER), especially in entity-rich scenarios. Furthermore, even when compared to the strong Whisper baseline, CopyNE still achieves notable reductions in CER and NE-CER. Qualitative comparisons with previous approaches demonstrate that CopyNE can better handle entities, effectively improving the accuracy of ASR."
1374,https://arxiv.org/abs/2305.12441,A Pilot Study on Dialogue-Level Dependency Parsing for Chinese,"Dialogue-level dependency parsing has received insufficient attention, especially for Chinese. To this end, we draw on ideas from syntactic dependency and rhetorical structure theory (RST), developing a high-quality human-annotated corpus, which contains 850 dialogues and 199,803 dependencies. Considering that such tasks suffer from high annotation costs, we investigate zero-shot and few-shot scenarios. Based on an existing syntactic treebank, we adopt a signal-based method to transform seen syntactic dependencies into unseen ones between elementary discourse units (EDUs), where the signals are detected by masked language modeling. Besides, we apply single-view and multi-view data selection to access reliable pseudo-labeled instances. Experimental results show the effectiveness of these baselines. Moreover, we discuss several crucial points about our dataset and approach."
1375,https://arxiv.org/abs/2305.12258,Constructing Code-mixed Universal Dependency Forest for Unbiased Cross-lingual Relation Extraction,"Latest efforts on cross-lingual relation extraction (XRE) aggressively leverage the language-consistent structural features from the universal dependency (UD) resource, while they may largely suffer from biased transfer (e.g., either target-biased or source-biased) due to the inevitable linguistic disparity between languages. In this work, we investigate an unbiased UD-based XRE transfer by constructing a type of code-mixed UD forest. We first translate the sentence of the source language to the parallel target-side language, for both of which we parse the UD tree respectively. Then, we merge the source-/target-side UD structures as a unified code-mixed UD forest. With such forest features, the gaps of UD-based XRE between the training and predicting phases can be effectively closed. We conduct experiments on the ACE XRE benchmark datasets, where the results demonstrate that the proposed code-mixed UD forests help unbiased UD-based XRE transfer, with which we achieve significant XRE performance gains."
1376,https://arxiv.org/abs/2305.12256,Scene Graph as Pivoting: Inference-time Image-free Unsupervised Multimodal Machine Translation with Visual Scene Hallucination,"In this work, we investigate a more realistic unsupervised multimodal machine translation (UMMT) setup, inference-time image-free UMMT, where the model is trained with source-text image pairs, and tested with only source-text inputs. First, we represent the input images and texts with the visual and language scene graphs (SG), where such fine-grained vision-language features ensure a holistic understanding of the semantics. To enable pure-text input during inference, we devise a visual scene hallucination mechanism that dynamically generates pseudo visual SG from the given textual SG. Several SG-pivoting based learning objectives are introduced for unsupervised translation training. On the benchmark Multi30K data, our SG-based method outperforms the best-performing baseline by significant BLEU scores on the task and setup, helping yield translations with better completeness, relevance and fluency without relying on paired images. Further in-depth analyses reveal how our model advances in the task setting."
1377,https://arxiv.org/abs/2305.11768,Generating Visual Spatial Description via Holistic 3D Scene Understanding,"Visual spatial description (VSD) aims to generate texts that describe the spatial relations of the given objects within images. Existing VSD work merely models the 2D geometrical vision features, thus inevitably falling prey to the problem of skewed spatial understanding of target objects. In this work, we investigate the incorporation of 3D scene features for VSD. With an external 3D scene extractor, we obtain the 3D objects and scene features for input images, based on which we construct a target object-centered 3D spatial scene graph (Go3D-S2G), such that we model the spatial semantics of target objects within the holistic 3D scenes. Besides, we propose a scene subgraph selecting mechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where the diverse local structure features are navigated to yield spatially-diversified text generation. Experimental results on two VSD datasets demonstrate that our framework outperforms the baselines significantly, especially improving on the cases with complex visual spatial relations. Meanwhile, our method can produce more spatially-diversified generation. Code is available at https://github.com/zhaoyucs/VSD."
1378,https://arxiv.org/abs/2305.04557,Toward Adversarial Training on Contextualized Language Representation,"Beyond the success story of adversarial training (AT) in the recent text domain on top of pre-trained language models (PLMs), our empirical study showcases the inconsistent gains from AT on some tasks, e.g. commonsense reasoning, named entity recognition. This paper investigates AT from the perspective of the contextualized language representation outputted by PLM encoders. We find the current AT attacks lean to generate sub-optimal adversarial examples that can fool the decoder part but have a minor effect on the encoder. However, we find it necessary to effectively deviate the latter one to allow AT to gain. Based on the observation, we propose simple yet effective \textit{Contextualized representation-Adversarial Training} (CreAT), in which the attack is explicitly optimized to deviate the contextualized representation of the encoder. It allows a global optimization of adversarial examples that can fool the entire model. We also find CreAT gives rise to a better direction to optimize the adversarial examples, to let them less sensitive to hyperparameters. Compared to AT, CreAT produces consistent performance gains on a wider range of tasks and is proven to be more effective for language pre-training where only the encoder part is kept for downstream tasks. We achieve the new state-of-the-art performances on a series of challenging benchmarks, e.g. AdvGLUE (59.1 $ \rightarrow $ 61.1), HellaSWAG (93.0 $ \rightarrow $ 94.9), ANLI (68.1 $ \rightarrow $ 69.3)."
1379,https://arxiv.org/abs/2305.04530,A Multi-Modal Context Reasoning Approach for Conditional Inference on Joint Textual and Visual Clues,"Conditional inference on joint textual and visual clues is a multi-modal reasoning task that textual clues provide prior permutation or external knowledge, which are complementary with visual content and pivotal to deducing the correct option. Previous methods utilizing pretrained vision-language models (VLMs) have achieved impressive performances, yet they show a lack of multimodal context reasoning capability, especially for text-modal information. To address this issue, we propose a Multi-modal Context Reasoning approach, named ModCR. Compared to VLMs performing reasoning via cross modal semantic alignment, it regards the given textual abstract semantic and objective image information as the pre-context information and embeds them into the language model to perform context reasoning. Different from recent vision-aided language models used in natural language processing, ModCR incorporates the multi-view semantic alignment information between language and vision by introducing the learnable alignment prefix between image and text in the pretrained language model. This makes the language model well-suitable for such multi-modal reasoning scenario on joint textual and visual clues. We conduct extensive experiments on two corresponding data sets and experimental results show significantly improved performance (exact gain by 4.8% on PMR test set) compared to previous strong baselines. Code Link: \url{https://github.com/YunxinLi/Multimodal-Context-Reasoning}."
1380,https://arxiv.org/abs/2305.04465,Can Diffusion Model Achieve Better Performance in Text Generation? Bridging the Gap between Training and Inference!,"Diffusion models have been successfully adapted to text generation tasks by mapping the discrete text into the continuous space. However, there exist nonnegligible gaps between training and inference, owing to the absence of the forward process during inference. Thus, the model only predicts based on the previously generated reverse noise rather than the noise computed by the forward process. Besides, the widely-used downsampling strategy in speeding up the inference will cause the mismatch of diffusion trajectories between training and inference. To understand and mitigate the above two types of training-inference discrepancies, we launch a thorough preliminary study. Based on our observations, we propose two simple yet effective methods to bridge the gaps mentioned above, named Distance Penalty and Adaptive Decay Sampling. Extensive experiments on \textbf{6} generation tasks confirm the superiority of our methods, which can achieve $100\times \rightarrow 200\times$ speedup with better performance."
1381,https://arxiv.org/abs/2305.03701,LMEye: An Interactive Perception Network for Large Language Models,"Training a Large Visual Language Model (LVLM) from scratch, like GPT-4, is resource-intensive. Our paper presents a play-and-plug module for Large Language Models (LLMs), namely Interactive Perception Network (IPN), aiming to achieve a LVLM by incorporating the image understanding capability into LLMs. Previous methods incorporate visual information into LLMs with a simple visual mapping network, where the image feature is projected into the embedding space of LLMs via a linear layer. Such mapping network projects the image feature once yet does not consider the interaction between the image and the human input query. Hence, the obtained visual information with no connections with human intention may be inadequate for LLMs to make intention-following responses, which we term as static visual information. IPN addresses this issue by allowing the LLM to request the desired visual information aligned with various human instructions, which we term as the dynamic interaction between the LLM and visual information. Specifically, IPN consists of a simple visual mapping network to provide the basic perception of an image for LLMs. It also contains additional modules responsible for acquiring requests from LLMs, performing request-based visual information interaction, and transmitting the resulting interacted visual information to LLMs, respectively. In this way, LLMs act to understand the human query, deliver the corresponding request to the request-based visual information interaction module, and generate the response based on the interleaved multimodal information. We evaluate IPN through extensive experiments on multimodal question answering, reasoning, and so on, demonstrating that it significantly improves the zero-shot performance of LVLMs on various multimodal tasks compared to previous methods."
1382,https://arxiv.org/abs/2305.02265,A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text,"Pretrained Vision-Language Models (VLMs) have achieved remarkable performance in image retrieval from text. However, their performance drops drastically when confronted with linguistically complex texts that they struggle to comprehend. Inspired by the Divide-and-Conquer algorithm and dual-process theory, in this paper, we regard linguistically complex texts as compound proposition texts composed of multiple simple proposition sentences and propose an end-to-end Neural Divide-and-Conquer Reasoning framework, dubbed NDCR. It contains three main components: 1) Divide: a proposition generator divides the compound proposition text into simple proposition sentences and produces their corresponding representations, 2) Conquer: a pretrained VLMs-based visual-linguistic interactor achieves the interaction between decomposed proposition sentences and images, 3) Combine: a neural-symbolic reasoner combines the above reasoning states to obtain the final solution via a neural logic reasoning approach. According to the dual-process theory, the visual-linguistic interactor and neural-symbolic reasoner could be regarded as analogical reasoning System 1 and logical reasoning System 2. We conduct extensive experiments on a challenging image retrieval from contextual descriptions data set. Experimental results and analyses indicate NDCR significantly improves performance in the complex image-text reasoning problem. Code link: https://github.com/YunxinLi/NDCR."
1383,https://arxiv.org/abs/2305.00864,On Chen's theorem over Piatetski-Shapiro type primes and almost-primes,"In this paper, we establish a new mean value theorem of Bombieri-Vinogradov type over Piatetski-Shapiro sequence. Namely, it is proved that for any given constant $A>0$ and any sufficiently small $\varepsilon>0$, there holds \begin{equation*}
  \sum_{\substack{d\leqslant x^ξ\\ (d,l)=1}}\Bigg|\sum_{\substack{A_1(x)\leqslant a<A_2(x)\\ (a,d)=1}}g(a)
  \Bigg(\sum_{\substack{ap\leqslant x\\ ap\equiv l\!\pmod d \\ ap=[k^{1/γ}]}}1
  -\frac{1}{\varphi(d)}\sum_{\substack{ap\leqslant x\\ ap=[k^{1/γ}] }}
  1\Bigg)\Bigg|\ll\frac{x^γ}{(\log x)^A}, \end{equation*} provided that $1\leqslant A_1(x)<A_2(x)\leqslant x^{1-\varepsilon}$ and $g(a)\ll τ_r^s(a)$, where $l\not=0$ is a fixed integer and \begin{equation*} ξ:=ξ(γ)=\frac{2^{38}+17}{38}γ-\frac{2^{38}-1}{38}-\varepsilon \end{equation*} with \begin{equation*}
  1-\frac{18}{2^{38}+17}<γ<1. \end{equation*} Moreover, for $γ$ satisfying \begin{equation*} 1-\frac{0.03208}{2^{38}+17}<γ<1, \end{equation*} we prove that there exist infinitely many primes $p$ such that $p+2=\mathcal{P}_2$ with $\mathcal{P}_2$ being Piatetski-Shapiro almost-primes of type $γ$, and there exist infinitely many Piatetski-Shapiro primes $p$ of type $γ$ such that $p+2=\mathcal{P}_2$. These results generalize the result of Pan and Ding [37] and constitutes an improvement upon a series of previous results of [29,31,39,47]."
1384,https://arxiv.org/abs/2304.12764,Test-Time Adaptation with Perturbation Consistency Learning,"Currently, pre-trained language models (PLMs) do not cope well with the distribution shift problem, resulting in models trained on the training set failing in real test scenarios. To address this problem, the test-time adaptation (TTA) shows great potential, which updates model parameters to suit the test data at the testing time. Existing TTA methods rely on well-designed auxiliary tasks or self-training strategies based on pseudo-label. However, these methods do not achieve good trade-offs regarding performance gains and computational costs. To obtain some insights into such a dilemma, we take two representative TTA methods, i.e., Tent and OIL, for exploration and find that stable prediction is the key to achieving a good balance. Accordingly, in this paper, we propose perturbation consistency learning (PCL), a simple test-time adaptation method to promote the model to make stable predictions for samples with distribution shifts. Extensive experiments on adversarial robustness and cross-lingual transferring demonstrate that our method can achieve higher or comparable performance with less inference time over strong PLM backbones and previous state-of-the-art TTA methods."
1385,https://arxiv.org/abs/2304.07450,Intent-aware Ranking Ensemble for Personalized Recommendation,"Ranking ensemble is a critical component in real recommender systems. When a user visits a platform, the system will prepare several item lists, each of which is generally from a single behavior objective recommendation model. As multiple behavior intents, e.g., both clicking and buying some specific item category, are commonly concurrent in a user visit, it is necessary to integrate multiple single-objective ranking lists into one. However, previous work on rank aggregation mainly focused on fusing homogeneous item lists with the same objective while ignoring ensemble of heterogeneous lists ranked with different objectives with various user intents.
  In this paper, we treat a user's possible behaviors and the potential interacting item categories as the user's intent. And we aim to study how to fuse candidate item lists generated from different objectives aware of user intents. To address such a task, we propose an Intent-aware ranking Ensemble Learning~(IntEL) model to fuse multiple single-objective item lists with various user intents, in which item-level personalized weights are learned. Furthermore, we theoretically prove the effectiveness of IntEL with point-wise, pair-wise, and list-wise loss functions via error-ambiguity decomposition. Experiments on two large-scale real-world datasets also show significant improvements of IntEL on multiple behavior objectives simultaneously compared to previous ranking ensemble models."
1386,https://arxiv.org/abs/2304.06248,LasUIE: Unifying Information Extraction with Latent Adaptive Structure-aware Generative Language Model,"Universally modeling all typical information extraction tasks (UIE) with one generative language model (GLM) has revealed great potential by the latest study, where various IE predictions are unified into a linearized hierarchical expression under a GLM. Syntactic structure information, a type of effective feature which has been extensively utilized in IE community, should also be beneficial to UIE. In this work, we propose a novel structure-aware GLM, fully unleashing the power of syntactic knowledge for UIE. A heterogeneous structure inductor is explored to unsupervisedly induce rich heterogeneous structural representations by post-training an existing GLM. In particular, a structural broadcaster is devised to compact various latent trees into explicit high-order forests, helping to guide a better generation during decoding. We finally introduce a task-oriented structure fine-tuning mechanism, further adjusting the learned structures to most coincide with the end-task's need. Over 12 IE benchmarks across 7 tasks our system shows significant improvements over the baseline UIE system. Further in-depth analyses show that our GLM learns rich task-adaptive structural bias that greatly resolves the UIE crux, the long-range dependence issue and boundary identifying. Source codes are open at https://github.com/ChocoWu/LasUIE."
1387,https://arxiv.org/abs/2303.17117,Learning Reliable Representations for Incomplete Multi-View Partial Multi-Label Classification,"As a cross-topic of multi-view learning and multi-label classification, multi-view multi-label classification has gradually gained traction in recent years. The application of multi-view contrastive learning has further facilitated this process, however, the existing multi-view contrastive learning methods crudely separate the so-called negative pair, which largely results in the separation of samples belonging to the same category or similar ones. Besides, plenty of multi-view multi-label learning methods ignore the possible absence of views and labels. To address these issues, in this paper, we propose an incomplete multi-view partial multi-label classification network named RANK. In this network, a label-driven multi-view contrastive learning strategy is proposed to leverage supervised information to preserve the structure within view and perform consistent alignment across views. Furthermore, we break through the view-level weights inherent in existing methods and propose a quality-aware sub-network to dynamically assign quality scores to each view of each sample. The label correlation information is fully utilized in the final multi-label cross-entropy classification loss, effectively improving the discriminative power. Last but not least, our model is not only able to handle complete multi-view multi-label datasets, but also works on datasets with missing instances and labels. Extensive experiments confirm that our RANK outperforms existing state-of-the-art methods."
1388,https://arxiv.org/abs/2303.13780,Towards Making the Most of ChatGPT for Machine Translation,"ChatGPT shows remarkable capabilities for machine translation (MT). Several prior studies have shown that it achieves comparable results to commercial systems for high-resource languages, but lags behind in complex tasks, e.g, low-resource and distant-language-pairs translation. However, they usually adopt simple prompts which can not fully elicit the capability of ChatGPT. In this report, we aim to further mine ChatGPT's translation ability by revisiting several aspects: temperature, task information, and domain information, and correspondingly propose two (simple but effective) prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts (DSP). We show that: 1) The performance of ChatGPT depends largely on temperature, and a lower temperature usually can achieve better performance; 2) Emphasizing the task information further improves ChatGPT's performance, particularly in complex MT tasks; 3) Introducing domain information can elicit ChatGPT's generalization ability and improve its performance in the specific domain; 4) ChatGPT tends to generate hallucinations for non-English-centric MT tasks, which can be partially addressed by our proposed prompts but still need to be highlighted for the MT/NLP community. We also explore the effects of advanced in-context learning strategies and find a (negative but interesting) observation: the powerful chain-of-thought prompt leads to word-by-word translation behavior, thus bringing significant translation degradation."
1389,https://arxiv.org/abs/2303.12036,Solving polynomial variational inequality problems via Lagrange multiplier expressions and Moment-SOS relaxations,"In this paper, we study variational inequality problems (VIPs) with involved mappings and feasible sets characterized by polynomial functions (namely, polynomial VIPs). We propose a numerical algorithm for computing solutions to polynomial VIPs based on Lagrange multiplier expressions and the Moment-SOS hierarchy of semidefinite relaxations. We also extend our approach to finding more or even all solutions to polynomial VIPs. We show that the method proposed in this paper can find solutions or detect the nonexistence of solutions within finitely many steps, under some general assumptions. In addition, we show that if the VIP is given by generic polynomials, then it has finitely many Karush-Kuhn-Tucker points, and our method can solve it within finitely many steps. Numerical experiments are conducted to illustrate the efficiency of the proposed methods."
1390,https://arxiv.org/abs/2303.11552,Boosting Verified Training for Robust Image Classifications via Abstraction,"This paper proposes a novel, abstraction-based, certified training method for robust image classifiers. Via abstraction, all perturbed images are mapped into intervals before feeding into neural networks for training. By training on intervals, all the perturbed images that are mapped to the same interval are classified as the same label, rendering the variance of training sets to be small and the loss landscape of the models to be smooth. Consequently, our approach significantly improves the robustness of trained models. For the abstraction, our training method also enables a sound and complete black-box verification approach, which is orthogonal and scalable to arbitrary types of neural networks regardless of their sizes and architectures. We evaluate our method on a wide range of benchmarks in different scales. The experimental results show that our method outperforms state of the art by (i) reducing the verified errors of trained models up to 95.64%; (ii) totally achieving up to 602.50x speedup; and (iii) scaling up to larger models with up to 138 million trainable parameters. The demo is available at https://github.com/zhangzhaodi233/ABSCERT.git."
1391,https://arxiv.org/abs/2303.10966,Towards Reliable Neural Machine Translation with Consistency-Aware Meta-Learning,"Neural machine translation (NMT) has achieved remarkable success in producing high-quality translations. However, current NMT systems suffer from a lack of reliability, as their outputs that are often affected by lexical or syntactic changes in inputs, resulting in large variations in quality. This limitation hinders the practicality and trustworthiness of NMT. A contributing factor to this problem is that NMT models trained with the one-to-one paradigm struggle to handle the source diversity phenomenon, where inputs with the same meaning can be expressed differently. In this work, we treat this problem as a bilevel optimization problem and present a consistency-aware meta-learning (CAML) framework derived from the model-agnostic meta-learning (MAML) algorithm to address it. Specifically, the NMT model with CAML (named CoNMT) first learns a consistent meta representation of semantically equivalent sentences in the outer loop. Subsequently, a mapping from the meta representation to the output sentence is learned in the inner loop, allowing the NMT model to translate semantically equivalent sentences to the same target sentence. We conduct experiments on the NIST Chinese to English task, three WMT translation tasks, and the TED M2O task. The results demonstrate that CoNMT effectively improves overall translation quality and reliably handles diverse inputs."
1392,https://arxiv.org/abs/2303.10898,A Tiny Machine Learning Model for Point Cloud Object Classification,"The design of a tiny machine learning model, which can be deployed in mobile and edge devices, for point cloud object classification is investigated in this work. To achieve this objective, we replace the multi-scale representation of a point cloud object with a single-scale representation for complexity reduction, and exploit rich 3D geometric information of a point cloud object for performance improvement. The proposed solution is named Green-PointHop due to its low computational complexity. We evaluate the performance of Green-PointHop on ModelNet40 and ScanObjectNN two datasets. Green-PointHop has a model size of 64K parameters. It demands 2.3M floating-point operations (FLOPs) to classify a ModelNet40 object of 1024 down-sampled points. Its classification performance gaps against the state-of-the-art DGCNN method are 3% and 7% for ModelNet40 and ScanObjectNN, respectively. On the other hand, the model size and inference complexity of DGCNN are 42X and 1203X of those of Green-PointHop, respectively."
1393,https://arxiv.org/abs/2303.07740,Efficient Image-Text Retrieval via Keyword-Guided Pre-Screening,"Under the flourishing development in performance, current image-text retrieval methods suffer from $N$-related time complexity, which hinders their application in practice. Targeting at efficiency improvement, this paper presents a simple and effective keyword-guided pre-screening framework for the image-text retrieval. Specifically, we convert the image and text data into the keywords and perform the keyword matching across modalities to exclude a large number of irrelevant gallery samples prior to the retrieval network. For the keyword prediction, we transfer it into a multi-label classification problem and propose a multi-task learning scheme by appending the multi-label classifiers to the image-text retrieval network to achieve a lightweight and high-performance keyword prediction. For the keyword matching, we introduce the inverted index in the search engine and create a win-win situation on both time and space complexities for the pre-screening. Extensive experiments on two widely-used datasets, i.e., Flickr30K and MS-COCO, verify the effectiveness of the proposed framework. The proposed framework equipped with only two embedding layers achieves $O(1)$ querying time complexity, while improving the retrieval efficiency and keeping its performance, when applied prior to the common image-text retrieval methods. Our code will be released."
1394,https://arxiv.org/abs/2303.07665,RenewNAT: Renewing Potential Translation for Non-Autoregressive Transformer,"Non-autoregressive neural machine translation (NAT) models are proposed to accelerate the inference process while maintaining relatively high performance. However, existing NAT models are difficult to achieve the desired efficiency-quality trade-off. For one thing, fully NAT models with efficient inference perform inferior to their autoregressive counterparts. For another, iterative NAT models can, though, achieve comparable performance while diminishing the advantage of speed. In this paper, we propose RenewNAT, a flexible framework with high efficiency and effectiveness, to incorporate the merits of fully and iterative NAT models. RenewNAT first generates the potential translation results and then renews them in a single pass. It can achieve significant performance improvements at the same expense as traditional NAT models (without introducing additional model parameters and decoding latency). Experimental results on various translation benchmarks (e.g., \textbf{4} WMT) show that our framework consistently improves the performance of strong fully NAT methods (e.g., GLAT and DSLP) without additional speed overhead."
1395,https://arxiv.org/abs/2303.07457,AMOM: Adaptive Masking over Masking for Conditional Masked Language Model,"Transformer-based autoregressive (AR) methods have achieved appealing performance for varied sequence-to-sequence generation tasks, e.g., neural machine translation, summarization, and code generation, but suffer from low inference efficiency. To speed up the inference stage, many non-autoregressive (NAR) strategies have been proposed in the past few years. Among them, the conditional masked language model (CMLM) is one of the most versatile frameworks, as it can support many different sequence generation scenarios and achieve very competitive performance on these tasks. In this paper, we further introduce a simple yet effective adaptive masking over masking strategy to enhance the refinement capability of the decoder and make the encoder optimization easier. Experiments on \textbf{3} different tasks (neural machine translation, summarization, and code generation) with \textbf{15} datasets in total confirm that our proposed simple method achieves significant performance improvement over the strong CMLM model. Surprisingly, our proposed model yields state-of-the-art performance on neural machine translation (\textbf{34.62} BLEU on WMT16 EN$\to$RO, \textbf{34.82} BLEU on WMT16 RO$\to$EN, and \textbf{34.84} BLEU on IWSLT De$\to$En) and even better performance than the \textbf{AR} Transformer on \textbf{7} benchmark datasets with at least \textbf{2.2$\times$} speedup. Our code is available at GitHub."
1396,https://arxiv.org/abs/2303.06679,RotoGBML: Towards Out-of-Distribution Generalization for Gradient-Based Meta-Learning,"Gradient-based meta-learning (GBML) algorithms are able to fast adapt to new tasks by transferring the learned meta-knowledge, while assuming that all tasks come from the same distribution (in-distribution, ID). However, in the real world, they often suffer from an out-of-distribution (OOD) generalization problem, where tasks come from different distributions. OOD exacerbates inconsistencies in magnitudes and directions of task gradients, which brings challenges for GBML to optimize the meta-knowledge by minimizing the sum of task gradients in each minibatch. To address this problem, we propose RotoGBML, a novel approach to homogenize OOD task gradients. RotoGBML uses reweighted vectors to dynamically balance diverse magnitudes to a common scale and uses rotation matrixes to rotate conflicting directions close to each other. To reduce overhead, we homogenize gradients with the features rather than the network parameters. On this basis, to avoid the intervention of non-causal features (e.g., backgrounds), we also propose an invariant self-information (ISI) module to extract invariant causal features (e.g., the outlines of objects). Finally, task gradients are homogenized based on these invariant causal features. Experiments show that RotoGBML outperforms other state-of-the-art methods on various few-shot image classification benchmarks."
1397,https://arxiv.org/abs/2303.06662,Fuzzy Alignments in Directed Acyclic Graph for Non-Autoregressive Machine Translation,"Non-autoregressive translation (NAT) reduces the decoding latency but suffers from performance degradation due to the multi-modality problem. Recently, the structure of directed acyclic graph has achieved great success in NAT, which tackles the multi-modality problem by introducing dependency between vertices. However, training it with negative log-likelihood loss implicitly requires a strict alignment between reference tokens and vertices, weakening its ability to handle multiple translation modalities. In this paper, we hold the view that all paths in the graph are fuzzily aligned with the reference sentence. We do not require the exact alignment but train the model to maximize a fuzzy alignment score between the graph and reference, which takes captured translations in all modalities into account. Extensive experiments on major WMT benchmarks show that our method substantially improves translation performance and increases prediction confidence, setting a new state of the art for NAT on the raw training data."
1398,https://arxiv.org/abs/2303.01391,The Ladder in Chaos: A Simple and Effective Improvement to General DRL Algorithms by Policy Path Trimming and Boosting,"Knowing the learning dynamics of policy is significant to unveiling the mysteries of Reinforcement Learning (RL). It is especially crucial yet challenging to Deep RL, from which the remedies to notorious issues like sample inefficiency and learning instability could be obtained. In this paper, we study how the policy networks of typical DRL agents evolve during the learning process by empirically investigating several kinds of temporal change for each policy parameter. On typical MuJoCo and DeepMind Control Suite (DMC) benchmarks, we find common phenomena for TD3 and RAD agents: 1) the activity of policy network parameters is highly asymmetric and policy networks advance monotonically along very few major parameter directions; 2) severe detours occur in parameter update and harmonic-like changes are observed for all minor parameter directions. By performing a novel temporal SVD along policy learning path, the major and minor parameter directions are identified as the columns of right unitary matrix associated with dominant and insignificant singular values respectively. Driven by the discoveries above, we propose a simple and effective method, called Policy Path Trimming and Boosting (PPTB), as a general plug-in improvement to DRL algorithms. The key idea of PPTB is to periodically trim the policy learning path by canceling the policy updates in minor parameter directions, while boost the learning path by encouraging the advance in major directions. In experiments, we demonstrate the general and significant performance improvements brought by PPTB, when combined with TD3 and RAD in MuJoCo and DMC environments respectively."
1399,https://arxiv.org/abs/2302.13242,Generation of Vortex N2+ Lasing,"Harnessing structured light is fascinating for its multi-disciplinary applications, e.g., in remote driving microrobots, sensing, communications, and ultrahigh resolution imaging. Here we experimentally demonstrated the generation of a vortex N2+ lasing pumped by a wavefront structured near-infrared femtosecond pulse with an orbital angular momentum. The topological charge of the new-born N2+ lasing is measured to be twofold that of the pump beam. As compared to the case with pump beam of plane wavefront, the N2+ lasing generation efficiency is much higher for the vortex pump beam at high pumping energy which has a higher clamping intensity by reducing the on-axis plasma density. Our results herald a stirring marching into the territory of remote structured N2+ lasing."
1400,https://arxiv.org/abs/2302.11506,S3I-PointHop: SO(3)-Invariant PointHop for 3D Point Cloud Classification,"Many point cloud classification methods are developed under the assumption that all point clouds in the dataset are well aligned with the canonical axes so that the 3D Cartesian point coordinates can be employed to learn features. When input point clouds are not aligned, the classification performance drops significantly. In this work, we focus on a mathematically transparent point cloud classification method called PointHop, analyze its reason for failure due to pose variations, and solve the problem by replacing its pose dependent modules with rotation invariant counterparts. The proposed method is named SO(3)-Invariant PointHop (or S3I-PointHop in short). We also significantly simplify the PointHop pipeline using only one single hop along with multiple spatial aggregation techniques. The idea of exploiting more spatial information is novel. Experiments on the ModelNet40 dataset demonstrate the superiority of S3I-PointHop over traditional PointHop-like methods."
1401,https://arxiv.org/abs/2302.05263,Metal-bonded perovskite lead hydride with phonon-mediated superconductivity up to 46 K under atmospheric pressure,"In the search for high-temperature superconductivity in hydrides, a plethora of multi-hydrogen superconductors have been theoretically predicted, and some have been synthesized experimentally under ultrahigh pressures of several hundred GPa. However, the impracticality of these high-pressure methods has been a persistent issue. In response, we propose a new approach to achieve high-temperature superconductivity under atmospheric pressure by implanting hydrogen into lead to create a stable few-hydrogen metal-bonded perovskite, Pb$_4$H. This approach diverges from the popular design methodology of multi-hydrogen covalent high critical temperature ($T_c$) superconductors under ultrahigh pressure. By solving the anisotropic Migdal-Eliashberg (ME) equations, we demonstrate that perovskite Pb$_4$H is a typical phonon-mediated superconductor with a $T_c$ of 46 K, which is six times higher than that of bulk Pb (7.22 K) and higher than that of MgB$_2$ (39 K). The high $T_c$ can be attributed to the strong electron-phonon coupling (EPC) strength of 2.45, which arises from hydrogen implantation in lead that induces several high-frequency optical phonon modes with a relatively large phonon linewidth resulting from H atom vibration. The metallic-bonding in perovskite Pb$_4$H not only improves the structural stability but also guarantees better ductility than the widely investigated multi-hydrogen, iron-based, and cuprate superconductors. These results suggest that there is potential for the exploration of new high-temperature superconductors under atmospheric pressure and may reignite interest in their experimental synthesis soon."
1402,https://arxiv.org/abs/2301.12699,KG-BERTScore: Incorporating Knowledge Graph into BERTScore for Reference-Free Machine Translation Evaluation,"BERTScore is an effective and robust automatic metric for referencebased machine translation evaluation. In this paper, we incorporate multilingual knowledge graph into BERTScore and propose a metric named KG-BERTScore, which linearly combines the results of BERTScore and bilingual named entity matching for reference-free machine translation evaluation. From the experimental results on WMT19 QE as a metric without references shared tasks, our metric KG-BERTScore gets higher overall correlation with human judgements than the current state-of-the-art metrics for reference-free machine translation evaluation.1 Moreover, the pre-trained multilingual model used by KG-BERTScore and the parameter for linear combination are also studied in this paper."
1403,https://arxiv.org/abs/2301.11912,OccRob: Efficient SMT-Based Occlusion Robustness Verification of Deep Neural Networks,"Occlusion is a prevalent and easily realizable semantic perturbation to deep neural networks (DNNs). It can fool a DNN into misclassifying an input image by occluding some segments, possibly resulting in severe errors. Therefore, DNNs planted in safety-critical systems should be verified to be robust against occlusions prior to deployment. However, most existing robustness verification approaches for DNNs are focused on non-semantic perturbations and are not suited to the occlusion case. In this paper, we propose the first efficient, SMT-based approach for formally verifying the occlusion robustness of DNNs. We formulate the occlusion robustness verification problem and prove it is NP-complete. Then, we devise a novel approach for encoding occlusions as a part of neural networks and introduce two acceleration techniques so that the extended neural networks can be efficiently verified using off-the-shelf, SMT-based neural network verification tools. We implement our approach in a prototype called OccRob and extensively evaluate its performance on benchmark datasets with various occlusion variants. The experimental results demonstrate our approach's effectiveness and efficiency in verifying DNNs' robustness against various occlusions, and its ability to generate counterexamples when these DNNs are not robust."
1404,https://arxiv.org/abs/2301.11749,A Multi-task Multi-stage Transitional Training Framework for Neural Chat Translation,"Neural chat translation (NCT) aims to translate a cross-lingual chat between speakers of different languages. Existing context-aware NMT models cannot achieve satisfactory performances due to the following inherent problems: 1) limited resources of annotated bilingual dialogues; 2) the neglect of modelling conversational properties; 3) training discrepancy between different stages. To address these issues, in this paper, we propose a multi-task multi-stage transitional (MMT) training framework, where an NCT model is trained using the bilingual chat translation dataset and additional monolingual dialogues. We elaborately design two auxiliary tasks, namely utterance discrimination and speaker discrimination, to introduce the modelling of dialogue coherence and speaker characteristic into the NCT model. The training process consists of three stages: 1) sentence-level pre-training on large-scale parallel corpus; 2) intermediate training with auxiliary tasks using additional monolingual dialogues; 3) context-aware fine-tuning with gradual transition. Particularly, the second stage serves as an intermediate phase that alleviates the training discrepancy between the pre-training and fine-tuning stages. Moreover, to make the stage transition smoother, we train the NCT model using a gradual transition strategy, i.e., gradually transiting from using monolingual to bilingual dialogues. Extensive experiments on two language pairs demonstrate the effectiveness and superiority of our proposed training framework."
1405,https://arxiv.org/abs/2301.09050,Pressure-induced superconductivity in quasi-one-dimensional semimetal $\mathrm{Ta}_2 \mathrm{PdSe}_6$,"Here we report the discovery of pressure-induced superconductivity in quasi-one-dimensional $\mathrm{Ta}_2 \mathrm{PdSe}_6$, through a combination of electrical transport, synchrotron x-ray diffraction, and theoretical calculations. Our transport measurements show that the superconductivity appears at a critical pressure $P_{\mathrm{c}} \sim 18.3$ GPa and is robust upon further compression up to $62.6$ GPa. The estimated upper critical field $μ_0 H_{\mathrm{c} 2}(0)$ in the pressurized $\mathrm{Ta}_2 \mathrm{PdSe}_6$ is much lower than the Pauli limiting field, in contrast to the case in its isostructural analogs $M_2 \mathrm{Pd}_{\mathrm{x}} X_5$ $(M=\mathrm{Nb}$, Ta; $X=\mathrm{S}, \mathrm{Se})$. Concomitant with the occurrence of superconductivity, anomalies in pressuredependent transport properties are observed, including sign reversal of Hall coefficient, abnormally enhanced resistance, and dramatically suppressed magnetoresistance. Meanwhile, room-temperature synchrotron x-ray diffraction experiments reveal the stability of the pristine monoclinic structure (space group $C 2 / m$ ) upon compression. Combined with the density functional theory calculations, we argue that a pressure-induced Lifshitz transition could be the electronic origin of the emergent superconductivity in $\mathrm{Ta}_2 \mathrm{PdSe}_6$."
1406,https://arxiv.org/abs/2212.11748,Nonconforming finite element methods of order two and order three for the Stokes flow in three dimensions,"In this study, the nonconforming finite elements of order two and order three are constructed and exploited for the Stokes problem. The moments of order up to $k-1$ ($k=2,3$) on all the facets of the tetrahedron are used for DoFs (degrees of freedom) to construct the unisolvent $k$-order nonconforming finite element with the bubble function space of $P_{k+1}$ explicitly represented. The pair of the $k$-order element and the discontinuous piecewise $P_{k}$ is proved to be stable for solving the Stokes problem with the element-wise divergence-free condition preserved. The main difficulty in establishing the discrete inf-sup condition comes from the fact that the usual Fortin operator can not be constructed. Thanks to the explicit representation of the bubble functions, its divergence space is proved to be identical to the orthogonal complement space of constants with respect to $P_k$ on the tetrahedron, which plays an important role to overcome the aforementioned difficulty and leads to the desirable well-posedness of the discrete problem. Furthermore, a reduced $k$-order nonconforming finite element with a discontinuous piecewise $P_{k-1}$ is designed and proved to be stable for solving the Stokes problem. The lack of the Fortin operator causes difficulty in analyzing the discrete inf-sup condition for the reduced third-order element pair. To deal with this problem, the so-called macro-element technique is adopted with a crucial algebraic result concerning the property of functions in the orthogonal complement space of the divergence of the discrete velocity space with respect to the discrete pressure space on the macro-element. Numerical experiments are provided to validate the theoretical results."
1407,https://arxiv.org/abs/2212.11538,SHLE: Devices Tracking and Depth Filtering for Stereo-based Height Limit Estimation,"Recently, over-height vehicle strike frequently occurs, causing great economic cost and serious safety problems. Hence, an alert system which can accurately discover any possible height limiting devices in advance is necessary to be employed in modern large or medium sized cars, such as touring cars. Detecting and estimating the height limiting devices act as the key point of a successful height limit alert system. Though there are some works research height limit estimation, existing methods are either too computational expensive or not accurate enough. In this paper, we propose a novel stereo-based pipeline named SHLE for height limit estimation. Our SHLE pipeline consists of two stages. In stage 1, a novel devices detection and tracking scheme is introduced, which accurately locate the height limit devices in the left or right image. Then, in stage 2, the depth is temporally measured, extracted and filtered to calculate the height limit device. To benchmark the height limit estimation task, we build a large-scale dataset named ""Disparity Height"", where stereo images, pre-computed disparities and ground-truth height limit annotations are provided. We conducted extensive experiments on ""Disparity Height"" and the results show that SHLE achieves an average error below than 10cm though the car is 70m away from the devices. Our method also outperforms all compared baselines and achieves state-of-the-art performance. Code is available at https://github.com/Yang-Kaixing/SHLE."
1408,https://arxiv.org/abs/2212.11138,QVIP: An ILP-based Formal Verification Approach for Quantized Neural Networks,"Deep learning has become a promising programming paradigm in software development, owing to its surprising performance in solving many challenging tasks. Deep neural networks (DNNs) are increasingly being deployed in practice, but are limited on resource-constrained devices owing to their demand for computational power. Quantization has emerged as a promising technique to reduce the size of DNNs with comparable accuracy as their floating-point numbered counterparts. The resulting quantized neural networks (QNNs) can be implemented energy-efficiently. Similar to their floating-point numbered counterparts, quality assurance techniques for QNNs, such as testing and formal verification, are essential but are currently less explored. In this work, we propose a novel and efficient formal verification approach for QNNs. In particular, we are the first to propose an encoding that reduces the verification problem of QNNs into the solving of integer linear constraints, which can be solved using off-the-shelf solvers. Our encoding is both sound and complete. We demonstrate the application of our approach on local robustness verification and maximum robustness radius computation. We implement our approach in a prototype tool QVIP and conduct a thorough evaluation. Experimental results on QNNs with different quantization bits confirm the effectiveness and efficiency of our approach, e.g., two orders of magnitude faster and able to solve more verification tasks in the same time limit than the state-of-the-art methods."
1409,https://arxiv.org/abs/2212.08632,Enhancing Multi-modal and Multi-hop Question Answering via Structured Knowledge and Unified Retrieval-Generation,"Multi-modal and multi-hop question answering aims to answer a question based on multiple input sources from different modalities. Previous methods retrieve the evidence separately and feed the retrieved evidence to a language model to generate the corresponding answer. However, these methods fail to build connections between candidates and thus cannot model the inter-dependent relation during retrieval. Moreover, the reasoning process over multi-modality candidates can be unbalanced without building alignments between different modalities. To address this limitation, we propose a Structured Knowledge and Unified Retrieval Generation based method (SKURG). We align the sources from different modalities via the shared entities and map them into a shared semantic space via structured knowledge. Then, we utilize a unified retrieval-generation decoder to integrate intermediate retrieval results for answer generation and adaptively determine the number of retrieval steps. We perform experiments on two multi-modal and multi-hop datasets: WebQA and MultimodalQA. The results demonstrate that SKURG achieves state-of-the-art performance on both retrieval and answer generation."
1410,https://arxiv.org/abs/2212.05830,P-Transformer: Towards Better Document-to-Document Neural Machine Translation,"Directly training a document-to-document (Doc2Doc) neural machine translation (NMT) via Transformer from scratch, especially on small datasets usually fails to converge. Our dedicated probing tasks show that 1) both the absolute position and relative position information gets gradually weakened or even vanished once it reaches the upper encoder layers, and 2) the vanishing of absolute position information in encoder output causes the training failure of Doc2Doc NMT. To alleviate this problem, we propose a position-aware Transformer (P-Transformer) to enhance both the absolute and relative position information in both self-attention and cross-attention. Specifically, we integrate absolute positional information, i.e., position embeddings, into the query-key pairs both in self-attention and cross-attention through a simple yet effective addition operation. Moreover, we also integrate relative position encoding in self-attention. The proposed P-Transformer utilizes sinusoidal position encoding and does not require any task-specified position embedding, segment embedding, or attention mechanism. Through the above methods, we build a Doc2Doc NMT model with P-Transformer, which ingests the source document and completely generates the target document in a sequence-to-sequence (seq2seq) way. In addition, P-Transformer can be applied to seq2seq-based document-to-sentence (Doc2Sent) and sentence-to-sentence (Sent2Sent) translation. Extensive experimental results of Doc2Doc NMT show that P-Transformer significantly outperforms strong baselines on widely-used 9 document-level datasets in 7 language pairs, covering small-, middle-, and large-scales, and achieves a new state-of-the-art. Experimentation on discourse phenomena shows that our Doc2Doc NMT models improve the translation quality in both BLEU and discourse coherence. We make our code available on Github."
1411,https://arxiv.org/abs/2212.04262,ConsistTL: Modeling Consistency in Transfer Learning for Low-Resource Neural Machine Translation,"Transfer learning is a simple and powerful method that can be used to boost model performance of low-resource neural machine translation (NMT). Existing transfer learning methods for NMT are static, which simply transfer knowledge from a parent model to a child model once via parameter initialization. In this paper, we propose a novel transfer learning method for NMT, namely ConsistTL, which can continuously transfer knowledge from the parent model during the training of the child model. Specifically, for each training instance of the child model, ConsistTL constructs the semantically-equivalent instance for the parent model and encourages prediction consistency between the parent and child for this instance, which is equivalent to the child model learning each instance under the guidance of the parent model. Experimental results on five low-resource NMT tasks demonstrate that ConsistTL results in significant improvements over strong transfer learning baselines, with a gain up to 1.7 BLEU over the existing back-translation model on the widely-used WMT17 Turkish-English benchmark. Further analysis reveals that ConsistTL can improve the inference calibration of the child model. Code and scripts are freely available at https://github.com/NLP2CT/ConsistTL."
1412,https://arxiv.org/abs/2212.01188,Improving Simultaneous Machine Translation with Monolingual Data,"Simultaneous machine translation (SiMT) is usually done via sequence-level knowledge distillation (Seq-KD) from a full-sentence neural machine translation (NMT) model. However, there is still a significant performance gap between NMT and SiMT. In this work, we propose to leverage monolingual data to improve SiMT, which trains a SiMT student on the combination of bilingual data and external monolingual data distilled by Seq-KD. Preliminary experiments on En-Zh and En-Ja news domain corpora demonstrate that monolingual data can significantly improve translation quality (e.g., +3.15 BLEU on En-Zh). Inspired by the behavior of human simultaneous interpreters, we propose a novel monolingual sampling strategy for SiMT, considering both chunk length and monotonicity. Experimental results show that our sampling strategy consistently outperforms the random sampling strategy (and other conventional typical NMT monolingual sampling strategies) by avoiding the key problem of SiMT -- hallucination, and has better scalability. We achieve +0.72 BLEU improvements on average against random sampling on En-Zh and En-Ja. Data and codes can be found at https://github.com/hexuandeng/Mono4SiMT."
1413,https://arxiv.org/abs/2211.12781,Breaking the Representation Bottleneck of Chinese Characters: Neural Machine Translation with Stroke Sequence Modeling,"Existing research generally treats Chinese character as a minimum unit for representation. However, such Chinese character representation will suffer two bottlenecks: 1) Learning bottleneck, the learning cannot benefit from its rich internal features (e.g., radicals and strokes); and 2) Parameter bottleneck, each individual character has to be represented by a unique vector. In this paper, we introduce a novel representation method for Chinese characters to break the bottlenecks, namely StrokeNet, which represents a Chinese character by a Latinized stroke sequence (e.g., ""ao1 (concave)"" to ""ajaie"" and ""tu1 (convex)"" to ""aeaqe""). Specifically, StrokeNet maps each stroke to a specific Latin character, thus allowing similar Chinese characters to have similar Latin representations. With the introduction of StrokeNet to neural machine translation (NMT), many powerful but not applicable techniques to non-Latin languages (e.g., shared subword vocabulary learning and ciphertext-based data augmentation) can now be perfectly implemented. Experiments on the widely-used NIST Chinese-English, WMT17 Chinese-English and IWSLT17 Japanese-English NMT tasks show that StrokeNet can provide a significant performance boost over the strong baselines with fewer model parameters, achieving 26.5 BLEU on the WMT17 Chinese-English task which is better than any previously reported results without using monolingual data. Code and scripts are freely available at https://github.com/zjwang21/StrokeNet."
1414,https://arxiv.org/abs/2211.11186,DualApp: Tight Over-Approximation for Neural Network Robustness Verification via Under-Approximation,"The robustness of neural networks is fundamental to the hosting system's reliability and security. Formal verification has been proven to be effective in providing provable robustness guarantees. To improve the verification scalability, over-approximating the non-linear activation functions in neural networks by linear constraints is widely adopted, which transforms the verification problem into an efficiently solvable linear programming problem. As over-approximations inevitably introduce overestimation, many efforts have been dedicated to defining the tightest possible approximations. Recent studies have however showed that the existing so-called tightest approximations are superior to each other. In this paper we identify and report an crucial factor in defining tight approximations, namely the approximation domains of activation functions. We observe that existing approaches only rely on overestimated domains, while the corresponding tight approximation may not necessarily be tight on its actual domain. We propose a novel under-approximation-guided approach, called dual-approximation, to define tight over-approximations and two complementary under-approximation algorithms based on sampling and gradient descent. The overestimated domain guarantees the soundness while the underestimated one guides the tightness. We implement our approach into a tool called DualApp and extensively evaluate it on a comprehensive benchmark of 84 collected and trained neural networks with different architectures. The experimental results show that DualApp outperforms the state-of-the-art approximation-based approaches, with up to 71.22% improvement to the verification result."
1415,https://arxiv.org/abs/2211.11127,BBReach: Tight and Scalable Black-Box Reachability Analysis of Deep Reinforcement Learning Systems,"Reachability analysis is a promising technique to automatically prove or disprove the reliability and safety of AI-empowered software systems that are developed by using Deep Reinforcement Learning (DRL). Existing approaches suffer however from limited scalability and large overestimation as they must over-approximate the complex and almost inexplicable system components, namely deep neural networks (DNNs). In this paper we propose a novel, tight and scalable reachability analysis approach for DRL systems. By training on abstract states, our approach treats the embedded DNNs as black boxes to avoid the over-approximation for neural networks in computing reachable sets. To tackle the state explosion problem inherent to abstraction-based approaches, we devise a novel adjacent interval aggregation algorithm which balances the growth of abstract states and the overestimation caused by the abstraction. We implement a tool, called BBReach, and assess it on an extensive benchmark of control systems to demonstrate its tightness, scalability, and efficiency."
1416,https://arxiv.org/abs/2211.10153,A generalization of Piatetski-Shapiro sequences (II),"Suppose that $α,β\in\mathbb{R}$. Let $α\geqslant1$ and $c$ be a real number in the range $1<c< 12/11$. In this paper, it is proved that there exist infinitely many primes in the generalized Piatetski--Shapiro sequence, which is defined by $(\lfloorαn^c+β\rfloor)_{n=1}^\infty$. Moreover, we also prove that there exist infinitely many Carmichael numbers composed entirely of primes from the generalized Piatetski--Shapiro sequences with $c\in(1,\frac{19137}{18746})$. The two theorems constitute improvements upon previous results by Guo and Qi."
1417,https://arxiv.org/abs/2211.06862,WR-ONE2SET: Towards Well-Calibrated Keyphrase Generation,"Keyphrase generation aims to automatically generate short phrases summarizing an input document. The recently emerged ONE2SET paradigm (Ye et al., 2021) generates keyphrases as a set and has achieved competitive performance. Nevertheless, we observe serious calibration errors outputted by ONE2SET, especially in the over-estimation of $\varnothing$ token (means ""no corresponding keyphrase""). In this paper, we deeply analyze this limitation and identify two main reasons behind: 1) the parallel generation has to introduce excessive $\varnothing$ as padding tokens into training instances; and 2) the training mechanism assigning target to each slot is unstable and further aggravates the $\varnothing$ token over-estimation. To make the model well-calibrated, we propose WR-ONE2SET which extends ONE2SET with an adaptive instance-level cost Weighting strategy and a target Re-assignment mechanism. The former dynamically penalizes the over-estimated slots for different instances thus smoothing the uneven training distribution. The latter refines the original inappropriate assignment and reduces the supervisory signals of over-estimated slots. Experimental results on commonly-used datasets demonstrate the effectiveness and generality of our proposed paradigm."
1418,https://arxiv.org/abs/2211.04198,Third-Party Aligner for Neural Word Alignments,"Word alignment is to find translationally equivalent words between source and target sentences. Previous work has demonstrated that self-training can achieve competitive word alignment results. In this paper, we propose to use word alignments generated by a third-party word aligner to supervise the neural word alignment training. Specifically, source word and target word of each word pair aligned by the third-party aligner are trained to be close neighbors to each other in the contextualized embedding space when fine-tuning a pre-trained cross-lingual language model. Experiments on the benchmarks of various language pairs show that our approach can surprisingly do self-correction over the third-party supervision by finding more accurate word alignments and deleting wrong word alignments, leading to better performance than various third-party word aligners, including the currently best one. When we integrate all supervisions from various third-party aligners, we achieve state-of-the-art word alignment performances, with averagely more than two points lower alignment error rates than the best third-party aligner. We released our code at https://github.com/sdongchuanqi/Third-Party-Supervised-Aligner."
1419,https://arxiv.org/abs/2211.01635,Revisiting Grammatical Error Correction Evaluation and Beyond,"Pretraining-based (PT-based) automatic evaluation metrics (e.g., BERTScore and BARTScore) have been widely used in several sentence generation tasks (e.g., machine translation and text summarization) due to their better correlation with human judgments over traditional overlap-based methods. Although PT-based methods have become the de facto standard for training grammatical error correction (GEC) systems, GEC evaluation still does not benefit from pretrained knowledge. This paper takes the first step towards understanding and improving GEC evaluation with pretraining. We first find that arbitrarily applying PT-based metrics to GEC evaluation brings unsatisfactory correlation results because of the excessive attention to inessential systems outputs (e.g., unchanged parts). To alleviate the limitation, we propose a novel GEC evaluation metric to achieve the best of both worlds, namely PT-M2 which only uses PT-based metrics to score those corrected parts. Experimental results on the CoNLL14 evaluation task show that PT-M2 significantly outperforms existing methods, achieving a new state-of-the-art result of 0.949 Pearson correlation. Further analysis reveals that PT-M2 is robust to evaluate competitive GEC systems. Source code and scripts are freely available at https://github.com/pygongnlp/PT-M2."
1420,https://arxiv.org/abs/2210.17127,Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change,"Recent research has revealed that neural language models at scale suffer from poor temporal generalization capability, i.e., the language model pre-trained on static data from past years performs worse over time on emerging data. Existing methods mainly perform continual training to mitigate such a misalignment. While effective to some extent but is far from being addressed on both the language modeling and downstream tasks. In this paper, we empirically observe that temporal generalization is closely affiliated with lexical semantic change, which is one of the essential phenomena of natural languages. Based on this observation, we propose a simple yet effective lexical-level masking strategy to post-train a converged language model. Experiments on two pre-trained language models, two different classification tasks, and four benchmark datasets demonstrate the effectiveness of our proposed method over existing temporal adaptation methods, i.e., continual training with new data. Our code is available at \url{https://github.com/zhaochen0110/LMLM}."
1421,https://arxiv.org/abs/2210.17122,Mining Word Boundaries in Speech as Naturally Annotated Word Segmentation Data,"Chinese word segmentation (CWS) models have achieved very high performance when the training data is sufficient and in-domain. However, the performance drops drastically when shifting to cross-domain and low-resource scenarios due to data sparseness issues. Considering that constructing large-scale manually annotated data is time-consuming and labor-intensive, in this work, we for the first time propose to mine word boundary information from pauses in speech to efficiently obtain large-scale CWS naturally annotated data. We present a simple yet effective complete-then-train method to utilize these natural annotations from speech for CWS model training. Extensive experiments demonstrate that the CWS performance in cross-domain and low-resource scenarios can be significantly improved by leveraging our naturally annotated data extracted from speech."
1422,https://arxiv.org/abs/2210.15231,Unsupervised Boundary-Aware Language Model Pretraining for Chinese Sequence Labeling,"Boundary information is critical for various Chinese language processing tasks, such as word segmentation, part-of-speech tagging, and named entity recognition. Previous studies usually resorted to the use of a high-quality external lexicon, where lexicon items can offer explicit boundary information. However, to ensure the quality of the lexicon, great human effort is always necessary, which has been generally ignored. In this work, we suggest unsupervised statistical boundary information instead, and propose an architecture to encode the information directly into pre-trained language models, resulting in Boundary-Aware BERT (BABERT). We apply BABERT for feature induction of Chinese sequence labeling tasks. Experimental results on ten benchmarks of Chinese sequence labeling demonstrate that BABERT can provide consistent improvements on all datasets. In addition, our method can complement previous supervised lexicon exploration, where further improvements can be achieved when integrated with external lexicon information."
1423,https://arxiv.org/abs/2210.12658,Extending Phrase Grounding with Pronouns in Visual Dialogues,"Conventional phrase grounding aims to localize noun phrases mentioned in a given caption to their corresponding image regions, which has achieved great success recently. Apparently, sole noun phrase grounding is not enough for cross-modal visual language understanding. Here we extend the task by considering pronouns as well. First, we construct a dataset of phrase grounding with both noun phrases and pronouns to image regions. Based on the dataset, we test the performance of phrase grounding by using a state-of-the-art literature model of this line. Then, we enhance the baseline grounding model with coreference information which should help our task potentially, modeling the coreference structures with graph convolutional networks. Experiments on our dataset, interestingly, show that pronouns are easier to ground than noun phrases, where the possible reason might be that these pronouns are much less ambiguous. Additionally, our final model with coreference information can significantly boost the grounding performance of both noun phrases and pronouns."
1424,https://arxiv.org/abs/2210.12484,SynGEC: Syntax-Enhanced Grammatical Error Correction with a Tailored GEC-Oriented Parser,"This work proposes a syntax-enhanced grammatical error correction (GEC) approach named SynGEC that effectively incorporates dependency syntactic information into the encoder part of GEC models. The key challenge for this idea is that off-the-shelf parsers are unreliable when processing ungrammatical sentences. To confront this challenge, we propose to build a tailored GEC-oriented parser (GOPar) using parallel GEC training data as a pivot. First, we design an extended syntax representation scheme that allows us to represent both grammatical errors and syntax in a unified tree structure. Then, we obtain parse trees of the source incorrect sentences by projecting trees of the target correct sentences. Finally, we train GOPar with such projected trees. For GEC, we employ the graph convolution network to encode source-side syntactic information produced by GOPar, and fuse them with the outputs of the Transformer encoder. Experiments on mainstream English and Chinese GEC datasets show that our proposed SynGEC approach consistently and substantially outperforms strong baselines and achieves competitive performance. Our code and data are all publicly available at https://github.com/HillZhang1999/SynGEC."
1425,https://arxiv.org/abs/2210.11109,Visual Spatial Description: Controlled Spatial-Oriented Image-to-Text Generation,"Image-to-text tasks, such as open-ended image captioning and controllable image description, have received extensive attention for decades. Here, we further advance this line of work by presenting Visual Spatial Description (VSD), a new perspective for image-to-text toward spatial semantics. Given an image and two objects inside it, VSD aims to produce one description focusing on the spatial perspective between the two objects. Accordingly, we manually annotate a dataset to facilitate the investigation of the newly-introduced task and build several benchmark encoder-decoder models by using VL-BART and VL-T5 as backbones. In addition, we investigate pipeline and joint end-to-end architectures for incorporating visual spatial relationship classification (VSRC) information into our model. Finally, we conduct experiments on our benchmark dataset to evaluate all our models. Results show that our models are impressive, providing accurate and human-like spatial-oriented text descriptions. Meanwhile, VSRC has great potential for VSD, and the joint end-to-end architecture is the better choice for their integration. We make the dataset and codes public for research purposes."
1426,https://arxiv.org/abs/2210.10293,Forging Multiple Training Objectives for Pre-trained Language Models via Meta-Learning,"Multiple pre-training objectives fill the vacancy of the understanding capability of single-objective language modeling, which serves the ultimate purpose of pre-trained language models (PrLMs), generalizing well on a mass of scenarios. However, learning multiple training objectives in a single model is challenging due to the unknown relative significance as well as the potential contrariety between them. Empirical studies have shown that the current objective sampling in an ad-hoc manual setting makes the learned language representation barely converge to the desired optimum. Thus, we propose \textit{MOMETAS}, a novel adaptive sampler based on meta-learning, which learns the latent sampling pattern on arbitrary pre-training objectives. Such a design is lightweight with negligible additional training overhead. To validate our approach, we adopt five objectives and conduct continual pre-training with BERT-base and BERT-large models, where MOMETAS demonstrates universal performance gain over other rule-based sampling strategies on 14 natural language processing tasks."
1427,https://arxiv.org/abs/2210.08218,Massive MIMO Evolution Towards 3GPP Release 18,"Since the introduction of fifth-generation new radio (5G-NR) in Third Generation Partnership Project (3GPP) Release 15, swift progress has been made to evolve 5G with 3GPP Release 18 emerging. A critical aspect is the design of massive multiple-input multiple-output (MIMO) technology. In this line, this paper makes several important contributions: We provide a comprehensive overview of the evolution of standardized massive MIMO features from 3GPP Release 15 to 17 for both time/frequency-division duplex operation across bands FR-1 and FR-2. We analyze the progress on channel state information (CSI) frameworks, beam management frameworks and present enhancements for uplink CSI. We shed light on emerging 3GPP Release 18 problems requiring imminent attention. These include advanced codebook design and sounding reference signal design for coherent joint transmission (CJT) with multiple transmission/reception points (multi- TRPs). We discuss advancements in uplink demodulation reference signal design, enhancements for mobility to provide accurate CSI estimates, and unified transmission configuration indicator framework tailored for FR-2 bands. For each concept, we provide system level simulation results to highlight their performance benefits. Via field trials in an outdoor environment at Shanghai Jiaotong University, we demonstrate the gains of multi-TRP CJT relative to single TRP at 3.7 GHz."
1428,https://arxiv.org/abs/2210.04525,SelfMix: Robust Learning Against Textual Label Noise with Self-Mixup Training,"The conventional success of textual classification relies on annotated data, and the new paradigm of pre-trained language models (PLMs) still requires a few labeled data for downstream tasks. However, in real-world applications, label noise inevitably exists in training data, damaging the effectiveness, robustness, and generalization of the models constructed on such data. Recently, remarkable achievements have been made to mitigate this dilemma in visual data, while only a few explore textual data. To fill this gap, we present SelfMix, a simple yet effective method, to handle label noise in text classification tasks. SelfMix uses the Gaussian Mixture Model to separate samples and leverages semi-supervised learning. Unlike previous works requiring multiple models, our method utilizes the dropout mechanism on a single model to reduce the confirmation bias in self-training and introduces a textual-level mixup training strategy. Experimental results on three text classification benchmarks with different types of text show that the performance of our proposed method outperforms these strong baselines designed for both textual and visual data under different noise ratios and noise types. Our code is available at https://github.com/noise-learning/SelfMix."
1429,https://arxiv.org/abs/2209.07696,Towards A Unified Policy Abstraction Theory and Representation Learning Approach in Markov Decision Processes,"Lying on the heart of intelligent decision-making systems, how policy is represented and optimized is a fundamental problem. The root challenge in this problem is the large scale and the high complexity of policy space, which exacerbates the difficulty of policy learning especially in real-world scenarios. Towards a desirable surrogate policy space, recently policy representation in a low-dimensional latent space has shown its potential in improving both the evaluation and optimization of policy. The key question involved in these studies is by what criterion we should abstract the policy space for desired compression and generalization. However, both the theory on policy abstraction and the methodology on policy representation learning are less studied in the literature. In this work, we make very first efforts to fill up the vacancy. First, we propose a unified policy abstraction theory, containing three types of policy abstraction associated to policy features at different levels. Then, we generalize them to three policy metrics that quantify the distance (i.e., similarity) of policies, for more convenient use in learning policy representation. Further, we propose a policy representation learning approach based on deep metric learning. For the empirical study, we investigate the efficacy of the proposed policy metrics and representations, in characterizing policy difference and conveying policy generalization respectively. Our experiments are conducted in both policy optimization and evaluation problems, containing trust-region policy optimization (TRPO), diversity-guided evolution strategy (DGES) and off-policy evaluation (OPE). Somewhat naturally, the experimental results indicate that there is no a universally optimal abstraction for all downstream learning problems; while the influence-irrelevance policy abstraction can be a generally preferred choice."
1430,https://arxiv.org/abs/2209.05701,$Bρ$-defined Isochronous Mass Spectrometry and Mass Measurements of $^{58}$Ni Fragments,"A novel isochronous mass spectrometry, termed as $Bρ$-defined IMS, is established at the experimental cooler-storage ring CSRe in Lanzhou. Its potential has been studied through high precision mass measurements of $^{58}$Ni projectile fragments. Two time-of-flight detectors were installed in one of the straight sections of CSRe, thus enabling simultaneous measurements of the velocity and the revolution time of each stored short-lived ion. This allows for calculating the magnetic rigidity $Bρ$ and the orbit length $C$ of each ion. The accurate $Bρ(C)$ function has been constructed, which is a universal calibration curve used to deduce the masses of the stored nuclides. The sensitivity to single stored ions, quickness, and background-free characteristics of the method are ideally suited to address nuclides with very short lifetimes and tiniest production yields. In the limiting case of just a single particle, the attained mass resolving power allows one us to determine its mass-over-charge ratio $m/q$ with a remarkable precision of merely $\sim5$ keV. Masses of $T_z = -3/2$ fp-shell nuclides are re-determined with high accuracy, and the validity of the isospin multiplet mass equation is tested up to the heaviest isospin quartet with $A = 55$. The new masses are also used to investigate the mirror symmetry of empirical residual proton-neutron interactions."
1431,https://arxiv.org/abs/2208.12711,SeSQL: Yet Another Large-scale Session-level Chinese Text-to-SQL Dataset,"As the first session-level Chinese dataset, CHASE contains two separate parts, i.e., 2,003 sessions manually constructed from scratch (CHASE-C), and 3,456 sessions translated from English SParC (CHASE-T). We find the two parts are highly discrepant and incompatible as training and evaluation data. In this work, we present SeSQL, yet another large-scale session-level text-to-SQL dataset in Chinese, consisting of 5,028 sessions all manually constructed from scratch. In order to guarantee data quality, we adopt an iterative annotation workflow to facilitate intense and in-time review of previous-round natural language (NL) questions and SQL queries. Moreover, by completing all context-dependent NL questions, we obtain 27,012 context-independent question/SQL pairs, allowing SeSQL to be used as the largest dataset for single-round multi-DB text-to-SQL parsing. We conduct benchmark session-level text-to-SQL parsing experiments on SeSQL by employing three competitive session-level parsers, and present detailed analysis."
1432,https://arxiv.org/abs/2208.10677,A Review of Machine Learning-based Failure Management in Optical Networks,"Failure management plays a significant role in optical networks. It ensures secure operation, mitigates potential risks, and executes proactive protection. Machine learning (ML) is considered to be an extremely powerful technique for performing comprehensive data analysis and complex network management and is widely utilized for failure management in optical networks to revolutionize the conventional manual methods. In this study, the background of failure management is introduced, where typical failure tasks, physical objects, ML algorithms, data source, and extracted information are illustrated in detail. An overview of the applications of ML in failure management is provided in terms of alarm analysis, failure prediction, failure detection, failure localization, and failure identification. Finally, the future directions on ML for failure management are discussed from the perspective of data, model, task, and emerging techniques."
1433,https://arxiv.org/abs/2208.09872,Provably Tightest Linear Approximation for Robustness Verification of Sigmoid-like Neural Networks,"The robustness of deep neural networks is crucial to modern AI-enabled systems and should be formally verified. Sigmoid-like neural networks have been adopted in a wide range of applications. Due to their non-linearity, Sigmoid-like activation functions are usually over-approximated for efficient verification, which inevitably introduces imprecision. Considerable efforts have been devoted to finding the so-called tighter approximations to obtain more precise verification results. However, existing tightness definitions are heuristic and lack theoretical foundations. We conduct a thorough empirical analysis of existing neuron-wise characterizations of tightness and reveal that they are superior only on specific neural networks. We then introduce the notion of network-wise tightness as a unified tightness definition and show that computing network-wise tightness is a complex non-convex optimization problem. We bypass the complexity from different perspectives via two efficient, provably tightest approximations. The results demonstrate the promising performance achievement of our approaches over state of the art: (i) achieving up to 251.28% improvement to certified lower robustness bounds; and (ii) exhibiting notably more precise verification results on convolutional networks."
1434,https://arxiv.org/abs/2208.08868,Physics-Informed Neural Operator for Fast and Scalable Optical Fiber Channel Modelling in Multi-Span Transmission,"We propose efficient modelling of optical fiber channel via NLSE-constrained physics-informed neural operator without reference solutions. This method can be easily scalable for distance, sequence length, launch power, and signal formats, and is implemented for ultra-fast simulations of 16-QAM signal transmission with ASE noise."
1435,https://arxiv.org/abs/2208.08097,Brain Topography Adaptive Network for Satisfaction Modeling in Interactive Information Access System,"With the growth of information on the Web, most users heavily rely on information access systems (e.g., search engines, recommender systems, etc.) in their daily lives. During this procedure, modeling users' satisfaction status plays an essential part in improving their experiences with the systems. In this paper, we aim to explore the benefits of using Electroencephalography (EEG) signals for satisfaction modeling in interactive information access system design. Different from existing EEG classification tasks, the arisen of satisfaction involves multiple brain functions, such as arousal, prototypicality, and appraisals, which are related to different brain topographical areas. Thus modeling user satisfaction raises great challenges to existing solutions. To address this challenge, we propose BTA, a Brain Topography Adaptive network with a multi-centrality encoding module and a spatial attention mechanism module to capture cognitive connectivities in different spatial distances. We explore the effectiveness of BTA for satisfaction modeling in two popular information access scenarios, i.e., search and recommendation. Extensive experiments on two real-world datasets verify the effectiveness of introducing brain topography adaptive strategy in satisfaction modeling. Furthermore, we also conduct search result re-ranking task and video rating prediction task based on the satisfaction inferred from brain signals on search and recommendation scenarios, respectively. Experimental results show that brain signals extracted with BTA help improve the performance of interactive information access systems significantly."
1436,https://arxiv.org/abs/2208.05753,Disentangled Modeling of Domain and Relevance for Adaptable Dense Retrieval,"Recent advance in Dense Retrieval (DR) techniques has significantly improved the effectiveness of first-stage retrieval. Trained with large-scale supervised data, DR models can encode queries and documents into a low-dimensional dense space and conduct effective semantic matching. However, previous studies have shown that the effectiveness of DR models would drop by a large margin when the trained DR models are adopted in a target domain that is different from the domain of the labeled data. One of the possible reasons is that the DR model has never seen the target corpus and thus might be incapable of mitigating the difference between the training and target domains. In practice, unfortunately, training a DR model for each target domain to avoid domain shift is often a difficult task as it requires additional time, storage, and domain-specific data labeling, which are not always available. To address this problem, in this paper, we propose a novel DR framework named Disentangled Dense Retrieval (DDR) to support effective and flexible domain adaptation for DR models. DDR consists of a Relevance Estimation Module (REM) for modeling domain-invariant matching patterns and several Domain Adaption Modules (DAMs) for modeling domain-specific features of multiple target corpora. By making the REM and DAMs disentangled, DDR enables a flexible training paradigm in which REM is trained with supervision once and DAMs are trained with unsupervised data. Comprehensive experiments in different domains and languages show that DDR significantly improves ranking performance compared to strong DR baselines and substantially outperforms traditional retrieval methods in most scenarios."
1437,https://arxiv.org/abs/2208.01260,Well-balanced fifth-order finite difference Hermite WENO scheme for the shallow water equations,"In this paper, we propose a well-balanced fifth-order finite difference Hermite WENO (HWENO) scheme for the shallow water equations with non-flat bottom topography in pre-balanced form. For achieving the well-balance property, we adopt the similar idea of WENO-XS scheme [Xing and Shu, J. Comput. Phys., 208 (2005), 206-227.] to balance the flux gradients and the source terms. The fluxes in the original equation are reconstructed by the nonlinear HWENO reconstructions while other fluxes in the derivative equations are approximated by the high-degree polynomials directly. And an HWENO limiter is applied for the derivatives of equilibrium variables in time discretization step to control spurious oscillations which maintains the well-balance property. Instead of using a five-point stencil in the same fifth-order WENO-XS scheme, the proposed HWENO scheme only needs a compact three-point stencil in the reconstruction. Various benchmark examples in one and two dimensions are presented to show the HWENO scheme is fifth-order accuracy, preserves steady-state solution, has better resolution, is more accurate and efficient, and is essentially non-oscillatory."
1438,https://arxiv.org/abs/2208.00114,Outcome Adaptive Propensity Score Methods for Handling Censoring and High-Dimensionality: Application to Insurance Claims,"Propensity scores are commonly used to reduce the confounding bias in non-randomized observational studies for estimating the average treatment effect. An important assumption underlying this approach is that all confounders that are associated with both the treatment and the outcome of interest are measured and included in the propensity score model. In the absence of strong prior knowledge about potential confounders, researchers may agnostically want to adjust for a high-dimensional set of pre-treatment variables. As such, variable selection procedure is needed for propensity score estimation. In addition, recent studies show that including variables related to treatment only in the propensity score model may inflate the variance of the treatment effect estimates, while including variables that are predictive of only the outcome can improve efficiency. In this paper, we propose a flexible approach to incorporating outcome-covariate relationship in the propensity score model by including the predicted binary outcome probability (OP) as a covariate. Our approach can be easily adapted to an ensemble of variable selection methods, including regularization methods and modern machine learning tools based on classification and regression trees. We evaluate our method to estimate the treatment effects on a binary outcome, which is possibly censored, among multiple treatment groups. Simulation studies indicate that incorporating OP for estimating the propensity scores can improve statistical efficiency and protect against model misspecification. The proposed methods are applied to a cohort of advanced stage prostate cancer patients identified from a private insurance claims database for comparing the adverse effects of four commonly used drugs for treating castration-resistant prostate cancer."
1439,https://arxiv.org/abs/2207.11844,Enhancing Image Rescaling using Dual Latent Variables in Invertible Neural Network,"Normalizing flow models have been used successfully for generative image super-resolution (SR) by approximating complex distribution of natural images to simple tractable distribution in latent space through Invertible Neural Networks (INN). These models can generate multiple realistic SR images from one low-resolution (LR) input using randomly sampled points in the latent space, simulating the ill-posed nature of image upscaling where multiple high-resolution (HR) images correspond to the same LR. Lately, the invertible process in INN has also been used successfully by bidirectional image rescaling models like IRN and HCFlow for joint optimization of downscaling and inverse upscaling, resulting in significant improvements in upscaled image quality. While they are optimized for image downscaling too, the ill-posed nature of image downscaling, where one HR image could be downsized to multiple LR images depending on different interpolation kernels and resampling methods, is not considered. A new downscaling latent variable, in addition to the original one representing uncertainties in image upscaling, is introduced to model variations in the image downscaling process. This dual latent variable enhancement is applicable to different image rescaling models and it is shown in extensive experiments that it can improve image upscaling accuracy consistently without sacrificing image quality in downscaled LR images. It is also shown to be effective in enhancing other INN-based models for image restoration applications like image hiding."
1440,https://arxiv.org/abs/2207.11401,Chunk-aware Alignment and Lexical Constraint for Visual Entailment with Natural Language Explanations,"Visual Entailment with natural language explanations aims to infer the relationship between a text-image pair and generate a sentence to explain the decision-making process. Previous methods rely mainly on a pre-trained vision-language model to perform the relation inference and a language model to generate the corresponding explanation. However, the pre-trained vision-language models mainly build token-level alignment between text and image yet ignore the high-level semantic alignment between the phrases (chunks) and visual contents, which is critical for vision-language reasoning. Moreover, the explanation generator based only on the encoded joint representation does not explicitly consider the critical decision-making points of relation inference. Thus the generated explanations are less faithful to visual-language reasoning. To mitigate these problems, we propose a unified Chunk-aware Alignment and Lexical Constraint based method, dubbed as CALeC. It contains a Chunk-aware Semantic Interactor (arr. CSI), a relation inferrer, and a Lexical Constraint-aware Generator (arr. LeCG). Specifically, CSI exploits the sentence structure inherent in language and various image regions to build chunk-aware semantic alignment. Relation inferrer uses an attention-based reasoning network to incorporate the token-level and chunk-level vision-language representations. LeCG utilizes lexical constraints to expressly incorporate the words or chunks focused by the relation inferrer into explanation generation, improving the faithfulness and informativeness of the explanations. We conduct extensive experiments on three datasets, and experimental results indicate that CALeC significantly outperforms other competitor models on inference accuracy and quality of generated explanations."
1441,https://arxiv.org/abs/2207.06989,Tree Structure-Aware Few-Shot Image Classification via Hierarchical Aggregation,"In this paper, we mainly focus on the problem of how to learn additional feature representations for few-shot image classification through pretext tasks (e.g., rotation or color permutation and so on). This additional knowledge generated by pretext tasks can further improve the performance of few-shot learning (FSL) as it differs from human-annotated supervision (i.e., class labels of FSL tasks). To solve this problem, we present a plug-in Hierarchical Tree Structure-aware (HTS) method, which not only learns the relationship of FSL and pretext tasks, but more importantly, can adaptively select and aggregate feature representations generated by pretext tasks to maximize the performance of FSL tasks. A hierarchical tree constructing component and a gated selection aggregating component is introduced to construct the tree structure and find richer transferable knowledge that can rapidly adapt to novel classes with a few labeled images. Extensive experiments show that our HTS can significantly enhance multiple few-shot methods to achieve new state-of-the-art performance on four benchmark datasets. The code is available at: https://github.com/remiMZ/HTS-ECCV22."
1442,https://arxiv.org/abs/2207.05258,A fifth-order finite difference HWENO scheme combined with limiter for hyperbolic conservation laws,"In this paper, a simple fifth-order finite difference Hermite WENO (HWENO) scheme combined with limiter is proposed for one- and two- dimensional hyperbolic conservation laws. The fluxes in the governing equation are approximated by the nonlinear HWENO reconstruction which is the combination of a quintic polynomial with two quadratic polynomials, where the linear weights can be artificial positive numbers only if the sum equals one. And other fluxes in the derivative equations are approximated by high-degree polynomials directly. For the purpose of controlling spurious oscillations, an HWENO limiter is applied to modify the derivatives. Instead of using the modified derivatives both in fluxes reconstruction and time discretization as in the modified HWENO scheme (J. Sci. Comput., 85:29, 2020), we only apply the modified derivatives in time discretization while remaining the original derivatives in fluxes reconstruction. Comparing with the modified HWENO scheme, the proposed HWENO scheme is simpler, more accurate, efficient and higher resolution. In addition, the HWENO scheme has a more compact spatial reconstructed stencil and greater efficiency than the classical fifth-order finite difference WENO scheme of Jiang and Shu. Various benchmark numerical examples are presented to show the fifth-order accuracy, great efficiency, high resolution and robustness of the proposed HWENO scheme."
1443,https://arxiv.org/abs/2206.12811,Towards Representation Alignment and Uniformity in Collaborative Filtering,"Collaborative filtering (CF) plays a critical role in the development of recommender systems. Most CF methods utilize an encoder to embed users and items into the same representation space, and the Bayesian personalized ranking (BPR) loss is usually adopted as the objective function to learn informative encoders. Existing studies mainly focus on designing more powerful encoders (e.g., graph neural network) to learn better representations. However, few efforts have been devoted to investigating the desired properties of representations in CF, which is important to understand the rationale of existing CF methods and design new learning objectives. In this paper, we measure the representation quality in CF from the perspective of alignment and uniformity on the hypersphere. We first theoretically reveal the connection between the BPR loss and these two properties. Then, we empirically analyze the learning dynamics of typical CF methods in terms of quantified alignment and uniformity, which shows that better alignment or uniformity both contribute to higher recommendation performance. Based on the analyses results, a learning objective that directly optimizes these two properties is proposed, named DirectAU. We conduct extensive experiments on three public datasets, and the proposed learning framework with a simple matrix factorization model leads to significant performance improvements compared to state-of-the-art CF methods. Our implementations are publicly available at https://github.com/THUwangcy/DirectAU."
1444,https://arxiv.org/abs/2206.12608,Adversarial Self-Attention for Language Understanding,"Deep neural models (e.g. Transformer) naturally learn spurious features, which create a ``shortcut'' between the labels and inputs, thus impairing the generalization and robustness. This paper advances the self-attention mechanism to its robust variant for Transformer-based pre-trained language models (e.g. BERT). We propose \textit{Adversarial Self-Attention} mechanism (ASA), which adversarially biases the attentions to effectively suppress the model reliance on features (e.g. specific keywords) and encourage its exploration of broader semantics. We conduct a comprehensive evaluation across a wide range of tasks for both pre-training and fine-tuning stages. For pre-training, ASA unfolds remarkable performance gains compared to naive training for longer steps. For fine-tuning, ASA-empowered models outweigh naive models by a large margin considering both generalization and robustness."
1445,https://arxiv.org/abs/2206.11556,A Federated Reinforcement Learning Method with Quantization for Cooperative Edge Caching in Fog Radio Access Networks,"In this paper, cooperative edge caching problem is studied in fog radio access networks (F-RANs). Given the non-deterministic polynomial hard (NP-hard) property of the problem, a dueling deep Q network (Dueling DQN) based caching update algorithm is proposed to make an optimal caching decision by learning the dynamic network environment. In order to protect user data privacy and solve the problem of slow convergence of the single deep reinforcement learning (DRL) model training, we propose a federated reinforcement learning method with quantization (FRLQ) to implement cooperative training of models from multiple fog access points (F-APs) in F-RANs. To address the excessive consumption of communications resources caused by model transmission, we prune and quantize the shared DRL models to reduce the number of model transfer parameters. The communications interval is increased and the communications rounds are reduced by periodical model global aggregation. We analyze the global convergence and computational complexity of our policy. Simulation results verify that our policy has better performance in reducing user request delay and improving cache hit rate compared to benchmark schemes. The proposed policy is also shown to have faster training speed and higher communications efficiency with minimal loss of model accuracy."
1446,https://arxiv.org/abs/2206.09718,3D homogenization of the T-A formulation for the analysis of coils with complex geometries,"The modeling and analysis of superconducting coils is an essential task in the design stage of most devices based on high-temperature superconductors (HTS). These calculations allow verifying basic estimations and assumptions, proposing improvements, and computing quantities that are not easy to calculate with an analytical approach. For instance, the estimation of losses in HTS is fundamental during the design stage since losses can strongly influence the cooling system requirements and operating temperature. Typically, 2D finite element analysis is used to calculate AC losses in HTS, due to the lack of analytical solutions that can accurately represent complex operating conditions such as AC transport current and AC external applied magnetic field in coils. These 2D models are usually a representation of an infinitely long arrangement. Therefore, they cannot be used to analyze end effects and complex 3D configurations. In this publication, we use the homogenization of the T-A formulation in 3D for the analysis of superconducting coils with complex geometries where a 2D approach can not provide accurate analyses and verification of assumptions. The modeling methodology allows an easier implementation in commercial software (COMSOL Multiphysics) in comparison with the currently available 3D H homogenization, despite the complexity of the geometry. This methodology is first validated with a racetrack coil (benchmark case) by comparing the results with the well-established H formulation. Then, the electromagnetic behavior of coils with more complex geometries is analyzed."
1447,https://arxiv.org/abs/2206.08611,Medical Dialogue Response Generation with Pivotal Information Recalling,"Medical dialogue generation is an important yet challenging task. Most previous works rely on the attention mechanism and large-scale pretrained language models. However, these methods often fail to acquire pivotal information from the long dialogue history to yield an accurate and informative response, due to the fact that the medical entities usually scatters throughout multiple utterances along with the complex relationships between them. To mitigate this problem, we propose a medical response generation model with Pivotal Information Recalling (MedPIR), which is built on two components, i.e., knowledge-aware dialogue graph encoder and recall-enhanced generator. The knowledge-aware dialogue graph encoder constructs a dialogue graph by exploiting the knowledge relationships between entities in the utterances, and encodes it with a graph attention network. Then, the recall-enhanced generator strengthens the usage of these pivotal information by generating a summary of the dialogue before producing the actual response. Experimental results on two large-scale medical dialogue datasets show that MedPIR outperforms the strong baselines in BLEU scores and medical entities F1 measure."
1448,https://arxiv.org/abs/2206.08493,Nonconforming finite elements for the Brinkman and $-\text{curl}Δ\text{curl}$ problems on cubical meshes,"We propose two families of nonconforming elements on cubical meshes: one for the $-\text{curl}Δ\text{curl}$ problem and the other for the Brinkman problem. The element for the $-\text{curl}Δ\text{curl}$ problem is the first nonconforming element on cubical meshes. The element for the Brinkman problem can yield a uniformly stable finite element method with respect to the parameter $ν$. The lowest-order elements for the $-\text{curl}Δ\text{curl}$ and the Brinkman problems have 48 and 30 degrees of freedom, respectively.
  The two families of elements are subspaces of $H(\text{curl};Ω)$ and $H(\text{div};Ω)$, and they, as nonconforming approximation to $H(\text{gradcurl};Ω)$ and $[H^1(Ω)]^3$, can form a discrete Stokes complex together with the Lagrange element and the $L^2$ element."
1449,https://arxiv.org/abs/2206.03761,A Survey on the Fairness of Recommender Systems,"Recommender systems are an essential tool to relieve the information overload challenge and play an important role in people's daily lives. Since recommendations involve allocations of social resources (e.g., job recommendation), an important issue is whether recommendations are fair. Unfair recommendations are not only unethical but also harm the long-term interests of the recommender system itself. As a result, fairness issues in recommender systems have recently attracted increasing attention. However, due to multiple complex resource allocation processes and various fairness definitions, the research on fairness in recommendation is scattered. To fill this gap, we review over 60 papers published in top conferences/journals, including TOIS, SIGIR, and WWW. First, we summarize fairness definitions in the recommendation and provide several views to classify fairness issues. Then, we review recommendation datasets and measurements in fairness studies and provide an elaborate taxonomy of fairness methods in the recommendation. Finally, we conclude this survey by outlining some promising future directions."
1450,https://arxiv.org/abs/2206.01375,Exploiting dynamic nonlinearity in upconversion nanoparticles for super-resolution imaging,"Single-beam super-resolution microscopy, also known as superlinear microscopy, exploits the nonlinear response of fluorescent probes in confocal microscopy. The technique requires no complex purpose-built system, light field modulation, or beam shaping. Here, we present a strategy to enhance spatial resolution of superlinear microscopy by modulating excitation intensity during image acquisition. This modulation induces dynamic optical nonlinearity in upconversion nanoparticles (UCNPs), resulting in variations of higher spatial-frequency information in the obtained images. The high-order information can be extracted with a proposed weighted finite difference imaging algorithm from raw fluorescence images, to generate an image with a higher resolution than superlinear microscopy images. We apply this approach to resolve two adjacent nanoparticles within a diffraction-limited area, improving the resolution to 130 nm. This work suggests a new scope for developing dynamic nonlinear fluorescent probes in super-resolution nanoscopy."
1451,https://arxiv.org/abs/2206.00512,Neural Network Verification with Proof Production,"Deep neural networks (DNNs) are increasingly being employed in safety-critical systems, and there is an urgent need to guarantee their correctness. Consequently, the verification community has devised multiple techniques and tools for verifying DNNs. When DNN verifiers discover an input that triggers an error, that is easy to confirm; but when they report that no error exists, there is no way to ensure that the verification tool itself is not flawed. As multiple errors have already been observed in DNN verification tools, this calls the applicability of DNN verification into question. In this work, we present a novel mechanism for enhancing Simplex-based DNN verifiers with proof production capabilities: the generation of an easy-to-check witness of unsatisfiability, which attests to the absence of errors. Our proof production is based on an efficient adaptation of the well-known Farkas' lemma, combined with mechanisms for handling piecewise-linear functions and numerical precision errors. As a proof of concept, we implemented our technique on top of the Marabou DNN verifier. Our evaluation on a safety-critical system for airborne collision avoidance shows that proof production succeeds in almost all cases and requires only minimal overhead."
1452,https://arxiv.org/abs/2205.14560,A well-balanced moving mesh discontinuous Galerkin method for the Ripa model on triangular meshes,"A well-balanced moving mesh discontinuous Galerkin (DG) method is proposed for the numerical solution of the Ripa model -- a generalization of the shallow water equations that accounts for effects of water temperature variations. Thermodynamic processes are important particularly in the upper layers of the ocean where the variations of sea surface temperature play a fundamental role in climate change. The well-balance property which requires numerical schemes to preserve the lake-at-rest steady state is crucial to the simulation of perturbation waves over that steady state such as waves on a lake or tsunami waves in the deep ocean. To ensure the well-balance, positivity-preserving, and high-order properties, a DG-interpolation scheme (with or without scaling positivity-preserving limiter) and special treatments pertaining to the Ripa model are employed in the transfer of both the flow variables and bottom topography from the old mesh to the new one and in the TVB limiting process. Mesh adaptivity is realized using an MMPDE moving mesh approach and a metric tensor based on an equilibrium variable and water depth. A motivation is to adapt the mesh according to both the perturbations of the lake-at-rest steady state and the water depth distribution (bottom structure). Numerical examples in one and two dimensions are presented to demonstrate the well-balance, high-order accuracy, and positivity-preserving properties of the method and its ability to capture small perturbations of the lake-at-rest steady state."
1453,https://arxiv.org/abs/2205.08052,An Inverse Probability Weighted Regression Method that Accounts for Right-censoring for Causal Inference with Multiple Treatments and a Binary Outcome,"Comparative effectiveness research often involves evaluating the differences in the risks of an event of interest between two or more treatments using observational data. Often, the post-treatment outcome of interest is whether the event happens within a pre-specified time window, which leads to a binary outcome. One source of bias for estimating the causal treatment effect is the presence of confounders, which are usually controlled using propensity score-based methods. An additional source of bias is right-censoring, which occurs when the information on the outcome of interest is not completely available due to dropout, study termination, or treatment switch before the event of interest. We propose an inverse probability weighted regression-based estimator that can simultaneously handle both confounding and right-censoring, calling the method CIPWR, with the letter C highlighting the censoring component. CIPWR estimates the average treatment effects by averaging the predicted outcomes obtained from a logistic regression model that is fitted using a weighted score function. The CIPWR estimator has a double robustness property such that estimation consistency can be achieved when either the model for the outcome or the models for both treatment and censoring are correctly specified. We establish the asymptotic properties of the CIPWR estimator for conducting inference, and compare its finite sample performance with that of several alternatives through simulation studies. The methods under comparison are applied to a cohort of prostate cancer patients from an insurance claims database for comparing the adverse effects of four candidate drugs for advanced stage prostate cancer."
1454,https://arxiv.org/abs/2205.06703,MuCPAD: A Multi-Domain Chinese Predicate-Argument Dataset,"During the past decade, neural network models have made tremendous progress on in-domain semantic role labeling (SRL). However, performance drops dramatically under the out-of-domain setting. In order to facilitate research on cross-domain SRL, this paper presents MuCPAD, a multi-domain Chinese predicate-argument dataset, which consists of 30,897 sentences and 92,051 predicates from six different domains. MuCPAD exhibits three important features. 1) Based on a frame-free annotation methodology, we avoid writing complex frames for new predicates. 2) We explicitly annotate omitted core arguments to recover more complete semantic structure, considering that omission of content words is ubiquitous in multi-domain Chinese texts. 3) We compile 53 pages of annotation guidelines and adopt strict double annotation for improving data quality. This paper describes in detail the annotation methodology and annotation process of MuCPAD, and presents in-depth data analysis. We also give benchmark results on cross-domain SRL based on MuCPAD."
1455,https://arxiv.org/abs/2204.11501,Graph Convolutional Network Based Semi-Supervised Learning on Multi-Speaker Meeting Data,"Unsupervised clustering on speakers is becoming increasingly important for its potential uses in semi-supervised learning. In reality, we are often presented with enormous amounts of unlabeled data from multi-party meetings and discussions. An effective unsupervised clustering approach would allow us to significantly increase the amount of training data without additional costs for annotations. Recently, methods based on graph convolutional networks (GCN) have received growing attention for unsupervised clustering, as these methods exploit the connectivity patterns between nodes to improve learning performance. In this work, we present a GCN-based approach for semi-supervised learning. Given a pre-trained embedding extractor, a graph convolutional network is trained on the labeled data and clusters unlabeled data with ""pseudo-labels"". We present a self-correcting training mechanism that iteratively runs the cluster-train-correct process on pseudo-labels. We show that this proposed approach effectively uses unlabeled data and improves speaker recognition accuracy."
1456,https://arxiv.org/abs/2204.11447,Evaluating Interpolation and Extrapolation Performance of Neural Retrieval Models,"A retrieval model should not only interpolate the training data but also extrapolate well to the queries that are different from the training data. While neural retrieval models have demonstrated impressive performance on ad-hoc search benchmarks, we still know little about how they perform in terms of interpolation and extrapolation. In this paper, we demonstrate the importance of separately evaluating the two capabilities of neural retrieval models. Firstly, we examine existing ad-hoc search benchmarks from the two perspectives. We investigate the distribution of training and test data and find a considerable overlap in query entities, query intent, and relevance labels. This finding implies that the evaluation on these test sets is biased toward interpolation and cannot accurately reflect the extrapolation capacity. Secondly, we propose a novel evaluation protocol to separately evaluate the interpolation and extrapolation performance on existing benchmark datasets. It resamples the training and test data based on query similarity and utilizes the resampled dataset for training and evaluation. Finally, we leverage the proposed evaluation protocol to comprehensively revisit a number of widely-adopted neural retrieval models. Results show models perform differently when moving from interpolation to extrapolation. For example, representation-based retrieval models perform almost as well as interaction-based retrieval models in terms of interpolation but not extrapolation. Therefore, it is necessary to separately evaluate both interpolation and extrapolation performance and the proposed resampling method serves as a simple yet effective evaluation tool for future IR studies."
1457,https://arxiv.org/abs/2204.11406,Robust Self-Augmentation for Named Entity Recognition with Meta Reweighting,"Self-augmentation has received increasing research interest recently to improve named entity recognition (NER) performance in low-resource scenarios. Token substitution and mixup are two feasible heterogeneous self-augmentation techniques for NER that can achieve effective performance with certain specialized efforts. Noticeably, self-augmentation may introduce potentially noisy augmented data. Prior research has mainly resorted to heuristic rule-based constraints to reduce the noise for specific self-augmentation methods individually. In this paper, we revisit these two typical self-augmentation methods for NER, and propose a unified meta-reweighting strategy for them to achieve a natural integration. Our method is easily extensible, imposing little effort on a specific self-augmentation method. Experiments on different Chinese and English NER benchmarks show that our token substitution and mixup method, as well as their integration, can achieve effective performance improvement. Based on the meta-reweighting mechanism, we can enhance the advantages of the self-augmentation techniques without much extra effort."
1458,https://arxiv.org/abs/2204.10994,MuCGEC: a Multi-Reference Multi-Source Evaluation Dataset for Chinese Grammatical Error Correction,"This paper presents MuCGEC, a multi-reference multi-source evaluation dataset for Chinese Grammatical Error Correction (CGEC), consisting of 7,063 sentences collected from three Chinese-as-a-Second-Language (CSL) learner sources. Each sentence is corrected by three annotators, and their corrections are carefully reviewed by a senior annotator, resulting in 2.3 references per sentence. We conduct experiments with two mainstream CGEC models, i.e., the sequence-to-sequence model and the sequence-to-edit model, both enhanced with large pretrained language models, achieving competitive benchmark performance on previous and our datasets. We also discuss CGEC evaluation methodologies, including the effect of multiple references and using a char-based metric. Our annotation guidelines, data, and code are available at \url{https://github.com/HillZhang1999/MuCGEC}."
1459,https://arxiv.org/abs/2204.10714,Identifying Chinese Opinion Expressions with Extremely-Noisy Crowdsourcing Annotations,"Recent works of opinion expression identification (OEI) rely heavily on the quality and scale of the manually-constructed training corpus, which could be extremely difficult to satisfy. Crowdsourcing is one practical solution for this problem, aiming to create a large-scale but quality-unguaranteed corpus. In this work, we investigate Chinese OEI with extremely-noisy crowdsourcing annotations, constructing a dataset at a very low cost. Following zhang et al. (2021), we train the annotator-adapter model by regarding all annotations as gold-standard in terms of crowd annotators, and test the model by using a synthetic expert, which is a mixture of all annotators. As this annotator-mixture for testing is never modeled explicitly in the training phase, we propose to generate synthetic training samples by a pertinent mixup strategy to make the training and testing highly consistent. The simulation experiments on our constructed dataset show that crowdsourcing is highly promising for OEI, and our proposed annotator-mixup can further enhance the crowdsourcing modeling."
1460,https://arxiv.org/abs/2204.09269,A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond,"Non-autoregressive (NAR) generation, which is first proposed in neural machine translation (NMT) to speed up inference, has attracted much attention in both machine learning and natural language processing communities. While NAR generation can significantly accelerate inference speed for machine translation, the speedup comes at the cost of sacrificed translation accuracy compared to its counterpart, auto-regressive (AR) generation. In recent years, many new models and algorithms have been designed/proposed to bridge the accuracy gap between NAR generation and AR generation. In this paper, we conduct a systematic survey with comparisons and discussions of various non-autoregressive translation (NAT) models from different aspects. Specifically, we categorize the efforts of NAT into several groups, including data manipulation, modeling methods, training criterion, decoding algorithms, and the benefit from pre-trained models. Furthermore, we briefly review other applications of NAR models beyond machine translation, such as dialogue generation, text summarization, grammar error correction, semantic parsing, speech synthesis, and automatic speech recognition. In addition, we also discuss potential directions for future exploration, including releasing the dependency of KD, dynamic length prediction, pre-training for NAR, and wider applications, etc. We hope this survey can help researchers capture the latest progress in NAR generation, inspire the design of advanced NAR models and algorithms, and enable industry practitioners to choose appropriate solutions for their applications. The web page of this survey is at \url{https://github.com/LitterBrother-Xiao/Overview-of-Non-autoregressive-Applications}."
1461,https://arxiv.org/abs/2204.07895,New conforming finite element divdiv complexes in three dimensions,"In this paper, the first family of conforming finite element divdiv complexes on cuboid grids in three dimensions is constructed. Besides, a new family of conforming finite element divdiv complexes with enhanced smoothness on tetrahedral grids is presented. These complexes are exact in the sense that the range of each discrete map is the kernel space of the succeeding one."
1462,https://arxiv.org/abs/2204.07837,BLISS: Robust Sequence-to-Sequence Learning via Self-Supervised Input Representation,"Data augmentations (DA) are the cores to achieving robust sequence-to-sequence learning on various natural language processing (NLP) tasks. However, most of the DA approaches force the decoder to make predictions conditioned on the perturbed input representation, underutilizing supervised information provided by perturbed input. In this work, we propose a framework-level robust sequence-to-sequence learning approach, named BLISS, via self-supervised input representation, which has the great potential to complement the data-level augmentation approaches. The key idea is to supervise the sequence-to-sequence framework with both the \textit{supervised} (""input$\rightarrow$output"") and \textit{self-supervised} (""perturbed input$\rightarrow$input"") information. We conduct comprehensive experiments to validate the effectiveness of BLISS on various tasks, including machine translation, grammatical error correction, and text summarization. The results show that BLISS outperforms significantly the vanilla Transformer and consistently works well across tasks than the other five contrastive baselines. Extensive analyses reveal that BLISS learns robust representations and rich linguistic knowledge, confirming our claim. Source code will be released upon publication."
1463,https://arxiv.org/abs/2204.02659,ConvSearch: A Open-Domain Conversational Search Behavior Dataset,"Conversational Search has been paid much attention recently with the increasing popularity of intelligent user interfaces. However, compared with the endeavour in designing effective conversational search algorithms, relatively much fewer researchers have focused on the construction of benchmark datasets. For most existing datasets, the information needs are defined by researchers and search requests are not proposed by actual users. Meanwhile, these datasets usually focus on the conversations between users and agents (systems), while largely ignores the search behaviors of agents before they return response to users. To overcome these problems, we construct a Chinese Open-Domain Conversational Search Behavior Dataset (ConvSearch) based on Wizard-of-Oz paradigm in the field study scenario. We develop a novel conversational search platform to collect dialogue contents, annotate dialogue quality and candidate search results and record agent search behaviors. 25 search agents and 51 users are recruited for the field study that lasts about 45 days. The ConvSearch dataset contains 1,131 dialogues together with annotated search results and corresponding search behaviors. We also provide the intent labels of each search behavior iteration to support intent understanding related researches. The dataset is already open to public for academic usage."
1464,https://arxiv.org/abs/2210.02305,Neuro-Planner: A 3D Visual Navigation Method for MAV with Depth Camera based on Neuromorphic Reinforcement Learning,"Traditional visual navigation methods of micro aerial vehicle (MAV) usually calculate a passable path that satisfies the constraints depending on a prior map. However, these methods have issues such as high demand for computing resources and poor robustness in face of unfamiliar environments. Aiming to solve the above problems, we propose a neuromorphic reinforcement learning method (Neuro-Planner) that combines spiking neural network (SNN) and deep reinforcement learning (DRL) to realize MAV 3D visual navigation with depth camera. Specifically, we design spiking actor network based on two-state LIF (TS-LIF) neurons and its encoding-decoding schemes for efficient inference. Then our improved hybrid deep deterministic policy gradient (HDDPG) and TS-LIF-based spatio-temporal back propagation (STBP) algorithms are used as the training framework for actor-critic network architecture. To verify the effectiveness of the proposed Neuro-Planner, we carry out detailed comparison experiments with various SNN training algorithm (STBP, BPTT and SLAYER) in the software-in-the-loop (SITL) simulation framework. The navigation success rate of our HDDPG-STBP is 4.3\% and 5.3\% higher than that of the original DDPG in the two evaluation environments. To the best of our knowledge, this is the first work combining neuromorphic computing and deep reinforcement learning for MAV 3D visual navigation task."
1465,https://arxiv.org/abs/2205.13296,Social Interpretable Tree for Pedestrian Trajectory Prediction,"Understanding the multiple socially-acceptable future behaviors is an essential task for many vision applications. In this paper, we propose a tree-based method, termed as Social Interpretable Tree (SIT), to address this multi-modal prediction task, where a hand-crafted tree is built depending on the prior information of observed trajectory to model multiple future trajectories. Specifically, a path in the tree from the root to leaf represents an individual possible future trajectory. SIT employs a coarse-to-fine optimization strategy, in which the tree is first built by high-order velocity to balance the complexity and coverage of the tree and then optimized greedily to encourage multimodality. Finally, a teacher-forcing refining operation is used to predict the final fine trajectory. Compared with prior methods which leverage implicit latent variables to represent possible future trajectories, the path in the tree can explicitly explain the rough moving behaviors (e.g., go straight and then turn right), and thus provides better interpretability. Despite the hand-crafted tree, the experimental results on ETH-UCY and Stanford Drone datasets demonstrate that our method is capable of matching or exceeding the performance of state-of-the-art methods. Interestingly, the experiments show that the raw built tree without training outperforms many prior deep neural network based approaches. Meanwhile, our method presents sufficient flexibility in long-term prediction and different best-of-$K$ predictions."
1466,https://arxiv.org/abs/2111.12324,How Speech is Recognized to Be Emotional - A Study Based on Information Decomposition,"The way that humans encode their emotion into speech signals is complex. For instance, an angry man may increase his pitch and speaking rate, and use impolite words. In this paper, we present a preliminary study on various emotional factors and investigate how each of them impacts modern emotion recognition systems. The key tool of our study is the SpeechFlow model presented recently, by which we are able to decompose speech signals into separate information factors (content, pitch, rhythm). Based on this decomposition, we carefully studied the performance of each information component and their combinations. We conducted the study on three different speech emotion corpora and chose an attention-based convolutional RNN as the emotion classifier. Our results show that rhythm is the most important component for emotional expression. Moreover, the cross-corpus results are very bad (even worse than guess), demonstrating that the present speech emotion recognition model is rather weak. Interestingly, by removing one or several unimportant components, the cross-corpus results can be improved. This demonstrates the potential of the decomposition approach towards a generalizable emotion recognition."
1467,https://arxiv.org/abs/2110.05087,A Multi-Resolution Front-End for End-to-End Speech Anti-Spoofing,"The choice of an optimal time-frequency resolution is usually a difficult but important step in tasks involving speech signal classification, e.g., speech anti-spoofing. The variations of the performance with different choices of timefrequency resolutions can be as large as those with different model architectures, which makes it difficult to judge what the improvement actually comes from when a new network architecture is invented and introduced as the classifier. In this paper, we propose a multi-resolution front-end for feature extraction in an end-to-end classification framework. Optimal weighted combinations of multiple time-frequency resolutions will be learned automatically given the objective of a classification task. Features extracted with different time-frequency resolutions are weighted and concatenated as inputs to the successive networks, where the weights are predicted by a learnable neural network inspired by the weighting block in squeeze-and-excitation networks (SENet). Furthermore, the refinement of the chosen timefrequency resolutions is investigated by pruning the ones with relatively low importance, which reduces the complexity and size of the model. The proposed method is evaluated on the tasks of speech anti-spoofing in ASVSpoof 2019 and its superiority has been justified by comparing with similar baselines."
1468,https://arxiv.org/abs/2108.00238,Unlimited Neighborhood Interaction for Heterogeneous Trajectory Prediction,"Understanding complex social interactions among agents is a key challenge for trajectory prediction. Most existing methods consider the interactions between pairwise traffic agents or in a local area, while the nature of interactions is unlimited, involving an uncertain number of agents and non-local areas simultaneously. Besides, they treat heterogeneous traffic agents the same, namely those among agents of different categories, while neglecting people's diverse reaction patterns toward traffic agents in ifferent categories. To address these problems, we propose a simple yet effective Unlimited Neighborhood Interaction Network (UNIN), which predicts trajectories of heterogeneous agents in multiple categories. Specifically, the proposed unlimited neighborhood interaction module generates the fused-features of all agents involved in an interaction simultaneously, which is adaptive to any number of agents and any range of interaction area. Meanwhile, a hierarchical graph attention module is proposed to obtain category-to-category interaction and agent-to-agent interaction. Finally, parameters of a Gaussian Mixture Model are estimated for generating the future trajectories. Extensive experimental results on benchmark datasets demonstrate a significant performance improvement of our method over the state-of-the-art methods."
1469,https://arxiv.org/abs/2012.12468,CN-Celeb: multi-genre speaker recognition,"Research on speaker recognition is extending to address the vulnerability in the wild conditions, among which genre mismatch is perhaps the most challenging, for instance, enrollment with reading speech while testing with conversational or singing audio. This mismatch leads to complex and composite inter-session variations, both intrinsic (i.e., speaking style, physiological status) and extrinsic (i.e., recording device, background noise). Unfortunately, the few existing multi-genre corpora are not only limited in size but are also recorded under controlled conditions, which cannot support conclusive research on the multi-genre problem. In this work, we firstly publish CN-Celeb, a large-scale multi-genre corpus that includes in-the-wild speech utterances of 3,000 speakers in 11 different genres. Secondly, using this dataset, we conduct a comprehensive study on the multi-genre phenomenon, in particular the impact of the multi-genre challenge on speaker recognition and the performance gain when the new dataset is used to conduct multi-genre training."
1470,https://arxiv.org/abs/2010.14243,Squeezing value of cross-domain labels: a decoupled scoring approach for speaker verification,"Domain mismatch often occurs in real applications and causes serious performance reduction on speaker verification systems. The common wisdom is to collect cross-domain data and train a multi-domain PLDA model, with the hope to learn a domain-independent speaker subspace. In this paper, we firstly present an empirical study to show that simply adding cross-domain data does not help performance in conditions with enrollment-test mismatch. Careful analysis shows that this striking result is caused by the incoherent statistics between the enrollment and test conditions. Based on this analysis, we present a decoupled scoring approach that can maximally squeeze the value of cross-domain labels and obtain optimal verification scores when the enrollment and test are mismatched. When the statistics are coherent, the new formulation falls back to the conventional PLDA. Experimental results on cross-channel test show that the proposed approach is highly effective and is a principle solution to domain mismatch."
1471,https://arxiv.org/abs/2010.14242,Deep generative factorization for speech signal,"Various information factors are blended in speech signals, which forms the primary difficulty for most speech information processing tasks. An intuitive idea is to factorize speech signal into individual information factors (e.g., phonetic content and speaker trait), though it turns out to be highly challenging. This paper presents a speech factorization approach based on a novel factorial discriminative normalization flow model (factorial DNF). Experiments conducted on a two-factor case that involves phonetic content and speaker trait demonstrates that the proposed factorial DNF has powerful capability to factorize speech signals and outperforms several comparative models in terms of information representation and manipulation."
1472,https://arxiv.org/abs/2009.06863,When Automatic Voice Disguise Meets Automatic Speaker Verification,"The technique of transforming voices in order to hide the real identity of a speaker is called voice disguise, among which automatic voice disguise (AVD) by modifying the spectral and temporal characteristics of voices with miscellaneous algorithms are easily conducted with softwares accessible to the public. AVD has posed great threat to both human listening and automatic speaker verification (ASV). In this paper, we have found that ASV is not only a victim of AVD but could be a tool to beat some simple types of AVD. Firstly, three types of AVD, pitch scaling, vocal tract length normalization (VTLN) and voice conversion (VC), are introduced as representative methods. State-of-the-art ASV methods are subsequently utilized to objectively evaluate the impact of AVD on ASV by equal error rates (EER). Moreover, an approach to restore disguised voice to its original version is proposed by minimizing a function of ASV scores w.r.t. restoration parameters. Experiments are then conducted on disguised voices from Voxceleb, a dataset recorded in real-world noisy scenario. The results have shown that, for the voice disguise by pitch scaling, the proposed approach obtains an EER around 7% comparing to the 30% EER of a recently proposed baseline using the ratio of fundamental frequencies. The proposed approach generalizes well to restore the disguise with nonlinear frequency warping in VTLN by reducing its EER from 34.3% to 18.5%. However, it is difficult to restore the source speakers in VC by our approach, where more complex forms of restoration functions or other paralinguistic cues might be necessary to restore the nonlinear transform in VC. Finally, contrastive visualization on ASV features with and without restoration illustrate the role of the proposed approach in an intuitive way."
1473,https://arxiv.org/abs/2005.11905,Neural Discriminant Analysis for Deep Speaker Embedding,"Probabilistic Linear Discriminant Analysis (PLDA) is a popular tool in open-set classification/verification tasks. However, the Gaussian assumption underlying PLDA prevents it from being applied to situations where the data is clearly non-Gaussian. In this paper, we present a novel nonlinear version of PLDA named as Neural Discriminant Analysis (NDA). This model employs an invertible deep neural network to transform a complex distribution to a simple Gaussian, so that the linear Gaussian model can be readily established in the transformed space. We tested this NDA model on a speaker recognition task where the deep speaker vectors (x-vectors) are presumably non-Gaussian. Experimental results on two datasets demonstrate that NDA consistently outperforms PLDA, by handling the non-Gaussian distributions of the x-vectors."
1474,https://arxiv.org/abs/2005.11902,ASR-Free Pronunciation Assessment,"Most of the pronunciation assessment methods are based on local features derived from automatic speech recognition (ASR), e.g., the Goodness of Pronunciation (GOP) score. In this paper, we investigate an ASR-free scoring approach that is derived from the marginal distribution of raw speech signals. The hypothesis is that even if we have no knowledge of the language (so cannot recognize the phones/words), we can still tell how good a pronunciation is, by comparatively listening to some speech data from the target language. Our analysis shows that this new scoring approach provides an interesting correction for the phone-competition problem of GOP. Experimental results on the ERJ dataset demonstrated that combining the ASR-free score and GOP can achieve better performance than the GOP baseline."
1475,https://arxiv.org/abs/2005.11900,Domain-Invariant Speaker Vector Projection by Model-Agnostic Meta-Learning,"Domain generalization remains a critical problem for speaker recognition, even with the state-of-the-art architectures based on deep neural nets. For example, a model trained on reading speech may largely fail when applied to scenarios of singing or movie. In this paper, we propose a domain-invariant projection to improve the generalizability of speaker vectors. This projection is a simple neural net and is trained following the Model-Agnostic Meta-Learning (MAML) principle, for which the objective is to classify speakers in one domain if it had been updated with speech data in another domain. We tested the proposed method on CNCeleb, a new dataset consisting of single-speaker multi-condition (SSMC) data. The results demonstrated that the MAML-based domain-invariant projection can produce more generalizable speaker vectors, and effectively improve the performance in unseen domains."
1476,https://arxiv.org/abs/2002.09283,MODMA dataset: a Multi-modal Open Dataset for Mental-disorder Analysis,"According to the World Health Organization, the number of mental disorder patients, especially depression patients, has grown rapidly and become a leading contributor to the global burden of disease. However, the present common practice of depression diagnosis is based on interviews and clinical scales carried out by doctors, which is not only labor-consuming but also time-consuming. One important reason is due to the lack of physiological indicators for mental disorders. With the rising of tools such as data mining and artificial intelligence, using physiological data to explore new possible physiological indicators of mental disorder and creating new applications for mental disorder diagnosis has become a new research hot topic. However, good quality physiological data for mental disorder patients are hard to acquire. We present a multi-modal open dataset for mental-disorder analysis. The dataset includes EEG and audio data from clinically depressed patients and matching normal controls. All our patients were carefully diagnosed and selected by professional psychiatrists in hospitals. The EEG dataset includes not only data collected using traditional 128-electrodes mounted elastic cap, but also a novel wearable 3-electrode EEG collector for pervasive applications. The 128-electrodes EEG signals of 53 subjects were recorded as both in resting state and under stimulation; the 3-electrode EEG signals of 55 subjects were recorded in resting state; the audio data of 52 subjects were recorded during interviewing, reading, and picture description. We encourage other researchers in the field to use it for testing their methods of mental-disorder analysis."
1477,https://arxiv.org/abs/1810.06067,A radio structure resolved at the deca-parsec scale in radio-quiet quasar PDS 456 with an extremely powerful X-ray outflow,"Active galactic nuclei (AGN) accreting at rates close to the Eddington limit can host radiatively driven mildly relativistic outflows. Some of these X-ray absorbing but powerful outflows may produce strong shocks resulting in a significant non-thermal emission. This outflow-driven radio emission may be detectable in the radio-quiet quasar PDS 456 since it has a bolometric luminosity reaching the Eddington limit and a relativistic wide-aperture X-ray outflow with a kinetic power high enough to quench the star formation in its host galaxy. To investigate this possibility, we performed very-long-baseline interferometric (VLBI) observations of the quasar with the European VLBI Network (EVN) at 5 GHz. The EVN image with the full resolution reveals two faint and diffuse radio components with a projected separation of about 20 pc and an average brightness temperature of around two million Kelvin. In relation to the optical sub-mas-accuracy position measured by the Gaia mission, the two components are very likely on opposite sides of an undetected radio core. The VLBI structure at the deca-pc scale can thus be either a young jet or a bidirectional radio-emitting outflow, launched in the vicinity of a strongly accreting central engine. Two diffuse components at the hecto-pc scale, likely the relic radio emission from the past AGN activity, are tentatively detected on each side in the low-resolution EVN image."
1478,https://arxiv.org/abs/1803.00886,Deep factorization for speech signal,"Various informative factors mixed in speech signals, leading to great difficulty when decoding any of the factors. An intuitive idea is to factorize each speech frame into individual informative factors, though it turns out to be highly difficult. Recently, we found that speaker traits, which were assumed to be long-term distributional properties, are actually short-time patterns, and can be learned by a carefully designed deep neural network (DNN). This discovery motivated a cascade deep factorization (CDF) framework that will be presented in this paper. The proposed framework infers speech factors in a sequential way, where factors previously inferred are used as conditional variables when inferring other factors. We will show that this approach can effectively factorize speech signals, and using these factors, the original speech spectrum can be recovered with a high accuracy. This factorization and reconstruction approach provides potential values for many speech processing tasks, e.g., speaker recognition and emotion recognition, as will be demonstrated in the paper."
1479,https://arxiv.org/abs/1802.08085,Precursors of gate oxide degradation in SiC power MOSFETs,"Gate oxide degradation is more critical in Silicon-Carbide (SiC) MOSFETs than in Silicon (Si) MOSFETs. This is because of the smaller gate oxide thickness and the higher electric field that develops across the gate oxide in SiC MOSFETs. While multiple precursors have been identified for monitoring the gate oxide degradation in Si MOSFETs, very few precursors have been identified for SiC MOSFETs. The purpose of this paper is to demonstrate that gate oxide degradation precursors used in Si MOSFETs: a) threshold voltage, b) gate plateau voltage and c) gate plateau time, can also be used as precursors for SiC MOSFETS. Moreover, all three precursors are found to exhibit a simultaneous increasing trend (during the stress time) leading to an increase in on-state loss, switching loss and switching time of the SiC MOSFET. The existing studies of gate oxide degradation mechanisms in SiC MOSFETs, and their effects on threshold voltage and mobility were extended to correlate a variation of all three precursors using analytical expressions. The increasing trends of precursors were experimentally confirmed by inducing gate oxide degradation in commercial SiC MOSFET samples."
1480,https://arxiv.org/abs/1711.00366,Full-info Training for Deep Speaker Feature Learning,"In recent studies, it has shown that speaker patterns can be learned from very short speech segments (e.g., 0.3 seconds) by a carefully designed convolutional & time-delay deep neural network (CT-DNN) model. By enforcing the model to discriminate the speakers in the training data, frame-level speaker features can be derived from the last hidden layer. In spite of its good performance, a potential problem of the present model is that it involves a parametric classifier, i.e., the last affine layer, which may consume some discriminative knowledge, thus leading to `information leak' for the feature learning. This paper presents a full-info training approach that discards the parametric classifier and enforces all the discriminative knowledge learned by the feature net. Our experiments on the Fisher database demonstrate that this new training scheme can produce more coherent features, leading to consistent and notable performance improvement on the speaker verification task."
1481,https://arxiv.org/abs/1710.01789,Enhanced Neural Machine Translation by Learning from Draft,"Neural machine translation (NMT) has recently achieved impressive results. A potential problem of the existing NMT algorithm, however, is that the decoding is conducted from left to right, without considering the right context. This paper proposes an two-stage approach to solve the problem. In the first stage, a conventional attention-based NMT system is used to produce a draft translation, and in the second stage, a novel double-attention NMT system is used to refine the translation, by looking at the original input as well as the draft translation. This drafting-and-refinement can obtain the right-context information from the draft, hence producing more consistent translations. We evaluated this approach using two Chinese-English translation tasks, one with 44k pairs and 1M pairs respectively. The experiments showed that our approach achieved positive improvements over the conventional NMT system: the improvements are 2.4 and 0.9 BLEU points on the small-scale and large-scale tasks, respectively."
1482,https://arxiv.org/abs/1706.07861,Cross-lingual Speaker Verification with Deep Feature Learning,"Existing speaker verification (SV) systems often suffer from performance degradation if there is any language mismatch between model training, speaker enrollment, and test. A major cause of this degradation is that most existing SV methods rely on a probabilistic model to infer the speaker factor, so any significant change on the distribution of the speech signal will impact the inference. Recently, we proposed a deep learning model that can learn how to extract the speaker factor by a deep neural network (DNN). By this feature learning, an SV system can be constructed with a very simple back-end model. In this paper, we investigate the robustness of the feature-based SV system in situations with language mismatch. Our experiments were conducted on a complex cross-lingual scenario, where the model training was in English, and the enrollment and test were in Chinese or Uyghur. The experiments demonstrated that the feature-based system outperformed the i-vector system with a large margin, particularly with language mismatch between enrollment and test."
1483,https://arxiv.org/abs/1706.07859,Deep Speaker Verification: Do We Need End to End?,"End-to-end learning treats the entire system as a whole adaptable black box, which, if sufficient data are available, may learn a system that works very well for the target task. This principle has recently been applied to several prototype research on speaker verification (SV), where the feature learning and classifier are learned together with an objective function that is consistent with the evaluation metric. An opposite approach to end-to-end is feature learning, which firstly trains a feature learning model, and then constructs a back-end classifier separately to perform SV. Recently, both approaches achieved significant performance gains on SV, mainly attributed to the smart utilization of deep neural networks. However, the two approaches have not been carefully compared, and their respective advantages have not been well discussed. In this paper, we compare the end-to-end and feature learning approaches on a text-independent SV task. Our experiments on a dataset sampled from the Fisher database and involving 5,000 speakers demonstrated that the feature learning approach outperformed the end-to-end approach. This is a strong support for the feature learning approach, at least with data and computation resources similar to ours."
1484,https://arxiv.org/abs/1706.02101,A Study on Replay Attack and Anti-Spoofing for Automatic Speaker Verification,"For practical automatic speaker verification (ASV) systems, replay attack poses a true risk. By replaying a pre-recorded speech signal of the genuine speaker, ASV systems tend to be easily fooled. An effective replay detection method is therefore highly desirable. In this study, we investigate a major difficulty in replay detection: the over-fitting problem caused by variability factors in speech signal. An F-ratio probing tool is proposed and three variability factors are investigated using this tool: speaker identity, speech content and playback & recording device. The analysis shows that device is the most influential factor that contributes the highest over-fitting risk. A frequency warping approach is studied to alleviate the over-fitting problem, as verified on the ASV-spoof 2017 database."
1485,https://arxiv.org/abs/1609.08419,Decision Making Based on Cohort Scores for Speaker Verification,"Decision making is an important component in a speaker verification system. For the conventional GMM-UBM architecture, the decision is usually conducted based on the log likelihood ratio of the test utterance against the GMM of the claimed speaker and the UBM. This single-score decision is simple but tends to be sensitive to the complex variations in speech signals (e.g. text content, channel, speaking style, etc.). In this paper, we propose a decision making approach based on multiple scores derived from a set of cohort GMMs (cohort scores). Importantly, these cohort scores are not simply averaged as in conventional cohort methods; instead, we employ a powerful discriminative model as the decision maker. Experimental results show that the proposed method delivers substantial performance improvement over the baseline system, especially when a deep neural network (DNN) is used as the decision maker, and the DNN input involves some statistical features derived from the cohort scores."
1486,https://arxiv.org/abs/1603.09460,System Combination for Short Utterance Speaker Recognition,"For text-independent short-utterance speaker recognition (SUSR), the performance often degrades dramatically. This paper presents a combination approach to the SUSR tasks with two phonetic-aware systems: one is the DNN-based i-vector system and the other is our recently proposed subregion-based GMM-UBM system. The former employs phone posteriors to construct an i-vector model in which the shared statistics offers stronger robustness against limited test data, while the latter establishes a phone-dependent GMM-UBM system which represents speaker characteristics with more details. A score-level fusion is implemented to integrate the respective advantages from the two systems. Experimental results show that for the text-independent SUSR task, both the DNN-based i-vector system and the subregion-based GMM-UBM system outperform their respective baselines, and the score-level system combination delivers performance improvement."
1487,https://arxiv.org/abs/1511.06066,Transfer Learning for Speech and Language Processing,"Transfer learning is a vital technique that generalizes models trained for one setting or task to other settings or tasks. For example in speech recognition, an acoustic model trained for one language can be used to recognize speech in another language, with little or no re-training data. Transfer learning is closely related to multi-task learning (cross-lingual vs. multilingual), and is traditionally studied in the name of `model adaptation'. Recent advance in deep learning shows that transfer learning becomes much easier and more effective with high-level abstract features learned by deep models, and the `transfer' can be conducted not only between data distributions and data types, but also between model structures (e.g., shallow nets and deep nets) or even model types (e.g., Bayesian models and neural models). This review paper summarizes some recent prominent research towards this direction, particularly for speech and language processing. We also report some results from our group and highlight the potential of this very interesting research field."
1488,https://arxiv.org/abs/1510.05940,Max-margin Metric Learning for Speaker Recognition,"Probabilistic linear discriminant analysis (PLDA) is a popular normalization approach for the i-vector model, and has delivered state-of-the-art performance in speaker recognition. A potential problem of the PLDA model, however, is that it essentially assumes Gaussian distributions over speaker vectors, which is not always true in practice. Additionally, the objective function is not directly related to the goal of the task, e.g., discriminating true speakers and imposters. In this paper, we propose a max-margin metric learning approach to solve the problems. It learns a linear transform with a criterion that the margin between target and imposter trials are maximized. Experiments conducted on the SRE08 core test show that compared to PLDA, the new approach can obtain comparable or even better performance, though the scoring is simply a cosine computation."
1489,https://arxiv.org/abs/1510.05937,Binary Speaker Embedding,"The popular i-vector model represents speakers as low-dimensional continuous vectors (i-vectors), and hence it is a way of continuous speaker embedding. In this paper, we investigate binary speaker embedding, which transforms i-vectors to binary vectors (codes) by a hash function. We start from locality sensitive hashing (LSH), a simple binarization approach where binary codes are derived from a set of random hash functions. A potential problem of LSH is that the randomly sampled hash functions might be suboptimal. We therefore propose an improved Hamming distance learning approach, where the hash function is learned by a variable-sized block training that projects each dimension of the original i-vectors to variable-sized binary codes independently. Our experiments show that binary speaker embedding can deliver competitive or even better results on both speaker verification and identification tasks, while the memory usage and the computation cost are significantly reduced."
1490,https://arxiv.org/abs/1509.01183,Parallel Knowledge Embedding with MapReduce on a Multi-core Processor,"This article firstly attempts to explore parallel algorithms of learning distributed representations for both entities and relations in large-scale knowledge repositories with {\it MapReduce} programming model on a multi-core processor. We accelerate the training progress of a canonical knowledge embedding method, i.e. {\it translating embedding} ({\bf TransE}) model, by dividing a whole knowledge repository into several balanced subsets, and feeding each subset into an individual core where local embeddings can concurrently run updating during the {\it Map} phase. However, it usually suffers from inconsistent low-dimensional vector representations of the same key, which are collected from different {\it Map} workers, and further leads to conflicts when conducting {\it Reduce} to merge the various vectors associated with the same key. Therefore, we try several strategies to acquire the merged embeddings which may not only retain the performance of {\it entity inference}, {\it relation prediction}, and even {\it triplet classification} evaluated by the single-thread {\bf TransE} on several well-known knowledge bases such as Freebase and NELL, but also scale up the learning speed along with the number of cores within a processor. So far, the empirical studies show that we could achieve comparable results as the single-thread {\bf TransE} performs by the {\it stochastic gradient descend} (SGD) algorithm, as well as increase the training speed multiple times via adapting the {\it batch gradient descend} (BGD) algorithm for {\it MapReduce} paradigm."
1491,https://arxiv.org/abs/1505.06427,Deep Speaker Vectors for Semi Text-independent Speaker Verification,"Recent research shows that deep neural networks (DNNs) can be used to extract deep speaker vectors (d-vectors) that preserve speaker characteristics and can be used in speaker verification. This new method has been tested on text-dependent speaker verification tasks, and improvement was reported when combined with the conventional i-vector method.
  This paper extends the d-vector approach to semi text-independent speaker verification tasks, i.e., the text of the speech is in a limited set of short phrases. We explore various settings of the DNN structure used for d-vector extraction, and present a phone-dependent training which employs the posterior features obtained from an ASR system. The experimental results show that it is possible to apply d-vectors on semi text-independent speaker recognition, and the phone-dependent training improves system performance."
1492,https://arxiv.org/abs/1505.03823,Distant Supervision for Entity Linking,"Entity linking is an indispensable operation of populating knowledge repositories for information extraction. It studies on aligning a textual entity mention to its corresponding disambiguated entry in a knowledge repository. In this paper, we propose a new paradigm named distantly supervised entity linking (DSEL), in the sense that the disambiguated entities that belong to a huge knowledge repository (Freebase) are automatically aligned to the corresponding descriptive webpages (Wiki pages). In this way, a large scale of weakly labeled data can be generated without manual annotation and fed to a classifier for linking more newly discovered entities. Compared with traditional paradigms based on solo knowledge base, DSEL benefits more via jointly leveraging the respective advantages of Freebase and Wikipedia. Specifically, the proposed paradigm facilitates bridging the disambiguated labels (Freebase) of entities and their textual descriptions (Wikipedia) for Web-scale entities. Experiments conducted on a dataset of 140,000 items and 60,000 features achieve a baseline F1-measure of 0.517. Furthermore, we analyze the feature performance and improve the F1-measure to 0.545."
1493,https://arxiv.org/abs/1505.02433,Probabilistic Belief Embedding for Knowledge Base Completion,"This paper contributes a novel embedding model which measures the probability of each belief $\langle h,r,t,m\rangle$ in a large-scale knowledge repository via simultaneously learning distributed representations for entities ($h$ and $t$), relations ($r$), and the words in relation mentions ($m$). It facilitates knowledge completion by means of simple vector operations to discover new beliefs. Given an imperfect belief, we can not only infer the missing entities, predict the unknown relations, but also tell the plausibility of the belief, just leveraging the learnt embeddings of remaining evidences. To demonstrate the scalability and the effectiveness of our model, we conduct experiments on several large-scale repositories which contain millions of beliefs from WordNet, Freebase and NELL, and compare it with other cutting-edge approaches via competing the performances assessed by the tasks of entity inference, relation prediction and triplet classification with respective metrics. Extensive experimental results show that the proposed model outperforms the state-of-the-arts with significant improvements."
1494,https://arxiv.org/abs/1504.01684,Large Margin Nearest Neighbor Embedding for Knowledge Representation,"Traditional way of storing facts in triplets ({\it head\_entity, relation, tail\_entity}), abbreviated as ({\it h, r, t}), makes the knowledge intuitively displayed and easily acquired by mankind, but hardly computed or even reasoned by AI machines. Inspired by the success in applying {\it Distributed Representations} to AI-related fields, recent studies expect to represent each entity and relation with a unique low-dimensional embedding, which is different from the symbolic and atomic framework of displaying knowledge in triplets. In this way, the knowledge computing and reasoning can be essentially facilitated by means of a simple {\it vector calculation}, i.e. ${\bf h} + {\bf r} \approx {\bf t}$. We thus contribute an effective model to learn better embeddings satisfying the formula by pulling the positive tail entities ${\bf t^{+}}$ to get together and close to {\bf h} + {\bf r} ({\it Nearest Neighbor}), and simultaneously pushing the negatives ${\bf t^{-}}$ away from the positives ${\bf t^{+}}$ via keeping a {\it Large Margin}. We also design a corresponding learning algorithm to efficiently find the optimal solution based on {\it Stochastic Gradient Descent} in iterative fashion. Quantitative experiments illustrate that our approach can achieve the state-of-the-art performance, compared with several latest methods on some benchmark datasets for two classical applications, i.e. {\it Link prediction} and {\it Triplet classification}. Moreover, we analyze the parameter complexities among all the evaluated models, and analytical results indicate that our model needs fewer computational resources on outperforming the other methods."
1495,https://arxiv.org/abs/1503.08155,Learning Embedding Representations for Knowledge Inference on Imperfect and Incomplete Repositories,"This paper considers the problem of knowledge inference on large-scale imperfect repositories with incomplete coverage by means of embedding entities and relations at the first attempt. We propose IIKE (Imperfect and Incomplete Knowledge Embedding), a probabilistic model which measures the probability of each belief, i.e. $\langle h,r,t\rangle$, in large-scale knowledge bases such as NELL and Freebase, and our objective is to learn a better low-dimensional vector representation for each entity ($h$ and $t$) and relation ($r$) in the process of minimizing the loss of fitting the corresponding confidence given by machine learning (NELL) or crowdsouring (Freebase), so that we can use $||{\bf h} + {\bf r} - {\bf t}||$ to assess the plausibility of a belief when conducting inference. We use subsets of those inexact knowledge bases to train our model and test the performances of link prediction and triplet classification on ground truth beliefs, respectively. The results of extensive experiments show that IIKE achieves significant improvement compared with the baseline and state-of-the-art approaches."
1496,https://arxiv.org/abs/1411.4455,Errata: Distant Supervision for Relation Extraction with Matrix Completion,"The essence of distantly supervised relation extraction is that it is an incomplete multi-label classification problem with sparse and noisy features. To tackle the sparsity and noise challenges, we propose solving the classification problem using matrix completion on factorized matrix of minimized rank. We formulate relation classification as completing the unknown labels of testing items (entity pairs) in a sparse matrix that concatenates training and testing textual features with training labels. Our algorithmic framework is based on the assumption that the rank of item-by-feature and item-by-label joint matrix is low. We apply two optimization models to recover the underlying low-rank matrix leveraging the sparsity of feature-label matrix. The matrix completion problem is then solved by the fixed point continuation (FPC) algorithm, which can find the global optimum. Experiments on two widely used datasets with different dimensions of textual features demonstrate that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods."
1497,https://arxiv.org/abs/1407.5513,Prime Coset Sum: A Systematic Method for Designing Multi-D Wavelet Filter Banks with Fast Algorithms,"As constructing multi-D wavelets remains a challenging problem, we propose a new method called prime coset sum to construct multi-D wavelets. Our method provides a systematic way to construct multi-D non-separable wavelet filter banks from two 1-D lowpass filters, with one of whom being interpolatory. Our method has many important features including the following: 1) it works for any spatial dimension, and any prime scalar dilation, 2) the vanishing moments of the multi-D wavelet filter banks are guaranteed by certain properties of the initial 1-D lowpass filters, and furthermore, 3) the resulting multi-D wavelet filter banks are associated with fast algorithms that are faster than the existing fast tensor product algorithms."
1498,https://arxiv.org/abs/1111.5092,Coset Sum: an alternative to the tensor product in wavelet construction,"A multivariate biorthogonal wavelet system can be obtained from a pair of multivariate biorthogonal refinement masks in Multiresolution Analysis setup. Some multivariate refinement masks may be decomposed into lower dimensional refinement masks. Tensor product is a popular way to construct a decomposable multivariate refinement mask from lower dimensional refinement masks.
  We present an alternative method, which we call coset sum, for constructing multivariate refinement masks from univariate refinement masks. The coset sum shares many essential features of the tensor product that make it attractive in practice: (1) it preserves the biorthogonality of univariate refinement masks, (2) it preserves the accuracy number of the univariate refinement mask, and (3) the wavelet system associated with it has fast algorithms for computing and inverting the wavelet coefficients. The coset sum can even provide a wavelet system with faster algorithms in certain cases than the tensor product. These features of the coset sum suggest that it is worthwhile to develop and practice alternative methods to the tensor product for constructing multivariate wavelet systems. Some experimental results using 2-D images are presented to illustrate our findings."
1499,https://arxiv.org/abs/2304.14351,Operator growth and black hole formation,"When two particles collide in an asymptotically AdS spacetime with high enough energy and small enough impact parameter, they can form a black hole. Motivated by dual quantum circuit considerations, we propose a threshold condition for black hole formation. Intuitively the condition can be understood as the onset of overlap of the butterfly cones describing the ballistic spread of the effect of the perturbations on the boundary systems. We verify the correctness of the condition in three bulk dimensions. We describe a six-point correlation function that can diagnose this condition and compute it in two-dimensional CFTs using eikonal resummation."
1500,https://arxiv.org/abs/2210.10351,Using deep convolutional neural networks to classify poisonous and edible mushrooms found in China,"Because of their abundance of amino acids, polysaccharides, and many other nutrients that benefit human beings, mushrooms are deservedly popular as dietary cuisine both worldwide and in China. However, if people eat poisonous fungi by mistake, they may suffer from nausea, vomiting, mental disorder, acute anemia, or even death. Each year in China, there are around 8000 people became sick, and 70 died as a result of eating toxic mushrooms by mistake. It is counted that there are thousands of kinds of mushrooms among which only around 900 types are edible, thus without specialized knowledge, the probability of eating toxic mushrooms by mistake is very high. Most people deem that the only characteristic of poisonous mushrooms is a bright colour, however, some kinds of them do not correspond to this trait. In order to prevent people from eating these poisonous mushrooms, we propose to use deep learning methods to indicate whether a mushroom is toxic through analyzing hundreds of edible and toxic mushrooms smartphone pictures. We crowdsource a mushroom image dataset that contains 250 images of poisonous mushrooms and 200 images of edible mushrooms. The Convolutional Neural Network (CNN) is a specialized type of artificial neural networks that use a mathematical operation called convolution in place of general matrix multiplication in at least one of their layers, which can generate a relatively precise result by analyzing a huge amount of images, and thus is very suitable for our research. The experimental results demonstrate that the proposed model has high credibility and can provide a decision-making basis for the selection of edible fungi, so as to reduce the morbidity and mortality caused by eating poisonous mushrooms. We also open source our hand collected mushroom image dataset so that peer researchers can also deploy their own model to advance poisonous mushroom identification."
1501,https://arxiv.org/abs/2209.15223,ASTF: Visual Abstractions of Time-Varying Patterns in Radio Signals,"A time-frequency diagram is a commonly used visualization for observing the time-frequency distribution of radio signals and analyzing their time-varying patterns of communication states in radio monitoring and management. While it excels when performing short-term signal analyses, it becomes inadaptable for long-term signal analyses because it cannot adequately depict signal time-varying patterns in a large time span on a space-limited screen. This research thus presents an abstract signal time-frequency (ASTF) diagram to address this problem. In the diagram design, a visual abstraction method is proposed to visually encode signal communication state changes in time slices. A time segmentation algorithm is proposed to divide a large time span into time slices.Three new quantified metrics and a loss function are defined to ensure the preservation of important time-varying information in the time segmentation. An algorithm performance experiment and a user study are conducted to evaluate the effectiveness of the diagram for long-term signal analyses."
1502,https://arxiv.org/abs/2207.10679,Bouncing inside the horizon and scrambling delays,"We study charged perturbations of the thermofield double state dual to a charged AdS black hole. We model the perturbation by a massless charged shell in the bulk. Unlike the neutral case, all such shells bounce at a definite radius, which can be behind the horizon. We show that the standard ""shock wave"" calculation of a scrambling time indicates that adding charge increases the scrambling time. We then give two arguments using the bounce that suggest that scrambling does not actually take longer when charge is added, but instead its onset is delayed. We also construct a boundary four point function which detects whether the shell bounces inside the black hole."
1503,https://arxiv.org/abs/2205.08964,Skew constacyclic codes over a class of finite commutative semisimple rings,"In this article, we study skew constacyclic codes over a class of finite commutative semisimple rings. The automorphism group of $\mathcal{R}=\prod_{i=1}^t F_q$ is determined, and we characterize skew constacyclic codes over ring by linear codes over finite field. We also define homomorphisms which map linear codes over $\mathcal{R}$ to matrix product codes over $F_q,$ some optimal linear codes over finite fields are obtained."
1504,https://arxiv.org/abs/2204.00079,Sensitive label-free and compact ultrasonic sensor based on double silicon-on-insulator slot micro-ring resonators,"We propose a new label-free ultrasonic sensor, which comprises a slot wave-guide and double silicon-on-insulator (SOI) slot micro-ring resonators. The all-optical sensors do not suffer from electromagnetic interference. We choose to integrate a silicon slot double micro-ring (SDMR) resonators in an acoustically resonant membrane. Optimization of the several key structural parameters is investigated to achieve the mode-field distributions of transmission spectrum based on Comsol Multiphysics software. Our numerical studies show that the proposed ultrasonic sensor offers higher sensitivity and a larger detection frequency range than conventional piezoelectric based ultrasound transducer. For a SDMR system with an area of 15um*30um, sensitivity as high as 2453.7mv/kpa, and over a bandwidth range of 1-150MHz. The sensitivity value is 36 times higher than that of single slot micro-ring ultrasonic sensor. The theoretical-factor of the SDMR can be approximately 1.24*10^6 with bending radius of 5um. The investigation on the SDMR system is a valuable exploration of the photo-acoustic microscopy for the ultra-high Q factor and large frequency range."
1505,https://arxiv.org/abs/2202.04661,Collisions of localized shocks and quantum circuits,"We study collisions between localized shockwaves inside a black hole interior. We give a holographic boundary description of this process in terms of the overlap of two growing perturbations in a shared quantum circuit. The perturbations grow both exponentially as well as ballistically. Due to a competition between different physical effects, the circuit analysis shows dependence on the transverse locations and exhibits four regimes of qualitatively different behaviors. On the gravity side we study properties of the post-collision geometry, using exact calculations in simple setups and estimations in more general circumstances. We show that the circuit analysis offers intuitive and surprisingly accurate predictions about gravity computations involving non-linear features of general relativity."
1506,https://arxiv.org/abs/2201.05301,The Doppler shifts of resonant fluorescence spectrum for a two-level 85Rb atom via multiphoton Compton scattering,"Usually, it's difficult for us to observe the Compton Scattering in an atom. One way to overcome this difficult is using multi-photon collide with an atom, which will come into being multi-photon Compton Scattering (MCS) phenomenon. Thus, we can investigate the MCS process in visible light region. During the MCS process, the cluster atoms moving as a whole, namely atomic Dicke states, the multi-photon interacting with cluster atoms. We can observe a significant Doppler shift of resonant fluorescence spectrum(RFS)in a room-temperature two-levelatomic system. In this paper, we present a detail analysis of the physics mechanism of the Doppler shift and propose a method to measure the component of the Dicke states (the atomic polymers with different masses)by using the Doppler shift of the RFS."
1507,https://arxiv.org/abs/2110.09414,BPPChecker: An SMT-based Model Checker on Basic Parallel Processes(Full Version),"Program verification on concurrent programs is a big challenge due to general undecidable results. Petri nets and its extensions are used in most works. However, existing verifiers based on Petri nets are difficult to be complete and efficient. Basic Parallel Process (BPP), as a subclass of Petri nets, can be used as a model for describing and verifying concurrent programs with lower complexity. We propose and implement BPPChecker, the first model checker for verifying a subclass of CTL on BPP. We propose constraint-based algorithms for the problem of model checking on BPPs and handle formulas by SMT solver Z3. For EF operator, we reduce the model checking of EF-formulas to the satisfiability problem of existential Presburger formula. For EG operator, we provide a k-step bounded semantics and reduce the model checking of EG-formulas to the satisfiability problem of linear integer arithmetic. Besides, we give Actor Communicating System (ACS) the over-approximation BPP-based semantics and evaluate BPPChecker on ACSs generated from real Erlang programs. Experimental results show that BPPChecker performs more efficiently than the existing tools for a series of branching-time property verification problems of Erlang programs."
1508,https://arxiv.org/abs/2109.10894,Evaluating Effects of Background Stories on Graph Perception,"A graph is an abstract model that represents relations among entities, for example, the interactions between characters in a novel. A background story endows entities and relations with real-world meanings and describes the semantics and context of the abstract model, for example, the actual story that the novel presents. Considering practical experience and prior research, human viewers who are familiar with the background story of a graph and those who do not know the background story may perceive the same graph differently. However, no previous research has adequately addressed this problem. This research paper thus presents an evaluation that investigated the effects of background stories on graph perception. Three hypotheses that focused on the role of visual focus areas, graph structure identification, and mental model formation on graph perception were formulated and guided three controlled experiments that evaluated the hypotheses using real-world graphs with background stories. An analysis of the resulting experimental data, which compared the performance of participants who read and did not read the background stories, obtained a set of instructive findings. First, having knowledge about a graph's background story influences participants' focus areas during interactive graph explorations. Second, such knowledge significantly affects one's ability to identify community structures but not high degree and bridge structures. Third, this knowledge influences graph recognition under blurred visual conditions. These findings can bring new considerations to the design of storytelling visualizations and interactive graph explorations."
1509,https://arxiv.org/abs/2109.01091,An Indoor Crowd Movement Trajectory Benchmark Dataset,"In recent years, technologies of indoor crowd positioning and movement data analysis have received widespread attention in the fields of reliability management, indoor navigation, and crowd behavior monitoring. However, only a few indoor crowd movement trajectory datasets are available to the public, thus restricting the development of related research and application. This paper contributes a new benchmark dataset of indoor crowd movement trajectories. This dataset records the movements of over 5000 participants at a three day large academic conference in a two story indoor venue. The conference comprises varied activities, such as academic seminars, business exhibitions, a hacking contest, interviews, tea breaks, and a banquet. The participants are divided into seven types according to participation permission to the activities. Some of them are involved in anomalous events, such as loss of items, unauthorized accesses, and equipment failures, forming a variety of spatial temporal movement patterns. In this paper, we first introduce the scenario design, entity and behavior modeling, and data generator of the dataset. Then, a detailed ground truth of the dataset is presented. Finally, we describe the process and experience of applying the dataset to the contest of ChinaVis Data Challenge 2019. Evaluation results of the 75 contest entries and the feedback from 359 contestants demonstrate that the dataset has satisfactory completeness, and usability, and can effectively identify the performance of methods, technologies, and systems for indoor trajectory analysis."
1510,https://arxiv.org/abs/2105.12755,Six-point functions and collisions in the black hole interior,"In the eternal AdS black hole geometry, we consider two signals sent from the boundaries into the black hole interior shared between the two asymptotic regions. We compute three different out-of-time-order six-point functions to quantify various properties of the collision of these signals behind the horizons: (i) We diagnose the strength of the collision by probing the two-signal state on a late time slice with boundary operators. (ii) We quantify two-sided operator growth, which provides a dual description of the signals meeting in the black hole interior, in terms of the quantum butterfly effect and quantum circuits. (iii) We consider an explicit coupling between the left and right CFTs to make the wormhole traversable and extract information about the collision product from behind the horizon. At a technical level, our results rely on the method of eikonal resummation to obtain the relevant gravitational contributions to Lorentzian six-point functions at all orders in the $G_N$-expansion. We observe that such correlation functions display an intriguing factorization property. We corroborate these results with geodesic computations of six-point functions in two- and three-dimensional gravity."
1511,https://arxiv.org/abs/2104.02736,Diagnosing collisions in the interior of a wormhole,"Two distant black holes can be connected in the interior through a wormhole. Such a wormhole has been interpreted as an entangled state shared between two exterior regions. If Alice and Bob send signals into each of the black holes, they can meet in the interior. In this letter, we interpret this meeting in terms of the quantum circuit that prepares the entangled state: Alice and Bob sending signals creates growing perturbations in the circuit, whose overlap represents their meeting inside the wormhole. We argue that such overlap in the circuit is quantified by a particular six-point correlation function. Therefore, exterior observers in possession of the entangled qubits can use this correlation function to diagnose the collision in the interior without having to jump in themselves."
1512,https://arxiv.org/abs/2103.12472,Non-intrusive reduced order modeling of parametric electromagnetic scattering problems through Gaussian process regression,"This paper is concerned with the design of a non-intrusive model order reduction (MOR) for the system of parametric time-domain Maxwell equations. A time- and parameter-independent reduced basis (RB) is constructed by using a two-step proper orthogonal decomposition (POD) technique from a collection of full-order electromagnetic field solutions, which are generated via a discontinuous Galerkin time-domain (DGTD) solver. The mapping between the time/parameter values and the projection coefficients onto the RB space is approximated by a Gaussian process regression (GPR). Based on the data characteristics of electromagnetic field solutions, the singular value decomposition (SVD) is applied to extract the principal components of the training data of each projection coefficient, and the GPR models are trained for time- and parameter-modes respectively, by which the final global regression function can be represented as a linear combination of these time- and parameter-Gaussian processes. The extraction of the RB and the training of GPR surrogate models are both completed in the offline stage. Then the field solution at any new input time/parameter point can be directly recovered in the online stage as a linear combination of the RB with the regression outputs as the coefficients. In virtue of its non-intrusive nature, the proposed POD-GPR framework, which is equation-free, decouples the offline and online stages completely, and hence can predict the electromagnetic solution fields at unseen parameter locations quickly and effectively. The performance of our method is illustrated by a scattering problem of a multi-layer dielectric cylinder."
1513,https://arxiv.org/abs/2102.05697,Size and momentum of an infalling particle in the black hole interior,"The future interior of black holes in AdS/CFT can be described in terms of a quantum circuit. We investigate boundary quantities detecting properties of this quantum circuit. We discuss relations between operator size, quantum complexity, and the momentum of an infalling particle in the black hole interior. We argue that the trajectory of the infalling particle in the interior close to the horizon is related to the growth of operator size. The notion of size here differs slightly from the size which has previously been related to momentum of exterior particles and provides an interesting generalization. The fact that both exterior and interior momentum are related to operator size growth is a manifestation of complementarity."
1514,https://arxiv.org/abs/2011.06016,Collision in the interior of wormhole,"The Schwarzschild wormhole has been interpreted as an entangled state. If Alice and Bob fall into each of the black hole, they can meet in the interior. We interpret this meeting in terms of the quantum circuit that prepares the entangled state. Alice and Bob create growing perturbations in the circuit, and we argue that the overlap of these perturbations represents their meeting. We compare the gravity picture with circuit analysis, and identify the post-collision region as the region storing the gates that are not affected by any of the perturbations."
1515,https://arxiv.org/abs/2010.13062,Transgender Community Sentiment Analysis from Social Media Data: A Natural Language Processing Approach,"Transgender community is experiencing a huge disparity in mental health conditions compared with the general population. Interpreting the social medial data posted by transgender people may help us understand the sentiments of these sexual minority groups better and apply early interventions. In this study, we manually categorize 300 social media comments posted by transgender people to the sentiment of negative, positive, and neutral. 5 machine learning algorithms and 2 deep neural networks are adopted to build sentiment analysis classifiers based on the annotated data. Results show that our annotations are reliable with a high Cohen's Kappa score over 0.8 across all three classes. LSTM model yields an optimal performance of accuracy over 0.85 and AUC of 0.876. Our next step will focus on using advanced natural language processing algorithms on a larger annotated dataset."
1516,https://arxiv.org/abs/2009.08093,An early prediction of covid-19 associated hospitalization surge using deep learning approach,"The global pandemic caused by COVID-19 affects our lives in all aspects. As of September 11, more than 28 million people have tested positive for COVID-19 infection, and more than 911,000 people have lost their lives in this virus battle. Some patients can not receive appropriate medical treatment due the limits of hospitalization volume and shortage of ICU beds. An estimated future hospitalization is critical so that medical resources can be allocated as needed. In this study, we propose to use 4 recurrent neural networks to infer hospitalization change for the following week compared with the current week. Results show that sequence to sequence model with attention achieves a high accuracy of 0.938 and AUC of 0.850 in the hospitalization prediction. Our work has the potential to predict the hospitalization need and send a warning to medical providers and other stakeholders when a re-surge initializes."
1517,https://arxiv.org/abs/2009.02651,SilkViser:A Visual Explorer of Blockchain-based Cryptocurrency Transaction Data,"Many blockchain-based cryptocurrencies provide users with online blockchain explorers for viewing online transaction data. However, traditional blockchain explorers mostly present transaction information in textual and tabular forms. Such forms make understanding cryptocurrency transaction mechanisms difficult for novice users (NUsers). They are also insufficiently informative for experienced users (EUsers) to recognize advanced transaction information. This study introduces a new online cryptocurrency transaction data viewing tool called SilkViser. Guided by detailed scenario and requirement analyses, we create a series of appreciating visualization designs, such as paper ledger-inspired block and blockchain visualizations and ancient copper coin-inspired transaction visualizations, to help users understand cryptocurrency transaction mechanisms and recognize advanced transaction information. We also provide a set of lightweight interactions to facilitate easy and free data exploration. Moreover, a controlled user study is conducted to quantitatively evaluate the usability and effectiveness of SilkViser. Results indicate that SilkViser can satisfy the requirements of NUsers and EUsers. Our visualization designs can compensate for the inexperience of NUsers in data viewing and attract potential users to participate in cryptocurrency transactions."
1518,https://arxiv.org/abs/2009.02498,Preserving Minority Structures in Graph Sampling,"Sampling is a widely used graph reduction technique to accelerate graph computations and simplify graph visualizations. By comprehensively analyzing the literature on graph sampling, we assume that existing algorithms cannot effectively preserve minority structures that are rare and small in a graph but are very important in graph analysis. In this work, we initially conduct a pilot user study to investigate representative minority structures that are most appealing to human viewers. We then perform an experimental study to evaluate the performance of existing graph sampling algorithms regarding minority structure preservation. Results confirm our assumption and suggest key points for designing a new graph sampling approach named mino-centric graph sampling (MCGS). In this approach, a triangle-based algorithm and a cut-point-based algorithm are proposed to efficiently identify minority structures. A set of importance assessment criteria are designed to guide the preservation of important minority structures. Three optimization objectives are introduced into a greedy strategy to balance the preservation between minority and majority structures and suppress the generation of new minority structures. A series of experiments and case studies are conducted to evaluate the effectiveness of the proposed MCGS."
1519,https://arxiv.org/abs/2009.02491,Reverse-engineering Bar Charts Using Neural Networks,"Reverse-engineering bar charts extracts textual and numeric information from the visual representations of bar charts to support application scenarios that require the underlying information. In this paper, we propose a neural network-based method for reverse-engineering bar charts. We adopt a neural network-based object detection model to simultaneously localize and classify textual information. This approach improves the efficiency of textual information extraction. We design an encoder-decoder framework that integrates convolutional and recurrent neural networks to extract numeric information. We further introduce an attention mechanism into the framework to achieve high accuracy and robustness. Synthetic and real-world datasets are used to evaluate the effectiveness of the method. To the best of our knowledge, this work takes the lead in constructing a complete neural network-based method of reverse-engineering bar charts."
1520,https://arxiv.org/abs/2008.01677,Simultaneous Semantic Alignment Network for Heterogeneous Domain Adaptation,"Heterogeneous domain adaptation (HDA) transfers knowledge across source and target domains that present heterogeneities e.g., distinct domain distributions and difference in feature type or dimension. Most previous HDA methods tackle this problem through learning a domain-invariant feature subspace to reduce the discrepancy between domains. However, the intrinsic semantic properties contained in data are under-explored in such alignment strategy, which is also indispensable to achieve promising adaptability. In this paper, we propose a Simultaneous Semantic Alignment Network (SSAN) to simultaneously exploit correlations among categories and align the centroids for each category across domains. In particular, we propose an implicit semantic correlation loss to transfer the correlation knowledge of source categorical prediction distributions to target domain. Meanwhile, by leveraging target pseudo-labels, a robust triplet-centroid alignment mechanism is explicitly applied to align feature representations for each category. Notably, a pseudo-label refinement procedure with geometric similarity involved is introduced to enhance the target pseudo-label assignment accuracy. Comprehensive experiments on various HDA tasks across text-to-image, image-to-image and text-to-text successfully validate the superiority of our SSAN against state-of-the-art HDA methods. The code is publicly available at https://github.com/BIT-DA/SSAN."
1521,https://arxiv.org/abs/2006.03019,Complexity and Momentum,"Previous work has explored the connections between three concepts -- operator size, complexity, and the bulk radial momentum of an infalling object -- in the context of JT gravity and the SYK model. In this paper we investigate the higher dimensional generalizations of these connections. We use a toy model to study the growth of an operator when perturbing the vacuum of a CFT. From circuit analysis we relate the operator growth to the rate of increase of complexity and check it by complexity-volume duality. We further give an empirical formula relating complexity and the bulk radial momentum that works from the time that the perturbation just comes in from the cutoff boundary, to after the scrambling time."
1522,https://arxiv.org/abs/2003.03406,Petz map and Python's lunch,"We look at the interior operator reconstruction from the point of view of Petz map and study its complexity. We show that Petz maps can be written as precursors under the condition of perfect recovery. When we have the entire boundary system its complexity is related to the volume / action of the wormhole from the bulk operator to the boundary. When we only have access to part of the system, Python's lunch appears and its restricted complexity depends exponentially on the size of the subsystem one loses access to."
1523,https://arxiv.org/abs/1912.00909,A quantum circuit interpretation of evaporating black hole geometry,"We give a quantum circuit interpretation of evaporating black hole geometry. We make an analogy between the appearance of island for evaporating black hole and the transition from two-sided to one-sided black hole in the familiar example of perturbed thermofield double. If Alice perturbs thermofield double and waits for scrambling time, she will have a one-sided black hole with interior of her own. We argue that by similar mechanism the radiation gets access to the interior (island forms) after Page time. The growth of the island happens as a result of the constant transitions from two-sided to one-sided black holes."
1524,https://arxiv.org/abs/1912.00742,Pythia: AI-assisted Code Completion System,"In this paper, we propose a novel end-to-end approach for AI-assisted code completion called Pythia. It generates ranked lists of method and API recommendations which can be used by software developers at edit time. The system is currently deployed as part of Intellicode extension in Visual Studio Code IDE. Pythia exploits state-of-the-art large-scale deep learning models trained on code contexts extracted from abstract syntax trees. It is designed to work at a high throughput predicting the best matching code completions on the order of 100 $ms$.
  We describe the architecture of the system, perform comparisons to frequency-based approach and invocation-based Markov Chain language model, and discuss challenges serving Pythia models on lightweight client devices.
  The offline evaluation results obtained on 2700 Python open source software GitHub repositories show a top-5 accuracy of 92\%, surpassing the baseline models by 20\% averaged over classes, for both intra and cross-project settings."
1525,https://arxiv.org/abs/1910.04364,Network Entropy based on Cluster Expansion on Motifs for Undirected Graphs,"The structure of the network can be described by motifs, which are subgraphs that often repeat themselves. In order to understand the structure of network motifs, it is of great importance to study subgraphs from the perspective of statistical mechanics. In this paper, we use clustering extensions in statistical physics to solve the problem of using motifs as network primitives. By projecting the network motifs to clusters in the gas model, we develop the partition function of the network, which enables us to calculate global thermodynamic quantities, such as energy, entropy, and vice versa. Then, we give the analytic expressions of the number of specific types of motifs and calculate their correlated entropy. We conduct algebraic experiments on datasets, both synthetic and in real life, and evaluate the qualitative and quantitative characterization of motif entropy derived from the partition function. Our findings show that the motif entropy of networks in real life, for instance, financial and stock market networks, is of high correlation to the change of network structure. Hence, our findings are consistent with recent studies about the similar topic that network motifs can be represented as basic elements of well-defined information processing functions."
1526,https://arxiv.org/abs/1910.02544,Using Deep Learning and Machine Learning to Detect Epileptic Seizure with Electroencephalography (EEG) Data,"The prediction of epileptic seizure has always been extremely challenging in medical domain. However, as the development of computer technology, the application of machine learning introduced new ideas for seizure forecasting. Applying machine learning model onto the predication of epileptic seizure could help us obtain a better result and there have been plenty of scientists who have been doing such works so that there are sufficient medical data provided for researchers to do training of machine learning models."
1527,https://arxiv.org/abs/1908.10996,The Page curve of Hawking radiation from semiclassical geometry,"We consider a gravity theory coupled to matter, where the matter has a higher-dimensional holographic dual. In such a theory, finding quantum extremal surfaces becomes equivalent to finding the RT/HRT surfaces in the higher-dimensional theory. Using this we compute the entropy of Hawking radiation and argue that it follows the Page curve, as suggested by recent computations of the entropy and entanglement wedges for old black holes. The higher-dimensional geometry connects the radiation to the black hole interior in the spirit of ER=EPR. The black hole interior then becomes part of the entanglement wedge of the radiation. Inspired by this, we propose a new rule for computing the entropy of quantum systems entangled with gravitational systems which involves searching for ""islands"" in determining the entanglement wedge."
1528,https://arxiv.org/abs/1908.02005,RSATree: Distribution-Aware Data Representation of Large-Scale Tabular Datasets for Flexible Visual Query,"Analysts commonly investigate the data distributions derived from statistical aggregations of data that are represented by charts, such as histograms and binned scatterplots, to visualize and analyze a large-scale dataset. Aggregate queries are implicitly executed through such a process. Datasets are constantly extremely large; thus, the response time should be accelerated by calculating predefined data cubes. However, the queries are limited to the predefined binning schema of preprocessed data cubes. Such limitation hinders analysts' flexible adjustment of visual specifications to investigate the implicit patterns in the data effectively. Particularly, RSATree enables arbitrary queries and flexible binning strategies by leveraging three schemes, namely, an R-tree-based space partitioning scheme to catch the data distribution, a locality-sensitive hashing technique to achieve locality-preserving random access to data items, and a summed area table scheme to support interactive query of aggregated values with a linear computational complexity. This study presents and implements a web-based visual query system that supports visual specification, query, and exploration of large-scale tabular data with user-adjustable granularities. We demonstrate the efficiency and utility of our approach by performing various experiments on real-world datasets and analyzing time and space complexity."
1529,https://arxiv.org/abs/1908.00403,Evaluating Perceptual Bias During Geometric Scaling of Scatterplots,"Scatterplots are frequently scaled to fit display areas in multi-view and multi-device data analysis environments. A common method used for scaling is to enlarge or shrink the entire scatterplot together with the inside points synchronously and proportionally. This process is called geometric scaling. However, geometric scaling of scatterplots may cause a perceptual bias, that is, the perceived and physical values of visual features may be dissociated with respect to geometric scaling. For example, if a scatterplot is projected from a laptop to a large projector screen, then observers may feel that the scatterplot shown on the projector has fewer points than that viewed on the laptop. This paper presents an evaluation study on the perceptual bias of visual features in scatterplots caused by geometric scaling. The study focuses on three fundamental visual features (i.e., numerosity, correlation, and cluster separation) and three hypotheses that are formulated on the basis of our experience. We carefully design three controlled experiments by using well-prepared synthetic data and recruit participants to complete the experiments on the basis of their subjective experience. With a detailed analysis of the experimental results, we obtain a set of instructive findings. First, geometric scaling causes a bias that has a linear relationship with the scale ratio. Second, no significant difference exists between the biases measured from normally and uniformly distributed scatterplots. Third, changing the point radius can correct the bias to a certain extent. These findings can be used to inspire the design decisions of scatterplots in various scenarios."
1530,https://arxiv.org/abs/1904.12820,Symmetries Near the Horizon,"We consider a nearly-AdS$_2$ gravity theory on the two-sided wormhole geometry. We construct three gauge-invariant operators in NAdS which move bulk matter relative to the dynamical boundaries. In a two-sided system, these operators satisfy an SL(2) algebra (up to non-perturbative corrections). In a semiclassical limit, these generators act like SL(2) transformations of the boundary time, or conformal symmetries of the two sided boundary theory. These can be used to define an operator-state mapping. A particular large N and low temperature limit of the SYK model has precisely the same structure, and this construction of the exact generators also applies. We also discuss approximate, but simpler, constructions of the generators in the SYK model. These are closely related to the ""size"" operator and are connected to the maximal chaos behavior captured by out of time order correlators."
1531,https://arxiv.org/abs/1810.08741,The Case of the Missing Gates: Complexity of Jackiw-Teitelboim Gravity,"The Jackiw-Teitelboim (JT) model arises from the dimensional reduction of charged black holes. Motivated by the holographic complexity conjecture, we calculate the late-time rate of change of action of a Wheeler-DeWitt patch in the JT theory. Surprisingly, the rate vanishes. This is puzzling because it contradicts both holographic expectations for the rate of complexification and also action calculations for charged black holes. We trace the discrepancy to an improper treatment of boundary terms when naively doing the dimensional reduction. Once the boundary term is corrected, we find exact agreement with expectations. We comment on the general lessons that this might hold for holographic complexity and beyond."
1532,https://arxiv.org/abs/1810.02282,Averaging principle for two dimensional stochastic Navier-Stokes equations,"The averaging principle is established for the slow component and the fast component being two dimensional stochastic Navier-Stokes equations and stochastic reaction-diffusion equations, respectively. The classical Khasminskii approach based on time discretization is used for the proof of the slow component strong convergence to the solution of the corresponding averaged equation under some suitable conditions. Meanwhile, some powerful techniques are used to overcome the difficulties caused by the nonlinear term and to release the regularity of the initial value."
1533,https://arxiv.org/abs/1806.06185,EdgeChain: An Edge-IoT Framework and Prototype Based on Blockchain and Smart Contracts,"The emerging Internet of Things (IoT) is facing significant scalability and security challenges. On the one hand, IoT devices are ""weak"" and need external assistance. Edge computing provides a promising direction addressing the deficiency of centralized cloud computing in scaling massive number of devices. On the other hand, IoT devices are also relatively ""vulnerable"" facing malicious hackers due to resource constraints. The emerging blockchain and smart contracts technologies bring a series of new security features for IoT and edge computing. In this paper, to address the challenges, we design and prototype an edge-IoT framework named ""EdgeChain"" based on blockchain and smart contracts. The core idea is to integrate a permissioned blockchain and the internal currency or ""coin"" system to link the edge cloud resource pool with each IoT device' account and resource usage, and hence behavior of the IoT devices. EdgeChain uses a credit-based resource management system to control how much resource IoT devices can obtain from edge servers, based on pre-defined rules on priority, application types and past behaviors. Smart contracts are used to enforce the rules and policies to regulate the IoT device behavior in a non-deniable and automated manner. All the IoT activities and transactions are recorded into blockchain for secure data logging and auditing. We implement an EdgeChain prototype and conduct extensive experiments to evaluate the ideas. The results show that while gaining the security benefits of blockchain and smart contracts, the cost of integrating them into EdgeChain is within a reasonable and acceptable range."
1534,https://arxiv.org/abs/1804.04156,Falling Toward Charged Black Holes,"The growth of the ""size"" of operators is an important diagnostic of quantum chaos. In arXiv:1802.01198 [hep-th] it was conjectured that the holographic dual of the size is proportional to the average radial component of the momentum of the particle created by the operator. Thus the growth of operators in the background of a black hole corresponds to the acceleration of the particle as it falls toward the horizon. In this note we will use the momentum-size correspondence as a tool to study scrambling in the field of a near-extremal charged black hole. The agreement with previous work provides a non-trivial test of the momentum-size relation, as well as an explanation of a paradoxical feature of scrambling previously discovered by Leichenauer [arXiv:1405.7365 [hep-th]]. Naively Leichenauer's result says that only the non-extremal entropy participates in scrambling. The same feature is also present in the SYK model. In this paper we find a quite different interpretation of Leichenauer's result which does not have to do with any decoupling of the extremal degrees of freedom. Instead it has to do with the buildup of momentum as a particle accelerates through the long throat of the Reissner-Nordstrom geometry."
1535,https://arxiv.org/abs/1711.03125,Uncomplexity and Black Hole Geometry,"We give a definition of uncomplexity of a mixed state without invoking any particular definitions of mixed state complexity, and argue that it gives the amount of computational power Bob has when he only has access to part of a system. We find geometric meanings of our definition in various black hole examples, and make a connection with subregion duality. We show that Bob's uncomplexity is the portion of his accessible interior spacetime inside his entanglement wedge. This solves a puzzle we encountered about the uncomplexity of thermofield double state. In this process, we identify different kinds of operations Bob can do as being responsible for the growth of different parts of spacetime."
1536,https://arxiv.org/abs/1707.04354,Teleportation Through the Wormhole,"ER=EPR allows us to think of quantum teleportation as communication of quantum information through space-time wormholes connecting entangled systems. The conditions for teleportation render the wormhole traversable so that a quantum system entering one end of the ERB will, after a suitable time, appear at the other end. Teleportation requires the transfer of classical information outside the horizon, but the classical bit-string carries no information about the teleported system; the teleported system passes through the ERB leaving no trace outside the horizon. In general the teleported system will retain a memory of what it encountered in the wormhole. This phenomenon could be observable in a laboratory equipped with quantum computers."
1537,https://arxiv.org/abs/1705.05068,Commissioning of te China-ADS injector-I testing facility,"The 10 MeV accelerator-driven subcritical system (ADS) Injector-I test stand at Institute of High Energy Physics (IHEP) is a testing facility dedicated to demonstrate one of the two injector design schemes [Injector Scheme-I, which works at 325 MHz], for the ADS project in China. The Injector adopted a four vane copper structure RFQ with output energy of 3.2 MeV and a superconducting (SC) section accommodating fourteen \b{eta}g=0.12 single spoke cavities, fourteen SC solenoids and fourteen cold BPMs. The ion source was installed since April of 2014, periods of commissioning are regularly scheduled between installation phases of the rest of the injector. Continuous wave (CW) beam was shooting through the injector and 10 MeV CW proton beam with average beam current around 2 mA was obtained recently. This contribution describe the results achieved so far and the difficulties encountered in CW commissioning."
1538,https://arxiv.org/abs/1702.03957,Complexity and Boost Symmetry,"We find that the time dependence of holographic complexity is controlled by the Rindler boost symmetry across the horizon. By studying the collision energy experienced by an infalling object, we see the breaking of this boost symmetry is closely related to firewalls, which in turn shows the connection between the time dependence of complexity and firewalls. We further identify the black and white hole interiors as two tapes storing different parts of the minimal circuit preparing the state. Depending on whether the quantum gates are being laid on the tape at a particular moment, each tape can be in two states: working, or locked. We interpret the existence of firewalls as the locking of tapes."
1539,https://arxiv.org/abs/1608.02612,Quantum Complexity and Negative Curvature,"As time passes, once simple quantum states tend to become more complex. For strongly coupled k-local Hamiltonians, this growth of computational complexity has been conjectured to follow a distinctive and universal pattern. In this paper we show that the same pattern is exhibited by a much simpler system: classical geodesics on a compact two-dimensional geometry of uniform negative curvature. This striking parallel persists whether the system is allowed to evolve naturally or is perturbed from the outside."
1540,https://arxiv.org/abs/1605.04644,Abnormal Subspace Sparse PCA for Anomaly Detection and Interpretation,"The main shortage of principle component analysis (PCA) based anomaly detection models is their interpretability. In this paper, our goal is to propose an interpretable PCA-based model for anomaly detection and interpretation. The propose ASPCA model constructs principal components with sparse and orthogonal loading vectors to represent the abnormal subspace, and uses them to interpret detected anomalies. Our experiments on a synthetic dataset and two real world datasets showed that the proposed ASPCA models achieved comparable detection accuracies as the PCA model, and can provide interpretations for individual anomalies."
1541,https://arxiv.org/abs/1604.00662,Hierarchically porous Ni monolith@branch-structured NiCo2O4 for high energy density supercapacitors,"NiCo2O4 of varying nanostrucutures ranging from nanowires, nanoplates to nano-plates@nanowires were successfully grown on microporous (MP) Ni foams via one-step hydrothermal process. The investigation of electrochemical capacitance favors Ni-Co2O4 of nanoplates@nanowires microstructures which possesses specific capacitance of 1380.3 F/g and 1033F/g at 5A/g and 50A/g respectively and 86.7% capacitance re-tention after 5000 cycles at 30A/g. The relationship between morphology and specific capacitance was further explored by the model of surface roughness factor (RF), which is indicative of the active electrode-electrolyte interface areas. The RF of porous Ni@NiCo2O4 was remarkably improved by employing hierarchically porous (HP) Ni monoliths as substrates, which illustrates the model of high energy density (12.6 F/cm2) electrodes for super-capacitors."
1542,https://arxiv.org/abs/1512.04993,"Complexity, action, and black holes","Our earlier paper ""Complexity Equals Action"" conjectured that the quantum computational complexity of a holographic state is given by the classical action of a region in the bulk (the ""Wheeler-DeWitt"" patch). We provide calculations for the results quoted in that paper, explain how it fits into a broader (tensor) network of ideas, and elaborate on the hypothesis that black holes are the fastest computers in nature."
1543,https://arxiv.org/abs/1511.06240,Phase-field study of electrochemical reactions at exterior and interior interfaces in Li-Ion battery electrode particles,"To study the electrochemical reaction on surfaces, phase interfaces, and crack surfaces in the lithium ion battery electrode particles, a phase-field model is developed, which describes fracture in large strains and anisotropic Cahn-Hilliard-Reaction. Thereby the concentration-dependency of the elastic properties and the anisotropy of diffusivity are also considered. The implementation in 3D is carried out by isogeometric finite element methods in order to treat the high order terms in a straightforward sense. The electrochemical reaction is modeled through a modified Butler-Volmer equation to account for the influence of the phase change on the reaction on exterior surfaces. The reaction on the crack surfaces is considered through a volume source term weighted by a term related to the fracture order parameter. Based on the model, three characteristic examples are considered to reveal the electrochemical reactions on particle surfaces, phase interfaces, and crack surfaces, as well as their influence on the particle material behavior. Results show that the ratio between the timescale of reaction and the diffusion can have a significant influence on phase segregation behavior, as well as the anisotropy of diffusivity. In turn, the distribution of the lithium concentration greatly influences the reaction on the surface, especially when the phase interfaces appear on exterior surfaces or crack surfaces. The reaction rate increases considerably at phase interfaces, due to the large lithium concentration gradient. Moreover, simulations demonstrate that the segregation of a Li-rich and a Li-poor phase during delithiation can drive the cracks to propagate. Results indicate that the model can capture the electrochemical reaction on the freshly cracked surfaces."
1544,https://arxiv.org/abs/1509.07876,Complexity Equals Action,"We conjecture that the quantum complexity of a holographic state is dual to the action of a certain spacetime region that we call a Wheeler-DeWitt patch. We illustrate and test the conjecture in the context of neutral, charged, and rotating black holes in AdS, as well as black holes perturbed with static shells and with shock waves. This conjecture evolved from a previous conjecture that complexity is dual to spatial volume, but appears to be a major improvement over the original. In light of our results, we discuss the hypothesis that black holes are the fastest computers in nature."
1545,https://arxiv.org/abs/1506.02433,Steady State of Pedestrian Flow in Bottleneck Experiments,"Experiments with pedestrians could depend strongly on initial conditions. Comparisons of the results of such experiments require to distinguish carefully between transient state and steady state. In this work, a feasible algorithm - Cumulative Sum Control Chart - is proposed and improved to automatically detect steady states from density and speed time series of bottleneck experiments. The threshold of the detection parameter in the algorithm is calibrated using an autoregressive model. Comparing the detected steady states with previous manually selected ones, the modified algorithm gives more reproducible results. For the applications, three groups of bottleneck experiments are analysed and the steady states are detected. The study about pedestrian flow shows that the difference between the flows in all states and in steady state mainly depends on the ratio of pedestrian number to bottleneck width. When the ratio is higher than a critical value (approximately 115 persons/m), the flow in all states is almost identical with the flow in steady state. Thus we have more possibilities to compare the flows from different experiments, especially when the detection of steady states is difficult."
1546,https://arxiv.org/abs/1411.0468,Fano-Josephson effect of Majorana bound states,"We investigate the Josephson current in a Fano-Josephson junction formed by the direct coupling between two topological superconducting wires and their indirect coupling via a quantum dot. It is found that when two Majorana zero modes respectively appear in the wires, the Fano interference causes abundant Josephson phase transition processes. What is notable is that in the presence of appropriate direct and indirect inter-wire couplings, the fractional Josephson effect disappears and then such a structure transforms into a $0$-phase normal Josephson junction. On the other hand, if finite coupling occurs between the Majorana bound states at the ends of each wire, the normal Josepshon current is robustly in the $0$ phase, weakly dependent on the Fano effect. We believe that the results in this work are helpful for describing the Fano-modified Josephson effect."
1547,https://arxiv.org/abs/1410.7185,Novel solid-state glycine-nitrate combustion for controllable synthesis of hierarchically porous Ni monolith,"We demonstrate a novel solid-state glycine-nitrate route for not only the scalable combustion synthesis of hierarchically porous Ni monolith, but also control over impurities, microstructure topography and size. The as-synthesized porous Ni monolith may find instant applications as electrode current collectors, catalyst and catalyst substrates or sensors."
1548,https://arxiv.org/abs/1408.2823,Switchbacks and the Bridge to Nowhere,"This paper is in three parts: Part 1 explains the relevance of Einstein-Rosen bridges for one-sided black holes. Like their two-sided counterparts, one-sided black holes are connected to ERBs whose growth tracks the increasing complexity of the quantum state. Quantitative solutions for one-sided ERBs are presented in the appendix.
  Part 2 calls attention to the work of Nielsen and collaborators on the geometry of quantum complexity. This geometric formulation of complexity provides a valuable tool for studying the evolution of complexity for systems such as black holes.
  Part 3 applies the Nielsen approach to geometrize two related black hole quantum phenomena: the rapid mixing of information through fast-scrambling; and the time dependence of the complexity of precursors, in particular the switchback effect."
1549,https://arxiv.org/abs/2305.17098,ControlVideo: Adding Conditional Control for One Shot Text-to-Video Editing,"In this paper, we present ControlVideo, a novel method for text-driven video editing. Leveraging the capabilities of text-to-image diffusion models and ControlNet, ControlVideo aims to enhance the fidelity and temporal consistency of videos that align with a given text while preserving the structure of the source video. This is achieved by incorporating additional conditions such as edge maps, fine-tuning the key-frame and temporal attention on the source video-text pair with carefully designed strategies. An in-depth exploration of ControlVideo's design is conducted to inform future research on one-shot tuning video diffusion models. Quantitatively, ControlVideo outperforms a range of competitive baselines in terms of faithfulness and consistency while still aligning with the textual prompt. Additionally, it delivers videos with high visual realism and fidelity w.r.t. the source content, demonstrating flexibility in utilizing controls containing varying degrees of source video information, and the potential for multiple control combinations. The project page is available at \href{https://ml.cs.tsinghua.edu.cn/controlvideo/}{https://ml.cs.tsinghua.edu.cn/controlvideo/}."
1550,https://arxiv.org/abs/2305.16213,ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation,"Score distillation sampling (SDS) has shown great promise in text-to-3D generation by distilling pretrained large-scale text-to-image diffusion models, but suffers from over-saturation, over-smoothing, and low-diversity problems. In this work, we propose to model the 3D parameter as a random variable instead of a constant as in SDS and present variational score distillation (VSD), a principled particle-based variational framework to explain and address the aforementioned issues in text-to-3D generation. We show that SDS is a special case of VSD and leads to poor samples with both small and large CFG weights. In comparison, VSD works well with various CFG weights as ancestral sampling from diffusion models and simultaneously improves the diversity and sample quality with a common CFG weight (i.e., $7.5$). We further present various improvements in the design space for text-to-3D such as distillation time schedule and density initialization, which are orthogonal to the distillation algorithm yet not well explored. Our overall approach, dubbed ProlificDreamer, can generate high rendering resolution (i.e., $512\times512$) and high-fidelity NeRF with rich structure and complex effects (e.g., smoke and drops). Further, initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and photo-realistic. Project page: https://ml.cs.tsinghua.edu.cn/prolificdreamer/"
1551,https://arxiv.org/abs/2305.15241,Robust Classification via a Single Diffusion Model,"Recently, diffusion models have been successfully applied to improving adversarial robustness of image classifiers by purifying the adversarial noises or generating realistic data for adversarial training. However, the diffusion-based purification can be evaded by stronger adaptive attacks while adversarial training does not perform well under unseen threats, exhibiting inevitable limitations of these methods. To better harness the expressive power of diffusion models, in this paper we propose Robust Diffusion Classifier (RDC), a generative classifier that is constructed from a pre-trained diffusion model to be adversarially robust. Our method first maximizes the data likelihood of a given input and then predicts the class probabilities of the optimized input using the conditional likelihood of the diffusion model through Bayes' theorem. Since our method does not require training on particular adversarial attacks, we demonstrate that it is more generalizable to defend against multiple unseen threats. In particular, RDC achieves $73.24\%$ robust accuracy against $\ell_\infty$ norm-bounded perturbations with $ε_\infty=8/255$ on CIFAR-10, surpassing the previous state-of-the-art adversarial training models by $+2.34\%$. The findings highlight the potential of generative classifiers by employing diffusion models for adversarial robustness compared with the commonly studied discriminative classifiers."
1552,https://arxiv.org/abs/2305.13746,Preferential bond formation and interstitial/vacancy annihilation rate drive atomic clustering in gallium ion sputtered compound materials,"The investigation of chemical reactions during the ion irradiation is a frontier for the study of the ion-material interaction. In order to derive the contribution of bond formation to chemistry of ion produced nanoclusters, the valence electron energy loss spectroscopy (VEELS) was exploited to investigate the Ga$^+$ ion damage in Al$_2$O$_3$, InP and InGaAs, where each target material has been shown to yield different process for altering the clustering of recoil atoms: metallic Ga, metallic In and InGaP clusters in Al$_2$O$_3$, InP and InGaAs respectively. Supporting simulations based on Monte Carlo and crystal orbital Hamiltonianindicate that the chemical constitution of cascade induced nano-precipitates is a result of a competition between interstitial/vacancy consumption rate and preferential bond formation."
1553,https://arxiv.org/abs/2305.04888,Phase diagram of the $ν= 2$ quantum Hall state in bilayer graphene,"Bilayer graphene exhibits a rich phase diagram in the quantum Hall regime, arising from a multitude of internal degrees of freedom, including spin, valley, and orbital indices. The variety of fractional quantum Hall states between filling factors $1 < ν\leq 2$ suggests, among other things, a quantum phase transition between valley-unpolarized and polarized states at a perpendicular electric field $D^{*}$. We find the behavior of $D^{*}$ with $ν$ changes markedly as $B$ is reduced. At $ν= 2$, $D^{*}$ may even vanish when $B$ is sufficiently small. We present a theoretical model for lattice-scale interactions which explains these observations; surprisingly, both repulsive and attractive components in the interactions are required. Within this model we analyze the nature of the $ν= 2$ state as a function of the magnetic and electric fields, and predict that valley-coherence may emerge for $D \sim D^{*}$ in the high $B$ regime. This suggests the system supports Kekule bond-ordering, which could in principle be verified via STM measurements."
1554,https://arxiv.org/abs/2304.10263,PREIM3D: 3D Consistent Precise Image Attribute Editing from a Single Image,"We study the 3D-aware image attribute editing problem in this paper, which has wide applications in practice. Recent methods solved the problem by training a shared encoder to map images into a 3D generator's latent space or by per-image latent code optimization and then edited images in the latent space. Despite their promising results near the input view, they still suffer from the 3D inconsistency of produced images at large camera poses and imprecise image attribute editing, like affecting unspecified attributes during editing. For more efficient image inversion, we train a shared encoder for all images. To alleviate 3D inconsistency at large camera poses, we propose two novel methods, an alternating training scheme and a multi-view identity loss, to maintain 3D consistency and subject identity. As for imprecise image editing, we attribute the problem to the gap between the latent space of real images and that of generated images. We compare the latent space and inversion manifold of GAN models and demonstrate that editing in the inversion manifold can achieve better results in both quantitative and qualitative evaluations. Extensive experiments show that our method produces more 3D consistent images and achieves more precise image editing than previous work. Source code and pretrained models can be found on our project page: https://mybabyyh.github.io/Preim3D/"
1555,https://arxiv.org/abs/2304.10127,Learning Sample Difficulty from Pre-trained Models for Reliable Prediction,"Large-scale pre-trained models have achieved remarkable success in a variety of scenarios and applications, but how to leverage them to improve the prediction reliability of downstream models is undesirably under-explored. Moreover, modern neural networks have been found to be poorly calibrated and make overconfident predictions regardless of inherent sample difficulty and data uncertainty. To address this issue, we propose to utilize large-scale pre-trained models to guide downstream model training with sample difficulty-aware entropy regularization. Pre-trained models that have been exposed to large-scale datasets and do not overfit the downstream training classes enable us to measure each training sample difficulty via feature-space Gaussian modeling and relative Mahalanobis distance computation. Importantly, by adaptively penalizing overconfident prediction based on the sample's difficulty, we simultaneously improve accuracy and uncertainty calibration on various challenging benchmarks, consistently surpassing competitive baselines for reliable prediction."
1556,https://arxiv.org/abs/2304.10091,Learning CLIP Guided Visual-Text Fusion Transformer for Video-based Pedestrian Attribute Recognition,"Existing pedestrian attribute recognition (PAR) algorithms are mainly developed based on a static image. However, the performance is not reliable for images with challenging factors, such as heavy occlusion, motion blur, etc. In this work, we propose to understand human attributes using video frames that can make full use of temporal information. Specifically, we formulate the video-based PAR as a vision-language fusion problem and adopt pre-trained big models CLIP to extract the feature embeddings of given video frames. To better utilize the semantic information, we take the attribute list as another input and transform the attribute words/phrase into the corresponding sentence via split, expand, and prompt. Then, the text encoder of CLIP is utilized for language embedding. The averaged visual tokens and text tokens are concatenated and fed into a fusion Transformer for multi-modal interactive learning. The enhanced tokens will be fed into a classification head for pedestrian attribute prediction. Extensive experiments on a large-scale video-based PAR dataset fully validated the effectiveness of our proposed framework."
1557,https://arxiv.org/abs/2304.09724,A compact simple HWENO scheme with ADER time discretization for hyperbolic conservation laws I: structured meshes,"In this paper, a compact and high order ADER (Arbitrary high order using DERivatives) scheme using the simple HWENO method (ADER-SHWENO) is proposed for hyperbolic conservation laws. The newly-developed method employs the Lax-Wendroff procedure to convert time derivatives to spatial derivatives, which provides the time evolution of the variables at the cell interfaces. This information is required for the simple HWENO reconstructions, which take advantages of the simple WENO and the classic HWENO. Compared with the original Runge-Kutta HWENO method (RK-HWENO), the new method has two advantages. Firstly, RK-HWENO method must solve the additional equations for reconstructions, which is avoided for the new method. Secondly, the SHWENO reconstruction is performed once with one stencil and is different from the classic HWENO methods, in which both the function and its derivative values are reconstructed with two different stencils, respectively. Thus the new method is more efficient than the RK-HWENO method. Moreover, the new method is more compact than the existing ADER-WENO method. Besides, the new method makes the best use of the information in the ADER method. Thus, the time evolution of the cell averages of the derivatives is simpler than that developed in the work [Li et. al., 447 (2021), 110661.]. Numerical tests indicate that the new method can achieve high order for smooth solutions both in space and time, keep non-oscillatory at discontinuities."
1558,https://arxiv.org/abs/2304.04742,Detection Transformer with Stable Matching,"This paper is concerned with the matching stability problem across different decoder layers in DEtection TRansformers (DETR). We point out that the unstable matching in DETR is caused by a multi-optimization path problem, which is highlighted by the one-to-one matching design in DETR. To address this problem, we show that the most important design is to use and only use positional metrics (like IOU) to supervise classification scores of positive examples. Under the principle, we propose two simple yet effective modifications by integrating positional metrics to DETR's classification loss and matching cost, named position-supervised loss and position-modulated cost. We verify our methods on several DETR variants. Our methods show consistent improvements over baselines. By integrating our methods with DINO, we achieve 50.4 and 51.5 AP on the COCO detection benchmark using ResNet-50 backbones under 12 epochs and 24 epochs training settings, achieving a new record under the same setting. We achieve 63.8 AP on COCO detection test-dev with a Swin-Large backbone. Our code will be made available at https://github.com/IDEA-Research/Stable-DINO."
1559,https://arxiv.org/abs/2303.18181,A Closer Look at Parameter-Efficient Tuning in Diffusion Models,"Large-scale diffusion models like Stable Diffusion are powerful and find various real-world applications while customizing such models by fine-tuning is both memory and time inefficient. Motivated by the recent progress in natural language processing, we investigate parameter-efficient tuning in large diffusion models by inserting small learnable modules (termed adapters). In particular, we decompose the design space of adapters into orthogonal factors -- the input position, the output position as well as the function form, and perform Analysis of Variance (ANOVA), a classical statistical approach for analyzing the correlation between discrete (design options) and continuous variables (evaluation metrics). Our analysis suggests that the input position of adapters is the critical factor influencing the performance of downstream tasks. Then, we carefully study the choice of the input position, and we find that putting the input position after the cross-attention block can lead to the best performance, validated by additional visualization analyses. Finally, we provide a recipe for parameter-efficient tuning in diffusion models, which is comparable if not superior to the fully fine-tuned baseline (e.g., DreamBooth) with only 0.75 \% extra parameters, across various customized tasks."
1560,https://arxiv.org/abs/2303.15818,Towards Effective Adversarial Textured 3D Meshes on Physical Face Recognition,"Face recognition is a prevailing authentication solution in numerous biometric applications. Physical adversarial attacks, as an important surrogate, can identify the weaknesses of face recognition systems and evaluate their robustness before deployed. However, most existing physical attacks are either detectable readily or ineffective against commercial recognition systems. The goal of this work is to develop a more reliable technique that can carry out an end-to-end evaluation of adversarial robustness for commercial systems. It requires that this technique can simultaneously deceive black-box recognition models and evade defensive mechanisms. To fulfill this, we design adversarial textured 3D meshes (AT3D) with an elaborate topology on a human face, which can be 3D-printed and pasted on the attacker's face to evade the defenses. However, the mesh-based optimization regime calculates gradients in high-dimensional mesh space, and can be trapped into local optima with unsatisfactory transferability. To deviate from the mesh-based space, we propose to perturb the low-dimensional coefficient space based on 3D Morphable Model, which significantly improves black-box transferability meanwhile enjoying faster search efficiency and better visual quality. Extensive experiments in digital and physical scenarios show that our method effectively explores the security vulnerabilities of multiple popular commercial services, including three recognition APIs, four anti-spoofing APIs, two prevailing mobile phones and two automated access control systems."
1561,https://arxiv.org/abs/2303.11040,Benchmarking Robustness of 3D Object Detection to Common Corruptions in Autonomous Driving,"3D object detection is an important task in autonomous driving to perceive the surroundings. Despite the excellent performance, the existing 3D detectors lack the robustness to real-world corruptions caused by adverse weathers, sensor noises, etc., provoking concerns about the safety and reliability of autonomous driving systems. To comprehensively and rigorously benchmark the corruption robustness of 3D detectors, in this paper we design 27 types of common corruptions for both LiDAR and camera inputs considering real-world driving scenarios. By synthesizing these corruptions on public datasets, we establish three corruption robustness benchmarks -- KITTI-C, nuScenes-C, and Waymo-C. Then, we conduct large-scale experiments on 24 diverse 3D object detection models to evaluate their corruption robustness. Based on the evaluation results, we draw several important findings, including: 1) motion-level corruptions are the most threatening ones that lead to significant performance drop of all models; 2) LiDAR-camera fusion models demonstrate better robustness; 3) camera-only models are extremely vulnerable to image corruptions, showing the indispensability of LiDAR point clouds. We release the benchmarks and codes at https://github.com/kkkcx/3D_Corruptions_AD. We hope that our benchmarks and findings can provide insights for future research on developing robust 3D object detection models."
1562,https://arxiv.org/abs/2303.09105,Rethinking Model Ensemble in Transfer-based Adversarial Attacks,"Deep learning models are vulnerable to adversarial examples. Transfer-based adversarial attacks attract tremendous attention as they can identify the weaknesses of deep learning models in a black-box manner. An effective strategy to improve the transferability of adversarial examples is attacking an ensemble of models. However, previous works simply average the outputs of different models, lacking an in-depth analysis on how and why model ensemble can strongly improve the transferability. In this work, we rethink the ensemble in adversarial attacks and define the common weakness of model ensemble with the properties of the flatness of loss landscape and the closeness to the local optimum of each model. We empirically and theoretically show that these two properties are strongly correlated with the transferability and propose a Common Weakness Attack (CWA) to generate more transferable adversarial examples by promoting these two properties. Experimental results on both image classification and object detection tasks validate the effectiveness of our approach to improve the adversarial transferability, especially when attacking adversarially trained models."
1563,https://arxiv.org/abs/2303.06555,One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale,"This paper proposes a unified diffusion framework (dubbed UniDiffuser) to fit all distributions relevant to a set of multi-modal data in one model. Our key insight is -- learning diffusion models for marginal, conditional, and joint distributions can be unified as predicting the noise in the perturbed data, where the perturbation levels (i.e. timesteps) can be different for different modalities. Inspired by the unified view, UniDiffuser learns all distributions simultaneously with a minimal modification to the original diffusion model -- perturbs data in all modalities instead of a single modality, inputs individual timesteps in different modalities, and predicts the noise of all modalities instead of a single modality. UniDiffuser is parameterized by a transformer for diffusion models to handle input types of different modalities. Implemented on large-scale paired image-text data, UniDiffuser is able to perform image, text, text-to-image, image-to-text, and image-text pair generation by setting proper timesteps without additional overhead. In particular, UniDiffuser is able to produce perceptually realistic samples in all tasks and its quantitative results (e.g., the FID and CLIP score) are not only superior to existing general-purpose models but also comparable to the bespoken models (e.g., Stable Diffusion and DALL-E 2) in representative tasks (e.g., text-to-image generation)."
1564,https://arxiv.org/abs/2303.05499,Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection,"In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection, and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves a $52.5$ AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean $26.1$ AP. Code will be available at \url{https://github.com/IDEA-Research/GroundingDINO}."
1565,https://arxiv.org/abs/2303.05092,Reward Informed Dreamer for Task Generalization in Reinforcement Learning,"A long-standing goal of reinforcement learning is that algorithms can learn on training tasks and generalize well on unseen tasks like humans, where different tasks share similar dynamic with different reward functions. A general challenge is that it is nontrivial to quantitatively measure the similarities between these different tasks, which is vital for analyzing the task distribution and further designing algorithms with stronger generalization. To address this, we present a novel metric named Task Distribution Relevance (TDR) via optimal Q functions to capture the relevance of the task distribution quantitatively. In the case of tasks with a high TDR, i.e., the tasks differ significantly, we demonstrate that the Markovian policies cannot distinguish them, yielding poor performance accordingly. Based on this observation, we propose a framework of Reward Informed Dreamer (RID) with reward-informed world models, which captures invariant latent features over tasks and encodes reward signals into policies for distinguishing different tasks. In RID, we calculate the corresponding variational lower bound of the log-likelihood on the data, which includes a novel term to distinguish different tasks via states, based on reward-informed world models. Finally, extensive experiments in DeepMind control suite demonstrate that RID can significantly improve the performance of handling different tasks at the same time, especially for those with high TDR, and further generalize to unseen tasks effectively."
1566,https://arxiv.org/abs/2303.02867,Dual Feedback Attention Framework via Boundary-Aware Auxiliary and Progressive Semantic Optimization for Salient Object Detection in Optical Remote Sensing Imagery,"Salient object detection in optical remote sensing image (ORSI-SOD) has gradually attracted attention thanks to the development of deep learning (DL) and salient object detection in natural scene image (NSI-SOD). However, NSI and ORSI are different in many aspects, such as large coverage, complex background, and large differences in target types and scales. Therefore, a new dedicated method is needed for ORSI-SOD. In addition, existing methods do not pay sufficient attention to the boundary of the object, and the completeness of the final saliency map still needs improvement. To address these issues, we propose a novel method called Dual Feedback Attention Framework via Boundary-Aware Auxiliary and Progressive Semantic Optimization (DFA-BASO). First, Boundary Protection Calibration (BPC) module is proposed to reduce the loss of edge position information during forward propagation and suppress noise in low-level features. Second, a Dual Feature Feedback Complementary (DFFC) module is proposed based on BPC module. It aggregates boundary-semantic dual features and provides effective feedback to coordinate features across different layers. Finally, a Strong Semantic Feedback Refinement (SSFR) module is proposed to obtain more complete saliency maps. This module further refines feature representation and eliminates feature differences through a unique feedback mechanism. Extensive experiments on two public datasets show that DFA-BASO outperforms 15 state-of-the-art methods. Furthermore, this paper strongly demonstrates the true contribution of DFA-BASO to ORSI-SOD by in-depth analysis of the visualization figure. All codes can be found at https://github.com/YUHsss/DFA-BASO."
1567,https://arxiv.org/abs/2303.01049,Model-based Constrained MDP for Budget Allocation in Sequential Incentive Marketing,"Sequential incentive marketing is an important approach for online businesses to acquire customers, increase loyalty and boost sales. How to effectively allocate the incentives so as to maximize the return (e.g., business objectives) under the budget constraint, however, is less studied in the literature. This problem is technically challenging due to the facts that 1) the allocation strategy has to be learned using historically logged data, which is counterfactual in nature, and 2) both the optimality and feasibility (i.e., that cost cannot exceed budget) needs to be assessed before being deployed to online systems. In this paper, we formulate the problem as a constrained Markov decision process (CMDP). To solve the CMDP problem with logged counterfactual data, we propose an efficient learning algorithm which combines bisection search and model-based planning. First, the CMDP is converted into its dual using Lagrangian relaxation, which is proved to be monotonic with respect to the dual variable. Furthermore, we show that the dual problem can be solved by policy learning, with the optimal dual variable being found efficiently via bisection search (i.e., by taking advantage of the monotonicity). Lastly, we show that model-based planing can be used to effectively accelerate the joint optimization process without retraining the policy for every dual variable. Empirical results on synthetic and real marketing datasets confirm the effectiveness of our methods."
1568,https://arxiv.org/abs/2303.00284,To Make Yourself Invisible with Adversarial Semantic Contours,"Modern object detectors are vulnerable to adversarial examples, which may bring risks to real-world applications. The sparse attack is an important task which, compared with the popular adversarial perturbation on the whole image, needs to select the potential pixels that is generally regularized by an $\ell_0$-norm constraint, and simultaneously optimize the corresponding texture. The non-differentiability of $\ell_0$ norm brings challenges and many works on attacking object detection adopted manually-designed patterns to address them, which are meaningless and independent of objects, and therefore lead to relatively poor attack performance.
  In this paper, we propose Adversarial Semantic Contour (ASC), an MAP estimate of a Bayesian formulation of sparse attack with a deceived prior of object contour. The object contour prior effectively reduces the search space of pixel selection and improves the attack by introducing more semantic bias. Extensive experiments demonstrate that ASC can corrupt the prediction of 9 modern detectors with different architectures (\e.g., one-stage, two-stage and Transformer) by modifying fewer than 5\% of the pixels of the object area in COCO in white-box scenario and around 10\% of those in black-box scenario. We further extend the attack to datasets for autonomous driving systems to verify the effectiveness. We conclude with cautions about contour being the common weakness of object detectors with various architecture and the care needed in applying them in safety-sensitive scenarios."
1569,https://arxiv.org/abs/2302.14376,GNOT: A General Neural Operator Transformer for Operator Learning,"Learning partial differential equations' (PDEs) solution operators is an essential problem in machine learning. However, there are several challenges for learning operators in practical applications like the irregular mesh, multiple input functions, and complexity of the PDEs' solution. To address these challenges, we propose a general neural operator transformer (GNOT), a scalable and effective transformer-based framework for learning operators. By designing a novel heterogeneous normalized attention layer, our model is highly flexible to handle multiple input functions and irregular mesh. Besides, we introduce a geometric gating mechanism which could be viewed as a soft domain decomposition to solve the multi-scale problems. The large model capacity of transformer architecture grants our model the possibility to scale to large datasets and practical problems. We conduct extensive experiments on multiple challenging datasets from different domains and achieve a remarkable improvement compared with alternative methods."
1570,https://arxiv.org/abs/2302.14301,A Comprehensive Study on Robustness of Image Classification Models: Benchmarking and Rethinking,"The robustness of deep neural networks is usually lacking under adversarial examples, common corruptions, and distribution shifts, which becomes an important research problem in the development of deep learning. Although new deep learning methods and robustness improvement techniques have been constantly proposed, the robustness evaluations of existing methods are often inadequate due to their rapid development, diverse noise patterns, and simple evaluation metrics. Without thorough robustness evaluations, it is hard to understand the advances in the field and identify the effective methods. In this paper, we establish a comprehensive robustness benchmark called \textbf{ARES-Bench} on the image classification task. In our benchmark, we evaluate the robustness of 55 typical deep learning models on ImageNet with diverse architectures (e.g., CNNs, Transformers) and learning algorithms (e.g., normal supervised training, pre-training, adversarial training) under numerous adversarial attacks and out-of-distribution (OOD) datasets. Using robustness curves as the major evaluation criteria, we conduct large-scale experiments and draw several important findings, including: 1) there is an inherent trade-off between adversarial and natural robustness for the same model architecture; 2) adversarial training effectively improves adversarial robustness, especially when performed on Transformer architectures; 3) pre-training significantly improves natural robustness based on more training data or self-supervised learning. Based on ARES-Bench, we further analyze the training tricks in large-scale adversarial training on ImageNet. By designing the training settings accordingly, we achieve the new state-of-the-art adversarial robustness. We have made the benchmarking results and code platform publicly available."
1571,https://arxiv.org/abs/2302.10586,Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels,"We propose a three-stage training strategy called dual pseudo training (DPT) for conditional image generation and classification in semi-supervised learning. First, a classifier is trained on partially labeled data and predicts pseudo labels for all data. Second, a conditional generative model is trained on all data with pseudo labels and generates pseudo images given labels. Finally, the classifier is trained on real data augmented by pseudo images with labels. We demonstrate large-scale diffusion models and semi-supervised learners benefit mutually with a few labels via DPT. In particular, on the ImageNet 256x256 generation benchmark, DPT can generate realistic, diverse, and semantically correct images with very few labels. With two (i.e., < 0.2%) and five (i.e., < 0.4%) labels per class, DPT achieves an FID of 3.44 and 3.37 respectively, outperforming strong diffusion models with full labels, such as IDDPM, CDM, ADM, and LDM. Besides, DPT outperforms competitive semi-supervised baselines substantially on ImageNet classification benchmarks with one, two, and five labels per class, achieving state-of-the-art top-1 accuracies of 59.0 (+2.8), 69.5 (+3.0), and 73.6 (+1.2) respectively."
1572,https://arxiv.org/abs/2302.05098,Confidence-based Reliable Learning under Dual Noises,"Deep neural networks (DNNs) have achieved remarkable success in a variety of computer vision tasks, where massive labeled images are routinely required for model optimization. Yet, the data collected from the open world are unavoidably polluted by noise, which may significantly undermine the efficacy of the learned models. Various attempts have been made to reliably train DNNs under data noise, but they separately account for either the noise existing in the labels or that existing in the images. A naive combination of the two lines of works would suffer from the limitations in both sides, and miss the opportunities to handle the two kinds of noise in parallel. This work provides a first, unified framework for reliable learning under the joint (image, label)-noise. Technically, we develop a confidence-based sample filter to progressively filter out noisy data without the need of pre-specifying noise ratio. Then, we penalize the model uncertainty of the detected noisy data instead of letting the model continue over-fitting the misleading information in them. Experimental results on various challenging synthetic and real-world noisy datasets verify that the proposed method can outperform competing baselines in the aspect of classification performance."
1573,https://arxiv.org/abs/2302.02334,Revisiting Discriminative vs. Generative Classifiers: Theory and Implications,"A large-scale deep model pre-trained on massive labeled or unlabeled data transfers well to downstream tasks. Linear evaluation freezes parameters in the pre-trained model and trains a linear classifier separately, which is efficient and attractive for transfer. However, little work has investigated the classifier in linear evaluation except for the default logistic regression. Inspired by the statistical efficiency of naive Bayes, the paper revisits the classical topic on discriminative vs. generative classifiers. Theoretically, the paper considers the surrogate loss instead of the zero-one loss in analyses and generalizes the classical results from binary cases to multiclass ones. We show that, under mild assumptions, multiclass naive Bayes requires $O(\log n)$ samples to approach its asymptotic error while the corresponding multiclass logistic regression requires $O(n)$ samples, where $n$ is the feature dimension. To establish it, we present a multiclass $\mathcal{H}$-consistency bound framework and an explicit bound for logistic loss, which are of independent interests. Simulation results on a mixture of Gaussian validate our theoretical findings. Experiments on various pre-trained deep vision models show that naive Bayes consistently converges faster as the number of data increases. Besides, naive Bayes shows promise in few-shot cases and we observe the ``two regimes'' phenomenon in pre-trained supervised models. Our code is available at https://github.com/ML-GSAI/Revisiting-Dis-vs-Gen-Classifiers."
1574,https://arxiv.org/abs/2302.00487,"A Comprehensive Survey of Continual Learning: Theory, Method and Application","To cope with real-world dynamics, an intelligent agent needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance drop of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative strategies address continual learning, and how they are adapted to particular challenges in various applications. Through an in-depth discussion of continual learning in terms of the current trends, cross-directional prospects and interdisciplinary connections with neuroscience, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond."
1575,https://arxiv.org/abs/2301.03747,Semiparametric Regression for Spatial Data via Deep Learning,"In this work, we propose a deep learning-based method to perform semiparametric regression analysis for spatially dependent data. To be specific, we use a sparsely connected deep neural network with rectified linear unit (ReLU) activation function to estimate the unknown regression function that describes the relationship between response and covariates in the presence of spatial dependence. Under some mild conditions, the estimator is proven to be consistent, and the rate of convergence is determined by three factors: (1) the architecture of neural network class, (2) the smoothness and (intrinsic) dimension of true mean function, and (3) the magnitude of spatial dependence. Our method can handle well large data set owing to the stochastic gradient descent optimization algorithm. Simulation studies on synthetic data are conducted to assess the finite sample performance, the results of which indicate that the proposed method is capable of picking up the intricate relationship between response and covariates. Finally, a real data analysis is provided to demonstrate the validity and effectiveness of the proposed method."
1576,https://arxiv.org/abs/2212.00362,Why Are Conditional Generative Models Better Than Unconditional Ones?,"Extensive empirical evidence demonstrates that conditional generative models are easier to train and perform better than unconditional ones by exploiting the labels of data. So do score-based diffusion models. In this paper, we analyze the phenomenon formally and identify that the key of conditional learning is to partition the data properly. Inspired by the analyses, we propose self-conditioned diffusion models (SCDM), which is trained conditioned on indices clustered by the k-means algorithm on the features extracted by a model pre-trained in a self-supervised manner. SCDM significantly improves the unconditional model across various datasets and achieves a record-breaking FID of 3.94 on ImageNet 64x64 without labels. Besides, SCDM achieves a slightly better FID than the corresponding conditional model on CIFAR10."
1577,https://arxiv.org/abs/2211.15516,DQ-DETR: Dual Query Detection Transformer for Phrase Extraction and Grounding,"In this paper, we study the problem of visual grounding by considering both phrase extraction and grounding (PEG). In contrast to the previous phrase-known-at-test setting, PEG requires a model to extract phrases from text and locate objects from images simultaneously, which is a more practical setting in real applications. As phrase extraction can be regarded as a $1$D text segmentation problem, we formulate PEG as a dual detection problem and propose a novel DQ-DETR model, which introduces dual queries to probe different features from image and text for object prediction and phrase mask prediction. Each pair of dual queries is designed to have shared positional parts but different content parts. Such a design effectively alleviates the difficulty of modality alignment between image and text (in contrast to a single query design) and empowers Transformer decoder to leverage phrase mask-guided attention to improve performance. To evaluate the performance of PEG, we also propose a new metric CMAP (cross-modal average precision), analogous to the AP metric in object detection. The new metric overcomes the ambiguity of Recall@1 in many-box-to-one-phrase cases in phrase grounding. As a result, our PEG pre-trained DQ-DETR establishes new state-of-the-art results on all visual grounding benchmarks with a ResNet-101 backbone. For example, it achieves $91.04\%$ and $83.51\%$ in terms of recall rate on RefCOCO testA and testB with a ResNet-101 backbone. Code will be availabl at \url{https://github.com/IDEA-Research/DQ-DETR}."
1578,https://arxiv.org/abs/2211.08064,"Physics-Informed Machine Learning: A Survey on Problems, Methods and Applications","Recent advances of data-driven machine learning have revolutionized fields like computer vision, reinforcement learning, and many scientific and engineering domains. In many real-world and scientific problems, systems that generate data are governed by physical laws. Recent work shows that it provides potential benefits for machine learning models by incorporating the physical prior and collected data, which makes the intersection of machine learning and physics become a prevailing paradigm. By integrating the data and mathematical physics models seamlessly, it can guide the machine learning model towards solutions that are physically plausible, improving accuracy and efficiency even in uncertain and high-dimensional contexts. In this survey, we present this learning paradigm called Physics-Informed Machine Learning (PIML) which is to build a model that leverages empirical data and available physical prior knowledge to improve performance on a set of tasks that involve a physical mechanism. We systematically review the recent development of physics-informed machine learning from three perspectives of machine learning tasks, representation of physical prior, and methods for incorporating physical prior. We also propose several important open research problems based on the current trends in the field. We argue that encoding different forms of physical prior into model architectures, optimizers, inference algorithms, and significant domain-specific applications like inverse engineering design and robotic control is far from being fully explored in the field of physics-informed machine learning. We believe that the interdisciplinary research of physics-informed machine learning will significantly propel research progress, foster the creation of more effective machine learning models, and also offer invaluable assistance in addressing long-standing problems in related disciplines."
1579,https://arxiv.org/abs/2211.04265,Photon shot-noise limited transient absorption soft X-ray spectroscopy at the European XFEL,"Femtosecond transient soft X-ray Absorption Spectroscopy (XAS) is a very promising technique that can be employed at X-ray Free Electron Lasers (FELs) to investigate out-of-equilibrium dynamics for material and energy research. Here we present a dedicated setup for soft X-rays available at the Spectroscopy & Coherent Scattering (SCS) instrument at the European X-ray Free Electron Laser (EuXFEL). It consists of a beam-splitting off-axis zone plate (BOZ) used in transmission to create three copies of the incoming beam, which are used to measure the transmitted intensity through the excited and unexcited sample, as well as to monitor the incoming intensity. Since these three intensity signals are detected shot-by-shot and simultaneously, this setup allows normalized shot-by-shot analysis of the transmission. For photon detection, the DSSC imaging detector, which is capable of recording up to 800 images at 4.5 MHz frame rate during the FEL burst, is employed and allows approaching the photon shot-noise limit. We review the setup and its capabilities, as well as the online and offline analysis tools provided to users."
1580,https://arxiv.org/abs/2211.01093,Improving transferability of 3D adversarial attacks with scale and shear transformations,"Previous work has shown that 3D point cloud classifiers can be vulnerable to adversarial examples. However, most of the existing methods are aimed at white-box attacks, where the parameters and other information of the classifiers are known in the attack, which is unrealistic for real-world applications. In order to improve the attack performance of the black-box classifiers, the research community generally uses the transfer-based black-box attack. However, the transferability of current 3D attacks is still relatively low. To this end, this paper proposes Scale and Shear (SS) Attack to generate 3D adversarial examples with strong transferability. Specifically, we randomly scale or shear the input point cloud, so that the attack will not overfit the white-box model, thereby improving the transferability of the attack. Extensive experiments show that the SS attack proposed in this paper can be seamlessly combined with the existing state-of-the-art (SOTA) 3D point cloud attack methods to form more powerful attack methods, and the SS attack improves the transferability over 3.6 times compare to the baseline. Moreover, while substantially outperforming the baseline methods, the SS attack achieves SOTA transferability under various defenses. Our code will be available online at https://github.com/cuge1995/SS-attack"
1581,https://arxiv.org/abs/2211.00942,Model-based Reinforcement Learning with a Hamiltonian Canonical ODE Network,"Model-based reinforcement learning usually suffers from a high sample complexity in training the world model, especially for the environments with complex dynamics. To make the training for general physical environments more efficient, we introduce Hamiltonian canonical ordinary differential equations into the learning process, which inspires a novel model of neural ordinary differential auto-encoder (NODA). NODA can model the physical world by nature and is flexible to impose Hamiltonian mechanics (e.g., the dimension of the physical equations) which can further accelerate training of the environment models. It can consequentially empower an RL agent with the robust extrapolation using a small amount of samples as well as the guarantee on the physical plausibility. Theoretically, we prove that NODA has uniform bounds for multi-step transition errors and value errors under certain conditions. Extensive experiments show that NODA can learn the environment dynamics effectively with a high sample efficiency, making it possible to facilitate reinforcement learning agents at the early stage."
1582,https://arxiv.org/abs/2210.16525,Spectral Representation Learning for Conditional Moment Models,"Many problems in causal inference and economics can be formulated in the framework of conditional moment models, which characterize the target function through a collection of conditional moment restrictions. For nonparametric conditional moment models, efficient estimation often relies on preimposed conditions on various measures of ill-posedness of the hypothesis space, which are hard to validate when flexible models are used. In this work, we address this issue by proposing a procedure that automatically learns representations with controlled measures of ill-posedness. Our method approximates a linear representation defined by the spectral decomposition of a conditional expectation operator, which can be used for kernelized estimators and is known to facilitate minimax optimal estimation in certain settings. We show this representation can be efficiently estimated from data, and establish L2 consistency for the resulting estimator. We evaluate the proposed method on proximal causal inference tasks, exhibiting promising performance on high-dimensional, semi-synthetic data."
1583,https://arxiv.org/abs/2210.15291,Isometric 3D Adversarial Examples in the Physical World,"3D deep learning models are shown to be as vulnerable to adversarial examples as 2D models. However, existing attack methods are still far from stealthy and suffer from severe performance degradation in the physical world. Although 3D data is highly structured, it is difficult to bound the perturbations with simple metrics in the Euclidean space. In this paper, we propose a novel $ε$-isometric ($ε$-ISO) attack to generate natural and robust 3D adversarial examples in the physical world by considering the geometric properties of 3D objects and the invariance to physical transformations. For naturalness, we constrain the adversarial example to be $ε$-isometric to the original one by adopting the Gaussian curvature as a surrogate metric guaranteed by a theoretical analysis. For invariance to physical transformations, we propose a maxima over transformation (MaxOT) method that actively searches for the most harmful transformations rather than random ones to make the generated adversarial example more robust in the physical world. Experiments on typical point cloud recognition models validate that our approach can significantly improve the attack success rate and naturalness of the generated 3D adversarial examples than the state-of-the-art attack methods."
1584,https://arxiv.org/abs/2210.13162,The interplay of local electron correlations and ultrafast spin dynamics in fcc Ni,"The complex electronic structure of metallic ferromagnets is determined by a balance between exchange interaction, electron hopping leading to band formation, and local Coulomb repulsion. The interplay between the respective terms of the Hamiltonian is of fundamental interest, since it produces most, if not all, of the exotic phenomena observed in the solid state. By combining high energy and temporal resolution in femtosecond time-resolved X-ray absorption spectroscopy with ab initio time-dependent density functional theory we analyze the electronic structure in fcc Ni on the time scale of these interactions in a pump-probe experiment. We distinguish transient broadening and energy shifts in the absorption spectra, which we demonstrate to be caused by electron repopulation and correlation-induced modifications of the electronic structure, respectively. Importantly, the theoretical description of this experimental result hence requires to take the local Coulomb interaction into account, revealing a temporal interplay between band formation, exchange interaction, and Coulomb repulsion."
1585,https://arxiv.org/abs/2210.12642,Accelerated Linearized Laplace Approximation for Bayesian Deep Learning,"Laplace approximation (LA) and its linearized variant (LLA) enable effortless adaptation of pretrained deep neural networks to Bayesian neural networks. The generalized Gauss-Newton (GGN) approximation is typically introduced to improve their tractability. However, LA and LLA are still confronted with non-trivial inefficiency issues and should rely on Kronecker-factored, diagonal, or even last-layer approximate GGN matrices in practical use. These approximations are likely to harm the fidelity of learning outcomes. To tackle this issue, inspired by the connections between LLA and neural tangent kernels (NTKs), we develop a Nystrom approximation to NTKs to accelerate LLA. Our method benefits from the capability of popular deep learning libraries for forward mode automatic differentiation, and enjoys reassuring theoretical guarantees. Extensive studies reflect the merits of the proposed method in aspects of both scalability and performance. Our method can even scale up to architectures like vision transformers. We also offer valuable ablation studies to diagnose our method. Code is available at \url{https://github.com/thudzj/ELLA}."
1586,https://arxiv.org/abs/2210.12637,Neural Eigenfunctions Are Structured Representation Learners,"In this paper, we introduce a scalable method for learning structured, adaptive-length deep representations. Our approach is to train neural networks such that they approximate the principal eigenfunctions of a kernel. We show that, when the kernel is derived from positive relations in a contrastive learning setup, our method outperforms a number of competitive baselines in visual representation learning and transfer learning benchmarks, and importantly, produces structured representations where the order of features indicates degrees of importance. We demonstrate using such representations as adaptive-length codes in image retrieval systems. By truncation according to feature importance, our method requires up to 16$\times$ shorter representation length than leading self-supervised learning methods to achieve similar retrieval performance. We further apply our method to graph data and report strong results on a node representation learning benchmark with more than one million nodes."
1587,https://arxiv.org/abs/2210.03895,ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial Viewpoints,"Recent studies have demonstrated that visual recognition models lack robustness to distribution shift. However, current work mainly considers model robustness to 2D image transformations, leaving viewpoint changes in the 3D world less explored. In general, viewpoint changes are prevalent in various real-world applications (e.g., autonomous driving), making it imperative to evaluate viewpoint robustness. In this paper, we propose a novel method called ViewFool to find adversarial viewpoints that mislead visual recognition models. By encoding real-world objects as neural radiance fields (NeRF), ViewFool characterizes a distribution of diverse adversarial viewpoints under an entropic regularizer, which helps to handle the fluctuations of the real camera pose and mitigate the reality gap between the real objects and their neural representations. Experiments validate that the common image classifiers are extremely vulnerable to the generated adversarial viewpoints, which also exhibit high cross-model transferability. Based on ViewFool, we introduce ImageNet-V, a new out-of-distribution dataset for benchmarking viewpoint robustness of image classifiers. Evaluation results on 40 classifiers with diverse architectures, objective functions, and data augmentations reveal a significant drop in model performance when tested on ImageNet-V, which provides a possibility to leverage ViewFool as an effective data augmentation strategy to improve viewpoint robustness."
1588,https://arxiv.org/abs/2210.03526,A Unified Hard-Constraint Framework for Solving Geometrically Complex PDEs,"We present a unified hard-constraint framework for solving geometrically complex PDEs with neural networks, where the most commonly used Dirichlet, Neumann, and Robin boundary conditions (BCs) are considered. Specifically, we first introduce the ""extra fields"" from the mixed finite element method to reformulate the PDEs so as to equivalently transform the three types of BCs into linear forms. Based on the reformulation, we derive the general solutions of the BCs analytically, which are employed to construct an ansatz that automatically satisfies the BCs. With such a framework, we can train the neural networks without adding extra loss terms and thus efficiently handle geometrically complex PDEs, alleviating the unbalanced competition between the loss terms corresponding to the BCs and PDEs. We theoretically demonstrate that the ""extra fields"" can stabilize the training process. Experimental results on real-world geometrically complex PDEs showcase the effectiveness of our method compared with state-of-the-art baselines."
1589,https://arxiv.org/abs/2209.15408,Equivariant Energy-Guided SDE for Inverse Molecular Design,"Inverse molecular design is critical in material science and drug discovery, where the generated molecules should satisfy certain desirable properties. In this paper, we propose equivariant energy-guided stochastic differential equations (EEGSDE), a flexible framework for controllable 3D molecule generation under the guidance of an energy function in diffusion models. Formally, we show that EEGSDE naturally exploits the geometric symmetry in 3D molecular conformation, as long as the energy function is invariant to orthogonal transformations. Empirically, under the guidance of designed energy functions, EEGSDE significantly improves the baseline on QM9, in inverse molecular design targeted to quantum properties and molecular structures. Furthermore, EEGSDE is able to generate molecules with multiple target properties by combining the corresponding energy functions linearly."
1590,https://arxiv.org/abs/2209.15215,INT: Towards Infinite-frames 3D Detection with An Efficient Framework,"It is natural to construct a multi-frame instead of a single-frame 3D detector for a continuous-time stream. Although increasing the number of frames might improve performance, previous multi-frame studies only used very limited frames to build their systems due to the dramatically increased computational and memory cost. To address these issues, we propose a novel on-stream training and prediction framework that, in theory, can employ an infinite number of frames while keeping the same amount of computation as a single-frame detector. This infinite framework (INT), which can be used with most existing detectors, is utilized, for example, on the popular CenterPoint, with significant latency reductions and performance improvements. We've also conducted extensive experiments on two large-scale datasets, nuScenes and Waymo Open Dataset, to demonstrate the scheme's effectiveness and efficiency. By employing INT on CenterPoint, we can get around 7% (Waymo) and 15% (nuScenes) performance boost with only 2~4ms latency overhead, and currently SOTA on the Waymo 3D Detection leaderboard."
1591,https://arxiv.org/abs/2209.14548,Offline Reinforcement Learning via High-Fidelity Generative Behavior Modeling,"In offline reinforcement learning, weighted regression is a common method to ensure the learned policy stays close to the behavior policy and to prevent selecting out-of-sample actions. In this work, we show that due to the limited distributional expressivity of policy models, previous methods might still select unseen actions during training, which deviates from their initial motivation. To address this problem, we adopt a generative approach by decoupling the learned policy into two parts: an expressive generative behavior model and an action evaluation model. The key insight is that such decoupling avoids learning an explicitly parameterized policy model with a closed-form expression. Directly learning the behavior policy allows us to leverage existing advances in generative modeling, such as diffusion-based methods, to model diverse behaviors. As for action evaluation, we combine our method with an in-sample planning technique to further avoid selecting out-of-sample actions and increase computational efficiency. Experimental results on D4RL datasets show that our proposed method achieves competitive or superior performance compared with state-of-the-art offline RL methods, especially in complex tasks such as AntMaze. We also empirically demonstrate that our method can successfully learn from a heterogeneous dataset containing multiple distinctive but similarly successful strategies, whereas previous unimodal policies fail."
1592,https://arxiv.org/abs/2209.12152,All are Worth Words: A ViT Backbone for Diffusion Models,"Vision transformers (ViT) have shown promise in various vision tasks while the U-Net based on a convolutional neural network (CNN) remains dominant in diffusion models. We design a simple and general ViT-based architecture (named U-ViT) for image generation with diffusion models. U-ViT is characterized by treating all inputs including the time, condition and noisy image patches as tokens and employing long skip connections between shallow and deep layers. We evaluate U-ViT in unconditional and class-conditional image generation, as well as text-to-image generation tasks, where U-ViT is comparable if not superior to a CNN-based U-Net of a similar size. In particular, latent diffusion models with U-ViT achieve record-breaking FID scores of 2.29 in class-conditional image generation on ImageNet 256x256, and 5.48 in text-to-image generation on MS-COCO, among methods without accessing large external datasets during the training of generative models. Our results suggest that, for diffusion-based image modeling, the long skip connection is crucial while the down-sampling and up-sampling operators in CNN-based U-Net are not always necessary. We believe that U-ViT can provide insights for future research on backbones in diffusion models and benefit generative modeling on large scale cross-modality datasets."
1593,https://arxiv.org/abs/2209.07075,Bi-level Physics-Informed Neural Networks for PDE Constrained Optimization using Broyden's Hypergradients,"Deep learning based approaches like Physics-informed neural networks (PINNs) and DeepONets have shown promise on solving PDE constrained optimization (PDECO) problems. However, existing methods are insufficient to handle those PDE constraints that have a complicated or nonlinear dependency on optimization targets. In this paper, we present a novel bi-level optimization framework to resolve the challenge by decoupling the optimization of the targets and constraints. For the inner loop optimization, we adopt PINNs to solve the PDE constraints only. For the outer loop, we design a novel method by using Broyden's method based on the Implicit Function Theorem (IFT), which is efficient and accurate for approximating hypergradients. We further present theoretical explanations and error analysis of the hypergradients computation. Extensive experiments on multiple large-scale and nonlinear PDE constrained optimization problems demonstrate that our method achieves state-of-the-art results compared with strong baselines."
1594,https://arxiv.org/abs/2209.07074,On the Reuse Bias in Off-Policy Reinforcement Learning,"Importance sampling (IS) is a popular technique in off-policy evaluation, which re-weights the return of trajectories in the replay buffer to boost sample efficiency. However, training with IS can be unstable and previous attempts to address this issue mainly focus on analyzing the variance of IS. In this paper, we reveal that the instability is also related to a new notion of Reuse Bias of IS -- the bias in off-policy evaluation caused by the reuse of the replay buffer for evaluation and optimization. We theoretically show that the off-policy evaluation and optimization of the current policy with the data from the replay buffer result in an overestimation of the objective, which may cause an erroneous gradient update and degenerate the performance. We further provide a high-probability upper bound of the Reuse Bias, and show that controlling one term of the upper bound can control the Reuse Bias by introducing the concept of stability for off-policy algorithms. Based on these analyses, we finally present a novel Bias-Regularized Importance Sampling (BIRIS) framework along with practical algorithms, which can alleviate the negative impact of the Reuse Bias. Experimental results show that our BIRIS-based methods can significantly improve the sample efficiency on a series of continuous control tasks in MuJoCo."
1595,https://arxiv.org/abs/2208.05622,Regret Analysis for Hierarchical Experts Bandit Problem,"We study an extension of standard bandit problem in which there are R layers of experts. Multi-layered experts make selections layer by layer and only the experts in the last layer can play arms. The goal of the learning policy is to minimize the total regret in this hierarchical experts setting. We first analyze the case that total regret grows linearly with the number of layers. Then we focus on the case that all experts are playing Upper Confidence Bound (UCB) strategy and give several sub-linear upper bounds for different circumstances. Finally, we design some experiments to help the regret analysis for the general case of hierarchical UCB structure and show the practical significance of our theoretical results. This article gives many insights about reasonable hierarchical decision structure."
1596,https://arxiv.org/abs/2207.06635,EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic Differential Equations,"Score-based diffusion models (SBDMs) have achieved the SOTA FID results in unpaired image-to-image translation (I2I). However, we notice that existing methods totally ignore the training data in the source domain, leading to sub-optimal solutions for unpaired I2I. To this end, we propose energy-guided stochastic differential equations (EGSDE) that employs an energy function pretrained on both the source and target domains to guide the inference process of a pretrained SDE for realistic and faithful unpaired I2I. Building upon two feature extractors, we carefully design the energy function such that it encourages the transferred image to preserve the domain-independent features and discard domain-specific ones. Further, we provide an alternative explanation of the EGSDE as a product of experts, where each of the three experts (corresponding to the SDE and two feature extractors) solely contributes to faithfulness or realism. Empirically, we compare EGSDE to a large family of baselines on three widely-adopted unpaired I2I tasks under four metrics. EGSDE not only consistently outperforms existing SBDMs-based methods in almost all settings but also achieves the SOTA realism results without harming the faithful performance. Furthermore, EGSDE allows for flexible trade-offs between realism and faithfulness and we improve the realism results further (e.g., FID of 51.04 in Cat to Dog and FID of 50.43 in Wild to Dog on AFHQ) by tuning hyper-parameters. The code is available at https://github.com/ML-GSAI/EGSDE."
1597,https://arxiv.org/abs/2207.06543,CoSCL: Cooperation of Small Continual Learners is Stronger than a Big One,"Continual learning requires incremental compatibility with a sequence of tasks. However, the design of model architecture remains an open question: In general, learning all tasks with a shared set of parameters suffers from severe interference between tasks; while learning each task with a dedicated parameter subspace is limited by scalability. In this work, we theoretically analyze the generalization errors for learning plasticity and memory stability in continual learning, which can be uniformly upper-bounded by (1) discrepancy between task distributions, (2) flatness of loss landscape and (3) cover of parameter space. Then, inspired by the robust biological learning system that processes sequential experiences with multiple parallel compartments, we propose Cooperation of Small Continual Learners (CoSCL) as a general strategy for continual learning. Specifically, we present an architecture with a fixed number of narrower sub-networks to learn all incremental tasks in parallel, which can naturally reduce the two errors through improving the three components of the upper bound. To strengthen this advantage, we encourage to cooperate these sub-networks by penalizing the difference of predictions made by their feature representations. With a fixed parameter budget, CoSCL can improve a variety of representative continual learning approaches by a large margin (e.g., up to 10.64% on CIFAR-100-SC, 9.33% on CIFAR-100-RS, 11.45% on CUB-200-2011 and 6.72% on Tiny-ImageNet) and achieve the new state-of-the-art performance."
1598,https://arxiv.org/abs/2207.05369,Aharonov-Bohm oscillations in bilayer graphene edge state Fabry-Pérot interferometers,"The charge and exchange statistics of an elementary excitation manifest in quantum coherent oscillations that can be explored in interferometry measurements. Quantum Hall interferometers are primary tools to uncover unconventional quantum statistics associated with fractional and non-Abelian anyons of a two-dimensional system, the latter being the foundation of topological quantum computing. Graphene interferometers offer new avenues to explore the physics of exotic excitations due to their relatively small charging energies and sharp confinement potentials. Bilayer graphene possesses a true band gap to facilitate the formation of quantum confinement and exhibits the most robust even-denominator fractional quantum Hall states that may host non-Abelian anyons. Here we present the design and fabrication of a split-gated bilayer graphene Fabry-Pérot interferometer and experimental evidence of Aharonov-Bohm interference at multiple integer quantum Hall states. The versatility of the device allows us to study a wide range of scenarios, determine the velocities of edge states, and assess dephasing mechanisms of the interferometer. These results pave the way to the quest of non-Abelian statistics in this promising device platform."
1599,https://arxiv.org/abs/2206.09150,Thompson Sampling for (Combinatorial) Pure Exploration,"Existing methods of combinatorial pure exploration mainly focus on the UCB approach. To make the algorithm efficient, they usually use the sum of upper confidence bounds within arm set $S$ to represent the upper confidence bound of $S$, which can be much larger than the tight upper confidence bound of $S$ and leads to a much higher complexity than necessary, since the empirical means of different arms in $S$ are independent. To deal with this challenge, we explore the idea of Thompson Sampling (TS) that uses independent random samples instead of the upper confidence bounds, and design the first TS-based algorithm TS-Explore for (combinatorial) pure exploration. In TS-Explore, the sum of independent random samples within arm set $S$ will not exceed the tight upper confidence bound of $S$ with high probability. Hence it solves the above challenge, and achieves a lower complexity upper bound than existing efficient UCB-based algorithms in general combinatorial pure exploration. As for pure exploration of classic multi-armed bandit, we show that TS-Explore achieves an asymptotically optimal complexity upper bound."
1600,https://arxiv.org/abs/2206.07309,Estimating the Optimal Covariance with Imperfect Mean in Diffusion Probabilistic Models,"Diffusion probabilistic models (DPMs) are a class of powerful deep generative models (DGMs). Despite their success, the iterative generation process over the full timesteps is much less efficient than other DGMs such as GANs. Thus, the generation performance on a subset of timesteps is crucial, which is greatly influenced by the covariance design in DPMs. In this work, we consider diagonal and full covariances to improve the expressive power of DPMs. We derive the optimal result for such covariances, and then correct it when the mean of DPMs is imperfect. Both the optimal and the corrected ones can be decomposed into terms of conditional expectations over functions of noise. Building upon it, we propose to estimate the optimal covariance and its correction given imperfect mean by learning these conditional expectations. Our method can be applied to DPMs with both discrete and continuous timesteps. We consider the diagonal covariance in our implementation for computational efficiency. For an efficient practical implementation, we adopt a parameter sharing scheme and a two-stage training process. Empirically, our method outperforms a wide variety of covariance design on likelihood results, and improves the sample quality especially on a small number of timesteps."
1601,https://arxiv.org/abs/2206.04436,Towards Safe Reinforcement Learning via Constraining Conditional Value-at-Risk,"Though deep reinforcement learning (DRL) has obtained substantial success, it may encounter catastrophic failures due to the intrinsic uncertainty of both transition and observation. Most of the existing methods for safe reinforcement learning can only handle transition disturbance or observation disturbance since these two kinds of disturbance affect different parts of the agent; besides, the popular worst-case return may lead to overly pessimistic policies. To address these issues, we first theoretically prove that the performance degradation under transition disturbance and observation disturbance depends on a novel metric of Value Function Range (VFR), which corresponds to the gap in the value function between the best state and the worst state. Based on the analysis, we adopt conditional value-at-risk (CVaR) as an assessment of risk and propose a novel reinforcement learning algorithm of CVaR-Proximal-Policy-Optimization (CPPO) which formalizes the risk-sensitive constrained optimization problem by keeping its CVaR under a given threshold. Experimental results show that CPPO achieves a higher cumulative reward and is more robust against both observation and transition disturbances on a series of continuous control tasks in MuJoCo."
1602,https://arxiv.org/abs/2206.04372,Diagnosing Ensemble Few-Shot Classifiers,"The base learners and labeled samples (shots) in an ensemble few-shot classifier greatly affect the model performance. When the performance is not satisfactory, it is usually difficult to understand the underlying causes and make improvements. To tackle this issue, we propose a visual analysis method, FSLDiagnotor. Given a set of base learners and a collection of samples with a few shots, we consider two problems: 1) finding a subset of base learners that well predict the sample collections; and 2) replacing the low-quality shots with more representative ones to adequately represent the sample collections. We formulate both problems as sparse subset selection and develop two selection algorithms to recommend appropriate learners and shots, respectively. A matrix visualization and a scatterplot are combined to explain the recommended learners and shots in context and facilitate users in adjusting them. Based on the adjustment, the algorithm updates the recommendation results for another round of improvement. Two case studies are conducted to demonstrate that FSLDiagnotor helps build a few-shot classifier efficiently and increases the accuracy by 12% and 21%, respectively."
1603,https://arxiv.org/abs/2206.04310,GSmooth: Certified Robustness against Semantic Transformations via Generalized Randomized Smoothing,"Certified defenses such as randomized smoothing have shown promise towards building reliable machine learning systems against $\ell_p$-norm bounded attacks. However, existing methods are insufficient or unable to provably defend against semantic transformations, especially those without closed-form expressions (such as defocus blur and pixelate), which are more common in practice and often unrestricted. To fill up this gap, we propose generalized randomized smoothing (GSmooth), a unified theoretical framework for certifying robustness against general semantic transformations via a novel dimension augmentation strategy. Under the GSmooth framework, we present a scalable algorithm that uses a surrogate image-to-image network to approximate the complex transformation. The surrogate model provides a powerful tool for studying the properties of semantic transformations and certifying robustness. Experimental results on several datasets demonstrate the effectiveness of our approach for robustness certification against multiple kinds of semantic transformations and corruptions, which is not achievable by the alternative baselines."
1604,https://arxiv.org/abs/2206.02782,Towards Job-Transition-Tag Graph for a Better Job Title Representation Learning,"Works on learning job title representation are mainly based on \textit{Job-Transition Graph}, built from the working history of talents. However, since these records are usually messy, this graph is very sparse, which affects the quality of the learned representation and hinders further analysis. To address this specific issue, we propose to enrich the graph with additional nodes that improve the quality of job title representation. Specifically, we construct \textit{Job-Transition-Tag Graph}, a heterogeneous graph containing two types of nodes, i.e., job titles and tags (i.e., words related to job responsibilities or functionalities). Along this line, we reformulate job title representation learning as the task of learning node embedding on the \textit{Job-Transition-Tag Graph}. Experiments on two datasets show the interest of our approach."
1605,https://arxiv.org/abs/2205.14497,BadDet: Backdoor Attacks on Object Detection,"Deep learning models have been deployed in numerous real-world applications such as autonomous driving and surveillance. However, these models are vulnerable in adversarial environments. Backdoor attack is emerging as a severe security threat which injects a backdoor trigger into a small portion of training data such that the trained model behaves normally on benign inputs but gives incorrect predictions when the specific trigger appears. While most research in backdoor attacks focuses on image classification, backdoor attacks on object detection have not been explored but are of equal importance. Object detection has been adopted as an important module in various security-sensitive applications such as autonomous driving. Therefore, backdoor attacks on object detection could pose severe threats to human lives and properties. We propose four kinds of backdoor attacks for object detection task: 1) Object Generation Attack: a trigger can falsely generate an object of the target class; 2) Regional Misclassification Attack: a trigger can change the prediction of a surrounding object to the target class; 3) Global Misclassification Attack: a single trigger can change the predictions of all objects in an image to the target class; and 4) Object Disappearance Attack: a trigger can make the detector fail to detect the object of the target class. We develop appropriate metrics to evaluate the four backdoor attacks on object detection. We perform experiments using two typical object detection models -- Faster-RCNN and YOLOv3 on different datasets. More crucially, we demonstrate that even fine-tuning on another benign dataset cannot remove the backdoor hidden in the object detection model. To defend against these backdoor attacks, we propose Detector Cleanse, an entropy-based run-time detection framework to identify poisoned testing samples for any deployed object detector."
1606,https://arxiv.org/abs/2205.13496,Censored Quantile Regression Neural Networks for Distribution-Free Survival Analysis,"This paper considers doing quantile regression on censored data using neural networks (NNs). This adds to the survival analysis toolkit by allowing direct prediction of the target variable, along with a distribution-free characterisation of uncertainty, using a flexible function approximator. We begin by showing how an algorithm popular in linear models can be applied to NNs. However, the resulting procedure is inefficient, requiring sequential optimisation of an individual NN at each desired quantile. Our major contribution is a novel algorithm that simultaneously optimises a grid of quantiles output by a single NN. To offer theoretical insight into our algorithm, we show firstly that it can be interpreted as a form of expectation-maximisation, and secondly that it exhibits a desirable `self-correcting' property. Experimentally, the algorithm produces quantiles that are better calibrated than existing methods on 10 out of 12 real datasets."
1607,https://arxiv.org/abs/2205.10772,Fast Instrument Learning with Faster Rates,"We investigate nonlinear instrumental variable (IV) regression given high-dimensional instruments. We propose a simple algorithm which combines kernelized IV methods and an arbitrary, adaptive regression algorithm, accessed as a black box. Our algorithm enjoys faster-rate convergence and adapts to the dimensionality of informative latent features, while avoiding an expensive minimax optimization procedure, which has been necessary to establish similar guarantees. It further brings the benefit of flexible machine learning models to quasi-Bayesian uncertainty quantification, likelihood-based model selection, and model averaging. Simulation studies demonstrate the competitive performance of our method."
1608,https://arxiv.org/abs/2205.02806,Proximity-Induced Superconductivity in Epitaxial Topological Insulator/Graphene/Gallium Heterostructures,"The introduction of superconductivity to the Dirac surface states of a topological insulator leads to a topological superconductor, which may support topological quantum computing through Majorana zero modes. The development of a scalable material platform is key to the realization of topological quantum computing. Here we report on the growth and properties of high-quality (Bi,Sb)2Te3/graphene/gallium heterostructures. Our synthetic approach enables atomically sharp layers at both hetero-interfaces, which in turn promotes proximity-induced superconductivity that originates in the gallium film. A lithography-free, van der Waals tunnel junction is developed to perform transport tunneling spectroscopy. We find a robust, proximity-induced superconducting gap formed in the Dirac surface states in 5-10 quintuple-layer (Bi,Sb)2Te3/graphene/gallium heterostructures. The presence of a single Abrikosov vortex, where the Majorana zero modes are expected to reside, manifests in discrete conductance changes. The present material platform opens up opportunities for understanding and harnessing the application potential of topological superconductivity."
1609,https://arxiv.org/abs/2205.00165,NeuralEF: Deconstructing Kernels by Deep Neural Networks,"Learning the principal eigenfunctions of an integral operator defined by a kernel and a data distribution is at the core of many machine learning problems. Traditional nonparametric solutions based on the Nystr{ö}m formula suffer from scalability issues. Recent work has resorted to a parametric approach, i.e., training neural networks to approximate the eigenfunctions. However, the existing method relies on an expensive orthogonalization step and is difficult to implement. We show that these problems can be fixed by using a new series of objective functions that generalizes the EigenGame~\citep{gemp2020eigengame} to function space. We test our method on a variety of supervised and unsupervised learning problems and show it provides accurate approximations to the eigenfunctions of polynomial, radial basis, neural network Gaussian process, and neural tangent kernels. Finally, we demonstrate our method can scale up linearised Laplace approximation of deep neural networks to modern image classification datasets through approximating the Gauss-Newton matrix. Code is available at \url{https://github.com/thudzj/neuraleigenfunction}."
1610,https://arxiv.org/abs/2203.06587,Policy Learning for Robust Markov Decision Process with a Mismatched Generative Model,"In high-stake scenarios like medical treatment and auto-piloting, it's risky or even infeasible to collect online experimental data to train the agent. Simulation-based training can alleviate this issue, but may suffer from its inherent mismatches from the simulator and real environment. It is therefore imperative to utilize the simulator to learn a robust policy for the real-world deployment. In this work, we consider policy learning for Robust Markov Decision Processes (RMDP), where the agent tries to seek a robust policy with respect to unexpected perturbations on the environments. Specifically, we focus on the setting where the training environment can be characterized as a generative model and a constrained perturbation can be added to the model during testing. Our goal is to identify a near-optimal robust policy for the perturbed testing environment, which introduces additional technical difficulties as we need to simultaneously estimate the training environment uncertainty from samples and find the worst-case perturbation for testing. To solve this issue, we propose a generic method which formalizes the perturbation as an opponent to obtain a two-player zero-sum game, and further show that the Nash Equilibrium corresponds to the robust policy. We prove that, with a polynomial number of samples from the generative model, our algorithm can find a near-optimal robust policy with a high probability. Our method is able to deal with general perturbations under some mild assumptions and can also be extended to more complex problems like robust partial observable Markov decision process, thanks to the game-theoretical formulation."
1611,https://arxiv.org/abs/2203.06560,Query-Efficient Black-box Adversarial Attacks Guided by a Transfer-based Prior,"Adversarial attacks have been extensively studied in recent years since they can identify the vulnerability of deep learning models before deployed. In this paper, we consider the black-box adversarial setting, where the adversary needs to craft adversarial examples without access to the gradients of a target model. Previous methods attempted to approximate the true gradient either by using the transfer gradient of a surrogate white-box model or based on the feedback of model queries. However, the existing methods inevitably suffer from low attack success rates or poor query efficiency since it is difficult to estimate the gradient in a high-dimensional input space with limited information. To address these problems and improve black-box attacks, we propose two prior-guided random gradient-free (PRGF) algorithms based on biased sampling and gradient averaging, respectively. Our methods can take the advantage of a transfer-based prior given by the gradient of a surrogate model and the query information simultaneously. Through theoretical analyses, the transfer-based prior is appropriately integrated with model queries by an optimal coefficient in each method. Extensive experiments demonstrate that, in comparison with the alternative state-of-the-arts, both of our methods require much fewer queries to attack black-box models with higher success rates."
1612,https://arxiv.org/abs/2203.04623,Controllable Evaluation and Generation of Physical Adversarial Patch on Face Recognition,"Recent studies have revealed the vulnerability of face recognition models against physical adversarial patches, which raises security concerns about the deployed face recognition systems. However, it is still challenging to ensure the reproducibility for most attack algorithms under complex physical conditions, which leads to the lack of a systematic evaluation of the existing methods. It is therefore imperative to develop a framework that can enable a comprehensive evaluation of the vulnerability of face recognition in the physical world. To this end, we propose to simulate the complex transformations of faces in the physical world via 3D-face modeling, which serves as a digital counterpart of physical faces. The generic framework allows us to control different face variations and physical conditions to conduct reproducible evaluations comprehensively. With this digital simulator, we further propose a Face3DAdv method considering the 3D face transformations and realistic physical variations. Extensive experiments validate that Face3DAdv can significantly improve the effectiveness of diverse physically realizable adversarial patches in both simulated and physical environments, against various white-box and black-box face recognition models."
1613,https://arxiv.org/abs/2203.03605,DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection,"We present DINO (\textbf{D}ETR with \textbf{I}mproved de\textbf{N}oising anch\textbf{O}r boxes), a state-of-the-art end-to-end object detector. % in this paper. DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction. DINO achieves $49.4$AP in $12$ epochs and $51.3$AP in $24$ epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a significant improvement of $\textbf{+6.0}$\textbf{AP} and $\textbf{+2.7}$\textbf{AP}, respectively, compared to DN-DETR, the previous best DETR-like model. DINO scales well in both model size and data size. Without bells and whistles, after pre-training on the Objects365 dataset with a SwinL backbone, DINO obtains the best results on both COCO \texttt{val2017} ($\textbf{63.2}$\textbf{AP}) and \texttt{test-dev} (\textbf{$\textbf{63.3}$AP}). Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results. Our code will be available at \url{https://github.com/IDEACVR/DINO}."
1614,https://arxiv.org/abs/2202.10103,Robustness and Accuracy Could Be Reconcilable by (Proper) Definition,"The trade-off between robustness and accuracy has been widely studied in the adversarial literature. Although still controversial, the prevailing view is that this trade-off is inherent, either empirically or theoretically. Thus, we dig for the origin of this trade-off in adversarial training and find that it may stem from the improperly defined robust error, which imposes an inductive bias of local invariance -- an overcorrection towards smoothness. Given this, we advocate employing local equivariance to describe the ideal behavior of a robust model, leading to a self-consistent robust error named SCORE. By definition, SCORE facilitates the reconciliation between robustness and accuracy, while still handling the worst-case uncertainty via robust optimization. By simply substituting KL divergence with variants of distance metrics, SCORE can be efficiently minimized. Empirically, our models achieve top-rank performance on RobustBench under AutoAttack. Besides, SCORE provides instructive insights for explaining the overfitting phenomenon and semantic input gradients observed on robust models. Code is available at https://github.com/P2333/SCORE."
1615,https://arxiv.org/abs/2202.06592,Memory Replay with Data Compression for Continual Learning,"Continual learning needs to overcome catastrophic forgetting of the past. Memory replay of representative old training samples has been shown as an effective solution, and achieves the state-of-the-art (SOTA) performance. However, existing work is mainly built on a small memory buffer containing a few original data, which cannot fully characterize the old data distribution. In this work, we propose memory replay with data compression (MRDC) to reduce the storage cost of old training samples and thus increase their amount that can be stored in the memory buffer. Observing that the trade-off between the quality and quantity of compressed data is highly nontrivial for the efficacy of memory replay, we propose a novel method based on determinantal point processes (DPPs) to efficiently determine an appropriate compression quality for currently-arrived training samples. In this way, using a naive data compression algorithm with a properly selected quality can largely boost recent strong baselines by saving more compressed data in a limited storage space. We extensively validate this across several benchmarks of class-incremental learning and in a realistic scenario of object detection for autonomous driving."
1616,https://arxiv.org/abs/2201.12329,DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR,"We present in this paper a novel query formulation using dynamic anchor boxes for DETR (DEtection TRansformer) and offer a deeper understanding of the role of queries in DETR. This new formulation directly uses box coordinates as queries in Transformer decoders and dynamically updates them layer-by-layer. Using box coordinates not only helps using explicit positional priors to improve the query-to-feature similarity and eliminate the slow training convergence issue in DETR, but also allows us to modulate the positional attention map using the box width and height information. Such a design makes it clear that queries in DETR can be implemented as performing soft ROI pooling layer-by-layer in a cascade manner. As a result, it leads to the best performance on MS-COCO benchmark among the DETR-like detection models under the same setting, e.g., AP 45.7\% using ResNet50-DC5 as backbone trained in 50 epochs. We also conducted extensive experiments to confirm our analysis and verify the effectiveness of our methods. Code is available at \url{https://github.com/SlongLiu/DAB-DETR}."
1617,https://arxiv.org/abs/2201.06503,Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models,"Diffusion probabilistic models (DPMs) represent a class of powerful generative models. Despite their success, the inference of DPMs is expensive since it generally needs to iterate over thousands of timesteps. A key problem in the inference is to estimate the variance in each timestep of the reverse process. In this work, we present a surprising result that both the optimal reverse variance and the corresponding optimal KL divergence of a DPM have analytic forms w.r.t. its score function. Building upon it, we propose Analytic-DPM, a training-free inference framework that estimates the analytic forms of the variance and KL divergence using the Monte Carlo method and a pretrained score-based model. Further, to correct the potential bias caused by the score-based model, we derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, our analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and meanwhile enjoys a 20x to 80x speed up."
1618,https://arxiv.org/abs/2201.06350,Megahertz-rate Ultrafast X-ray Scattering and Holographic Imaging at the European XFEL,"The advent of X-ray free-electron lasers (XFELs) has revolutionized fundamental science, from atomic to condensed matter physics, from chemistry to biology, giving researchers access to X-rays with unprecedented brightness, coherence, and pulse duration. All XFEL facilities built until recently provided X-ray pulses at a relatively low repetition rate, with limited data statistics. Here, we present the results from the first megahertz repetition rate X-ray scattering experiments at the Spectroscopy and Coherent Scattering (SCS) instrument of the European XFEL. We illustrate the experimental capabilities that the SCS instrument offers, resulting from the operation at MHz repetition rates and the availability of the novel DSSC 2D imaging detector. Time-resolved magnetic X-ray scattering and holographic imaging experiments in solid state samples were chosen as representative, providing an ideal test-bed for operation at megahertz rates. Our results are relevant and applicable to any other non-destructive XFEL experiments in the soft X-ray range."
1619,https://arxiv.org/abs/2201.05769,Mixed Diagnostics for Longitudinal Properties of Electron Bunches in a Free-Electron Laser,"Longitudinal properties of electron bunches are critical for the performance of a wide range of scientific facilities. In a free-electron laser, for example, the existing diagnostics only provide very limited longitudinal information of the electron bunch during online tuning and optimization. We leverage the power of artificial intelligence to build a neural network model using experimental data, in order to bring the destructive longitudinal phase space (LPS) diagnostics online virtually and improve the existing current profile online diagnostics which uses a coherent transition radiation (CTR) spectrometer. The model can also serve as a digital twin of the real machine on which algorithms can be tested efficiently and effectively. We demonstrate at the FLASH facility that the encoder-decoder model with more than one decoder can make highly accurate predictions of megapixel LPS images and coherent transition radiation spectra concurrently for electron bunches in a bunch train with broad ranges of LPS shapes and peak currents, which are obtained by scanning all the major control knobs for LPS manipulation. Furthermore, we propose a way to significantly improve the CTR spectrometer online measurement by combining the predicted and measured spectra. Our work showcases how to combine virtual and real diagnostics in order to provide heterogeneous and reliable mixed diagnostics for scientific facilities."
1620,https://arxiv.org/abs/2111.15086,Scalable Semiparametric Spatio-temporal Regression for Large Data Analysis,"With the rapid advances of data acquisition techniques, spatio-temporal data are becoming increasingly abundant in a diverse array of disciplines. Here we develop spatio-temporal regression methodology for analyzing large amounts of spatially referenced data collected over time, motivated by environmental studies utilizing remotely sensed satellite data. In particular, we specify a semiparametric autoregressive model without the usual Gaussian assumption and devise a computationally scalable procedure that enables the regression analysis of large datasets. We estimate the model parameters by quasi maximum likelihood and show that the computational complexity can be reduced from cubic to linear of the sample size. Asymptotic properties under suitable regularity conditions are further established that inform the computational procedure to be efficient and scalable. A simulation study is conducted to evaluate the finite-sample properties of the parameter estimation and statistical inference. We illustrate our methodology by a dataset with over 2.96 million observations of annual land surface temperature and the comparison with an existing state-of-the-art approach highlights the advantages of our method."
1621,https://arxiv.org/abs/2111.11296,Improving Next-Application Prediction with Deep Personalized-Attention Neural Network,"Recently, due to the ubiquity and supremacy of E-recruitment platforms, job recommender systems have been largely studied. In this paper, we tackle the next job application problem, which has many practical applications. In particular, we propose to leverage next-item recommendation approaches to consider better the job seeker's career preference to discover the next relevant job postings (referred to jobs for short) they might apply for. Our proposed model, named Personalized-Attention Next-Application Prediction (PANAP), is composed of three modules. The first module learns job representations from textual content and metadata attributes in an unsupervised way. The second module learns job seeker representations. It includes a personalized-attention mechanism that can adapt the importance of each job in the learned career preference representation to the specific job seeker's profile. The attention mechanism also brings some interpretability to learned representations. Then, the third module models the Next-Application Prediction task as a top-K search process based on the similarity of representations. In addition, the geographic location is an essential factor that affects the preferences of job seekers in the recruitment domain. Therefore, we explore the influence of geographic location on the model performance from the perspective of negative sampling strategies. Experiments on the public CareerBuilder12 dataset show the interest in our approach."
1622,https://arxiv.org/abs/2110.12187,AFEC: Active Forgetting of Negative Transfer in Continual Learning,"Continual learning aims to learn a sequence of tasks from dynamic data distributions. Without accessing to the old training samples, knowledge transfer from the old tasks to each new task is difficult to determine, which might be either positive or negative. If the old knowledge interferes with the learning of a new task, i.e., the forward knowledge transfer is negative, then precisely remembering the old tasks will further aggravate the interference, thus decreasing the performance of continual learning. By contrast, biological neural networks can actively forget the old knowledge that conflicts with the learning of a new experience, through regulating the learning-triggered synaptic expansion and synaptic convergence. Inspired by the biological active forgetting, we propose to actively forget the old knowledge that limits the learning of new tasks to benefit continual learning. Under the framework of Bayesian continual learning, we develop a novel approach named Active Forgetting with synaptic Expansion-Convergence (AFEC). Our method dynamically expands parameters to learn each new task and then selectively combines them, which is formally consistent with the underlying mechanism of biological active forgetting. We extensively evaluate AFEC on a variety of continual learning benchmarks, including CIFAR-10 regression tasks, visual classification tasks and Atari reinforcement tasks, where AFEC effectively improves the learning of new tasks and achieves the state-of-the-art performance in a plug-and-play way."
1623,https://arxiv.org/abs/2110.09903,Unrestricted Adversarial Attacks on ImageNet Competition,"Many works have investigated the adversarial attacks or defenses under the settings where a bounded and imperceptible perturbation can be added to the input. However in the real-world, the attacker does not need to comply with this restriction. In fact, more threats to the deep model come from unrestricted adversarial examples, that is, the attacker makes large and visible modifications on the image, which causes the model classifying mistakenly, but does not affect the normal observation in human perspective. Unrestricted adversarial attack is a popular and practical direction but has not been studied thoroughly. We organize this competition with the purpose of exploring more effective unrestricted adversarial attack algorithm, so as to accelerate the academical research on the model robustness under stronger unbounded attacks. The competition is held on the TianChi platform (\url{https://tianchi.aliyun.com/competition/entrance/531853/introduction}) as one of the series of AI Security Challengers Program."
1624,https://arxiv.org/abs/2110.08256,Model-Agnostic Meta-Attack: Towards Reliable Evaluation of Adversarial Robustness,"The vulnerability of deep neural networks to adversarial examples has motivated an increasing number of defense strategies for promoting model robustness. However, the progress is usually hampered by insufficient robustness evaluations. As the de facto standard to evaluate adversarial robustness, adversarial attacks typically solve an optimization problem of crafting adversarial examples with an iterative process. In this work, we propose a Model-Agnostic Meta-Attack (MAMA) approach to discover stronger attack algorithms automatically. Our method learns the optimizer in adversarial attacks parameterized by a recurrent neural network, which is trained over a class of data samples and defenses to produce effective update directions during adversarial example generation. Furthermore, we develop a model-agnostic training algorithm to improve the generalization ability of the learned optimizer when attacking unseen defenses. Our approach can be flexibly incorporated with various attacks and consistently improves the performance with little extra computational cost. Extensive experiments demonstrate the effectiveness of the learned attacks by MAMA compared to the state-of-the-art attacks on different defenses, leading to a more reliable evaluation of adversarial robustness."
1625,https://arxiv.org/abs/2110.08042,Adversarial Attacks on ML Defense Models Competition,"Due to the vulnerability of deep neural networks (DNNs) to adversarial examples, a large number of defense techniques have been proposed to alleviate this problem in recent years. However, the progress of building more robust models is usually hampered by the incomplete or incorrect robustness evaluation. To accelerate the research on reliable evaluation of adversarial robustness of the current defense models in image classification, the TSAIL group at Tsinghua University and the Alibaba Security group organized this competition along with a CVPR 2021 workshop on adversarial machine learning (https://aisecure-workshop.github.io/amlcvpr2021/). The purpose of this competition is to motivate novel attack algorithms to evaluate adversarial robustness more effectively and reliably. The participants were encouraged to develop stronger white-box attack algorithms to find the worst-case robustness of different defenses. This competition was conducted on an adversarial robustness evaluation platform -- ARES (https://github.com/thu-ml/ares), and is held on the TianChi platform (https://tianchi.aliyun.com/competition/entrance/531847/introduction) as one of the series of AI Security Challengers Program. After the competition, we summarized the results and established a new adversarial robustness benchmark at https://ml.cs.tsinghua.edu.cn/ares-bench/, which allows users to upload adversarial attack algorithms and defense models for evaluation."
1626,https://arxiv.org/abs/2109.15009,Adversarial Semantic Contour for Object Detection,"Modern object detectors are vulnerable to adversarial examples, which brings potential risks to numerous applications, e.g., self-driving car. Among attacks regularized by $\ell_p$ norm, $\ell_0$-attack aims to modify as few pixels as possible. Nevertheless, the problem is nontrivial since it generally requires to optimize the shape along with the texture simultaneously, which is an NP-hard problem. To address this issue, we propose a novel method of Adversarial Semantic Contour (ASC) guided by object contour as prior. With this prior, we reduce the searching space to accelerate the $\ell_0$ optimization, and also introduce more semantic information which should affect the detectors more. Based on the contour, we optimize the selection of modified pixels via sampling and their colors with gradient descent alternately. Extensive experiments demonstrate that our proposed ASC outperforms the most commonly manually designed patterns (e.g., square patches and grids) on task of disappearing. By modifying no more than 5\% and 3.5\% of the object area respectively, our proposed ASC can successfully mislead the mainstream object detectors including the SSD512, Yolov4, Mask RCNN, Faster RCNN, etc."
1627,https://arxiv.org/abs/2109.13069,Cluster Attack: Query-based Adversarial Attacks on Graphs with Graph-Dependent Priors,"While deep neural networks have achieved great success in graph analysis, recent work has shown that they are vulnerable to adversarial attacks. Compared with adversarial attacks on image classification, performing adversarial attacks on graphs is more challenging because of the discrete and non-differential nature of the adjacent matrix for a graph. In this work, we propose Cluster Attack -- a Graph Injection Attack (GIA) on node classification, which injects fake nodes into the original graph to degenerate the performance of graph neural networks (GNNs) on certain victim nodes while affecting the other nodes as little as possible. We demonstrate that a GIA problem can be equivalently formulated as a graph clustering problem; thus, the discrete optimization problem of the adjacency matrix can be solved in the context of graph clustering. In particular, we propose to measure the similarity between victim nodes by a metric of Adversarial Vulnerability, which is related to how the victim nodes will be affected by the injected fake node, and to cluster the victim nodes accordingly. Our attack is performed in a practical and unnoticeable query-based black-box manner with only a few nodes on the graphs that can be accessed. Theoretical analysis and extensive experiments demonstrate the effectiveness of our method by fooling the node classifiers with only a small number of queries."
1628,https://arxiv.org/abs/2107.14171,Tianshou: a Highly Modularized Deep Reinforcement Learning Library,"In this paper, we present Tianshou, a highly modularized Python library for deep reinforcement learning (DRL) that uses PyTorch as its backend. Tianshou intends to be research-friendly by providing a flexible and reliable infrastructure of DRL algorithms. It supports online and offline training with more than 20 classic algorithms through a unified interface. To facilitate related research and prove Tianshou's reliability, we have released Tianshou's benchmark of MuJoCo environments, covering eight classic algorithms with state-of-the-art performance. We open-sourced Tianshou at https://github.com/thu-ml/tianshou/."
1629,https://arxiv.org/abs/2107.10834,Query2Label: A Simple Transformer Way to Multi-Label Classification,"This paper presents a simple and effective approach to solving the multi-label classification problem. The proposed approach leverages Transformer decoders to query the existence of a class label. The use of Transformer is rooted in the need of extracting local discriminative features adaptively for different labels, which is a strongly desired property due to the existence of multiple objects in one image. The built-in cross-attention module in the Transformer decoder offers an effective way to use label embeddings as queries to probe and pool class-related features from a feature map computed by a vision backbone for subsequent binary classifications. Compared with prior works, the new framework is simple, using standard Transformers and vision backbones, and effective, consistently outperforming all previous works on five multi-label classification data sets, including MS-COCO, PASCAL VOC, NUS-WIDE, and Visual Genome. Particularly, we establish $91.3\%$ mAP on MS-COCO. We hope its compact structure, simple implementation, and superior performance serve as a strong baseline for multi-label classification tasks and future studies. The code will be available soon at https://github.com/SlongLiu/query2labels."
1630,https://arxiv.org/abs/2107.10110,On the Convergence of Prior-Guided Zeroth-Order Optimization Algorithms,"Zeroth-order (ZO) optimization is widely used to handle challenging tasks, such as query-based black-box adversarial attacks and reinforcement learning. Various attempts have been made to integrate prior information into the gradient estimation procedure based on finite differences, with promising empirical results. However, their convergence properties are not well understood. This paper makes an attempt to fill up this gap by analyzing the convergence of prior-guided ZO algorithms under a greedy descent framework with various gradient estimators. We provide a convergence guarantee for the prior-guided random gradient-free (PRGF) algorithms. Moreover, to further accelerate over greedy descent methods, we present a new accelerated random search (ARS) algorithm that incorporates prior information, together with a convergence analysis. Finally, our theoretical results are confirmed by experiments on several numerical benchmarks as well as adversarial attacks."
1631,https://arxiv.org/abs/2107.01809,Boosting Transferability of Targeted Adversarial Examples via Hierarchical Generative Networks,"Transfer-based adversarial attacks can evaluate model robustness in the black-box setting. Several methods have demonstrated impressive untargeted transferability, however, it is still challenging to efficiently produce targeted transferability. To this end, we develop a simple yet effective framework to craft targeted transfer-based adversarial examples, applying a hierarchical generative network. In particular, we contribute to amortized designs that well adapt to multi-class targeted attacks. Extensive experiments on ImageNet show that our method improves the success rates of targeted black-box attacks by a significant margin over the existing methods -- it reaches an average success rate of 29.1\% against six diverse models based only on one substitute white-box model, which significantly outperforms the state-of-the-art gradient-based attack methods. Moreover, the proposed method is also more efficient beyond an order of magnitude than gradient-based methods."
1632,https://arxiv.org/abs/2106.15860,Understanding Adversarial Attacks on Observations in Deep Reinforcement Learning,"Deep reinforcement learning models are vulnerable to adversarial attacks that can decrease a victim's cumulative expected reward by manipulating the victim's observations. Despite the efficiency of previous optimization-based methods for generating adversarial noise in supervised learning, such methods might not be able to achieve the lowest cumulative reward since they do not explore the environmental dynamics in general. In this paper, we provide a framework to better understand the existing methods by reformulating the problem of adversarial attacks on reinforcement learning in the function space. Our reformulation generates an optimal adversary in the function space of the targeted attacks, repelling them via a generic two-stage framework. In the first stage, we train a deceptive policy by hacking the environment, and discover a set of trajectories routing to the lowest reward or the worst-case performance. Next, the adversary misleads the victim to imitate the deceptive policy by perturbing the observations. Compared to existing approaches, we theoretically show that our adversary is stronger under an appropriate noise level. Extensive experiments demonstrate our method's superiority in terms of efficiency and effectiveness, achieving the state-of-the-art performance in both Atari and MuJoCo environments."
1633,https://arxiv.org/abs/2106.15128,Regularized OFU: an Efficient UCB Estimator forNon-linear Contextual Bandit,"Balancing exploration and exploitation (EE) is a fundamental problem in contex-tual bandit. One powerful principle for EE trade-off isOptimism in Face of Uncer-tainty(OFU), in which the agent takes the action according to an upper confidencebound (UCB) of reward. OFU has achieved (near-)optimal regret bound for lin-ear/kernel contextual bandits. However, it is in general unknown how to deriveefficient and effective EE trade-off methods for non-linearcomplex tasks, suchas contextual bandit with deep neural network as the reward function. In thispaper, we propose a novel OFU algorithm namedregularized OFU(ROFU). InROFU, we measure the uncertainty of the reward by a differentiable function andcompute the upper confidence bound by solving a regularized optimization prob-lem. We prove that, for multi-armed bandit, kernel contextual bandit and neuraltangent kernel bandit, ROFU achieves (near-)optimal regret bounds with certainuncertainty measure, which theoretically justifies its effectiveness on EE trade-off.Importantly, ROFU admits a very efficient implementation with gradient-basedoptimizer, which easily extends to general deep neural network models beyondneural tangent kernel, in sharp contrast with previous OFU methods. The em-pirical evaluation demonstrates that ROFU works extremelywell for contextualbandits under various settings."
1634,https://arxiv.org/abs/2106.15058,Improving Transferability of Adversarial Patches on Face Recognition with Generative Models,"Face recognition is greatly improved by deep convolutional neural networks (CNNs). Recently, these face recognition models have been used for identity authentication in security sensitive applications. However, deep CNNs are vulnerable to adversarial patches, which are physically realizable and stealthy, raising new security concerns on the real-world applications of these models. In this paper, we evaluate the robustness of face recognition models using adversarial patches based on transferability, where the attacker has limited accessibility to the target models. First, we extend the existing transfer-based attack techniques to generate transferable adversarial patches. However, we observe that the transferability is sensitive to initialization and degrades when the perturbation magnitude is large, indicating the overfitting to the substitute models. Second, we propose to regularize the adversarial patches on the low dimensional data manifold. The manifold is represented by generative models pre-trained on legitimate human face images. Using face-like features as adversarial perturbations through optimization on the manifold, we show that the gaps between the responses of substitute models and the target models dramatically decrease, exhibiting a better transferability. Extensive digital world experiments are conducted to demonstrate the superiority of the proposed method in the black-box setting. We apply the proposed method in the physical world as well."
1635,https://arxiv.org/abs/2106.09993,Accumulative Poisoning Attacks on Real-time Data,"Collecting training data from untrusted sources exposes machine learning services to poisoning adversaries, who maliciously manipulate training data to degrade the model accuracy. When trained on offline datasets, poisoning adversaries have to inject the poisoned data in advance before training, and the order of feeding these poisoned batches into the model is stochastic. In contrast, practical systems are more usually trained/fine-tuned on sequentially captured real-time data, in which case poisoning adversaries could dynamically poison each data batch according to the current model state. In this paper, we focus on the real-time settings and propose a new attacking strategy, which affiliates an accumulative phase with poisoning attacks to secretly (i.e., without affecting accuracy) magnify the destructive effect of a (poisoned) trigger batch. By mimicking online learning and federated learning on MNIST and CIFAR-10, we show that model accuracy significantly drops by a single update step on the trigger batch after the accumulative phase. Our work validates that a well-designed but straightforward attacking strategy can dramatically amplify the poisoning effects, with no need to explore complex techniques."
1636,https://arxiv.org/abs/2106.08750,Quasi-Bayesian Dual Instrumental Variable Regression,"Recent years have witnessed an upsurge of interest in employing flexible machine learning models for instrumental variable (IV) regression, but the development of uncertainty quantification methodology is still lacking. In this work we present a novel quasi-Bayesian procedure for IV regression, building upon the recently developed kernelized IV models and the dual/minimax formulation of IV regression. We analyze the frequentist behavior of the proposed method, by establishing minimax optimal contraction rates in $L_2$ and Sobolev norms, and discussing the frequentist validity of credible balls. We further derive a scalable inference algorithm which can be extended to work with wide neural network models. Empirical evaluation shows that our method produces informative uncertainty estimates on complex high-dimensional problems."
1637,https://arxiv.org/abs/2210.08731,A High Fidelity Simulation Framework for Potential Safety Benefits Estimation of Cooperative Pedestrian Perception,"This paper proposes a high-fidelity simulation framework that can estimate the potential safety benefits of vehicle-to-infrastructure (V2I) pedestrian safety strategies. This simulator can support cooperative perception algorithms in the loop by simulating the environmental conditions, traffic conditions, and pedestrian characteristics at the same time. Besides, the benefit estimation model applied in our framework can systematically quantify both the risk conflict (non-crash condition) and the severity of the pedestrian's injuries (crash condition). An experiment was conducted in this paper that built a digital twin of a crowded urban intersection in China. The result shows that our framework is efficient for safety benefit estimation of V2I pedestrian safety strategies."
1638,https://arxiv.org/abs/2103.08259,The QXS-SAROPT Dataset for Deep Learning in SAR-Optical Data Fusion,"Deep learning techniques have made an increasing impact on the field of remote sensing. However, deep neural networks based fusion of multimodal data from different remote sensors with heterogenous characteristics has not been fully explored, due to the lack of availability of big amounts of perfectly aligned multi-sensor image data with diverse scenes of high resolutions, especially for synthetic aperture radar (SAR) data and optical imagery. To promote the development of deep learning based SAR-optical fusion approaches, we release the QXS-SAROPT dataset, which contains 20,000 pairs of SAR-optical image patches. We obtain the SAR patches from SAR satellite GaoFen-3 images and the optical patches from Google Earth images. These images cover three port cities: San Diego, Shanghai and Qingdao. Here, we present a detailed introduction of the construction of the dataset, and show its two representative exemplary applications, namely SAR-optical image matching and SAR ship detection boosted by cross-modal information from optical images. As a large open SAR-optical dataset with multiple scenes of a high resolution, we believe QXS-SAROPT will be of potential value for further research in SAR-optical data fusion technology based on deep learning."
1639,https://arxiv.org/abs/2103.08251,Boosting ship detection in SAR images with complementary pretraining techniques,"Deep learning methods have made significant progress in ship detection in synthetic aperture radar (SAR) images. The pretraining technique is usually adopted to support deep neural networks-based SAR ship detectors due to the scarce labeled SAR images. However, directly leveraging ImageNet pretraining is hardly to obtain a good ship detector because of different imaging perspective and geometry. In this paper, to resolve the problem of inconsistent imaging perspective between ImageNet and earth observations, we propose an optical ship detector (OSD) pretraining technique, which transfers the characteristics of ships in earth observations to SAR images from a large-scale aerial image dataset. On the other hand, to handle the problem of different imaging geometry between optical and SAR images, we propose an optical-SAR matching (OSM) pretraining technique, which transfers plentiful texture features from optical images to SAR images by common representation learning on the optical-SAR matching task. Finally, observing that the OSD pretraining based SAR ship detector has a better recall on sea area while the OSM pretraining based SAR ship detector can reduce false alarms on land area, we combine the predictions of the two detectors through weighted boxes fusion to further improve detection results. Extensive experiments on four SAR ship detection datasets and two representative CNN-based detection benchmarks are conducted to show the effectiveness and complementarity of the two proposed detectors, and the state-of-the-art performance of the combination of the two detectors. The proposed method won the sixth place of ship detection in SAR images in 2020 Gaofen challenge."
1640,https://arxiv.org/abs/2101.09163,The Next Decade of Telecommunications Artificial Intelligence,"It has been an exciting journey since the mobile communications and artificial intelligence were conceived 37 years and 64 years ago. While both fields evolved independently and profoundly changed communications and computing industries, the rapid convergence of 5G and deep learning is beginning to significantly transform the core communication infrastructure, network management and vertical applications. The paper first outlines the individual roadmaps of mobile communications and artificial intelligence in the early stage, with a concentration to review the era from 3G to 5G when AI and mobile communications started to converge. With regard to telecommunications artificial intelligence, the paper further introduces in detail the progress of artificial intelligence in the ecosystem of mobile communications. The paper then summarizes the classifications of AI in telecom ecosystems along with its evolution paths specified by various international telecommunications standardization bodies. Towards the next decade, the paper forecasts the prospective roadmap of telecommunications artificial intelligence. In line with 3GPP and ITU-R timeline of 5G & 6G, the paper further explores the network intelligence following 3GPP and ORAN routes respectively, experience and intention driven network management and operation, network AI signalling system, intelligent middle-office based BSS, intelligent customer experience management and policy control driven by BSS and OSS convergence, evolution from SLA to ELA, and intelligent private network for verticals. The paper is concluded with the vision that AI will reshape the future B5G or 6G landscape and we need pivot our R&D, standardizations, and ecosystem to fully take the unprecedented opportunities."
1641,https://arxiv.org/abs/1904.08637,ConvLab: Multi-Domain End-to-End Dialog System Platform,"We present ConvLab, an open-source multi-domain end-to-end dialog system platform, that enables researchers to quickly set up experiments with reusable components and compare a large set of different approaches, ranging from conventional pipeline systems to end-to-end neural models, in common environments. ConvLab offers a set of fully annotated datasets and associated pre-trained reference models. As a showcase, we extend the MultiWOZ dataset with user dialog act annotations to train all component models and demonstrate how ConvLab makes it easy and effortless to conduct complicated experiments in multi-domain end-to-end dialog settings."
1642,https://arxiv.org/abs/2305.17030,The First LHAASO Catalog of Gamma-Ray Sources,"We present the first catalog of very-high energy and ultra-high energy $γ$-ray sources detected by the Large High Altitude Air Shower Observatory (LHAASO), using 508 days of data collected by the Water Cherenkov Detector Array (WCDA) from March 2021 to September 2022 and 933 days of data recorded by the Kilometer Squared Array (KM2A) from January 2020 to September 2022. This catalog represents the most sensitive $E > 1$ TeV gamma-ray survey of the sky covering declination from $-$20$^{\circ}$ to 80$^{\circ}$. In total, the catalog contains 90 sources with extended size smaller than $2^\circ$ and with significance of detection at $> 5σ$. For each source, we provide its position, extension and spectral characteristics. Furthermore, based on our source association criteria, 32 new TeV sources are proposed in this study. Additionally, 43 sources are detected with ultra-high energy ($E > 100$ TeV) emission at $> 4σ$ significance level."
1643,https://arxiv.org/abs/2305.13598,FAST search for circumstellar atomic hydrogen. II. Is BD+303639 an interacting planetary nebula?,"The young, compact, very high surface brightness but low excitation planetary nebula (PN) BD+303639 is one of the very few PNe that have been reported to exhibit the 21cm HI emission line. As part of a long-term programme to search for circumstellar atomic hydrogen, we observed the 21cm feature toward BD+303639 with the Five-hundred-meter Aperture Spherical radio Telescope (FAST). Assuming a direct association between the PN and the detected HI emission, these new observations show that this surrounding emission is significantly more spatially extended than indicated by previous interferometric observations, and can be resolved into two velocity components. The estimated HI mass is larger than 100M_sun, invalidating an origin from the host star itself or its ejecta for the emitting material. We discuss the possibility that the extended HI emission stems from the interstellar medium (ISM) swept out over time by the stellar wind. Moreover, we report tentative detections of HI absorption features lying near and blueward of the systemic velocity of this PN, which are probably from a stalled asterosphere at the outer boundary of the expanding ionized region. The mass of the gas producing the HI absorption is insufficient to solve the so-called `PN missing mass problem'. We demonstrate the capability of FAST to investigate the interaction process between a PN and the surrounding ISM."
1644,https://arxiv.org/abs/2305.05372,Measurement of ultra-high-energy diffuse gamma-ray emission of the Galactic plane from 10 TeV to 1 PeV with LHAASO-KM2A,"The diffuse Galactic $γ$-ray emission, mainly produced via interactions between cosmic rays and the diffuse interstellar medium, is a very important probe of the distribution, propagation, and interaction of cosmic rays in the Milky Way. In this work we report the measurements of diffuse $γ$-rays from the Galactic plane between 10 TeV and 1 PeV energies, with the square kilometer array of the Large High Altitude Air Shower Observatory (LHAASO). Diffuse emissions from the inner ($15^{\circ}<l<125^{\circ}$, $|b|<5^{\circ}$) and outer ($125^{\circ}<l<235^{\circ}$, $|b|<5^{\circ}$) Galactic plane are detected with $29.1σ$ and $12.7σ$ significance, respectively. The outer Galactic plane diffuse emission is detected for the first time in the very- to ultra-high-energy domain ($E>10$~TeV). The energy spectrum in the inner Galaxy regions can be described by a power-law function with an index of $-2.99\pm0.04$, which is different from the curved spectrum as expected from hadronic interactions between locally measured cosmic rays and the line-of-sight integrated gas content. Furthermore, the measured flux is higher by a factor of $\sim3$ than the prediction. A similar spectrum with an index of $-2.99\pm0.07$ is found in the outer Galaxy region, and the absolute flux for $10\lesssim E\lesssim60$ TeV is again higher than the prediction for hadronic cosmic ray interactions. The latitude distributions of the diffuse emission are consistent with the gas distribution, while the longitude distributions show slight deviation from the gas distribution. The LHAASO measurements imply that either additional emission sources exist or cosmic ray intensities have spatial variations."
1645,https://arxiv.org/abs/2304.13949,UCF: Uncovering Common Features for Generalizable Deepfake Detection,"Deepfake detection remains a challenging task due to the difficulty of generalizing to new types of forgeries. This problem primarily stems from the overfitting of existing detection methods to forgery-irrelevant features and method-specific patterns. The latter is often ignored by previous works. This paper presents a novel approach to address the two types of overfitting issues by uncovering common forgery features. Specifically, we first propose a disentanglement framework that decomposes image information into three distinct components: forgery-irrelevant, method-specific forgery, and common forgery features. To ensure the decoupling of method-specific and common forgery features, a multi-task learning strategy is employed, including a multi-class classification that predicts the category of the forgery method and a binary classification that distinguishes the real from the fake. Additionally, a conditional decoder is designed to utilize forgery features as a condition along with forgery-irrelevant features to generate reconstructed images. Furthermore, a contrastive regularization technique is proposed to encourage the disentanglement of the common and specific forgery features. Ultimately, we only utilize the common forgery features for the purpose of generalizable deepfake detection. Extensive evaluations demonstrate that our framework can perform superior generalization than current state-of-the-art methods."
1646,https://arxiv.org/abs/2304.09075,Multi-User Matching and Resource Allocation in Vision Aided Communications,"Visual perception is an effective way to obtain the spatial characteristics of wireless channels and to reduce the overhead for communications system. A critical problem for the visual assistance is that the communications system needs to match the radio signal with the visual information of the corresponding user, i.e., to identify the visual user that corresponds to the target radio signal from all the environmental objects. In this paper, we propose a user matching method for environment with a variable number of objects. Specifically, we apply 3D detection to extract all the environmental objects from the images taken by multiple cameras. Then, we design a deep neural network (DNN) to estimate the location distribution of users by the images and beam pairs at multiple moments, and thereby identify the users from all the extracted environmental objects. Moreover, we present a resource allocation method based on the taken images to reduce the time and spectrum overhead compared to traditional resource allocation methods. Simulation results show that the proposed user matching method outperforms the existing methods, and the proposed resource allocation method can achieve $92\%$ transmission rate of the traditional resource allocation method but with the time and spectrum overhead significantly reduced."
1647,https://arxiv.org/abs/2304.07351,Generalized Color Orderings: CEGM Integrands and Decoupling Identities,"In a recent paper we defined generalized color orderings (GCO) and Feynman diagrams (GFD) to compute color-dressed generalized biadjoint amplitudes. In this work we study the Cachazo-Early-Guevara-Mizera (CEGM) representation of generalized partial amplitudes and ``decoupling"" identities. This representation is a generalization of the Cachazo-He-Yuan (CHY) formulation as an integral over the configuration space $X(k,n)$ of $n$ points on $\mathbb{CP}^{k-1}$ in generic position.
  Unlike the $k=2$ case, Parke-Taylor-like integrands are not enough to compute all partial amplitudes for $k>2$. Here we give a set of constraints that integrands associated to GCOs must satisfy and use them to construct all $(3,n<9)$ integrands, all $(3,9)$ integrands up to four undetermined constants, and $95 \%$ of $(4,8)$ integrands up to 24 undetermined constants.
  $k=2$ partial amplitudes are known to satisfy identities. Among them, the so-called $U(1)$ decoupling identities are the simplest ones. These are characterized by a label $i$ and a color ordering in $X(2,|[n]\setminus \{i\}|)$. Here we introduce decoupling identities for $k>2$ determined combinatorially using GCOs. Moreover, we identify the natural analog of $U(1)$ identities as those characterized by a pair of labels $i\neq j$, and a pair of GCOs, one in $X(k,|[n]\setminus \{i\}|)$ and the other in $X(k-1,|[n]\setminus \{j\}|)$. We call them {\it double extension} identities.
  We also provide explicit connections among different ways of representing GCOs, such as configurations of lines, configurations of points, and reorientation classes of uniform oriented matroids (chirotopes)."
1648,https://arxiv.org/abs/2304.05550,Sign-changing solution for an overdetermined elliptic problem on unbounded domain,"We prove the existence of two smooth families of unbounded domains in $\mathbb{R}^{N+1}$ with $N\geq1$ such that \begin{equation} -Δu=λu\,\, \text{in}\,\,Ω, \,\, u=0,\,\,\partial_νu=\text{const}\,\,\text{on}\,\,\partialΩ\nonumber \end{equation} admits a sign-changing solution. The domains bifurcate from the straight cylinder $B_1\times \mathbb{R}$, where $B_1$ is the unit ball in $\mathbb{R}^N$. These results can be regarded as counterexamples to the Berenstein conjecture on unbounded domain. Unlike most previous papers in this direction, a very delicate issue here is that there may be two-dimensional kernel space at some bifurcation point. Thus a Crandall-Rabinowitz type bifurcation theorem from high-dimensional kernel space is also established to achieve the goal."
1649,https://arxiv.org/abs/2304.04525,Bifurcation of sign-changing solutions for an overdetermined boundary problem in bounded domains,"We obtain a continuous family of nontrivial domains $Ω_s\subset \mathbb{R}^N$ ($N=2,3$ or $4$), bifurcating from a small ball, such that the problem \begin{equation} -Δu=u-\left(u^+\right)^3\,\, \text{in}\,\,Ω_s, \,\, u=0,\,\,\partial_νu=\text{const}\,\,\text{on}\,\,\partialΩ_s \nonumber \end{equation} has a sign-changing bounded solution. Compared with the recent result obtained by Ruiz, here we obtain a family domains $Ω_s$ by using Crandall-Rabinowitz bifurcation theorem instead of a sequence of domains."
1650,https://arxiv.org/abs/2304.04494,Improved Test-Time Adaptation for Domain Generalization,"The main challenge in domain generalization (DG) is to handle the distribution shift problem that lies between the training and test data. Recent studies suggest that test-time training (TTT), which adapts the learned model with test data, might be a promising solution to the problem. Generally, a TTT strategy hinges its performance on two main factors: selecting an appropriate auxiliary TTT task for updating and identifying reliable parameters to update during the test phase. Both previous arts and our experiments indicate that TTT may not improve but be detrimental to the learned model if those two factors are not properly considered. This work addresses those two factors by proposing an Improved Test-Time Adaptation (ITTA) method. First, instead of heuristically defining an auxiliary objective, we propose a learnable consistency loss for the TTT task, which contains learnable parameters that can be adjusted toward better alignment between our TTT task and the main prediction task. Second, we introduce additional adaptive parameters for the trained model, and we suggest only updating the adaptive parameters during the test phase. Through extensive experiments, we show that the proposed two strategies are beneficial for the learned model (see Figure 1), and ITTA could achieve superior performance to the current state-of-the-art methods on several DG benchmarks. Code is available at https://github.com/liangchen527/ITTA."
1651,https://arxiv.org/abs/2304.03903,High-Fidelity Clothed Avatar Reconstruction from a Single Image,"This paper presents a framework for efficient 3D clothed avatar reconstruction. By combining the advantages of the high accuracy of optimization-based methods and the efficiency of learning-based methods, we propose a coarse-to-fine way to realize a high-fidelity clothed avatar reconstruction (CAR) from a single image. At the first stage, we use an implicit model to learn the general shape in the canonical space of a person in a learning-based way, and at the second stage, we refine the surface detail by estimating the non-rigid deformation in the posed space in an optimization way. A hyper-network is utilized to generate a good initialization so that the convergence o f the optimization process is greatly accelerated. Extensive experiments on various datasets show that the proposed CAR successfully produces high-fidelity avatars for arbitrarily clothed humans in real scenes."
1652,https://arxiv.org/abs/2304.01581,Generation of rotational ground state HD$^+$ ions in an ion trap using a resonance-enhanced threshold photoionization process,"We report a method for producing ultracold HD+ molecular ions populated in a rotational ground state in an ion trap based on [2+1'] resonance-enhanced threshold photoionization (RETPI) and sympathetic cooling with the laser-cooled Be$^+$ ions. The effect of electric field of the ion trap on the RETPI process of neutral HD molecules and the blackbody radiation (BBR) on the population evolution of rotational states of the generated polar HD+ ions have been studied. The initial rotational ground state population of HD$^+$ ions is 0.93(12). After the cumulation time of 5 s, the rotational ground state population is reduced to 0.77(8) due to the BBR coupling. This method of generating ultracold state-selected HD$^+$ ions is beneficial for the studies in precision rovibrational spectroscopy, state-controlled cold chemical reaction, and quantum logic spectroscopy."
1653,https://arxiv.org/abs/2304.00202,Improving Fast Adversarial Training with Prior-Guided Knowledge,"Fast adversarial training (FAT) is an efficient method to improve robustness. However, the original FAT suffers from catastrophic overfitting, which dramatically and suddenly reduces robustness after a few training epochs. Although various FAT variants have been proposed to prevent overfitting, they require high training costs. In this paper, we investigate the relationship between adversarial example quality and catastrophic overfitting by comparing the training processes of standard adversarial training and FAT. We find that catastrophic overfitting occurs when the attack success rate of adversarial examples becomes worse. Based on this observation, we propose a positive prior-guided adversarial initialization to prevent overfitting by improving adversarial example quality without extra training costs. This initialization is generated by using high-quality adversarial perturbations from the historical training process. We provide theoretical analysis for the proposed initialization and propose a prior-guided regularization method that boosts the smoothness of the loss function. Additionally, we design a prior-guided ensemble FAT method that averages the different model weights of historical models using different decay rates. Our proposed method, called FGSM-PGK, assembles the prior-guided knowledge, i.e., the prior-guided initialization and model weights, acquired during the historical training process. Evaluations of four datasets demonstrate the superiority of the proposed method."
1654,https://arxiv.org/abs/2303.10816,IMF: Interactive Multimodal Fusion Model for Link Prediction,"Link prediction aims to identify potential missing triples in knowledge graphs. To get better results, some recent studies have introduced multimodal information to link prediction. However, these methods utilize multimodal information separately and neglect the complicated interaction between different modalities. In this paper, we aim at better modeling the inter-modality information and thus introduce a novel Interactive Multimodal Fusion (IMF) model to integrate knowledge from different modalities. To this end, we propose a two-stage multimodal fusion framework to preserve modality-specific knowledge as well as take advantage of the complementarity between different modalities. Instead of directly projecting different modalities into a unified space, our multimodal fusion module limits the representations of different modalities independent while leverages bilinear pooling for fusion and incorporates contrastive learning as additional constraints. Furthermore, the decision fusion module delivers the learned weighted average over the predictions of all modalities to better incorporate the complementarity of different modalities. Our approach has been demonstrated to be effective through empirical evaluations on several real-world datasets. The implementation code is available online at https://github.com/HestiaSky/IMF-Pytorch."
1655,https://arxiv.org/abs/2303.09723,On the Diophantine system involving pairs of triangles with the same area and the same perimeter,"Many authors studied the problem that rational triangle pairs (triangle-parallelogram pairs) with the same area and the same perimeter. They investigated this problem by solving the rational solutions of the corresponding Diophantine equations. In this paper, we give a unified description of this problem by using the affine transformation in a rectangular coordinate system. According to the fact that two triangles with the same area and the same perimeter determine an affine transformation, this problem can be reduced to solving a specific Diophantine system. Moreover, we will give some rational solutions to this Diophantine system."
1656,https://arxiv.org/abs/2303.09535,FateZero: Fusing Attentions for Zero-shot Text-based Video Editing,"The diffusion-based generative models have achieved remarkable success in text-based image generation. However, since it contains enormous randomness in generation progress, it is still challenging to apply such models for real-world visual content editing, especially in videos. In this paper, we propose FateZero, a zero-shot text-based editing method on real-world videos without per-prompt training or use-specific mask. To edit videos consistently, we propose several techniques based on the pre-trained models. Firstly, in contrast to the straightforward DDIM inversion technique, our approach captures intermediate attention maps during inversion, which effectively retain both structural and motion information. These maps are directly fused in the editing process rather than generated during denoising. To further minimize semantic leakage of the source video, we then fuse self-attentions with a blending mask obtained by cross-attention features from the source prompt. Furthermore, we have implemented a reform of the self-attention mechanism in denoising UNet by introducing spatial-temporal attention to ensure frame consistency. Yet succinct, our method is the first one to show the ability of zero-shot text-driven video style and local attribute editing from the trained text-to-image model. We also have a better zero-shot shape-aware editing ability based on the text-to-video model. Extensive experiments demonstrate our superior temporal consistency and editing capability than previous works."
1657,https://arxiv.org/abs/2303.08524,CoordFill: Efficient High-Resolution Image Inpainting via Parameterized Coordinate Querying,"Image inpainting aims to fill the missing hole of the input. It is hard to solve this task efficiently when facing high-resolution images due to two reasons: (1) Large reception field needs to be handled for high-resolution image inpainting. (2) The general encoder and decoder network synthesizes many background pixels synchronously due to the form of the image matrix. In this paper, we try to break the above limitations for the first time thanks to the recent development of continuous implicit representation. In detail, we down-sample and encode the degraded image to produce the spatial-adaptive parameters for each spatial patch via an attentional Fast Fourier Convolution(FFC)-based parameter generation network. Then, we take these parameters as the weights and biases of a series of multi-layer perceptron(MLP), where the input is the encoded continuous coordinates and the output is the synthesized color value. Thanks to the proposed structure, we only encode the high-resolution image in a relatively low resolution for larger reception field capturing. Then, the continuous position encoding will be helpful to synthesize the photo-realistic high-frequency textures by re-sampling the coordinate in a higher resolution. Also, our framework enables us to query the coordinates of missing pixels only in parallel, yielding a more efficient solution than the previous methods. Experiments show that the proposed method achieves real-time performance on the 2048$\times$2048 images using a single GTX 2080 Ti GPU and can handle 4096$\times$4096 images, with much better performance than existing state-of-the-art methods visually and numerically. The code is available at: https://github.com/NiFangBaAGe/CoordFill."
1658,https://arxiv.org/abs/2303.08233,NL4Opt Competition: Formulating Optimization Problems Based on Their Natural Language Descriptions,"The Natural Language for Optimization (NL4Opt) Competition was created to investigate methods of extracting the meaning and formulation of an optimization problem based on its text description. Specifically, the goal of the competition is to increase the accessibility and usability of optimization solvers by allowing non-experts to interface with them using natural language. We separate this challenging goal into two sub-tasks: (1) recognize and label the semantic entities that correspond to the components of the optimization problem; (2) generate a meaning representation (i.e., a logical form) of the problem from its detected problem entities. The first task aims to reduce ambiguity by detecting and tagging the entities of the optimization problems. The second task creates an intermediate representation of the linear programming (LP) problem that is converted into a format that can be used by commercial solvers. In this report, we present the LP word problem dataset and shared tasks for the NeurIPS 2022 competition. Furthermore, we investigate and compare the performance of the ChatGPT large language model against the winning solutions. Through this competition, we hope to bring interest towards the development of novel machine learning applications and datasets for optimization modeling."
1659,https://arxiv.org/abs/2301.06281,DPE: Disentanglement of Pose and Expression for General Video Portrait Editing,"One-shot video-driven talking face generation aims at producing a synthetic talking video by transferring the facial motion from a video to an arbitrary portrait image. Head pose and facial expression are always entangled in facial motion and transferred simultaneously. However, the entanglement sets up a barrier for these methods to be used in video portrait editing directly, where it may require to modify the expression only while maintaining the pose unchanged. One challenge of decoupling pose and expression is the lack of paired data, such as the same pose but different expressions. Only a few methods attempt to tackle this challenge with the feat of 3D Morphable Models (3DMMs) for explicit disentanglement. But 3DMMs are not accurate enough to capture facial details due to the limited number of Blenshapes, which has side effects on motion transfer. In this paper, we introduce a novel self-supervised disentanglement framework to decouple pose and expression without 3DMMs and paired data, which consists of a motion editing module, a pose generator, and an expression generator. The editing module projects faces into a latent space where pose motion and expression motion can be disentangled, and the pose or expression transfer can be performed in the latent space conveniently via addition. The two generators render the modified latent codes to images, respectively. Moreover, to guarantee the disentanglement, we propose a bidirectional cyclic training strategy with well-designed constraints. Evaluations demonstrate our method can control pose or expression independently and be used for general video editing."
1660,https://arxiv.org/abs/2301.06052,T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations,"In this work, we investigate a simple and must-known conditional generative framework based on Vector Quantised-Variational AutoEncoder (VQ-VAE) and Generative Pre-trained Transformer (GPT) for human motion generation from textural descriptions. We show that a simple CNN-based VQ-VAE with commonly used training recipes (EMA and Code Reset) allows us to obtain high-quality discrete representations. For GPT, we incorporate a simple corruption strategy during the training to alleviate training-testing discrepancy. Despite its simplicity, our T2M-GPT shows better performance than competitive approaches, including recent diffusion-based approaches. For example, on HumanML3D, which is currently the largest dataset, we achieve comparable performance on the consistency between text and generated motion (R-Precision), but with FID 0.116 largely outperforming MotionDiffuse of 0.630. Additionally, we conduct analyses on HumanML3D and observe that the dataset size is a limitation of our approach. Our work suggests that VQ-VAE still remains a competitive approach for human motion generation."
1661,https://arxiv.org/abs/2301.03144,Evidence of high-mass star formation through multi-scale mass accretion in hub-filament-system clouds,"We present a statistical study of a sample of 17 hub-filament-system (HFS) clouds of high-mass star formation using high-angular resolution ($\sim$1-2 arcsecond) ALMA 1.3mm and 3mm continuum data. The sample includes 8 infrared (IR)-dark and 9 IR-bright types, which correspond to an evolutionary sequence from the IR-dark to IR-bright stage. The central massive clumps and their associated most massive cores are observed to follow a trend of increasing mass ($M$) and mass surface density ($Σ$) with evolution from IR-dark to IR-bright stage. In addition, a mass-segregated cluster of young stellar objects (YSOs) are revealed in both IR-dark and IR-bright HFSs with massive YSOs located in the hub and the population of low-mass YSOs distributed over larger areas. Moreover, outflow feedback in all HFSs are found to escape preferentially through the inter-filamentary diffuse cavities, suggesting that outflows would render a limited effect on the disruption of the HFSs and ongoing high-mass star formation therein. From the above observations, we suggest that high-mass star formation in the HFSs can be described by a multi-scale mass accretion/transfer scenario, from hub-composing filaments through clumps down to cores, that can naturally lead to a mass-segregated cluster of stars."
1662,https://arxiv.org/abs/2301.01895,ATOMS: ALMA Three-millimeter Observations of Massive Star-forming regions -- XV. Steady Accretion from Global Collapse to Core Feeding in Massive Hub-filament System SDC335,"We present ALMA Band-3/7 observations towards ""the Heart"" of a massive hub-filament system (HFS) SDC335, to investigate its fragmentation and accretion. At a resolution of $\sim0.03$ pc, 3 mm continuum emission resolves two massive dense cores MM1 and MM2, with $383(^{+234}_{-120})$ $M_\odot$ (10-24% mass of ""the Heart"") and $74(^{+47}_{-24})$ $M_\odot$, respectively. With a resolution down to 0.01 pc, 0.87 mm continuum emission shows MM1 further fragments into six condensations and multi-transition lines of H$_2$CS provide temperature estimation. The relation between separation and mass of condensations at a scale of 0.01 pc favors turbulent Jeans fragmentation where the turbulence seems to be scale-free rather than scale-dependent. We use the H$^{13}$CO$^+$ (1-0) emission line to resolve the complex gas motion inside ""the Heart"" in position-position-velocity space. We identify four major gas streams connected to large-scale filaments, inheriting the anti-clockwise spiral pattern. Along these streams, gas feeds the central massive core MM1. Assuming an inclination angle of $45(\pm15)^{\circ}$ and a H$^{13}$CO$^+$ abundance of $5(\pm3)\times10^{-11}$, the total mass infall rate is estimated to be $2.40(\pm0.78)\times10^{-3}$ $M_\odot$ yr$^{-1}$, numerically consistent with the accretion rates derived from the clump-scale spherical infall model and the core-scale outflows. The consistency suggests a continuous, near steady-state, and efficient accretion from global collapse, therefore ensuring core feeding. Our comprehensive study of SDC335 showcases the detailed gas kinematics in a prototypical massive infalling clump and calls for further systematic and statistical analyses in a large sample."
1663,https://arxiv.org/abs/2301.00364,Generalizable Black-Box Adversarial Attack with Meta Learning,"In the scenario of black-box adversarial attack, the target model's parameters are unknown, and the attacker aims to find a successful adversarial perturbation based on query feedback under a query budget. Due to the limited feedback information, existing query-based black-box attack methods often require many queries for attacking each benign example. To reduce query cost, we propose to utilize the feedback information across historical attacks, dubbed example-level adversarial transferability. Specifically, by treating the attack on each benign example as one task, we develop a meta-learning framework by training a meta-generator to produce perturbations conditioned on benign examples. When attacking a new benign example, the meta generator can be quickly fine-tuned based on the feedback information of the new task as well as a few historical attacks to produce effective perturbations. Moreover, since the meta-train procedure consumes many queries to learn a generalizable generator, we utilize model-level adversarial transferability to train the meta-generator on a white-box surrogate model, then transfer it to help the attack against the target model. The proposed framework with the two types of adversarial transferability can be naturally combined with any off-the-shelf query-based attack methods to boost their performance, which is verified by extensive experiments."
1664,https://arxiv.org/abs/2212.11468,IPProtect: protecting the intellectual property of visual datasets during data valuation,"Data trading is essential to accelerate the development of data-driven machine learning pipelines. The central problem in data trading is to estimate the utility of a seller's dataset with respect to a given buyer's machine learning task, also known as data valuation. Typically, data valuation requires one or more participants to share their raw dataset with others, leading to potential risks of intellectual property (IP) violations. In this paper, we tackle the novel task of preemptively protecting the IP of datasets that need to be shared during data valuation. First, we identify and formalize two kinds of novel IP risks in visual datasets: data-item (image) IP and statistical (dataset) IP. Then, we propose a novel algorithm to convert the raw dataset into a sanitized version, that provides resistance to IP violations, while at the same time allowing accurate data valuation. The key idea is to limit the transfer of information from the raw dataset to the sanitized dataset, thereby protecting against potential intellectual property violations. Next, we analyze our method for the likely existence of a solution and immunity against reconstruction attacks. Finally, we conduct extensive experiments on three computer vision datasets demonstrating the advantages of our method in comparison to other baselines."
1665,https://arxiv.org/abs/2212.11243,Color-Dressed Generalized Biadjoint Scalar Amplitudes: Local Planarity,"The biadjoint scalar theory has cubic interactions and fields transforming in the biadjoint representation of $SU(N)\times SU({\tilde N})$. Amplitudes are ``color'' decomposed in terms of partial amplitudes computed using Feynman diagrams which are simultaneously planar with respect to two orderings. In 2019, a generalization of biadjoint scalar amplitudes based on generalized Feynman diagrams (GFDs) was introduced. GFDs are collections of Feynman diagrams derived by incorporating an additional constraint of ``local planarity'' into the construction of the arrangements of metric trees in combinatorics. In this work we propose a natural generalization of color orderings which leads to color-dressed amplitudes. A generalized color ordering (GCO) is defined as a collection of standard color orderings that is induced, in a precise sense, from an arrangement of projective lines on $\mathbb{RP}^2$. We present results for $n\leq 9$ generalized color orderings and GFDs, uncovering new phenomena in each case. We discover generalized decoupling identities and propose a definition of the ``colorless'' generalized scalar amplitude. We also propose a notion of GCOs for arbitrary $\mathbb{RP}^{k-1}$, discuss some of their properties and comment on their GFDs. In a companion paper, we explore the definition of partial amplitudes using CEGM integral formulas."
1666,https://arxiv.org/abs/2211.16927,3D GAN Inversion with Facial Symmetry Prior,"Recently, a surge of high-quality 3D-aware GANs have been proposed, which leverage the generative power of neural rendering. It is natural to associate 3D GANs with GAN inversion methods to project a real image into the generator's latent space, allowing free-view consistent synthesis and editing, referred as 3D GAN inversion. Although with the facial prior preserved in pre-trained 3D GANs, reconstructing a 3D portrait with only one monocular image is still an ill-pose problem. The straightforward application of 2D GAN inversion methods focuses on texture similarity only while ignoring the correctness of 3D geometry shapes. It may raise geometry collapse effects, especially when reconstructing a side face under an extreme pose. Besides, the synthetic results in novel views are prone to be blurry. In this work, we propose a novel method to promote 3D GAN inversion by introducing facial symmetry prior. We design a pipeline and constraints to make full use of the pseudo auxiliary view obtained via image flipping, which helps obtain a robust and reasonable geometry shape during the inversion process. To enhance texture fidelity in unobserved viewpoints, pseudo labels from depth-guided 3D warping can provide extra supervision. We design constraints aimed at filtering out conflict areas for optimization in asymmetric situations. Comprehensive quantitative and qualitative evaluations on image reconstruction and editing demonstrate the superiority of our method."
1667,https://arxiv.org/abs/2211.15064,High-fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors,"High-fidelity facial avatar reconstruction from a monocular video is a significant research problem in computer graphics and computer vision. Recently, Neural Radiance Field (NeRF) has shown impressive novel view rendering results and has been considered for facial avatar reconstruction. However, the complex facial dynamics and missing 3D information in monocular videos raise significant challenges for faithful facial reconstruction. In this work, we propose a new method for NeRF-based facial avatar reconstruction that utilizes 3D-aware generative prior. Different from existing works that depend on a conditional deformation field for dynamic modeling, we propose to learn a personalized generative prior, which is formulated as a local and low dimensional subspace in the latent space of 3D-GAN. We propose an efficient method to construct the personalized generative prior based on a small set of facial images of a given individual. After learning, it allows for photo-realistic rendering with novel views and the face reenactment can be realized by performing navigation in the latent space. Our proposed method is applicable for different driven signals, including RGB images, 3DMM coefficients, and audios. Compared with existing works, we obtain superior novel view synthesis results and faithfully face reenactment performance."
1668,https://arxiv.org/abs/2211.14758,VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild,"We present VideoReTalking, a new system to edit the faces of a real-world talking head video according to input audio, producing a high-quality and lip-syncing output video even with a different emotion. Our system disentangles this objective into three sequential tasks: (1) face video generation with a canonical expression; (2) audio-driven lip-sync; and (3) face enhancement for improving photo-realism. Given a talking-head video, we first modify the expression of each frame according to the same expression template using the expression editing network, resulting in a video with the canonical expression. This video, together with the given audio, is then fed into the lip-sync network to generate a lip-syncing video. Finally, we improve the photo-realism of the synthesized faces through an identity-aware face enhancement network and post-processing. We use learning-based approaches for all three steps and all our modules can be tackled in a sequential pipeline without any user intervention. Furthermore, our system is a generic approach that does not need to be retrained to a specific person. Evaluations on two widely-used datasets and in-the-wild examples demonstrate the superiority of our framework over other state-of-the-art methods in terms of lip-sync accuracy and visual quality."
1669,https://arxiv.org/abs/2211.14068,Fine-Grained Face Swapping via Regional GAN Inversion,"We present a novel paradigm for high-fidelity face swapping that faithfully preserves the desired subtle geometry and texture details. We rethink face swapping from the perspective of fine-grained face editing, \textit{i.e., ``editing for swapping'' (E4S)}, and propose a framework that is based on the explicit disentanglement of the shape and texture of facial components. Following the E4S principle, our framework enables both global and local swapping of facial features, as well as controlling the amount of partial swapping specified by the user. Furthermore, the E4S paradigm is inherently capable of handling facial occlusions by means of facial masks. At the core of our system lies a novel Regional GAN Inversion (RGI) method, which allows the explicit disentanglement of shape and texture. It also allows face swapping to be performed in the latent space of StyleGAN. Specifically, we design a multi-scale mask-guided encoder to project the texture of each facial component into regional style codes. We also design a mask-guided injection module to manipulate the feature maps with the style codes. Based on the disentanglement, face swapping is reformulated as a simplified problem of style and mask swapping. Extensive experiments and comparisons with current state-of-the-art methods demonstrate the superiority of our approach in preserving texture and shape details, as well as working with high resolution images. The project page is http://e4s2022.github.io"
1670,https://arxiv.org/abs/2211.13640,Molecules in the peculiar age-defying source IRAS 19312+1950,"Context. IRAS 19312+1950 is an isolated infrared source that exhibits a characteristic quasi-point-symmetric morphology in the near- and mid-infrared images and is also very bright in molecular radio lines. Because of its unique observational characteristics, various observational studies have been conducted and several hypotheses have been proposed regarding its origin, which is still unclear. So far, it has been suggested that it could be a peculiar evolved star, a young stellar object, or even a red nova remnant. Regardless of which type of object it is ultimately classified as, IRAS 19312+1950 is exceptionally bright in the infrared and molecular radio lines and therefore will undoubtedly be crucial as a prototype of this kind of object having a peculiar nature or unusual evolutionary phase.
  Aims. This study aims to reveal the molecular composition of the central part of IRAS 19312+1950 by performing an unbiased molecular radio line survey and discussing the origin of the object from a molecular chemical point of view.
  Methods. We carried out a spectral line survey with the IRAM 30 m telescope towards the center of IRAS 19312+1950 in the 3 and 1.3 mm windows.
  Results. In total, 28 transition lines of 22 molecular species and those isotopologues are detected towards IRAS 19312+1950, some of which exhibit a broad and a narrow components. Seventeen thermal lines and 1 maser line are newly detected. The molecular species of C$^{17}$O, $^{30}$SiO, HN$^{13}$C, HC$^{18}$O$^{+}$, H$_{2}$CO, and $c$-C$_{3}$H$_{2}$ are detected for the first time in this object.
  Conclusions. Our results, in combination with previous studies, favor the hypothesis that IRAS 19312+1950 might be a red nova remnant, in which the progenitors that merged to become a red nova may have contained at least two evolved stars with oxygen-rich and carbon-rich chemistry, respectively."
1671,https://arxiv.org/abs/2211.13221,Latent Video Diffusion Models for High-Fidelity Long Video Generation,"AI-generated content has attracted lots of attention recently, but photo-realistic video synthesis is still challenging. Although many attempts using GANs and autoregressive models have been made in this area, the visual quality and length of generated videos are far from satisfactory. Diffusion models have shown remarkable results recently but require significant computational resources. To address this, we introduce lightweight video diffusion models by leveraging a low-dimensional 3D latent space, significantly outperforming previous pixel-space video diffusion models under a limited computational budget. In addition, we propose hierarchical diffusion in the latent space such that longer videos with more than one thousand frames can be produced. To further overcome the performance degradation issue for long video generation, we propose conditional latent perturbation and unconditional guidance that effectively mitigate the accumulated errors during the extension of video length. Extensive experiments on small domain datasets of different categories suggest that our framework generates more realistic and longer videos than previous strong baselines. We additionally provide an extension to large-scale text-to-video generation to demonstrate the superiority of our work. Our code and models will be made publicly available."
1672,https://arxiv.org/abs/2211.12194,SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation,"Generating talking head videos through a face image and a piece of speech audio still contains many challenges. ie, unnatural head movement, distorted expression, and identity modification. We argue that these issues are mainly because of learning from the coupled 2D motion fields. On the other hand, explicitly using 3D information also suffers problems of stiff expression and incoherent video. We present SadTalker, which generates 3D motion coefficients (head pose, expression) of the 3DMM from audio and implicitly modulates a novel 3D-aware face render for talking head generation. To learn the realistic motion coefficients, we explicitly model the connections between audio and different types of motion coefficients individually. Precisely, we present ExpNet to learn the accurate facial expression from audio by distilling both coefficients and 3D-rendered faces. As for the head pose, we design PoseVAE via a conditional VAE to synthesize head motion in different styles. Finally, the generated 3D motion coefficients are mapped to the unsupervised 3D keypoints space of the proposed face render, and synthesize the final video. We conducted extensive experiments to demonstrate the superiority of our method in terms of motion and video quality."
1673,https://arxiv.org/abs/2211.11208,Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars,"3D-aware generative adversarial networks (GANs) synthesize high-fidelity and multi-view-consistent facial images using only collections of single-view 2D imagery. Towards fine-grained control over facial attributes, recent efforts incorporate 3D Morphable Face Model (3DMM) to describe deformation in generative radiance fields either explicitly or implicitly. Explicit methods provide fine-grained expression control but cannot handle topological changes caused by hair and accessories, while implicit ones can model varied topologies but have limited generalization caused by the unconstrained deformation fields. We propose a novel 3D GAN framework for unsupervised learning of generative, high-quality and 3D-consistent facial avatars from unstructured 2D images. To achieve both deformation accuracy and topological flexibility, we propose a 3D representation called Generative Texture-Rasterized Tri-planes. The proposed representation learns Generative Neural Textures on top of parametric mesh templates and then projects them into three orthogonal-viewed feature planes through rasterization, forming a tri-plane feature representation for volume rendering. In this way, we combine both fine-grained expression control of mesh-guided explicit deformation and the flexibility of implicit volumetric representation. We further propose specific modules for modeling mouth interior which is not taken into account by 3DMM. Our method demonstrates state-of-the-art 3D-aware synthesis quality and animation ability through extensive experiments. Furthermore, serving as 3D prior, our animatable 3D representation boosts multiple applications including one-shot facial avatars and 3D-aware stylization."
1674,https://arxiv.org/abs/2211.10682,DiffStyler: Controllable Dual Diffusion for Text-Driven Image Stylization,"Despite the impressive results of arbitrary image-guided style transfer methods, text-driven image stylization has recently been proposed for transferring a natural image into the stylized one according to textual descriptions of the target style provided by the user. Unlike previous image-to-image transfer approaches, text-guided stylization progress provides users with a more precise and intuitive way to express the desired style. However, the huge discrepancy between cross-modal inputs/outputs makes it challenging to conduct text-driven image stylization in a typical feed-forward CNN pipeline. In this paper, we present DiffStyler on the basis of diffusion models. The cross-modal style information can be easily integrated as guidance during the diffusion progress step-by-step. In particular, we use a dual diffusion processing architecture to control the balance between the content and style of the diffused results. Furthermore, we propose a content image-based learnable noise on which the reverse denoising process is based, enabling the stylization results to better preserve the structure information of the content image. We validate the proposed DiffStyler beyond the baseline methods through extensive qualitative and quantitative experiments."
1675,https://arxiv.org/abs/2211.02916,Electronic properties of monolayer copper selenide with one-dimensional moiré patterns,"Strain engineering is a vital way to manipulate the electronic properties of two-dimensional (2D) materials. As a typical representative of transition metal mono-chalcogenides (TMMs), a honeycomb CuSe monolayer features with one-dimensional (1D) moiré patterns owing to the uniaxial strain along one of three equivalent orientations of Cu(111) substrates. Here, by combining low-temperature scanning tunneling microscopy/spectroscopy (STM/S) experiments and density functional theory (DFT) calculations, we systematically investigate the electronic properties of the strained CuSe monolayer on the Cu(111) substrate. Our results show the semiconducting feature of CuSe monolayer with a band gap of 1.28 eV and the 1D periodical modulation of electronic properties by the 1D moiré patterns. Except for the uniaxially strained CuSe monolayer, we observed domain boundary and line defects in the CuSe monolayer, where the biaxial-strain and strain-free conditions can be investigated respectively. STS measurements for the three different strain regions show that the first peak in conduction band will move downward with the increasing strain. DFT calculations based on the three CuSe atomic models with different strain inside reproduced the peak movement. The present findings not only enrich the fundamental comprehension toward the influence of strain on electronic properties at 2D limit, but also offer the benchmark for the development of 2D semiconductor materials."
1676,https://arxiv.org/abs/2211.00638,Perfecting one-loop BCJ numerators in SYM and supergravity,"We take a major step towards computing $D$-dimensional one-loop amplitudes in general gauge theories, compatible with the principles of unitarity and the color-kinematics duality. For $n$-point amplitudes with either supersymmetry multiplets or generic non-supersymmetric matter in the loop, simple all-multiplicity expressions are obtained for the maximal cuts of kinematic numerators of $n$-gon diagrams. At $n=6,7$ points with maximal supersymmetry, we extend the cubic-diagram numerators to encode all contact terms, and thus solve the long-standing problem of \emph{simultaneously} realizing the following properties: color-kinematics duality, manifest locality, optimal power counting of loop momenta, quadratic rather than linearized Feynman propagators, compatibility with double copy as well as all graph symmetries. Color-kinematics dual representations with similar properties are presented in the half-maximally supersymmetric case at $n=4,5$ points. The resulting gauge-theory integrands and their supergravity counterparts obtained from the double copy are checked to reproduce the expected ultraviolet divergences."
1677,https://arxiv.org/abs/2210.05968,Boosting the Transferability of Adversarial Attacks with Reverse Adversarial Perturbation,"Deep neural networks (DNNs) have been shown to be vulnerable to adversarial examples, which can produce erroneous predictions by injecting imperceptible perturbations. In this work, we study the transferability of adversarial examples, which is significant due to its threat to real-world applications where model architecture or parameters are usually unknown. Many existing works reveal that the adversarial examples are likely to overfit the surrogate model that they are generated from, limiting its transfer attack performance against different target models. To mitigate the overfitting of the surrogate model, we propose a novel attack method, dubbed reverse adversarial perturbation (RAP). Specifically, instead of minimizing the loss of a single adversarial point, we advocate seeking adversarial example located at a region with unified low loss value, by injecting the worst-case perturbation (the reverse adversarial perturbation) for each step of the optimization procedure. The adversarial attack with RAP is formulated as a min-max bi-level optimization problem. By integrating RAP into the iterative process for attacks, our method can find more stable adversarial examples which are less sensitive to the changes of decision boundary, mitigating the overfitting of the surrogate model. Comprehensive experimental comparisons demonstrate that RAP can significantly boost adversarial transferability. Furthermore, RAP can be naturally combined with many existing black-box attack techniques, to further boost the transferability. When attacking a real-world image recognition system, Google Cloud Vision API, we obtain 22% performance improvement of targeted attacks over the compared method. Our codes are available at https://github.com/SCLBD/Transfer_attack_RAP."
1678,https://arxiv.org/abs/2210.02284,Unsupervised Sentence Textual Similarity with Compositional Phrase Semantics,"Measuring Sentence Textual Similarity (STS) is a classic task that can be applied to many downstream NLP applications such as text generation and retrieval. In this paper, we focus on unsupervised STS that works on various domains but only requires minimal data and computational resources. Theoretically, we propose a light-weighted Expectation-Correction (EC) formulation for STS computation. EC formulation unifies unsupervised STS approaches including the cosine similarity of Additively Composed (AC) sentence embeddings, Optimal Transport (OT), and Tree Kernels (TK). Moreover, we propose the Recursive Optimal Transport Similarity (ROTS) algorithm to capture the compositional phrase semantics by composing multiple recursive EC formulations. ROTS finishes in linear time and is faster than its predecessors. ROTS is empirically more effective and scalable than previous approaches. Extensive experiments on 29 STS tasks under various settings show the clear advantage of ROTS over existing approaches. Detailed ablation studies demonstrate the effectiveness of our approaches."
1679,https://arxiv.org/abs/2209.15565,Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions,"We describe an augmented intelligence system for simplifying and enhancing the modeling experience for operations research. Using this system, the user receives a suggested formulation of an optimization problem based on its description. To facilitate this process, we build an intuitive user interface system that enables the users to validate and edit the suggestions. We investigate controlled generation techniques to obtain an automatic suggestion of formulation. Then, we evaluate their effectiveness with a newly created dataset of linear programming problems drawn from various application domains."
1680,https://arxiv.org/abs/2209.12515,Routing and QoS Policy Optimization in SD-WAN,"In modern SD-WAN networks, a global controller continuously optimizes application and user intents by selecting the proper routing policies for each application. Nevertheless, the competition between flows can still occur at each overlay link and it may degrade Quality of Service (QoS). To mitigate performance degradations in case of congestion, QoS policies can also be dynamically optimized to limit the rate of low-priority flows and share the bandwidth among all flows in a fair manner. This demonstration presents a comprehensive control plane architecture to coordinate path selection and rate allocation, in order to meet application requirements while optimizing a global objective (e.g., low congestion, best quality, and minimum cost)."
1681,https://arxiv.org/abs/2209.12180,On optimal zero-padding of kernel truncation method,"The kernel truncation method (KTM) is a commonly-used algorithm to compute the convolution-type nonlocal potential $Φ(x)=(U\ast ρ)(x), ~x \in {\mathbb R^d}$, where the convolution kernel $U(x)$ might be singular at the origin and/or far-field and the density $ρ(x)$ is smooth and fast-decaying. In KTM, in order to capture the Fourier integrand's oscillations that is brought by the kernel truncation, one needs to carry out a zero-padding of the density, which means a larger physical computation domain and a finer mesh in the Fourier space by duality. The empirical fourfold zero-padding [ Vico et al J. Comput. Phys. (2016) ] puts a heavy burden on memory requirement especially for higher dimension problems. In this paper, we derive the optimal zero-padding factor, that is, $\sqrt{d}+1$, for the first time together with a rigorous proof. The memory cost is greatly reduced to a small fraction, i.e., $(\frac{\sqrt{d}+1}{4})^d$, of what is needed in the original fourfold algorithm. For example, in the precomputation step, a double-precision computation on a $256^3$ grid requires a minimum $3.4$ Gb memory with the optimal threefold zero-padding, while the fourfold algorithm requires around $8$ Gb where the reduction factor is $\frac{37}{64}\approx \frac{3}{5}$. Then, we present the error estimates of the potential and density in $d$ dimension. Next, we re-investigate the optimal zero-padding factor for the anisotropic density. Finally, extensive numerical results are provided to confirm the accuracy, efficiency, optimal zero-padding factor for the anisotropic density, together with some applications to different types of nonlocal potential, including the 1D/2D/3D Poisson, 2D Coulomb, quasi-2D/3D Dipole-Dipole Interaction and 3D quadrupolar potential."
1682,https://arxiv.org/abs/2209.05262,Squeezed spectra and elliptic flow of bosons and anti-bosons with different in-medium masses,"We study the influence of the in-medium mass difference between boson and anti-boson on their spectra and elliptic flow. The in-medium mass difference may lead to a difference between the transverse momentum spectra of boson and anti-boson, and it also leads to the splitting between the elliptic flow of boson and anti-boson.This effect increases with the increasing in-medium mass difference between boson and anti-boson. With the increasing pseudorapidity, the splitting effect of the transverse momentum spectra increases and the splitting effect of the elliptic flow decreases. These phenomena may provide a new sight to study the interactions between the bosons and source medium in high-energy heavy-ion collisions."
1683,https://arxiv.org/abs/2209.03918,A multi view multi stage and multi window framework for pulmonary artery segmentation from CT scans,"This is the technical report of the 9th place in the final result of PARSE2022 Challenge. We solve the segmentation problem of the pulmonary artery by using a two-stage method based on a 3D CNN network. The coarse model is used to locate the ROI, and the fine model is used to refine the segmentation result. In addition, in order to improve the segmentation performance, we adopt multi-view and multi-window level method, at the same time we employ a fine-tune strategy to mitigate the impact of inconsistent labeling."
1684,https://arxiv.org/abs/2208.13184,Towards Real-World Video Deblurring by Exploring Blur Formation Process,"This paper aims at exploring how to synthesize close-to-real blurs that existing video deblurring models trained on them can generalize well to real-world blurry videos. In recent years, deep learning-based approaches have achieved promising success on video deblurring task. However, the models trained on existing synthetic datasets still suffer from generalization problems over real-world blurry scenarios with undesired artifacts. The factors accounting for the failure remain unknown. Therefore, we revisit the classical blur synthesis pipeline and figure out the possible reasons, including shooting parameters, blur formation space, and image signal processor~(ISP). To analyze the effects of these potential factors, we first collect an ultra-high frame-rate (940 FPS) RAW video dataset as the data basis to synthesize various kinds of blurs. Then we propose a novel realistic blur synthesis pipeline termed as RAW-Blur by leveraging blur formation cues. Through numerous experiments, we demonstrate that synthesizing blurs in the RAW space and adopting the same ISP as the real-world testing data can effectively eliminate the negative effects of synthetic data. Furthermore, the shooting parameters of the synthesized blurry video, e.g., exposure time and frame-rate play significant roles in improving the performance of deblurring models. Impressively, the models trained on the blurry data synthesized by the proposed RAW-Blur pipeline can obtain more than 5dB PSNR gain against those trained on the existing synthetic blur datasets. We believe the novel realistic synthesis pipeline and the corresponding RAW video dataset can help the community to easily construct customized blur datasets to improve real-world video deblurring performance largely, instead of laboriously collecting real data pairs."
1685,https://arxiv.org/abs/2208.11883,Magnetic field control of the near-field radiative heat transfer in three-body planar systems,"Recently, the application of an external magnetic field to actively control the near-field heat transfer has emerged as an appealing and promising technique. Existing studies have shown that an external static magnetic field tends to reduce the subwavelength radiative flux exchanged between two planar structures containing magneto-optical (MO) materials, but so far the nearfield thermomagnetic effects in systems with more such structures at different temperatures have not been reported. Here, we are focused on examining how the presence of an external magnetic field modifies the radiative energy transfer in a many-body configuration consisting of three MO n-doped semiconductors slabs, separated by subwavelength vacuum gaps. To exactly calculate the radiative flux transferred in such an anisotropic planar system, a general Green-function-based approach is offered, which allows one to investigate the radiative heat transfer in arbitrary manybody systems with planar geometry. We demonstrate that, under specific choices of the geometrical and thermal parameters, the applied magnetic field is able to either reduce or enhance the near-field energy transfer in three-element MO planar systems, depending on the interplay between the damped evanescent fields of the zero-field surface waves and the propagating hyperbolic modes induced by magnetic fields. Our study broadens the understanding concerning to the use of external fields to actively control the heat transfer in subwavelength regimes, and may be leveraged for potential applications in the realm of nanoscale thermal management."
1686,https://arxiv.org/abs/2208.11323,Functional central limit theorems for spatial averages of the parabolic Anderson model with delta initial condition in dimension $d\geq 1$,"Let $\{u(t,x)\}_{t>0,x\in{{\mathbb R}^{d}}}$ be the solution to a $d$-dimensional parabolic Anderson model with initial condition $δ_{0}$ and driven by a Gaussian noise that is white in time and has a spatially homogeneous covariance given by a nonnegative-definite measure $f$ which satisfies Dalang's condition. Let $S_{N,t}:=N^{-d}\int_{{[0,N]}^d}{[U(t,x)-1]}~{\rm d}x$ denote the spatial average on ${{\mathbb R}^{d}}$.
  We obtain various functional central limit theorems (CLTs) for spatial averages based on the quantitative analysis of $f$ and spatial dimension $d$. In particular, when $f$ is given by Riesz kernel, i.e., $f({\rm x})={\Vert x \Vert}^{-β}{\rm d}x$, $β\in(0,2\wedge d)$, the functional CLT is also based on the index $β$."
1687,https://arxiv.org/abs/2208.09877,ATOMS: ALMA Three-millimeter Observations of Massive Star-forming regions -- XII: Fragmentation and multi-scale gas kinematics in protoclusters G12.42+0.50 and G19.88-0.53,"We present new continuum and molecular line data from the ALMA Three-millimeter Observations of Massive Star-forming regions (ATOMS) survey for the two protoclusters, G12.42+0.50 and G19.88-0.53. The 3 mm continuum maps reveal seven cores in each of the two globally contracting protoclusters. These cores satisfy the radius-mass relation and the surface mass density criteria for high-mass star formation. Similar to their natal clumps, the virial analysis of the cores suggests that they are undergoing gravitational collapse ($\rm α_{vir} << 2$). The clump to core scale fragmentation is investigated and the derived core masses and separations are found to be consistent with thermal Jeans fragmentation. We detect large-scale filamentary structures with velocity gradients and multiple outflows in both regions. Dendrogram analysis of the H$^{13}$CO$^{+}$ map identifies several branch and leaf structures with sizes $\sim$ 0.1 and 0.03 pc, respectively. The supersonic gas motion displayed by the branch structures is in agreement with the Larson power-law indicating that the gas kinematics at this spatial scale is driven by turbulence. The transition to transonic/subsonic gas motion is seen to occur at spatial scales of $\sim$0.1 pc indicating the dissipation of turbulence. In agreement with this, the leaf structures reveal gas motions that deviate from the slope of Larson's law. From the large-scale converging filaments to the collapsing cores, the gas dynamics in G12.42+0.50 and G19.88-0.53 show scale-dependent dominance of turbulence and gravity and the combination of these two driving mechanisms needs to be invoked to explain massive star formation in the protoclusters."
1688,https://arxiv.org/abs/2208.07530,Knowledge-Injected Federated Learning,"Federated learning is an emerging technique for training models from decentralized data sets. In many applications, data owners participating in the federated learning system hold not only the data but also a set of domain knowledge. Such knowledge includes human know-how and craftsmanship that can be extremely helpful to the federated learning task. In this work, we propose a federated learning framework that allows the injection of participants' domain knowledge, where the key idea is to refine the global model with knowledge locally. The scenario we consider is motivated by a real industry-level application, and we demonstrate the effectiveness of our approach to this application."
1689,https://arxiv.org/abs/2208.07407,SemAug: Semantically Meaningful Image Augmentations for Object Detection Through Language Grounding,"Data augmentation is an essential technique in improving the generalization of deep neural networks. The majority of existing image-domain augmentations either rely on geometric and structural transformations, or apply different kinds of photometric distortions. In this paper, we propose an effective technique for image augmentation by injecting contextually meaningful knowledge into the scenes. Our method of semantically meaningful image augmentation for object detection via language grounding, SemAug, starts by calculating semantically appropriate new objects that can be placed into relevant locations in the image (the what and where problems). Then it embeds these objects into their relevant target locations, thereby promoting diversity of object instance distribution. Our method allows for introducing new object instances and categories that may not even exist in the training set. Furthermore, it does not require the additional overhead of training a context network, so it can be easily added to existing architectures. Our comprehensive set of evaluations showed that the proposed method is very effective in improving the generalization, while the overhead is negligible. In particular, for a wide range of model architectures, our method achieved ~2-4% and ~1-2% mAP improvements for the task of object detection on the Pascal VOC and COCO datasets, respectively."
1690,https://arxiv.org/abs/2208.06758,The ultraviolet CII lines as a diagnostic of kappa-distributed electrons in planetary nebulae,"Non-Maxwellian $κ$ electron energy distributions (EEDs) have been proposed in recent years to resolve the so-called ``electron temperature and abundance discrepancy problem'' in the study of planetary nebulae (PNe). Thus the need to develop diagnostic tools to determine from observations the EED of PNe is raised. Arising from high energy levels, the ultraviolet (UV) emission lines from PNe present intensities that depend sensitively on the high-energy tail of the EED. In this work, we investigate the feasibility of using the \ion{C}{2}]$λ$2326/\ion{C}{2}$λ$1335 intensity ratios as a diagnostic of the deviation of the EED from the Maxwellian distribution (as represented by the $κ$ index). We use a Maxwellian decomposition approach to derive the theoretical $κ$-EED-based collisionally excited coefficients of \ion{C}{2}, and then compute the \ion{C}{2} UV intensity ratio as a function of the $κ$ index. We analyze the archival spectra acquired by the {\it International Ultraviolet Explorer} and measure the intensities of \ion{C}{2} UV lines from 12 PNe. By comparing the observed line ratios and the theoretical predictions, we can infer their $κ$ values. With the Maxwellian-EED hypothesis, the observed \ion{C}{2}]$λ$2326/\ion{C}{2}$λ$1335 ratios are found to be generally lower than those predicted from the observed optical spectra. This discrepancy can be explained in terms of the $κ$ EED. Our results show that the $κ$ values inferred range from 15 to infinity, suggesting a mild or modest deviation from the Maxwellian distribution. However, the $κ$-distributed electrons are unlikely to exist throughout the whole nebulae. A toy model shows that if just about 1--5 percent of the free electrons in a PN had a $κ$-EED as small as $κ=3$, it would be sufficient to account for the observations."
1691,https://arxiv.org/abs/2208.02337,Estimating Visual Information From Audio Through Manifold Learning,"We propose a new framework for extracting visual information about a scene only using audio signals. Audio-based methods can overcome some of the limitations of vision-based methods i.e., they do not require ""line-of-sight"", are robust to occlusions and changes in illumination, and can function as a backup in case vision/lidar sensors fail. Therefore, audio-based methods can be useful even for applications in which only visual information is of interest Our framework is based on Manifold Learning and consists of two steps. First, we train a Vector-Quantized Variational Auto-Encoder to learn the data manifold of the particular visual modality we are interested in. Second, we train an Audio Transformation network to map multi-channel audio signals to the latent representation of the corresponding visual sample. We show that our method is able to produce meaningful images from audio using a publicly available audio/visual dataset. In particular, we consider the prediction of the following visual modalities from audio: depth and semantic segmentation. We hope the findings of our work can facilitate further research in visual information extraction from audio. Code is available at: https://github.com/ubc-vision/audio_manifold."
1692,https://arxiv.org/abs/2207.12601,Flux Variations of Cosmic Ray Air Showers Detected by LHAASO-KM2A During a Thunderstorm on 10 June 2021,"The Large High Altitude Air Shower Observatory (LHAASO) has three sub-arrays, KM2A, WCDA and WFCTA. The flux variations of cosmic ray air showers were studied by analyzing the KM2A data during the thunderstorm on 10 June 2021. The number of shower events that meet the trigger conditions increases significantly in atmospheric electric fields, with maximum fractional increase of 20%. The variations of trigger rates (increases or decreases) are found to be strongly dependent on the primary zenith angle. The flux of secondary particles increases significantly, following a similar trend with that of the shower events. To better understand the observed behavior, Monte Carlo simulations are performed with CORSIKA and G4KM2A (a code based on GEANT4). We find that the experimental data (in saturated negative fields) are in good agreement with simulations, assuming the presence of a uniform upward electric field of 700 V/cm with a thickness of 1500 m in the atmosphere above the observation level. Due to the acceleration/deceleration and deflection by the atmospheric electric field, the number of secondary particles with energy above the detector threshold is modified, resulting in the changes in shower detection rate."
1693,https://arxiv.org/abs/2207.09035,PackCache: An Online Cost-driven Data Caching Algorithm in the Cloud,"In this paper, we study a data caching problem in the cloud environment, where multiple frequently co-utilised data items could be packed as a single item being transferred to serve a sequence of data requests dynamically with reduced cost. To this end, we propose an online algorithm with respect to a homogeneous cost model, called PackCache, that can leverage the FP-Tree technique to mine those frequently co-utilised data items for packing whereby the incoming requests could be cost-effectively served online by exploiting the concept of anticipatory caching. We show the algorithm is 2αcompetitive, reaching the lower bound of the competitive ratio for any deterministic online algorithm on the studied caching problem, and also time and space efficient to serve the requests. Finally, we evaluate the performance of the algorithm via experimental studies to show its actual cost-effectiveness and scalability."
1694,https://arxiv.org/abs/2207.08859,Prior-Guided Adversarial Initialization for Fast Adversarial Training,"Fast adversarial training (FAT) effectively improves the efficiency of standard adversarial training (SAT). However, initial FAT encounters catastrophic overfitting, i.e.,the robust accuracy against adversarial attacks suddenly and dramatically decreases. Though several FAT variants spare no effort to prevent overfitting, they sacrifice much calculation cost. In this paper, we explore the difference between the training processes of SAT and FAT and observe that the attack success rate of adversarial examples (AEs) of FAT gets worse gradually in the late training stage, resulting in overfitting. The AEs are generated by the fast gradient sign method (FGSM) with a zero or random initialization. Based on the observation, we propose a prior-guided FGSM initialization method to avoid overfitting after investigating several initialization strategies, improving the quality of the AEs during the whole training process. The initialization is formed by leveraging historically generated AEs without additional calculation cost. We further provide a theoretical analysis for the proposed initialization method. We also propose a simple yet effective regularizer based on the prior-guided initialization,i.e., the currently generated perturbation should not deviate too much from the prior-guided initialization. The regularizer adopts both historical and current adversarial perturbations to guide the model learning. Evaluations on four datasets demonstrate that the proposed method can prevent catastrophic overfitting and outperform state-of-the-art FAT methods. The code is released at https://github.com/jiaxiaojunQAQ/FGSM-PGI."
1695,https://arxiv.org/abs/2207.07291,"A phase transition driven by subtle distortion without broken symmetry on spin, charge and lattice in Layered LnCu4-δP2(Ln=Eu, Sr)","In the scenario of Landau phase transition theory in condensed matter physics, any thermal dynamic phase transition must be subject to some kind of broken symmetries, that are relative to its spin, charge, orbital and lattice. Here we report a rare phase transition at Tp ~120 K or 140 K in layered materials LnCu4-δP2 (Ln=Eu, Sr) driven by a subtle structural-distortion without any broken symmetry on charge, spin and lattice. The variations of the lattice parameters, (ΔLc/Lc) ~ 0.013% or 0.062%, verified by thermal expansion, is much less than that for a typical crystalline phase transition (~0.5-1%), but the significant anomaly in heat capacity provides clear evidence of its intrinsic nature of thermodynamic transition."
1696,https://arxiv.org/abs/2207.05811,Revealing Unfair Models by Mining Interpretable Evidence,"The popularity of machine learning has increased the risk of unfair models getting deployed in high-stake applications, such as justice system, drug/vaccination design, and medical diagnosis. Although there are effective methods to train fair models from scratch, how to automatically reveal and explain the unfairness of a trained model remains a challenging task. Revealing unfairness of machine learning models in interpretable fashion is a critical step towards fair and trustworthy AI. In this paper, we systematically tackle the novel task of revealing unfair models by mining interpretable evidence (RUMIE). The key idea is to find solid evidence in the form of a group of data instances discriminated most by the model. To make the evidence interpretable, we also find a set of human-understandable key attributes and decision rules that characterize the discriminated data instances and distinguish them from the other non-discriminated data. As demonstrated by extensive experiments on many real-world data sets, our method finds highly interpretable and solid evidence to effectively reveal the unfairness of trained models. Moreover, it is much more scalable than all of the baseline methods."
1697,https://arxiv.org/abs/2207.05299,Multi-Camera View Based Proactive BS Selection and Beam Switching for V2X,"Due to the short wavelength and large attenuation of millimeter-wave (mmWave), mmWave BSs are densely distributed and require beamforming with high directivity. When the user moves out of the coverage of the current BS or is severely blocked, the mmWave BS must be switched to ensure the communication quality. In this paper, we proposed a multi-camera view based proactive BS selection and beam switching that can predict the optimal BS of the user in the future frame and switch the corresponding beam pair. Specifically, we extract the features of multi-camera view images and a small part of channel state information (CSI) in historical frames, and dynamically adjust the weight of each modality feature. Then we design a multi-task learning module to guide the network to better understand the main task, thereby enhancing the accuracy and the robustness of BS selection and beam switching. Using the outputs of all tasks, a prior knowledge based fine tuning network is designed to further increase the BS switching accuracy. After the optimal BS is obtained, a beam pair switching network is proposed to directly predict the optimal beam pair of the corresponding BS. Simulation results in an outdoor intersection environment show the superior performance of our proposed solution under several metrics such as predicting accuracy, achievable rate, harmonic mean of precision and recall."
1698,https://arxiv.org/abs/2207.04402,Global bifurcation structure and geometric properties for steady periodic water waves with vorticity,"This paper studies the classical water wave problem with vorticity described by the Euler equations with a free surface under the influence of gravity over a flat bottom. Based on fundamental work \cite{ConstantinStrauss}, we first obtain two continuous bifurcation curves which meet the laminar flow only one time by using modified analytic bifurcation theorem. They are symmetric waves whose profiles are monotone between each crest and trough. Furthermore, we find that there is at least one inflection point on the wave profile between successive crests and troughs and the free surface is strictly concave at any crest and strictly convex at any trough. In addition, for favorable vorticity, we prove that the vertical displacement of water waves decreases with depth."
1699,https://arxiv.org/abs/2206.12588,Photon Tunneling Reconstitution in Black Phosphorus/hBN Heterostructure,"Excitation of hybrid modes constituted by different material-supported polaritons is a common way to enhance the near-field radiative energy transport, which has fascinating promise in applications of thermal photonics. Here, we investigate near-field thermal radiation mechanisms in heterostructure composed of hBN film and black phosphorus single layer. The results show that this heterostructured system can give rise to a remarkable enhancement for photon tunneling, outperforming the near-field thermal radiation properties of its building blocks, as well as some other representative heterostructures. Moreover, we find that the anisotropic hybrid effect can induce a remarkable topological reconstitution of polaritons for hBN film and black phosphorus, forming a novel anisotropic hybrid polaritons. Notably, such hybrid modes show significant topological differences compared to hBN film and black phosphorus in the type-I Reststrahlen band due to the anisotropic anticrossing hybridization effect. Lastly, we systematically analyze the evolution of such hybrid polariton modes as a function of hBN film thickness and the corresponding influence on radiative properties of the heterostructure. This work may benefit the applications of near-field energy harvesting and radiative cooling based on hybrid polaritons in anisotropic two-dimensional material and hyperbolic film."
1700,https://arxiv.org/abs/2206.08505,ATOMS: ALMA Three-millimeter Observations of Massive Star-forming regions -- XI. From inflow to infall in hub-filament systems,"We investigate the presence of hub-filament systems in a large sample of 146 active proto-clusters, using H$^{13}$CO$^{+}$ J=1-0 molecular line data obtained from the ATOMS survey. We find that filaments are ubiquitous in proto-clusters, and hub-filament systems are very common from dense core scales ($\sim$0.1 pc) to clump/cloud scales ($\sim$1-10 pc). The proportion of proto-clusters containing hub-filament systems decreases with increasing dust temperature ($T_d$) and luminosity-to-mass ratios ($L/M$) of clumps, indicating that stellar feedback from H{\sc ii} regions gradually destroys the hub-filament systems as proto-clusters evolve. Clear velocity gradients are seen along the longest filaments with a mean velocity gradient of 8.71 km s$^{-1}$pc$^{-1}$ and a median velocity gradient of 5.54 km s$^{-1}$pc$^{-1}$. We find that velocity gradients are small for filament lengths larger than $\sim$1~pc, probably hinting at the existence of inertial inflows, although we cannot determine whether the latter are driven by large-scale turbulence or large-scale gravitational contraction. In contrast, velocity gradients below $\sim$1~pc dramatically increase as filament lengths decrease, indicating that the gravity of the hubs or cores starts to dominate gas infall at small scales. We suggest that self-similar hub-filament systems and filamentary accretion at all scales may play a key role in high-mass star formation."
1701,https://arxiv.org/abs/2206.06965,Deep Reinforcement Learning for Exact Combinatorial Optimization: Learning to Branch,"Branch-and-bound is a systematic enumerative method for combinatorial optimization, where the performance highly relies on the variable selection strategy. State-of-the-art handcrafted heuristic strategies suffer from relatively slow inference time for each selection, while the current machine learning methods require a significant amount of labeled data. We propose a new approach for solving the data labeling and inference latency issues in combinatorial optimization based on the use of the reinforcement learning (RL) paradigm. We use imitation learning to bootstrap an RL agent and then use Proximal Policy Optimization (PPO) to further explore global optimal actions. Then, a value network is used to run Monte-Carlo tree search (MCTS) to enhance the policy network. We evaluate the performance of our method on four different categories of combinatorial optimization problems and show that our approach performs strongly compared to the state-of-the-art machine learning and heuristics based methods."
1702,https://arxiv.org/abs/2206.06959,AuxMix: Semi-Supervised Learning with Unconstrained Unlabeled Data,"Semi-supervised learning (SSL) has seen great strides when labeled data is scarce but unlabeled data is abundant. Critically, most recent work assume that such unlabeled data is drawn from the same distribution as the labeled data. In this work, we show that state-of-the-art SSL algorithms suffer a degradation in performance in the presence of unlabeled auxiliary data that does not necessarily possess the same class distribution as the labeled set. We term this problem as Auxiliary-SSL and propose AuxMix, an algorithm that leverages self-supervised learning tasks to learn generic features in order to mask auxiliary data that are not semantically similar to the labeled set. We also propose to regularize learning by maximizing the predicted entropy for dissimilar auxiliary samples. We show an improvement of 5% over existing baselines on a ResNet-50 model when trained on CIFAR10 dataset with 4k labeled samples and all unlabeled data is drawn from the Tiny-ImageNet dataset. We report competitive results on several datasets and conduct ablation studies."
1703,https://arxiv.org/abs/2206.06291,Exploring Structure-aware Transformer over Interaction Proposals for Human-Object Interaction Detection,"Recent high-performing Human-Object Interaction (HOI) detection techniques have been highly influenced by Transformer-based object detector (i.e., DETR). Nevertheless, most of them directly map parametric interaction queries into a set of HOI predictions through vanilla Transformer in a one-stage manner. This leaves rich inter- or intra-interaction structure under-exploited. In this work, we design a novel Transformer-style HOI detector, i.e., Structure-aware Transformer over Interaction Proposals (STIP), for HOI detection. Such design decomposes the process of HOI set prediction into two subsequent phases, i.e., an interaction proposal generation is first performed, and then followed by transforming the non-parametric interaction proposals into HOI predictions via a structure-aware Transformer. The structure-aware Transformer upgrades vanilla Transformer by encoding additionally the holistically semantic structure among interaction proposals as well as the locally spatial structure of human/object within each interaction proposal, so as to strengthen HOI predictions. Extensive experiments conducted on V-COCO and HICO-DET benchmarks have demonstrated the effectiveness of STIP, and superior results are reported when comparing with the state-of-the-art HOI detectors. Source code is available at \url{https://github.com/zyong812/STIP}."
1704,https://arxiv.org/abs/2206.05028,Spatial Cross-Attention Improves Self-Supervised Visual Representation Learning,"Unsupervised representation learning methods like SwAV are proved to be effective in learning visual semantics of a target dataset. The main idea behind these methods is that different views of a same image represent the same semantics. In this paper, we further introduce an add-on module to facilitate the injection of the knowledge accounting for spatial cross correlations among the samples. This in turn results in distilling intra-class information including feature level locations and cross similarities between same-class instances. The proposed add-on can be added to existing methods such as the SwAV. We can later remove the add-on module for inference without any modification of the learned weights. Through an extensive set of empirical evaluations, we verify that our method yields an improved performance in detecting the class activation maps, top-1 classification accuracy, and down-stream tasks such as object detection, with different configuration settings."
1705,https://arxiv.org/abs/2206.04676,Extending Momentum Contrast with Cross Similarity Consistency Regularization,"Contrastive self-supervised representation learning methods maximize the similarity between the positive pairs, and at the same time tend to minimize the similarity between the negative pairs. However, in general the interplay between the negative pairs is ignored as they do not put in place special mechanisms to treat negative pairs differently according to their specific differences and similarities. In this paper, we present Extended Momentum Contrast (XMoCo), a self-supervised representation learning method founded upon the legacy of the momentum-encoder unit proposed in the MoCo family configurations. To this end, we introduce a cross consistency regularization loss, with which we extend the transformation consistency to dissimilar images (negative pairs). Under the cross consistency regularization rule, we argue that semantic representations associated with any pair of images (positive or negative) should preserve their cross-similarity under pretext transformations. Moreover, we further regularize the training loss by enforcing a uniform distribution of similarity over the negative pairs across a batch. The proposed regularization can easily be added to existing self-supervised learning algorithms in a plug-and-play fashion. Empirically, we report a competitive performance on the standard Imagenet-1K linear head classification benchmark. In addition, by transferring the learned representations to common downstream tasks, we show that using XMoCo with the prevalently utilized augmentations can lead to improvements in the performance of such tasks. We hope the findings of this paper serve as a motivation for researchers to take into consideration the important interplay among the negative examples in self-supervised learning."
1706,https://arxiv.org/abs/2206.03022,Radiative heat transfer in low-symmetry Bravais crystal,"Over the last few years, broken symmetry within crystals has attracted extensive attention since it can improve the control of light propagation. In particular, low-symmetry Bravais crystal can support shear polaritons which has great potential in thermal photonics. In this work, we report a twist-induced near-field thermal control system based on the low-symmetry Bravais crystal medium (\b{eta}-Ga2O3). The near-field thermal radiation (NFTR) between such crystal slabs is nearly four orders of magnitude larger than the blackbody limit, exceeding the NFTR from other traditional dielectric materials. Moreover, we show that this crystal can serve as an excellent platform for twist-induced near-field thermal control. Due to the intrinsic shear effect, the twist-induced modulation supported by low-symmetry Bravais crystal exceeds that by high-symmetry crystal. We further clarify how the shear effect affects the twist-induced thermal-radiation modulation supported by hyperbolic and elliptical polaritons and show that the shear effect significantly enhances the twist-induced thermal control induced by the elliptical polariton mode. These results open new directions for thermal-radiation control in low-symmetry materials, including geological minerals, common oxides, and organic crystals."
1707,https://arxiv.org/abs/2206.02417,Fast Adversarial Training with Adaptive Step Size,"While adversarial training and its variants have shown to be the most effective algorithms to defend against adversarial attacks, their extremely slow training process makes it hard to scale to large datasets like ImageNet. The key idea of recent works to accelerate adversarial training is to substitute multi-step attacks (e.g., PGD) with single-step attacks (e.g., FGSM). However, these single-step methods suffer from catastrophic overfitting, where the accuracy against PGD attack suddenly drops to nearly 0% during training, destroying the robustness of the networks. In this work, we study the phenomenon from the perspective of training instances. We show that catastrophic overfitting is instance-dependent and fitting instances with larger gradient norm is more likely to cause catastrophic overfitting. Based on our findings, we propose a simple but effective method, Adversarial Training with Adaptive Step size (ATAS). ATAS learns an instancewise adaptive step size that is inversely proportional to its gradient norm. The theoretical analysis shows that ATAS converges faster than the commonly adopted non-adaptive counterparts. Empirically, ATAS consistently mitigates catastrophic overfitting and achieves higher robust accuracy on CIFAR10, CIFAR100 and ImageNet when evaluated on various adversarial budgets."
1708,https://arxiv.org/abs/2206.00910,A Real-time Critical-scenario-generation Framework for Testing Autonomous Driving System,"In order to find the most likely failure scenarios which may occur under certain given operation domain, critical-scenario-based test is supposed as an effective and widely used method, which gives suggestions for designers to improve the developing algorithm. However, for the state of art, critical-scenario generation approaches commonly utilize random-search or reinforcement learning methods to generate series of scenarios for a specific algorithm, which takes amounts of computing resource for testing a developing target that is always changing, and inapplicable for testing a real-time system. In this paper, we proposed a real-time critical-scenario-generation (RTCSG) framework to address the above challenges. In our framework, an aggressive-driving algorithm is proposed in controlling the virtual agent vehicles, a specially designed cost function is presented to guide scenarios to evolve towards critical conditions, and a self-adaptive coefficient iteration is designed that enable the approach to operate successfully in different conditions. With our proposed method, the critical-scenarios can be directly generated for the target under test which is a black-box system, and the real-time critical-scenario test can be brought into reality. The simulation results show that our approach is able to obtain more critical scenarios in most conditions than current methods, with a higher stability of success. For a real-time testing, our approach improves the efficiency around 16 times."
1709,https://arxiv.org/abs/2205.08787,Cross-subject Action Unit Detection with Meta Learning and Transformer-based Relation Modeling,"Facial Action Unit (AU) detection is a crucial task for emotion analysis from facial movements. The apparent differences of different subjects sometimes mislead changes brought by AUs, resulting in inaccurate results. However, most of the existing AU detection methods based on deep learning didn't consider the identity information of different subjects. The paper proposes a meta-learning-based cross-subject AU detection model to eliminate the identity-caused differences. Besides, a transformer-based relation learning module is introduced to learn the latent relations of multiple AUs. To be specific, our proposed work is composed of two sub-tasks. The first sub-task is meta-learning-based AU local region representation learning, called MARL, which learns discriminative representation of local AU regions that incorporates the shared information of multiple subjects and eliminates identity-caused differences. The second sub-task uses the local region representation of AU of the first sub-task as input, then adds relationship learning based on the transformer encoder architecture to capture AU relationships. The entire training process is cascaded. Ablation study and visualization show that our MARL can eliminate identity-caused differences, thus obtaining a robust and generalized AU discriminative embedding representation. Our results prove that on the two public datasets BP4D and DISFA, our method is superior to the state-of-the-art technology, and the F1 score is improved by 1.3% and 1.4%, respectively."
1710,https://arxiv.org/abs/2205.07300,The effect of dressing on thermalization of interacting waves,"We propose a more general setup for prethermalization in the system of interacting waves. The idea lies in dividing the multi-wave interactions into trivial and nontrivial ones. The trivial interactions will dress waves and lead to a less strongly interacting system which is statistically equivalent to the original one. With this in mind, we find that prethermalization occurs not only in the weakly interacting regime but also in the strongly interacting regime. The irreversible process towards equilibrium is governed by the Zakharov equation, from which double scaling of the thermalization time is expected. Finally, the theory is well confirmed in numerical experiments."
1711,https://arxiv.org/abs/2205.06170,FAST search for circumstellar atomic hydrogen--I: the young planetary nebula IC 4997,"Using the Five-hundred-meter Aperture Spherical radio Telescope (FAST) in Guizhou, China, we detect the 21cm neutral atomic hydrogen absorption in the young planetary nebula IC 4997. The absorption arises from a shell also associated with Na I D lines. The H I shell has a mass of $1.46\times10^{-2}$ M$_\odot$ and a dynamic age of 990yr. The column density of H I is estimated to be $7.1\times10^{20}$ cm$^{-2}$, which can be well explained in terms of a photodissociation region around the ionized nebula, limited by self shielding of H$_2$. We find that the atomic-to-ionized hydrogen ratio is 0.6, suggesting that H I substantially contributes to overall nebular mass."
1712,https://arxiv.org/abs/2205.02380,A characteristic-spectral-mixed scheme for six-dimensional Wigner-Coulomb dynamics,"Numerical resolution for 6-D Wigner dynamics under the Coulomb potential faces with the combined challenges of high dimensionality, nonlocality, oscillation and singularity. In particular, the extremely huge memory storage of 6-D grids hinders the usage of all existing deterministic numerical scheme, which is well-known as the curse of dimensionality. To surmount these difficulties, we propose a massively parallel solver, termed the CHAracteristic-Spectral-Mixed (CHASM) scheme, by fully exploiting two distinct features of the Wigner equation: Locality of spatial advection and nonlocality of quantum interaction. Our scheme utilizes the local cubic B-spline basis to interpolate the local spatial advection. The key is to use a perfectly matched boundary condition to give a closure of spline coefficients, so that distributed pieces can recover the global one as accurately as possible owing to the rapid decay of wavelet basis in the dual space, and communication costs are significantly reduced. To resolve the nonlocal pseudodifferential operator with weakly singular symbol, CHASM further adopts the truncated kernel method to attain a highly efficient approximation. Several typical experiments including the quantum harmonic oscillator and Hydrogen 1s state demonstrate the accuracy and efficiency of CHASM. The non-equilibrium electron-proton couplings are also clearly displayed and reveal the uncertainty principle and quantum tunneling in phase space. Finally, the scalability of CHASM up to 16000 cores is presented."
1713,https://arxiv.org/abs/2205.01922,Performance evaluations on the parallel CHAracteristic-Spectral-Mixed (CHASM) scheme,"Performance evaluations on the deterministic algorithms for 6-D problems are rarely found in literatures except some recent advances in the Vlasov and Boltzmann community [Dimarco et al. (2018), Kormann et al. (2019)], due to the extremely high complexity. Thus a detailed comparison among various techniques shall be useful to the researchers in the related fields. We try to make a thorough evaluation on a parallel CHAracteristic-Spectral-Mixed (CHASM) scheme to support its usage. CHASM utilizes the cubic B-spline expansion in the spatial space and spectral expansion in the momentum space, which many potentially overcome the computational burden in solving classical and quantum kinetic equations in 6-D phase space. Our purpose is three-pronged. First, we would like show that by imposing some effective Hermite boundary conditions, the local cubic spline can approximate to the global one as accurately as possible. Second, we will illustrate the necessity of adopting the truncated kernel method in calculating the pseudodifferential operator with a singular symbol, since the widely used pseudo-spectral method [Ringhofer (1990)] might fail to properly tackle the singularity. Finally, we make a comparison among non-splitting Lawson schemes and Strang operator splitting. Our numerical results demonstrate the advantage of the one-stage Lawson predictor-corrector scheme over multi-stage ones as well as the splitting scheme in both accuracy and stability."
1714,https://arxiv.org/abs/2204.13659,One-loop diagrams with quadratic propagators from the worldsheet,"It is well known that forward limits of tree-level amplitudes (and those trivalent diagrams they consist of) produce one-loop amplitudes and trivalent diagrams with propagators linear in the loop momentum. They naturally arise from one-loop worldsheet formulae, and an important open problem is how to recombine them into usual one-loop diagrams with quadratic propagators. In this paper, we study a new collection of worldsheet functions: generalized one-loop Parke-Taylor factors with tensor numerators, which are conjectured to serve as a basis for one-loop worldsheet functions with this nice property. We present all-multiplicity, closed-form expressions for combinations of one-loop trivalent diagrams with quadratic propagators and tensor numerators to arbitrary rank (including possible tadpole contributions), produced by any pair of Parke-Taylor factors. We also briefly comment on reducing worldsheet functions onto such a basis, and applications to one-loop amplitudes in physical theories."
1715,https://arxiv.org/abs/2204.12671,The symmetry for two class of steady stratified periodic water waves,"In this paper, we mainly consider two class of travelling stratified periodic water waves, one with negative (or without) surface tension and the other with constant Bernoulli's function and stagnation points. We first establish the symmetry result for stratified water waves with negative (or without) surface tension, but without stagnation by using the modified maximum principle. Furthermore, the symmetry property of stratified water waves with constant Bernoulli's function and stagnation points is also obtained provided the monotonic property is known."
1716,https://arxiv.org/abs/2204.08023,VDTR: Video Deblurring with Transformer,"Video deblurring is still an unsolved problem due to the challenging spatio-temporal modeling process. While existing convolutional neural network-based methods show a limited capacity for effective spatial and temporal modeling for video deblurring. This paper presents VDTR, an effective Transformer-based model that makes the first attempt to adapt Transformer for video deblurring. VDTR exploits the superior long-range and relation modeling capabilities of Transformer for both spatial and temporal modeling. However, it is challenging to design an appropriate Transformer-based model for video deblurring due to the complicated non-uniform blurs, misalignment across multiple frames and the high computational costs for high-resolution spatial modeling. To address these problems, VDTR advocates performing attention within non-overlapping windows and exploiting the hierarchical structure for long-range dependencies modeling. For frame-level spatial modeling, we propose an encoder-decoder Transformer that utilizes multi-scale features for deblurring. For multi-frame temporal modeling, we adapt Transformer to fuse multiple spatial features efficiently. Compared with CNN-based methods, the proposed method achieves highly competitive results on both synthetic and real-world video deblurring benchmarks, including DVD, GOPRO, REDS and BSD. We hope such a Transformer-based architecture can serve as a powerful alternative baseline for video deblurring and other video restoration tasks. The source code will be available at \url{https://github.com/ljzycmd/VDTR}."
1717,https://arxiv.org/abs/2204.02796,Molecules in the carbon-rich protoplanetary nebula CRL 2688,"We present observations of the carbon-rich protoplanetary nebula (PPN) CRL 2688 made with the Institut de Radioastronomie Millimetrique (IRAM) 30 m telescope in the 3mm and 2mm bands. In total, 196 transition lines belonging to 38 molecular species and isotopologues are detected, among which, to our best knowledge, 153 transition lines and 13 species are the first report for this object. Additionally, in order to contribute to future research, we have collected observational data on the molecular lines of CRL 2688 from the literature and compiled them into a single unified catalog. We find that the molecular abundance of CRL 2688 cannot be explained by the standard model of a circumstellar envelope. The implications of metal-bearing molecules on circumstellar chemistry are discussed."
1718,https://arxiv.org/abs/2203.17059,Room Temperature Gate Tunable Non Reciprocal Charge Transport in Lattice Matched InSb/CdTe Heterostructures,"The manipulation of symmetry provides an effective way to tailor the physical orders in solid-state systems. With the breaking of both the inversion and time-reversal symmetries, non-reciprocal magneto-transport may emerge in assorted non-magnetic systems to enrich spintronic physics. Here, we report the observation of the uni-directional magneto-resistance (UMR) in the lattice-matched InSb/CdTe film up to room temperature. Benefiting from the strong built-in electric field of $0.13 \mathrm{~V} \cdot \mathrm{nm}^{-1}$ in the hetero-junction region, the resulting Rashba-type spin-orbit coupling and quantum confinement warrant stable angular-dependent second-order charge current with the non-reciprocal coefficient 1-2 orders of magnitude larger than most non-centrosymmetric materials at 298 K. More importantly, this heterostructure configuration enables highly-efficient gate tuning of the rectification response in which the enhancement of the UMR amplitude by 40% is realized. Our results advocate the narrow-gap semiconductor-based hybrid system with the robust two-dimensional interfacial spin texture as a suitable platform for the pursuit of controllable chiral spin-orbit devices and applications."
1719,https://arxiv.org/abs/2203.15925,"Asynchronous, Option-Based Multi-Agent Policy Gradient: A Conditional Reasoning Approach","Multi-agent policy gradient methods have demonstrated success in games and robotics but are often limited to problems with low-level action space. However, when agents take higher-level, temporally-extended actions (i.e. options), when and how to derive a centralized control policy, its gradient as well as sampling options for all agents while not interrupting current option executions, becomes a challenge. This is mostly because agents may choose and terminate their options \textit{asynchronously}. In this work, we propose a conditional reasoning approach to address this problem, and empirically validate its effectiveness on representative option-based multi-agent cooperative tasks."
1720,https://arxiv.org/abs/2203.12208,Self-supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection,"Recent studies in deepfake detection have yielded promising results when the training and testing face forgeries are from the same dataset. However, the problem remains challenging when one tries to generalize the detector to forgeries created by unseen methods in the training dataset. This work addresses the generalizable deepfake detection from a simple principle: a generalizable representation should be sensitive to diverse types of forgeries. Following this principle, we propose to enrich the ""diversity"" of forgeries by synthesizing augmented forgeries with a pool of forgery configurations and strengthen the ""sensitivity"" to the forgeries by enforcing the model to predict the forgery configurations. To effectively explore the large forgery augmentation space, we further propose to use the adversarial training strategy to dynamically synthesize the most challenging forgeries to the current model. Through extensive experiments, we show that the proposed strategies are surprisingly effective (see Figure 1), and they could achieve superior performance than the current state-of-the-art methods. Code is available at \url{https://github.com/liangchen527/SLADD}."
1721,https://arxiv.org/abs/2203.06616,LAS-AT: Adversarial Training with Learnable Attack Strategy,"Adversarial training (AT) is always formulated as a minimax problem, of which the performance depends on the inner optimization that involves the generation of adversarial examples (AEs). Most previous methods adopt Projected Gradient Decent (PGD) with manually specifying attack parameters for AE generation. A combination of the attack parameters can be referred to as an attack strategy. Several works have revealed that using a fixed attack strategy to generate AEs during the whole training phase limits the model robustness and propose to exploit different attack strategies at different training stages to improve robustness. But those multi-stage hand-crafted attack strategies need much domain expertise, and the robustness improvement is limited. In this paper, we propose a novel framework for adversarial training by introducing the concept of ""learnable attack strategy"", dubbed LAS-AT, which learns to automatically produce attack strategies to improve the model robustness. Our framework is composed of a target network that uses AEs for training to improve robustness and a strategy network that produces attack strategies to control the AE generation. Experimental evaluations on three benchmark databases demonstrate the superiority of the proposed method. The code is released at https://github.com/jiaxiaojunQAQ/LAS-AT."
1722,https://arxiv.org/abs/2203.05212,Membership Privacy Protection for Image Translation Models via Adversarial Knowledge Distillation,"Image-to-image translation models are shown to be vulnerable to the Membership Inference Attack (MIA), in which the adversary's goal is to identify whether a sample is used to train the model or not. With daily increasing applications based on image-to-image translation models, it is crucial to protect the privacy of these models against MIAs.
  We propose adversarial knowledge distillation (AKD) as a defense method against MIAs for image-to-image translation models. The proposed method protects the privacy of the training samples by improving the generalizability of the model. We conduct experiments on the image-to-image translation models and show that AKD achieves the state-of-the-art utility-privacy tradeoff by reducing the attack performance up to 38.9% compared with the regular training model at the cost of a slight drop in the quality of the generated output images. The experimental results also indicate that the models trained by AKD generalize better than the regular training models. Furthermore, compared with existing defense methods, the results show that at the same privacy protection level, image translation models trained by AKD generate outputs with higher quality; while at the same quality of outputs, AKD enhances the privacy protection over 30%."
1723,https://arxiv.org/abs/2203.04036,StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN,"One-shot talking face generation aims at synthesizing a high-quality talking face video from an arbitrary portrait image, driven by a video or an audio segment. One challenging quality factor is the resolution of the output video: higher resolution conveys more details. In this work, we investigate the latent feature space of a pre-trained StyleGAN and discover some excellent spatial transformation properties. Upon the observation, we explore the possibility of using a pre-trained StyleGAN to break through the resolution limit of training datasets. We propose a novel unified framework based on a pre-trained StyleGAN that enables a set of powerful functionalities, i.e., high-resolution video generation, disentangled control by driving video or audio, and flexible face editing. Our framework elevates the resolution of the synthesized talking face to 1024*1024 for the first time, even though the training dataset has a lower resolution. We design a video-based motion generation module and an audio-based one, which can be plugged into the framework either individually or jointly to drive the video generation. The predicted motion is used to transform the latent features of StyleGAN for visual animation. To compensate for the transformation distortion, we propose a calibration network as well as a domain loss to refine the features. Moreover, our framework allows two types of facial editing, i.e., global editing via GAN inversion and intuitive editing based on 3D morphable models. Comprehensive experiments show superior video quality, flexible controllability, and editability over state-of-the-art methods."
1724,https://arxiv.org/abs/2203.03877,Effect of surface H$_2$ on molecular hydrogen formation on interstellar grains,"We investigate how the existence of hydrogen molecules on grain surfaces may affect H$_2$ formation efficiency in diffuse and translucent clouds. Hydrogen molecules are able to reduce the desorption energy of H atoms on grain surfaces in models. The detailed microscopic Monte Carlo method is used to perform model simulations. We found that the impact of the existence of H$_2$ on H$_2$ formation efficiency strongly depends on the diffusion barriers of H$_2$ on grain surfaces. Diffuse cloud models that do not consider surface H$_2$ predict that H atom recombination efficiency is above 0.5 over a grain temperature (T) range 10 K and 14 K. The adopted H$_2$ diffusion barriers in diffuse cloud models that consider surface H$_2$ are 80$\%$ H$_2$ desorption energies so that H$_2$ can be trapped in stronger binding sites. Depending on model parameters, these diffuse cloud models predict that the recombination efficiency is between nearly 0 and 0.5 at 10 K $\leq$ T $\leq$ 14 K. Translucent cloud model results show that H$_2$ formation efficiency is not affected by the existence of surface H$_2$ if the adopted average H$_2$ diffusion barrier on grain surfaces is low (194 K) so that H$_2$ can diffuse rapidly on grain surfaces. However, the recombination efficiency can drop to below 0.002 at T $\geq$ 10 K if higher average H$_2$ diffusion barrier is used (255 K) in translucent cloud models."
1725,https://arxiv.org/abs/2203.02777,Cosine Model Watermarking Against Ensemble Distillation,"Many model watermarking methods have been developed to prevent valuable deployed commercial models from being stealthily stolen by model distillations. However, watermarks produced by most existing model watermarking methods can be easily evaded by ensemble distillation, because averaging the outputs of multiple ensembled models can significantly reduce or even erase the watermarks. In this paper, we focus on tackling the challenging task of defending against ensemble distillation. We propose a novel watermarking technique named CosWM to achieve outstanding model watermarking performance against ensemble distillation. CosWM is not only elegant in design, but also comes with desirable theoretical guarantees. Our extensive experiments on public data sets demonstrate the excellent performance of CosWM and its advantages over the state-of-the-art baselines."
1726,https://arxiv.org/abs/2203.00748,E-LANG: Energy-Based Joint Inferencing of Super and Swift Language Models,"Building huge and highly capable language models has been a trend in the past years. Despite their great performance, they incur high computational cost. A common solution is to apply model compression or choose light-weight architectures, which often need a separate fixed-size model for each desirable computational budget, and may lose performance in case of heavy compression. This paper proposes an effective dynamic inference approach, called E-LANG, which distributes the inference between large accurate Super-models and light-weight Swift models. To this end, a decision making module routes the inputs to Super or Swift models based on the energy characteristics of the representations in the latent space. This method is easily adoptable and architecture agnostic. As such, it can be applied to black-box pre-trained models without a need for architectural manipulations, reassembling of modules, or re-training. Unlike existing methods that are only applicable to encoder-only backbones and classification tasks, our method also works for encoder-decoder structures and sequence-to-sequence tasks such as translation. The E-LANG performance is verified through a set of experiments with T5 and BERT backbones on GLUE, SuperGLUE, and WMT. In particular, we outperform T5-11B with an average computations speed-up of 3.3$\times$ on GLUE and 2.9$\times$ on SuperGLUE. We also achieve BERT-based SOTA on GLUE with 3.2$\times$ less computations. Code and demo are available in the supplementary materials."
1727,https://arxiv.org/abs/2202.13829,How and what to learn:The modes of machine learning,"Despite their great success, neural networks still remain as black-boxes due to the lack of interpretability. Here we propose a new analyzing method, namely the weight pathway analysis (WPA), to make them transparent. We consider weights in pathways that link neurons longitudinally from input neurons to output neurons, or simply weight pathways, as the basic units for understanding a neural network, and decompose a neural network into a series of subnetworks of such weight pathways. A visualization scheme of the subnetworks is presented that gives longitudinal perspectives of the network like radiographs, making the internal structures of the network visible. Impacts of parameter adjustments or structural changes to the network can be visualized via such radiographs. Characteristic maps are established for subnetworks to characterize the enhancement or suppression of the influence of input samples on each output neuron. Using WPA, we discover that neural network store and utilize information in a holographic way, that is, subnetworks encode all training samples in a coherent structure and thus only by investigating the weight pathways can one explore samples stored in the network. Furthermore, with WPA, we reveal fundamental learning modes of a neural network: the linear learning mode and the nonlinear learning mode. The former extracts linearly separable features while the latter extracts linearly inseparable features. The hidden-layer neurons self-organize into different classes for establishing learning modes and for reaching the training goal. The finding of learning modes provides us the theoretical ground for understanding some of the fundamental problems of machine learning, such as the dynamics of learning process, the role of linear and nonlinear neurons, as well as the role of network width and depth."
1728,https://arxiv.org/abs/2202.12160,Self-Attention for Incomplete Utterance Rewriting,"Incomplete utterance rewriting (IUR) has recently become an essential task in NLP, aiming to complement the incomplete utterance with sufficient context information for comprehension. In this paper, we propose a novel method by directly extracting the coreference and omission relationship from the self-attention weight matrix of the transformer instead of word embeddings and edit the original text accordingly to generate the complete utterance. Benefiting from the rich information in the self-attention weight matrix, our method achieved competitive results on public IUR datasets."
1729,https://arxiv.org/abs/2202.11307,ATOMS: ALMA Three-millimeter Observations of Massive Star-forming regions-IX. A pilot study towards IRDC G034.43+00.24 on multi-scale structures and gas kinematics,"We present a comprehensive study of the gas kinematics associated with density structures at different spatial scales in the filamentary infrared dark cloud, G034.43+00.24 (G34). This study makes use of the H13CO+ (1-0) molecular line data from the ALMA Three-millimeter Observations of Massive Star-forming regions (ATOMS) survey, which has spatial and velocity resolution of 0.04 pc and 0.2 km/s, respectively. Several tens of dendrogram structures have been extracted in the position-position-velocity space of H13CO+, which include 21 small-scale leaves and 20 larger-scale branches. Overall, their gas motions are supersonic but they exhibit the interesting behavior where leaves tend to be less dynamically supersonic than the branches. For the larger-scale, branch structures, the observed velocity-size relation (i.e., velocity variation/dispersion versus size) are seen to follow the Larson scaling exponent while the smaller-scale, leaf structures show a systematic deviation and display a steeper slope. We argue that the origin of the observed kinematics of the branch structures is likely to be a combination of turbulence and gravity-driven ordered gas flows. In comparison, gravity-driven chaotic gas motion is likely at the level of small-scale leaf structures. The results presented in our previous paper and this current follow-up study suggest that the main driving mechanism for mass accretion/inflow observed in G34 varies at different spatial scales. We therefore conclude that a scale-dependent combined effect of turbulence and gravity is essential to explain the star-formation processes in G34."
1730,https://arxiv.org/abs/2202.08552,EBHI:A New Enteroscope Biopsy Histopathological H&E Image Dataset for Image Classification Evaluation,"Background and purpose: Colorectal cancer has become the third most common cancer worldwide, accounting for approximately 10% of cancer patients. Early detection of the disease is important for the treatment of colorectal cancer patients. Histopathological examination is the gold standard for screening colorectal cancer. However, the current lack of histopathological image datasets of colorectal cancer, especially enteroscope biopsies, hinders the accurate evaluation of computer-aided diagnosis techniques. Methods: A new publicly available Enteroscope Biopsy Histopathological H&E Image Dataset (EBHI) is published in this paper. To demonstrate the effectiveness of the EBHI dataset, we have utilized several machine learning, convolutional neural networks and novel transformer-based classifiers for experimentation and evaluation, using an image with a magnification of 200x. Results: Experimental results show that the deep learning method performs well on the EBHI dataset. Traditional machine learning methods achieve maximum accuracy of 76.02% and deep learning method achieves a maximum accuracy of 95.37%. Conclusion: To the best of our knowledge, EBHI is the first publicly available colorectal histopathology enteroscope biopsy dataset with four magnifications and five types of images of tumor differentiation stages, totaling 5532 images. We believe that EBHI could attract researchers to explore new classification algorithms for the automated diagnosis of colorectal cancer, which could help physicians and patients in clinical settings."
1731,https://arxiv.org/abs/2202.06064,Self-Adaptive Real-Time Time-Dependent Density Functional Theory for X-ray Absorptions,"Real-time time-dependent density functional theory (RT-TDDFT) can in principle access the whole absorption spectrum of a many-electron system exposed to a narrow pulse. However, this requires an accurate and efficient propagator for the numerical integration of the time-dependent Kohn-Sham equation. While a low-order time propagator is already sufficient for the low-lying valence absorption spectra, it is no longer the case for the X-ray absorption spectra (XAS) of systems composed even only of light elements, for which the use of a high-order propagator is indispensable. It is then crucial to choose a largest possible time step and a shortest possible simulation time, so as to minimize the computational cost. To this end, we propose here a robust AutoPST approach to determine automatically (Auto) the propagator (P), step (S), and time (T) for relativistic RT-TDDFT simulations of XAS."
1732,https://arxiv.org/abs/2201.11249,Jointly Learning Knowledge Embedding and Neighborhood Consensus with Relational Knowledge Distillation for Entity Alignment,"Entity alignment aims at integrating heterogeneous knowledge from different knowledge graphs. Recent studies employ embedding-based methods by first learning the representation of Knowledge Graphs and then performing entity alignment via measuring the similarity between entity embeddings. However, they failed to make good use of the relation semantic information due to the trade-off problem caused by the different objectives of learning knowledge embedding and neighborhood consensus. To address this problem, we propose Relational Knowledge Distillation for Entity Alignment (RKDEA), a Graph Convolutional Network (GCN) based model equipped with knowledge distillation for entity alignment. We adopt GCN-based models to learn the representation of entities by considering the graph structure and incorporating the relation semantic information into GCN via knowledge distillation. Then, we introduce a novel adaptive mechanism to transfer relational knowledge so as to jointly learn entity embedding and neighborhood consensus. Experimental results on several benchmarking datasets demonstrate the effectiveness of our proposed model."
1733,https://arxiv.org/abs/2201.10044,"ATOMS: ALMA Three-millimeter Observations of Massive Star-forming regions -- VIII. A search for hot cores by using C$_2$H$_5$CN, CH$_3$OCHO and CH$_3$OH lines","Hot cores characterized by rich lines of complex organic molecules are considered as ideal sites for investigating the physical and chemical environments of massive star formation. We present a search for hot cores by using typical nitrogen- and oxygen-bearing complex organic molecules (C$_2$H$_5$CN, CH$_3$OCHO and CH$_3$OH), based on ALMA Three-millimeter Observations of Massive Star-forming regions (ATOMS). The angular resolutions and line sensitivities of the ALMA observations are better than 2 arcsec and 10 mJy/beam, respectively. A total of 60 hot cores are identified with 45 being newly detected, in which the complex organic molecules have high gas temperatures ($>$ 100 K) and small source sizes ($<$ 0.1 pc). So far this is the largest sample of hot cores observed with similar angular resolution and spectral coverage. The observations have also shown nitrogen and oxygen differentiation in both line emission and gas distribution in 29 hot cores. Column densities of CH$_3$OH and CH$_3$OCHO increase as rotation temperatures rise. The column density of CH$_3$OCHO correlates tightly with that of CH$_3$OH. The pathways for production of different species are discussed. Based on the spatial position difference between hot cores and UC~H{\sc ii} regions, we conclude that 24 hot cores are externally heated while the other hot cores are internally heated. The observations presented here will potentially help establish a hot core template for studying massive star formation and astrochemistry."
1734,https://arxiv.org/abs/2201.08787,The Properties and Evolutions of Starspots on Three Detached Eclipsing Binaries in the LAMOST-Kepler survey,"The spotted detached eclipsing binary (DEB) offers insights into starspots on the binary. Three spotted DEBs, KIC 8097825, KIC 6859813, and KIC 5527172, which were observed by the Kepler photometry and LAMOST spectroscopy, are studied in this work. The physical parameters of binaries are determined by binary modeling. The sizes, lifetimes, and single/double-dip ratio (SDR) of starspots are derived by starspot analysis. KIC 8097825 has large starspots. KIC 6859813 has a spot rotation period shorter than its orbital period but the system should be synchronized inferred from timescale estimation. The difference may be the result of the surface differential rotation. The KIC 5527172 has a long spot lifetime and an M dwarf component with an inflation radius. The primaries of these binaries and the secondary of KIC 8097825 have spots. Adding spotted DEBs of literature, we compare the starspots on binaries with those on the single stars. The spot sizes of starspots on 65% binaries are smaller than the median of those on single stars. The lifetimes of starspots on binaries are consistent with those on single stars when the rotation periods are larger than 3 days. SDRs for half of the binaries are consistent with those of single star systems, while another half are smaller. The relative lifetime positively correlates with the RMS and SDR but negatively correlates with the rotation period. These relations are similar to those of spots on the single star systems. Binaries with luminosity ratios close to the unit tend to have more double dips."
1735,https://arxiv.org/abs/2201.07533,ATOMS: ALMA Three-millimeter Observations of Massive Star-forming regions -- VII. A catalogue of SiO clumps from ACA observations,"To understand the nature of SiO emission, we conducted ACA observations of the SiO (2-1) lines toward 146 massive star-forming regions, as part of the ALMA Three-millimeter Observations of Massive Star-forming regions (ATOMS) survey. We detected SiO emission in 128 (87.7$\%$) sources and identified 171 SiO clumps, 105 of which are spatially separated from 3 mm continuum emission. A large amount of the SiO line profiles (60$\%$) are non-Gaussian. The velocity dispersion of the SiO lines ranges from 0.3 to 5.43 km s$^{-1}$. In 63 sources the SiO clumps are associated with H$_\rm{II}$ regions characterized by H40$α$ emission. We find that 68$\%$ (116) of the SiO clumps are associated with strong outflows. The median velocity dispersion of the SiO line for outflow sources and non-outflow sources is 1.91 km s$^{-1}$ and 0.99 km s$^{-1}$, respectively. These results indicate that outflow activities could be connected to strongly shocked gas. The velocity dispersion and [SiO]/[H$^{13}$CO$^+$] intensity ratio do not show any correlation with the dust temperature and particle number density of clumps. We find a positive correlation between the SiO line luminosity and the bolometric luminosity, implying stronger shock activities are associated with more luminous proto-clusters. The SiO clumps in associations with H$_\rm{II}$ regions were found to show a steeper feature in $L_\rm{sio}$/$L_\rm{bol}$. The SiO line luminosity and the fraction of shocked gas have no apparent evidence of correlation with the evolutionary stages traced by luminosity to mass ratio ($L_\rm{bol}/M$)."
1736,https://arxiv.org/abs/2201.05112,PDRs4All: A JWST Early Release Science Program on radiative feedback from massive stars,"Massive stars disrupt their natal molecular cloud material through radiative and mechanical feedback processes. These processes have profound effects on the evolution of interstellar matter in our Galaxy and throughout the Universe, from the era of vigorous star formation at redshifts of 1-3 to the present day. The dominant feedback processes can be probed by observations of the Photo-Dissociation Regions (PDRs) where the far-ultraviolet photons of massive stars create warm regions of gas and dust in the neutral atomic and molecular gas. PDR emission provides a unique tool to study in detail the physical and chemical processes that are relevant for most of the mass in inter- and circumstellar media including diffuse clouds, proto-planetary disks and molecular cloud surfaces, globules, planetary nebulae, and star-forming regions. PDR emission dominates the infrared (IR) spectra of star-forming galaxies. Most of the Galactic and extragalactic observations obtained with the James Webb Space Telescope (JWST) will therefore arise in PDR emission. In this paper we present an Early Release Science program using the MIRI, NIRSpec, and NIRCam instruments dedicated to the observations of an emblematic and nearby PDR: the Orion Bar. These early JWST observations will provide template datasets designed to identify key PDR characteristics in JWST observations. These data will serve to benchmark PDR models and extend them into the JWST era. We also present the Science-Enabling products that we will provide to the community. These template datasets and Science-Enabling products will guide the preparation of future proposals on star-forming regions in our Galaxy and beyond and will facilitate data analysis and interpretation of forthcoming JWST observations."
1737,https://arxiv.org/abs/2201.02658,Fair and efficient contribution valuation for vertical federated learning,"Federated learning is a popular technology for training machine learning models on distributed data sources without sharing data. Vertical federated learning or feature-based federated learning applies to the cases that different data sources share the same sample ID space but differ in feature space. To ensure the data owners' long-term engagement, it is critical to objectively assess the contribution from each data source and recompense them accordingly. The Shapley value (SV) is a provably fair contribution valuation metric originated from cooperative game theory. However, computing the SV requires extensively retraining the model on each subset of data sources, which causes prohibitively high communication costs in federated learning. We propose a contribution valuation metric called vertical federated Shapley value (VerFedSV) based on SV. We show that VerFedSV not only satisfies many desirable properties for fairness but is also efficient to compute, and can be adapted to both synchronous and asynchronous vertical federated learning algorithms. Both theoretical analysis and extensive experimental results verify the fairness, efficiency, and adaptability of VerFedSV."
1738,https://arxiv.org/abs/2112.12896,Dewetting Characteristics of Contact Lenses Coated with Wetting Agents,"Hypothesis:
  Although wetting agents have been developed to limit tear film dewetting over contact lenses, systematic analyses correlating wetting agents properties to mechanisms of the tear film destabilization are not readily available. Clarifying destabilization characteristics across key physio-chemical variables will provide a rational basis for identifying optimal wetting agents.
  Experiments:
  We employ an in-house, in vitro platform to comprehensively evaluate drainage and dewetting dynamics of five wetting agents across seventeen different formulations and two model tear film solutions: phosphate-buffered saline (PBS) and artificial tear solution (ATS). We consider the film thickness evolution, film thickness at breakup, dewetted front propagation, and develop correlations to contact angle to compare the samples.
  Findings:
  Zwitterionic wetting agents effectively stabilize the tear film by reducing the film thickness at the onset of dewetting, and delaying dewetted region propagation across the lens. Furthermore, tuning wetting agent surface concentrations in binary mixtures can enhance wetting characteristics. Finally, despite disparities in wetting agent molecular properties, the time to dewet $50\%$ of the lens scales linearly with the product of the receding contact angle and contact angle hysteresis. Hence, we fundamentally establish the importance of minimizing both the absolute contact angle values and contact angle hysteresis for effective wetting performance."
1739,https://arxiv.org/abs/2112.12251,"ML4CO: Is GCNN All You Need? Graph Convolutional Neural Networks Produce Strong Baselines For Combinatorial Optimization Problems, If Tuned and Trained Properly, on Appropriate Data","The 2021 NeurIPS Machine Learning for Combinatorial Optimization (ML4CO) competition was designed with the goal of improving state-of-the-art combinatorial optimization solvers by replacing key heuristic components with machine learning models. The competition's main scientific question was the following: is machine learning a viable option for improving traditional combinatorial optimization solvers on specific problem distributions, when historical data is available? This was motivated by the fact that in many practical scenarios, the data changes only slightly between the repetitions of a combinatorial optimization problem, and this is an area where machine learning models are particularly powerful at. This paper summarizes the solution and lessons learned by the Huawei EI-OROAS team in the dual task of the competition. The submission of our team achieved the second place in the final ranking, with a very close distance to the first spot. In addition, our solution was ranked first consistently for several weekly leaderboard updates before the final evaluation. We provide insights gained from a large number of experiments, and argue that a simple Graph Convolutional Neural Network (GCNNs) can achieve state-of-the-art results if trained and tuned properly."
1740,https://arxiv.org/abs/2112.07835,Mining Minority-class Examples With Uncertainty Estimates,"In the real world, the frequency of occurrence of objects is naturally skewed forming long-tail class distributions, which results in poor performance on the statistically rare classes. A promising solution is to mine tail-class examples to balance the training dataset. However, mining tail-class examples is a very challenging task. For instance, most of the otherwise successful uncertainty-based mining approaches struggle due to distortion of class probabilities resulting from skewness in data. In this work, we propose an effective, yet simple, approach to overcome these challenges. Our framework enhances the subdued tail-class activations and, thereafter, uses a one-class data-centric approach to effectively identify tail-class examples. We carry out an exhaustive evaluation of our framework on three datasets spanning over two computer vision tasks. Substantial improvements in the minority-class mining and fine-tuned model's performance strongly corroborate the value of our proposed solution."
1741,https://arxiv.org/abs/2112.05372,A recurrent neural network approach for remaining useful life prediction utilizing a novel trend features construction method,"Data-driven methods for remaining useful life (RUL) prediction normally learn features from a fixed window size of a priori of degradation, which may lead to less accurate prediction results on different datasets because of the variance of local features. This paper proposes a method for RUL prediction which depends on a trend feature representing the overall time sequence of degradation. Complete ensemble empirical mode decomposition, followed by a reconstruction procedure, is created to build the trend features. The probability distribution of sensors' measurement learned by conditional neural processes is used to evaluate the trend features. With the best trend feature, a data-driven model using long short-term memory is developed to predict the RUL. To prove the effectiveness of the proposed method, experiments on a benchmark C-MAPSS dataset are carried out and compared with other state-of-the-art methods. Comparison results show that the proposed method achieves the smallest root mean square values in prediction of all RUL."
